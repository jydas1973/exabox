#
# $Header: ecs/exacloud/exabox/infrapatching/test/utils.txt /main/77 2025/12/04 04:10:46 araghave Exp $
#
#
# utils.py
#
# Copyright (c) 2020, 2025, Oracle and/or its affiliates.
#
#    NAME
#      utils.py - Contains all the common modules to be used in test cases.
#
#    DESCRIPTION
#      This module contains common methods for each of the Test target handlers.
#
#    NOTES
#      This module contains common methods for each of the Test target handlers.
#
#    MODIFIED   (MM/DD/YY)
#    araghave    10/03/25 - Enhancement Request 38444755 - INFRAPATCHING TEST
#                           AUTOMATION - TEST ADDITION TO PERFORM DOMU ELU
#                           PATCH OPERATIONS
#    sdevasek    09/10/25   Bug 38411834 - FIX THE TEST CODE TO SEND PROPERLY
#                           FORMATTED JSON PAYLOAD IN THE PATCH REQUEST
#    sdevasek    09/03/25 - Enh 38351663 - TEST ADDITION TO VALIDATE PLUGIN
#                           EXECUTION FAILURE SCENARIO
#       apotluri 08/26/25 - Bug 38301663 - EXACS:25.2.2.1:RAISE AN ERROR WHEN
#                           THE EXTERNAL LAUNCH NODE IS UNAVAILABLE DUE TO
#                           REACHING THE MAXIMUM ALLOWED CONCURRENT SESSIONS
#    antamil     07/04/25   Bug 37994044 - Testscript changes to verify
#                           node_state during clusterless patching
#    apotluri    06/24/25 - Enhancement Request 38109516 - INFRAPATCH TEST
#                           AUTOMATION : CREATE PRECHECK RETRY FAILURE FOR DOMU
#                           AND SWITCH
#    apotluri    06/13/25 - Enhancement Request 37981656 - INFRAPATCH TEST
#                           AUTOMATION : ADD TEST FOR PATCH RETRY CASE FOR
#                           DOM0, DOMU AND SWITCH
#    antamil     04/06/25 - Bug 36842057 test to validate registered launchnode
#    antamil     04/23/25 - Enable cell test for clusterless patching
#    apotluri    04/02/25 - Bug 37775168 - INFRAPATCH TEST AUTOMATION :
#                           GENERATING TIME PROFILE DATA IN CODE COVERAGE IS
#                           FAILING
#    sdevasek    04/02/25 - Enh 37501751 -TEST ADDITION FOR THE VALIDATION OF
#                           JOB SCHEDULER TO CLEAN PATCHES ON THE LAUNCHNODE 
#                           FOR SINGLE VM CASE TEST IN X9M ENV
#    apotluri    03/10/25 - Enhancement Request 37606958 - INFRAPATCH TEST
#                           AUTOMATION - ADD RETRY TEST FOR SWITCH PATCH OP
#    apotluri    01/31/25 - Enhancement Request 37507403 - INFRAPATCHING TEST
#                           AUTOMATION: ADD TEST CASE WITH SELINUX ENFORCING ON
#                           DB NODES AND RUN PATCH_PREREQ_CHECK AND PATCH
#                           OPERATIONS
#    apotluri    12/19/24 - Bug 37407667 - INFRAPATCH TEST AUTOMATION : PATCHING
#                           REQUEST USING CURL IS SENDING TWO RESPONSE CODES 
#    emekala     12/10/24 - ENH 37374442 - SUPPORT INFRA PATCH MOCK FWK TO
#                           ACCEPT MOCK RESPONSE IN JSON FORMAT VIA REST API
#    emekala     11/28/24 - ENH 37328901 - Add support to initialize infra
#                           patch mock setup when payload has mock request
#                           attribute
#    araghave    11/22/24 - Enh 37241595 - TEST CHANGES TO REPLACE ALL IBSWITCH
#                           REFERENCE WITH GENERIC SWITCH REFERENCES IN INFRA
#                           PATCHING TEST CODE
#    apotluri    11/20/24 - Enhancement Request 37091565 - ADDITION OF
#                           AUTOMATION TEST TO VALIDATE PATCHMGR SESSION
#                           DETECTION AFTER SWITCHOVER
#    apotluri    11/19/24 - Bug 37270845 - INFRAPATCH TEST AUTOMATION:
#                           ENHANCING RESILIENCE IN VERIFYING CDB/PDB
#                           DEGRADATION SCENARIOS
#    apotluri    11/07/24 - Enhancement Request 36845243 - TEST ADDITION TO
#                           PERFORM PATCH OPERATIONS ON SINGLE VM CLUSTER
#    apotluri    09/26/24 - Enhancement Request 36846185 - INFRAPATCHING TEST
#                           AUTOMATION - TEST ADDITION TO VALIDATE CDB/PDB
#                           DEGRADATION
#    apotluri    09/16/24 - Bug 37044172 - INFRAPATCH TEST AUTOMATION : TESTS
#                           FAILING DUE TO CHANGE IN DIR STRUCTURE, FROM
#                           REQUESTS TO REQUESTS.BAK DIR
#    araghave    07/16/24 - Enh 36830077 - CLEANUP KSPLICE CODE FROM
#                           INFRAPATCHING FILES
#    apotluri    07/08/24 - Enhancement Request 36664566 - INFRAPATCHING TEST
#                           AUTOMATION : ADDITION OF TEST TO VALIDATE PDB
#                           DOWNTIME SCENARIO
#    apotluri    06/28/24 - Enhancement Request 36750543 - PYTHON UPGRADE -
#                           INFRAPATCHING FILES SHOULD HAVE DYNAMICALLY SET
#                           PYTHON PATHS TO AVOID REGRESSIONS
#    apotluri    06/26/24 - Bug 36775654 - INFRAPATCH TEST AUTOMATION : ON X9
#                           RACK CELL_KSPLICE_LIST TEST FAILING WHILE CHECKING
#                           FOR KSPLICE SCRIPT CONSOLE LOG
#    apotluri    06/21/24 - Enhancement Request 36750543 - PYTHON UPGRADE -
#                           INFRAPATCHING FILES SHOULD HAVE DYNAMICALLY SET
#                           PYTHON PATHS TO AVOID REGRESSIONS
#    apotluri    05/30/24 - Enhancement Request 36254182 - INFRAPATCHING TEST
#                           AUTOMATION - REMOVE DEPENDENCY ON GLOBAL PAYLOADOBJ
#                           IN THE TEST CODE
#    apotluri    05/16/24 - Bug 36623715 - INFRAPATCH TEST AUTOMATION :
#                           INCREASE PRECHECK SANITY CHECK POLL TIME BY 1
#                           MINUTE
#    apotluri    04/08/24 - Enhancement Request 35846277 - TEST ADDITION TO
#                           AUTOMATION TO VALIDATE ZERO VALUE FOR METEROCPU
#                           SCENARIO IN INFRAPATCHING
#    sdevasek    03/19/24 - Enh 36317237 - TEST AUTOMATION - ADD TEST FOR
#                           ONEOFF V2 IMPLEMENTATION
#    apotluri    02/01/24 - Enhancement Request 36171207 - ADDITION OF TEST TO
#                           VALIDATE PRECHECK/PATCH FAILURE IN EXACOMPUTE
#                           PATCHING
#    jyotdas     01/30/24 - Enhancement Request 36157656 - enhance infrapatch
#                           domu details payload to pass vms shutdown by
#                           customer
#    emekala     01/17/24 - ENH 36094940 - Support ECRA mock mode for
#                           Infrapatching operations
#    sdevasek    12/15/23 - Enh 36097928 - ADDITION OF TESTS TO MODIFY DEFAULT
#                           INFRAPATCHING.CONF PARAMS TO INCREASE CODE COVERAGE
#    antamil     12/08/23 - Enh 35976409 common function for patch retry
#    apotluri    11/22/23 - Enhancement Request 35888310 - INFRAPATCH TEST
#                           AUTMATION : ENABLE EXACOMPUTE TESTS IN X9
#    jyotdas     11/08/23 - Abort ecra wf when the infra patching is failed
#    sdevasek    10/20/23 - ENH 35930718 - TEST ADDITION TO VALIDATE EXACOMPUTE
#                           PATCH WHEN COMPUTE NODE HAS VMS RUNNING
#    apotluri    09/19/23 - BUG 35642430 - INFRAPATCHING TEST AUTOMATION IN
#                           EXACS R1 SETUP:
#                           DOMU_PATCH_PREREQ_CHECK_PATCH_MGR_FAILURE IS
#                           FAILING
#    apotluri    09/05/23 - ENH 35681550 - INFRAPATCHING TEST AUTOMATION -
#                           PLUGIN EXECUTION ON MULTI CLUSTER REQUIRES CHANGES
#    apotluri    08/16/23 - BUG 35702353 - SV23.4.1.1CS: CLEAR TEXT PASSWORDS
#                           IN EXACLOUD INFRAPATCHING UTIL FILE
#    apotluri    08/08/23 - ENH 35681403 - INFRAPATCHING TEST AUTOMATION -
#                           TIMEPROFILEDATAVALIDATOR FAILS ON X9 RACK
#                           (PRODUCTION AUTOMATION SETUP) FOR DOM0 / DOMU
#                           INCLUDE NODE LIST PATCHING
#    sdevasek    08/07/23 - ENH 35670127 - CHANGE OEDA REQUEST PATH AS
#                           {EXAUNIT_ID}_{OPERATION_UUID} IN TEST CODE
#    araghave    08/03/23 - Enh 35661378 - ADD DOM0 PATCHING CRS TESTS : DOM0
#                           POSTCHECK SHOULD STARTUP CRS ON ALL VMS IF CRS IS
#                           DOWN
#    apotluri    07/24/23 - ENH 35610019 - INFRAPATCHING TEST AUTOMATION : DOMU
#                           INCLUDE NODELIST TEST CASES ARE FAILING IF
#                           NATHOSTNAMES ARE USED
#    araghave    06/30/23 - Enh 35552878 - EXACOMPUTE TEST ADDITION FOR
#                           EXASPLICE PATCH OPERATIONS
#    sdevasek    06/19/23 - ENH 35432876 - ADDITION OF TEST TO VALIDATE
#                           PATCH RETRY SCENARIO
#    sdevasek    05/16/23 - ENH 35293729 - TEST ADDITION TO VALIDATE HEARTBEAT
#                           CHECK BY MAKING CRS TO DISABLE IN ONE OF THE DOMU
#    apotluri    05/10/23 - Enh 35371736 - INFRAPATCHING TEST AUTOMATION : MOVE
#                           CLEAN UP SPACE TASK TO A GENERIC PLACE INSTEAD OF
#                           DOING FOR EVERY TEST
#    sdevasek    04/19/23 - ENH 35293707 - TEST ADDITION TO VALIDATE SINGLENODE
#                           UPGRADENAME INCLUDENODELIST COMBINATIONS
#    emekala     04/06/23 - ENH 35204565 - TEST FRAMEWORK NOT TO RETRY PATCH
#                           OPERATION FOR THE CASES OTHER THAN EXACLOUD
#                           CONNECTIVITY FAILURE
#    apotluri    02/16/23 - ENH 34292618 - INFRAPATCHING AUTOMATION STABILITY -
#                           CLEAR UP SPACE IN THE NODES IN AUTOMATION RACK
#    sdevasek    01/23/23 - ENH 34862465 - ENABLE TIME PROFILE DIFF TO SHOW
#                           SWITCH DATA
#    sdevasek    01/13/23 - ENH 33893463 - UPDATE INFRAPATCH TEST AUTOMATION TO
#                           PROVIDE DIFFS OF TIME PROFILE FOR MAJOR OPERATIONS
#                           ACROSS CURRENT AND PREVIOUS RUNS
#    sdevasek    12/21/22 - BUG 34907692 - TIME_PROFILE_DATA VALIDATION SHOULD
#                           NOT LOOK FOR NODE_PATCHING_TIME_STATS WHEN ALL THE
#                           NODES ARE UP TO DATE
#    sdevasek    12/12/22 - Bug 34884954 - INFRAPATCHING AUTOMATION-EXACOMPUTE
#                           TEST TO USE LATEST INFRAPATCHING.CONF PARAMETERS
#    antamil     12/05/22 - ENH 34564371-IGNORE VALIDATION OF
#                           TIME PROFILE DATA AND PLUGIN CONSOLE LOG FOR
#                           ALREADY PATCHED NODE
#    sdevasek    11/09/22 - ENH 34743194 - UPDATE INFRAPATCHING TEST CODE
#                           TO MAKE AUTOMATION TO WORK IN X9M R1 ENV
#    antamil     11/15/22 - ENH 34513424 Test addition for exacompute patch
#    sdevasek    09/20/22 - ENH 34547838 - TEST ADDITION TO AUTOMATION TO
#                           VALIDATE NEW ERROR CODES FROM PATCH_MGR
#    sdevasek    09/14/22 - ENH 33924998 - TEST ADDITION TO DELETE ANY PENDING
#                           RACK_PATCH_UPDATE OPERATIONS USING ABORT REQUEST
#    sdevasek    08/11/22 - ENH 34465298 - TEST ADDITION TO AUTOMATION TO LOOK
#                           FOR TIME_PROFILE_DATA IN STATUS REPORT
#    jyotdas     05/25/22 - ENH 34161120 - expose nodeprogressingstatus field
#                           from fleetpatchingresponse to cp
#    sdevasek    04/21/22 - ENH 34088744 - ENABLE TESTS FOR KSPLICE AND ONEOFF
#                           OPERATIONS IN INFRAPTACHING AUTOMATION
#    sdevasek    04/26/22 - ENH 34108685 - ADD RETRY MECHANISM TO EXECUTE PATCH
#                           COMMANDS FOR CERTAIN FAILURES IN AUTOMATION
#    sdevasek    04/18/22 - ENH 34073709 - USE EXASSH TO EXECUTE COMMANDS AND
#                           COPY FILES TO NODES IN INFRAPATCHING AUTOMATION
#    sdevasek    03/07/22 - Bug-33928270 - STAGE CUSTOM DOM0DOMU SCRIPT FOR
#                           NON-ROLLING DOM0 PATCH TEST DURING CELL PATCH TEST
#    sdevasek    02/14/22 - Enh-33737906 - TEST ADDITION TO THE INFRAPATCHING
#                           AUTOMATION FOR EXACLOUD PLUGINS
#    sdevasek    02/04/22 - Enh-33819329 - TEST ADDITION TO THE INFRAPATCHING
#                           AUTOMATION FOR INCLUDELIST FEATURE
#    sdevasek    12/03/21 - Enh-33310641 - TEST ADDITION TO THE INFRAPATCHING
#                           AUTOMATION FOR DCS AGENT BASED SANITY CHECKS
#    sdevasek    11/23/21 - Enh33504205 - IMAGE REFRESHING INCLUSION IN
#                           INFRAPATCHING AUTOMATION
#    nmallego    10/25/21 - Bug-33310632 - Test for non-rolling upgrade
#    sdevasek    09/13/21 - Enh 32929805 - Infrapatching CI/CD pipeline
#                           implementation
#    araghave    05/27/21 - Enh 32929805 - INFRA PATCHING TEST FRAMEWORK
#    rkhemcha    08/28/20 - Refactor infra patching code
#    rkhemcha    08/28/20 - Creation
#
import abc
import json
import os
import base64
import random
from datetime import datetime
import time
import re
import traceback
import subprocess
import glob
from pathlib import Path
from subprocess import Popen, PIPE, STDOUT
from constants import *

try:
    from collections import OrderedDict
except ImportError:
    from collections.abc import OrderedDict

global invokeTime
logdir = os.path.join(os.getcwd() + RELATIVE_PATH_OF_EXACLOUD_THREAD_LOG_LOCATION_FROM_TEST_EXECUTION)


def mPatchLogInfo(aMsg):
    print(time.strftime("%Y-%m-%d %H:%M:%S%z"), ' - ' + PATCH_TEST_HANDLER + ' -INFO- ' + aMsg)


def mPatchLogError(aMsg):
    print(time.strftime("%Y-%m-%d %H:%M:%S%z"), ' - ' + PATCH_TEST_HANDLER + ' -ERROR- ' + aMsg)


def mPatchLogWarn(aMsg):
    print(time.strftime("%Y-%m-%d %H:%M:%S%z"), ' - ' + PATCH_TEST_HANDLER + ' -WARN- ' + aMsg)


def mPatchLogPrint(aMsg):
    print("\n%s\n" % aMsg)


def mLoadJson(aJsonFilePath):
    try:
        _file_handler = open(aJsonFilePath)
        _json_obj = json.loads(_file_handler.read())
        _file_handler.close()
        return _json_obj
    except Exception as e:
        mPatchLogError("Could not load the JSON file with path : " + aJsonFilePath + " (or) Invalid Json file" + str(e))
        mPatchLogPrint(traceback.format_exc())
        return None

def mRebootNode(aNode, aNodeType):
    """
    Reboot a node and wait for it to come online

    :param aNode: node name
    :return: True if successful, False otherwise
    """

    mPatchLogInfo(f"Rebooting {aNode}...")
    mExecuteRemoteExasshCmd("reboot", [aNode])

    _max_wait_time = 1800 # 30 mins
    _sleep_time = 30
    time.sleep(_sleep_time)

    _start_time = time.time()
    while time.time() - _start_time < _max_wait_time:
        try:
            _ret, _ = mExecuteRemoteExasshCmd("uptime", [aNode])
            if _ret:
                mPatchLogInfo(f"Node {aNode} is back online.")
                mPatchLogInfo(f"waiting for 5 mins to fully come online.")
                time.sleep(300)
                # this is requried for x5 dom0
                if aNodeType == "dom0":
                    if mGetInfraPatchingTestConfigParam("is_r1_env") == "False":
                        mExecuteRemoteExasshCmd("/opt/exacloud/network/dom0_iptables_setup.sh",
                                                                    [aNode])
                return True
        except Exception:
            pass

        mPatchLogInfo(f"Node {aNode} is still inaccessible. Retrying in {_sleep_time} seconds...")
        time.sleep(_sleep_time)

    mPatchLogError(f"Node {aNode} did not come online after reboot.")
    return False

def mUpdateSelinux(aNodes, aFlag, aNodeType):
    """
    Update SELinux configuration on given aNodes.
    Supports enabling (permissive -> enforcing) or disabling SELinux.

    :param aNodes: List of node names
    :param aflag: "enabled" or "disabled"
    :return: True if successful, False otherwise
    """

    def _mGetSelinuxStatus(aNode):
        """
        Get SELinux status for a node

        :param aNode: node name
        :return: True if successful, False otherwise
        """

        ret_val, output = mExecuteRemoteExasshCmd("/usr/sbin/getenforce", [aNode])
        status = output[aNode].strip() if ret_val else None
        if status is None:
            mPatchLogError(f"Failed to retrieve SELinux status for {aNode}")
            return None
        return status

    def _mUpdateSelinuxConfig(aNode, aValue, aNodeType):
        """
        Update SELinux configuration on given aNode and reboot

        :param aNode: node name
        :param aValue: permissive or enforcing or disabled
        :return: True if successful, False otherwise
        """""

        mExecuteRemoteExasshCmd(f'sed -i "s/^SELINUX=.*/SELINUX={aValue}/" /etc/selinux/config', [aNode])
        return mRebootNode(aNode, aNodeType)

    def _mApplySelinux(aNodes, aFlag, aNodeType):
        """
        Apply SELinux configuration on given nodes

        :param aNodes: node name
        :param aFlag: enabled or disabled
        :param aNodeType: dom0 or domu
        :return: True if successful, False otherwise
        """

        for _node in aNodes:
            _current_status = _mGetSelinuxStatus(_node)
            if _current_status is None:
                return False

            mPatchLogInfo(f"Current SELinux status on node {_node} : {_current_status}")
            _previous_status = _current_status

            if aFlag == "enabled" and _current_status != "Enforcing":
                if _current_status != "Permissive":
                    mPatchLogInfo(f"Updating {_node} to Permissive mode...")
                    # When SELinux is set to permissive mode and a file is modified, the SELinux context can be
                    # set to unlabeled_t which will deny access to that file when selinux is set to enforcing mode.
                    # So relabel all of the files on the node with the correct SELinux contexts in the next boot by
                    # creating /.autorelabel file
                    mExecuteRemoteExasshCmd('touch /.autorelabel', [_node])
                    if not _mUpdateSelinuxConfig(_node, "permissive", aNodeType):
                        return False
                mPatchLogInfo(f"Updating {_node} to Enforcing mode...")
                mExecuteRemoteExasshCmd('touch /.autorelabel', [_node])
                if not _mUpdateSelinuxConfig(_node, "enforcing", aNodeType):
                    return False
            elif aFlag == "disabled" and _current_status != "Disabled":
                mPatchLogInfo(f"Disabling SELinux on {_node}...")
                if not _mUpdateSelinuxConfig(_node, "disabled", aNodeType):
                    return False
            else:
                mPatchLogInfo(f"SELinux on {_node} is already on expected mode")

            # Verify final state
            _final_status = _mGetSelinuxStatus(_node)
            if _final_status is None:
                mPatchLogError(f"Unable to retrieve final SELinux status for {_node}")
                return False

            if _previous_status == "Enforcing" and _final_status == "Disabled":
                mPatchLogInfo(f"SELinux on {_node} successfully moved from Enforcing to Disabled.")
            elif _previous_status == "Disabled" and _final_status == "Enforcing":
                mPatchLogInfo(f"SELinux on {_node} successfully moved from Disabled to Enforcing.")

            return True

    _ret_val = _mApplySelinux(aNodes, aFlag, aNodeType)

    return _ret_val

def mGenerateNewIdemToken():
    """
    This method is used to generate new idemtoken
    """
    _username = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrausername')).decode('utf-8')
    _password = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrapassword')).decode('utf-8')
    _ecra_url = mGetInfraPatchingTestConfigParam('ecraurl')

    _curl_cmd = "curl --silent  -u %s:%s -k -X POST --header 'Content-Type: application/json' --header 'Accept: " \
                "application/json' '%s/idemtokens?new'" % (
                    _username, _password, _ecra_url)

    _idemtoken = mExecuteCmd(aCmd=_curl_cmd)

    _idemtoken_json = json.loads(_idemtoken)
    _new_idemtoken = _idemtoken_json["idemtoken"]
    return _new_idemtoken


def mSetMeteredOcpus(aClusterName, aCores):
    """
    This method is used to update given cores on a given cluster
    """
    _username = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrausername')).decode('utf-8')
    _password = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrapassword')).decode('utf-8')
    _ecra_url = mGetInfraPatchingTestConfigParam('ecraurl')

    # get the domus and cluster ocid
    _curl_cmd = "curl --silent  -u %s:%s -k -X GET --header 'Content-Type: application/json' --header 'Accept: " \
                "application/json' '%s/cluster/details?rackname=%s'" % (
                    _username, _password, _ecra_url, aClusterName)
    _cluster_details = mExecuteCmd(aCmd=_curl_cmd)
    mPatchLogInfo("Cluster details output is : %s " % _cluster_details)

    _data = json.loads(_cluster_details)
    _clu_ocid = _data["exadataInfrastructureOCID"]
    _exaunit_id = _data["exaunitid"]

    # get the list of domu present in the given cluster and construct payload
    _domu_list_with_core = []
    for item in _data["domUs"]:
        _domu_list = {"cores": aCores, "hostName": item["HostName"]}
        _domu_list_with_core.append(_domu_list)

    _domu_list_with_core_str = json.dumps(_domu_list_with_core)
    _pay_load = '{ "clusterAllocations": %s, "idemtoken": "%s", "operation": ' \
                '"updateAllocation"}' % (_domu_list_with_core_str, mGenerateNewIdemToken())

    # Actual curl cmd for setting cores to desired value
    _curl_cmd = "curl --silent  -u %s:%s -i -k -X PUT --header 'Content-Type: application/json' --header 'Accept: " \
                "application/json' -d '%s'  '%s/exadatainfrastructure/%s/exaunit/%s'" % (
                    _username, _password, _pay_load, _ecra_url, _clu_ocid, _exaunit_id)
    _payload_out = mExecuteCmd(aCmd=_curl_cmd)
    mPatchLogInfo("MeteredOcpu output is : %s " % _payload_out)
    _status_url = mGetHTTPHeaderValueFromHTTPResponse(_payload_out, STATUS_URI)
    _ret, _status_output = mCheckPatchOperationStatus(_status_url)
    mPatchLogInfo("MeteredOcpu status output  is : %s " % _status_output)
    return _ret


def mVmCntrlOperation(aExaunitId, aVM, aOperation):
    mPatchLogInfo("vm %s operation is fired on vm - %s and exaunit - %s." % (aOperation, aVM, aExaunitId))
    _username = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrausername')).decode('utf-8')
    _password = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrapassword')).decode('utf-8')
    _ecra_url = mGetInfraPatchingTestConfigParam('ecraurl')
    _pay_load = '{ "action": "%s" , "exaunitID": "%s"}' % (aOperation, aExaunitId)
    _curl_cmd = "curl --silent  -u %s:%s -i -k -X PUT --header 'Content-Type: application/json' --header 'Accept: application/json' -d '%s'  '%s/exaunit/%s/vms/%s'" % (
        _username, _password, _pay_load, _ecra_url, aExaunitId, aVM)
    mPatchLogInfo("mVmCntrlOperation curl command : %s " % _curl_cmd)
    _payload_out = mExecuteCmd(aCmd=_curl_cmd)
    mPatchLogInfo("mVmCntrlOperation output is : %s " % _payload_out)
    _status_url = mGetHTTPHeaderValueFromHTTPResponse(_payload_out, STATUS_URI)
    _ret, _status_output = mCheckPatchOperationStatus(_status_url)
    mPatchLogInfo("mVmCntrlOperation status output  is : %s " % _status_output)
    return _ret


def mUpdateAdditionalOptionsInPayload(aParam, aValue):
    """
    This method is used to update a parameter with a value in additional_options of payload.json
    """
    mPatchLogInfo("Updating the parameter %s in AdditionalOptions with %s." % (aParam, aValue))
    try:
        _json = mLoadJson(TEST_PAYLOAD_FILE)
        _additional_options = _json.get("additional_options", [{}])  # Get additional_options or an empty dictionary if not found
        if _additional_options and aParam in _additional_options[0]:
            _additional_options[0][aParam] = aValue
        else:
            # Key not found, add a new key-value pair
            _additional_options[0][aParam] = aValue

        _json["additional_options"] = _additional_options

        with open(TEST_PAYLOAD_FILE, 'w') as file:
            json.dump(_json, file, indent=4)
        mPatchLogInfo("The parameter %s in AdditionalOptions is updated with %s." % (aParam, aValue))
        return EXIT_SUCCESS

    except Exception as E:
        mPatchLogError("Updation of the parameter %s in AdditionalOptions with %s failed." % (aParam, aValue))
        mPatchLogPrint(traceback.format_exc())
        return EXIT_FAILURE


def mDeleteAdditionalOptionsFromPayload(aParam):
    """
    This method is used to delete a parameter from additional_options of payload.json
    """
    mPatchLogInfo("Deleting the parameter %s from AdditionalOptions." % aParam)
    try:
        _json = mLoadJson(TEST_PAYLOAD_FILE)
        _additional_options = _json.get("additional_options", [{}])

        if _additional_options and aParam in _additional_options[0]:
            del _additional_options[0][aParam]
            _json["additional_options"] = _additional_options

            with open(TEST_PAYLOAD_FILE, 'w') as file:
                json.dump(_json, file, indent=4)
            mPatchLogInfo("The parameter %s is deleted from AdditionalOptions." % aParam)
            return EXIT_SUCCESS
        else:
            mPatchLogInfo("The parameter %s not found in AdditionalOptions. No action taken." % aParam)
            return EXIT_SUCCESS

    except Exception as E:
        mPatchLogError("Deletion of the parameter %s from AdditionalOptions failed." % aParam)
        mPatchLogPrint(traceback.format_exc())


def mUpdateParamInJsonFile(aFile, aParam, aValue):
    """
    This is a generic method to update any parameter with a value in json file.
    """
    try:
        _json = mLoadJson(aFile)
        _json[aParam] = aValue
        with open(aFile, 'w') as file:
            json.dump(_json, file, indent=4)
        mPatchLogInfo("The parameter %s is updated with %s." % (aParam, aValue))
        return EXIT_SUCCESS
    except Exception as E:
        mPatchLogError("Updation of the parameter %s with %s failed." % (aParam, aValue))
        mPatchLogPrint(traceback.format_exc())
        return EXIT_FAILURE


def mUpdateParamInPayload(aParam, aValue):
    """
    This is method is used to update any parameter with a value in payload.json file.
    """
    mPatchLogInfo("Updating the parameter %s in payload.json with %s." % (aParam, aValue))
    return mUpdateParamInJsonFile(TEST_PAYLOAD_FILE, aParam, aValue)


def mUpdateInfraPatchingTestConfigParam(aParam, aValue):
    """
    This is method is used to update any parameter with a value in test_infrapatching.conf file.
    """
    mPatchLogInfo("Updating the parameter %s in test_infrapatching.conf with %s." % (aParam, aValue))
    return mUpdateParamInJsonFile(TEST_INFRAPATCHING_CONFIG_FILE_LOCATION, aParam, aValue)


def mComposeDecos(aDecos):
    def composition(func):
        for deco in reversed(aDecos):
            func = deco(func)
        return func

    return composition


def mInvokeInfrapatchCurlCmd(aOperationStyle):
    mPatchLogInfo("Invoking the patch type operation.")
    _username = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrausername')).decode('utf-8')
    _password = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrapassword')).decode('utf-8')
    _ecra_url = mGetInfraPatchingTestConfigParam('ecraurl')
    # _ecra_url = mGetECRAUrl()
    _single_node_vm_cluster_patching = (mGetInfraPatchingTestConfigParam('single_node_vm_cluster_patching') == 'True')
    if _single_node_vm_cluster_patching:
        _cluster = mGetInfraPatchingTestConfigParam('single_node_vm_cluster')
    else:
        _cluster = mGetInfraPatchingTestConfigParam('cluster')
    _is_exacompute = mGetInfraPatchingTestConfigParam('is_exacompute')
    _is_clusterless = mGetInfraPatchingTestConfigParam('is_clusterless')
    global invokeTime
    invokeTime = datetime.now()
    _curl_cmd = "curl --silent -H 'Expect:' -u %s:%s -i -k -X PUT --header 'Content-Type: application/json' --header 'Accept: application/json' -d" % (
        _username, _password)

    _additional_options = mGetParamValueFromPayLoadJson("additional_options")

    _node_selection_method = mGetInfraPatchingTestConfigParam('node_selection_method')
    # SingleNodeUpgrade is chosen, remove IncludeNodeList from additional_options
    if _node_selection_method.lower() == "SingleNodeUpgrade".lower():
        mDeleteAdditionalOptionsFromPayload("IncludeNodeList")
    # When none option is chosen for node_selection_method, it is considered as cluster patching
    # so removing both isSingleNodeUpgrade and IncludeNodeList from additional option sof the payload
    elif _node_selection_method.lower() == "none":
        mDeleteAdditionalOptionsFromPayload("IncludeNodeList")
        mDeleteAdditionalOptionsFromPayload("SingleUpgradeNodeName")
        mDeleteAdditionalOptionsFromPayload("isSingleNodeUpgrade")
    # When IncludeNodeList is chosen for node_selection_method, remove SingleUpgradeNodeName and isSingleNodeUpgrade
    # from additional_options
    elif _node_selection_method.lower() == "IncludeNodeList".lower():
        mDeleteAdditionalOptionsFromPayload("SingleUpgradeNodeName")
        mDeleteAdditionalOptionsFromPayload("isSingleNodeUpgrade")

    _additional_options_str = json.dumps(_additional_options)

    _operation = mGetParamValueFromPayLoadJson("operation")
    _oneoff_script_alias_str = '"OneoffScriptAlias":  "%s",' % mGetParamValueFromPayLoadJson("OneoffScriptAlias")

    if _is_exacompute == "True":
        _payload_cmd = '{ "Operation": "%s", ' \
                       '"LaunchNodes": [ "%s" ],' \
                       '"NodeList": %s,' \
                       '"TargetVersion": %s,' \
                       '"exasplice": "%s"}' % (
                           _operation,
                           mGetParamValueFromPayLoadJson("LaunchNodes"), mGetParamValueFromPayLoadJson("NodeList"),
                           mGetParamValueFromPayLoadJson("TargetVersion"),
                           _additional_options[0]["exasplice"])
        _patch_cmd = "%s '%s' '%s/exacompute/patching'" % (_curl_cmd, _payload_cmd, _ecra_url)
    elif _is_clusterless == "True":
        _payload_cmd = '{"Params":[{ "BackupMode": "%s", ' \
                       '"EnablePlugins": "%s", ' \
                       '"PluginTypes": "%s", ' \
                       '"Fedramp": "DISABLED", ' \
                       '"Retry": "no", ' \
                       '"Clusters": ["%s"], ' \
                       '"Operation": "%s", ' \
                       '%s ' \
                       '"OperationStyle": "%s",' \
                       '"PayloadType": "exadata_release", ' \
                       '"TargetType": ["%s"], ' \
                       '"TargetVersion": %s,' \
                       '"ClusterLess": "yes",' \
                       '"AdditionalOptions":%s}]}' % (
                           mGetParamValueFromPayLoadJson("BackupMode"), mGetParamValueFromPayLoadJson("EnablePlugins"), mGetParamValueFromPayLoadJson("PluginTypes"), _cluster,
                           _operation, "%s" % _oneoff_script_alias_str if _operation == TASK_ONEOFFV2 else "",
                           aOperationStyle, mGetParamValueFromPayLoadJson("target"), mGetParamValueFromPayLoadJson("TargetVersion"), _additional_options_str)
        _patch_cmd = "%s '%s' '%s/racks/patching'" % (_curl_cmd, _payload_cmd, _ecra_url)
    else:
        _payload_cmd = '{"Params":[{ "BackupMode": "%s", ' \
                       '"EnablePlugins": "%s", ' \
                       '"PluginTypes": "%s", ' \
                       '"Fedramp": "DISABLED", ' \
                       '"Retry": "no", ' \
                       '"Clusters": ["%s"], ' \
                       '"Operation": "%s", ' \
                       '%s ' \
                       '"OperationStyle": "%s",' \
                       '"PayloadType": "exadata_release", ' \
                       '"TargetType": ["%s"], ' \
                       '"TargetVersion": %s,' \
                       '"AdditionalOptions":%s}]}' % (
                           mGetParamValueFromPayLoadJson("BackupMode"), mGetParamValueFromPayLoadJson("EnablePlugins"), mGetParamValueFromPayLoadJson("PluginTypes"), _cluster,
                           _operation, "%s" % _oneoff_script_alias_str if _operation == TASK_ONEOFFV2 else "",
                           aOperationStyle, mGetParamValueFromPayLoadJson("target"), mGetParamValueFromPayLoadJson("TargetVersion"), _additional_options_str)
        _patch_cmd = "%s '%s' '%s/racks/patching'" % (_curl_cmd, _payload_cmd, _ecra_url)
    mPatchLogInfo("curl patch_cmd : %s " % _patch_cmd)
    _payload_out = mExecuteCmd(aCmd=_patch_cmd)
    mPatchLogInfo("curl patch_cmd output : %s " % _payload_out)
    return _payload_out


def mGetInfrapatchCurlCmd(aEcraInstallFolder, aOperation, aTarget, aOperationStyle, aClusterName, aMockModeViaRestApi=None):
    """
    Method for returning infrapatch curl cmd. Useful for mock mode where 100s of operations
    executed in a short span of time
    """

    mPatchLogInfo("Invoking the patch type operation.")
    _username = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrausername')).decode('utf-8')
    _password = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrapassword')).decode('utf-8')
    _ecra_url = mGetInfraPatchingTestConfigParam('ecraurl')
    # _ecra_url = mGetECRAUrl()
    _is_exacompute = mGetInfraPatchingTestConfigParam('is_exacompute')
    global invokeTime
    invokeTime = datetime.now()
    _curl_cmd = "curl --silent  -u %s:%s -i -k -X PUT --header 'Content-Type: application/json' --header 'Accept: application/json' -d" % (
        _username, _password)

    _additional_options = mGetParamValueFromPayLoadJson("additional_options")
    _node_selection_method = mGetInfraPatchingTestConfigParam('node_selection_method')
    # SingleNodeUpgrade is chosen, remove IncludeNodeList from additional_options
    if _node_selection_method.lower() == "SingleNodeUpgrade".lower():
        mDeleteAdditionalOptionsFromPayload("IncludeNodeList")
    # When none option is chosen for node_selection_method, it is considered as cluster patching
    # so removing both isSingleNodeUpgrade and IncludeNodeList from additional option sof the payload
    elif _node_selection_method.lower() == "none":
        mDeleteAdditionalOptionsFromPayload("IncludeNodeList")
        mDeleteAdditionalOptionsFromPayload("SingleUpgradeNodeName")
        mDeleteAdditionalOptionsFromPayload("isSingleNodeUpgrade")
    # When IncludeNodeList is chosen for node_selection_method, remove SingleUpgradeNodeName and isSingleNodeUpgrade
    # from additional_options
    elif _node_selection_method.lower() == "IncludeNodeList".lower():
        mDeleteAdditionalOptionsFromPayload("SingleUpgradeNodeName")
        mDeleteAdditionalOptionsFromPayload("isSingleNodeUpgrade")

    if aMockModeViaRestApi:
        _mock_payload = {}

        _mock_rack_details_for_target_type = mGetMockRackDetailsForTargetType(aEcraInstallFolder=aEcraInstallFolder, aTargetType=aTarget)
        if aTarget == PATCH_DOMU:
            _mock_rack_details_for_target_type = mGetMockRackDetailsForTargetType(aEcraInstallFolder=aEcraInstallFolder, aTargetType="dom0domu_mapping")
        if len(_mock_rack_details_for_target_type) > 0:
            _mock_rack_details_for_target_type_payload = {}
            _mock_rack_details_for_target_type_payload[aTarget] = _mock_rack_details_for_target_type
            _mock_payload["mock_rack_details"] = _mock_rack_details_for_target_type_payload

        _mock_response_details_for_target_in_task_type = mGetMockResponseDetailsForTargetInTaskType(aEcraInstallFolder=aEcraInstallFolder, aTaskType=aOperation, aTargetType=aTarget)
        if len(_mock_response_details_for_target_in_task_type) > 0:
            _mock_response_details_for_target_in_task_type_payload = {}
            _mock_response_details_for_target_in_task_type_payload[aOperation] = [ _mock_response_details_for_target_in_task_type ]
            _mock_payload["mock_response_details"] = _mock_response_details_for_target_in_task_type_payload
        
        _additional_options[0]["mockMode"] = _mock_payload
    _additional_options_str = json.dumps(_additional_options)
    mPatchLogInfo(f"\n_additional_options_str = \n{_additional_options_str}\n")

    if _is_exacompute == "True":
        _payload_cmd = '{ "Operation": "%s", ' \
                       '"LaunchNodes": [ "%s" ],' \
                       '"NodeList": %s,' \
                       '"TargetVersion": %s,' \
                       '"exasplice": "%s"}' % (
                           aOperation,
                           mGetParamValueFromPayLoadJson("LaunchNodes"),mGetParamValueFromPayLoadJson("NodeList"),
                           mGetParamValueFromPayLoadJson("TargetVersion"),
                           _additional_options[0]["exasplice"])
        _patch_cmd = "%s '%s' '%s/exacompute/patching'" % (_curl_cmd, _payload_cmd, _ecra_url)
    else:
        _payload_cmd = '{"Params":[{ "BackupMode": "%s", ' \
                       '"EnablePlugins": "%s", ' \
                       '"PluginTypes": "%s", ' \
                       '"Fedramp": "DISABLED", ' \
                       '"Retry": "no", ' \
                       '"Clusters": ["%s"], ' \
                       '"Operation": "%s", ' \
                       '"OperationStyle": "%s",' \
                       '"PayloadType": "exadata_release", ' \
                       '"TargetType": ["%s"], ' \
                       '"TargetVersion": %s,' \
                       '"AdditionalOptions":%s}]}"' % (
                           mGetParamValueFromPayLoadJson("BackupMode"), mGetParamValueFromPayLoadJson("EnablePlugins"), mGetParamValueFromPayLoadJson("PluginTypes"),
                           aClusterName,
                           aOperation,
                           aOperationStyle, aTarget, mGetParamValueFromPayLoadJson("TargetVersion"), _additional_options_str)
        _patch_cmd = "%s '%s' '%s/racks/patching'" % (_curl_cmd, _payload_cmd, _ecra_url)
    mPatchLogInfo("curl patch_cmd : %s " % _patch_cmd)
    return _patch_cmd


def mInvokeWorkflowRetry(aEcraRequestId):
    """
    curl command used to execute workflow retry is
    curl --silent  -u user_name:*** -i -k -X POST --header 'Content-Type: application/json' --header
    'Accept: application/json' -d '{"taskName":"InfraPatchService","workflowId":"7ad917b8-d27e-4805-af3a-182aaae9585e"}'
     'http://phoenix238047.dev3sub3phx.databasede3phx.oraclevcn.com:9001/ecra/endpoint/workflows/retry'
    """

    mPatchLogInfo("Invoking the patch retry  operation.")
    _username = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrausername')).decode('utf-8')
    _password = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrapassword')).decode('utf-8')
    _ecra_url = mGetInfraPatchingTestConfigParam('ecraurl')
    _workflow_payload_data = '{ "taskName": "%s" ,"workflowId": "%s" }' % (
        "InfraPatchService", aEcraRequestId)
    _curl_cmd = "curl --silent  -u %s:%s -i -k -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' -d '%s'  '%s/workflows/retry'" % (
        _username, _password, _workflow_payload_data, _ecra_url)

    mPatchLogInfo("workflow_retry_cmd is : %s " % _curl_cmd)
    _payload_out = mExecuteCmd(aCmd=_curl_cmd)
    mPatchLogInfo("workflow_retry_cmd output is : %s " % _payload_out)
    return _payload_out


def mSetECRAProperty(aPropertyName, aPropertyValue):
    """
    curl command used to set ecra property is
    curl --silent  -u *******  -i -k -X PUT  --header 'Content-Type: application/json' --header  'Accept: application/json'
    -d '{"value":"ENABLED"}' 'http://phoenix238047.dev3sub3phx.databasede3phx.oraclevcn.com:9001/ecra/endpoint/properties/CP_ROLLING_UPGRADE'
    """

    mPatchLogInfo("Setting ECRA property %s to %s." % (aPropertyName, aPropertyValue))
    _username = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrausername')).decode('utf-8')
    _password = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrapassword')).decode('utf-8')
    _ecra_url = mGetInfraPatchingTestConfigParam('ecraurl')
    _ecra_property_set_data = '{ "value": "%s" }' % (aPropertyValue)
    _curl_cmd = "curl --silent  -u %s:%s -i -k -X PUT --header 'Content-Type: application/json' --header 'Accept: application/json' -d '%s'  '%s/properties/%s'" % (
        _username, _password, _ecra_property_set_data, _ecra_url, aPropertyName)

    mPatchLogInfo("ECRA set property curl command : %s " % _curl_cmd)
    mExecuteCmd(aCmd=_curl_cmd)


def mExecuteCmd(aCmd):
    _out = os.popen(aCmd)
    _output = _out.read().strip()
    return _output


def mExtractLogFiles(aPayloadObj):
    _operation = aPayloadObj["operation"]
    os.chdir(logdir)
    _op_log_file_cmd = "ls -latr *%s* | tail -1 | awk '{print $9}'" % (_operation)
    _disp_log_file_cmd = "ls -latr *patch.patchclu_apply* | tail -1 | awk '{print $9}'"

    _op_log_file, _stat = mExecuteLocal(_op_log_file_cmd)
    _disp_log_file, _stat = mExecuteLocal(_disp_log_file_cmd)
    os.chdir(os.getcwd())

    return _op_log_file, _disp_log_file


def mCheckLogsTimeStamp(aOplogs, aDisplogs):
    _op_log_time = datetime.fromtimestamp(os.path.getctime(logdir + aOplogs))
    _disp_log_time = datetime.fromtimestamp(os.path.getctime(logdir + aDisplogs))
    mPatchLogInfo("invokeTime " + str(invokeTime))
    mPatchLogInfo("oplogTime " + str(_op_log_time) + "  " + aOplogs)
    mPatchLogInfo("displogTime " + str(_disp_log_time) + "  " + aDisplogs)
    if _op_log_time > invokeTime and _disp_log_time > invokeTime:
        return True
    else:
        return False


def mGetECRAUrl():
    try:
        _user = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrausername')).decode('utf-8')
        _passwd = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrapassword')).decode('utf-8')
        _ecra_path = os.getcwd() + "/../../../../../../../"
        _geturl_cmd = "echo %s >> /tmp/password.txt; %secracli/ecracli info --username %s --password=/tmp/password.txt | grep host | awk '{print $3}'" % (
            _passwd, _ecra_path, _user)
        _url = mExecuteCmd(aCmd=_geturl_cmd)
        # When Error occurs, grep 'host' on ecracli info returns the following
        # Error: *** ECRA communication. Please ensure host 'http://slc16ofa.us.oracle.com:9001/ecra/endpoint' is up and running and credentials are ok.
        if _url == "ECRA":
            mPatchLogInfo("Ecra services down on this node. Restart ecra and retry patch test.")
            exit()
        return _url
    except Exception as E:
        mPatchLogError("Could not get ECRA endpoint.")
        mPatchLogPrint(traceback.format_exc())
        return None


def mGetStatusFromLogs(aPayloadObj):
    """

    This method uses checking of dispatcher log and worker log to check patch operation is succeded or not.
    TODO : This method is currently not used and might be applicable and helpful during future implementations.

    """
    _oplogTV = False
    _displogTV = False

    mPatchLogInfo("Logs are monitored. Starting in a moment.")
    time.sleep(60)
    # Suspend the check for a couple of for creation of the log files
    _op_logs, _disp_logs = mExtractLogFiles(aPayloadObj)
    mPatchLogInfo("Logs being monitored are " + _op_logs + " and " + _disp_logs)
    op_log_file = os.path.join(logdir, _op_logs)
    disp_log_file = os.path.join(logdir, _disp_logs)
    _test_log_seek = 'egrep -i "Releasing IBFabric lock" %s' % (op_log_file)
    _disp_log_seek = 'egrep -i "status:Done       ret_code:" %s' % (disp_log_file)
    _op_log_success_str = "Operation " + aPayloadObj["operation"] + " completed successfully"
    _validate_status_entry = 'egrep -i "%s|%s" %s' % (_op_log_success_str, "ret_code = 0", op_log_file)
    _testlog_start_time = datetime.now()
    _current_time_in_sec = 0
    _patch_mgr_run = True

    if mCheckLogsTimeStamp(_op_logs, _disp_logs):
        while _patch_mgr_run and _current_time_in_sec < EXADATA_PATCHMGR_CONSOLE_READ_TIMEOUT_SEC:
            _out, _stat = mExecuteLocal(_disp_log_seek)
            if _stat == 0:
                mPatchLogInfo("_out" + _out)
                _patch_mgr_run = False
                if "ret_code:" in _out:
                    if "ret_code:0" in _out or "ret_code:701-614" in _out:
                        _displogTV = True
                    else:
                        _displogTV = False

                _out, _stat = mExecuteLocal(_test_log_seek)
                mPatchLogInfo("_out" + _out)
                if _stat == 0:
                    _patch_mgr_run = False
                    _out, _stat = mExecuteLocal(_validate_status_entry)
                    if _out:
                        _oplogTV = True
                    else:
                        _oplogTV = False

        _patch_progress_time = datetime.now()
        _current_time_in_sec = int((_patch_progress_time - _testlog_start_time).
                                   total_seconds())

    mPatchLogInfo("Patch operation task complete.")
    return (_oplogTV and _displogTV)


def mExecuteLocal(aCmd):
    """
     Common method for all commands executions and generate log to
     exacloud/log/thread/ location along with remote management log.
     Example:
             mgnt-e88c015e-b37d-11e9-8044-0010e0eba926.log

    It returns:

      zero -> if success
      non-zero -> if failure

    """

    _status = 1
    with os.popen('{ ' + aCmd + '; } 2>&1', 'r') as pipe:
        try:
            _output = pipe.read()
            _status = pipe.close()
        except:
            process = pipe._proc
            process.kill()
            process.wait()
            raise
    if _status is None:
        _status = 0
    if _output[-1:] == '\n':
        _output = _output[:-1]
    return _output, _status


def mCheckPatchOperationStatus(aURL, aMockMode=None):
    """

    Method to poll the progress status of the patch operation initiated through curl command.

    It takes status url as the input which is like below
    http://slc16ofa.us.oracle.com:9001/ecra/endpoint/statuses/1ace227a-32e1-41a6-812e-21f14544b88e

    It returns:
      1)True  -> when infra patch operation succeeds.
        False -> when infra patch operation fails.
      2) status call output if present otherwise empty

    It looks for "status" field in the response. Initially the "status" value is 202. Whenever status changes to
    other value within timeout, the result value is determined as fail or success based on the "status" value.
    Currently status:200 and status:300 are considered as success, status:400 and status:500 are considered as
    failure. Currently max poll time(EXADATA_PATCHMGR_CONSOLE_READ_TIMEOUT_SEC) is marked as 12 hrs.

    """

    _user = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrausername')).decode('utf-8')
    _passwd = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrapassword')).decode('utf-8')

    _curl_cmd = "curl --silent -u %s:%s -k -X GET --header 'Content-Type: application/json' --header 'Accept: application/json'" % (
        _user, _passwd)
    _curl_cmd = _curl_cmd + " " + aURL
    mPatchLogInfo("Curl command fired to monitor the patch operation progress status is " + _curl_cmd)
    _patch_operation_start_time = datetime.now()
    _duration_passed_in_sec = 0
    _result = False
    _status_out_put = ""
    while _duration_passed_in_sec < EXADATA_PATCHMGR_CONSOLE_READ_TIMEOUT_SEC:
        _status_out_put = mExecuteCmd(_curl_cmd)
        mPatchLogInfo("Infra Patch operation progress status output : \n" + _status_out_put)
        if "\"status\"" in _status_out_put:
            if "\"status\":202" not in _status_out_put:
                # Currently monitoring only for "status":200, "status":300, "status":400 and "status":500
                if "\"status\":500" in _status_out_put or "\"status\":400" in _status_out_put:
                    mPatchLogInfo("Infra Patch operation failed.")
                    break
                elif "\"status\":200" in _status_out_put or "\"status\":300" in _status_out_put:
                    mPatchLogInfo("Infra Patch operation succeeded.")
                    _result = True
                    break
        else:
            mPatchLogError("Curl command progress status output does not contain status field.")
            break
        if aMockMode is not None and len(aMockMode) > 0:
            time.sleep(2)
        else:
            time.sleep(60)
        _patch_progress_time = datetime.now()
        _duration_passed_in_sec = int((_patch_progress_time - _patch_operation_start_time).total_seconds())
    return _result, _status_out_put


def mGetHTTPHeaderValueFromHTTPResponse(aResponse, aHeader):
    """

    Method to read raw HTTP response and return the HTTP header value of the HTTP header name passed.

    It returns:
      HTTP header value of HTTP header name -> when the requested HTTP header is present in the HTTP Response passed.
                               empty string -> when any of the input parameter passed is empty or if HTTP header name is
                                               not present in the response.

    """
    _result = ""
    if len(aResponse) != 0 and len(aHeader) != 0:
        for _line in aResponse.splitlines():
            if _line.startswith(aHeader + ":"):
                _split_list = _line.split(aHeader + ":")
                if len(_split_list) == 2:
                    _result = _split_list[1]
                    mPatchLogInfo("Http header value for " + aHeader + " is " + _result)
                break
    else:
        mPatchLogInfo("Either http response or http header name is empty or both are empty.")
    return _result


def mExecuteInfraPatchCommand(aOperation, aTarget, aOperationStyle):
    """
     Internal method to execute curl to command to initiate infrapatch operation and to monitor the progress of it.

    It returns:
      1)True  -> when infra patch operation succeeds.
        False -> when infra patch operation fails. It can fail if HTTP response does return 202 or HTTP response does not
               have status_uri HTTP header or Infrapatch operation itself failed in the backend.
      2) status call output if present otherwise empty

    This method does the following
        1)Update the payload json with the operation and target passed as arguments.
        2)Use the payload json and initiate patch operation with curl command.
        3)Parse for status_uri HTTPHeader in the response.
        4)Poll for the progress status.

    """
    _ret = True
    _status_output = ""
    if mUpdateParamInPayload("operation", aOperation) == EXIT_SUCCESS and \
            mUpdateParamInPayload("target", aTarget) == EXIT_SUCCESS:
        _response = mInvokeInfrapatchCurlCmd(aOperationStyle)
        if _response.startswith(HTTP_202_RESPONSE_CODE):
            mPatchLogInfo("Patch operation started in background.")
            _status_url = mGetHTTPHeaderValueFromHTTPResponse(_response, STATUS_URI)
            if len(_status_url) == 0:
                mPatchLogError("Getting http header value from http response failed.")
                _ret = False
            else:
                _ret, _status_output = mCheckPatchOperationStatus(_status_url)
        else:
            mPatchLogError("HTTP 202 response is not returned for the HTTP request of patch operation initiation.")
            try:
                _response_lines = _response.splitlines()
                _json_start = False
                _json_output = ""
                for line in _response_lines:
                    if _json_start:
                        _json_output += line + "\n"
                    if line.strip() == "":
                        _json_start = True
                _status_output = _json_output.strip()
            except Exception as e:
                mPatchLogError(f"Failed to extract JSON error message: {str(e)}")
            _ret = False
    else:
        _ret = False

    return _ret, _status_output


def mExecuteInfraPatchCommandWithDCSAgentChecks(aOperation, aTarget, aOperationStyle=OP_STYLE_ROLLING):
    """
    Method to execute infrapatch operation by validating dcsagent based Infrapatching sanity checks
    on all applicable targets.

    It returns:
         1) True  -> when infra patch operation succeeds along with dcsagent based pre and post sanity checks.
            False -> otherwise.
         2) status call output if present otherwise empty

     Dcsagent Infrapatching sanity checks are applicable only for dom0,cell and domu for patch and rollback operations.

     This method does the following.
     1.Get PRECHECK sanity status through dcsagent from all the applicable VMs by passing proper payload.
     2.If PRECHECK sanity is successful then only run patch/rollback operation.
     3.If patch/rollback is successful then get the POSTCHECK sanity status through dcsagent
       from all the applicable VMs.
     4.If POSTCHECK sanity is also successful then return True. If failure occurs in
       any of the above steps return False.

    Note:
        DCS agent based Infrapatching sanity checks are applicable only for dom0,cell and domu for patch and rollback.

        During non-rolling patch/rollback operation, the below steps are followed for the overall patch flow.
        1.Get dcsagent based precheck data before calling CELL patching.
        2.If precheck sanity is successful then execute patching on cell and dom0.
        3.Get dcsagent based postcheck data after dom0 patching completion.

        During non-rolling cell patching, VMs are shutdown and are brought up after dom0 patch completion,
        so dcsagent checks are not feasible for this period of cell and dom0 patching.

        So here, postcheck sanity for cell patching and precheck sanity for dom0 are not run.
        At the end, after completion of dom0 patching, cell postcheck sanity is run.
    """
    _status_output = ""
    if aTarget in [PATCH_DOMU, PATCH_DOM0, PATCH_CELL] and aOperation in [TASK_PATCH, TASK_ROLLBACK]:

        _ignore_dcs_agent_sanity_checks = (mGetInfraPatchingTestConfigParam('ignore_dcs_agent_sanity_checks') == 'True')
        if _ignore_dcs_agent_sanity_checks:
            mPatchLogInfo("DCS agent based Infrapatching sanity checks are ignored.")

        # For non-rolling dom0 patch operations, dcsagent based precheck is not run.
        if (aOperationStyle == OP_STYLE_NON_ROLLING and aTarget == PATCH_DOM0):
            mPatchLogInfo("Infrapatching %s sanity is not run for %s %s in %s."
                          % (TASK_PRECHECK, aOperationStyle, aOperation, aTarget))
        else:
            if not _ignore_dcs_agent_sanity_checks:
                # Execute PRECHECK sanity through dcsagent API
                _dcsagent_infrapatching_sanity_status = mGetInfrapatchingSanityCheckStatus(aTarget,
                                                                                           TASK_PRECHECK, aOperationStyle)

                if _dcsagent_infrapatching_sanity_status == EXIT_FAILURE:
                    mPatchLogError("Infrapatching %s sanity is failed for %s %s operation in %s."
                                   % (TASK_PRECHECK, aOperationStyle, aOperation, aTarget))
                    return False, _status_output
                else:
                    mPatchLogInfo("Infrapatching %s sanity is successful for %s %s operation in %s."
                                  % (TASK_PRECHECK, aOperationStyle, aOperation, aTarget))

        mPatchLogInfo("Executing %s %s operation in %s."
                      % (aOperationStyle, aOperation, aTarget))
        # Execute actual patch/rollback operation
        _patch_op_ret, _status_output = mExecuteInfraPatchCommandWithRetry(aOperation, aTarget, aOperationStyle)
        if _patch_op_ret:
            # infrapatching operation completed successfully and now dcsagent postcheck sanity would be started.

            # For non-rolling cell patch operations, dcsagent based postcheck is not run.
            if (aOperationStyle == OP_STYLE_NON_ROLLING and aTarget == PATCH_CELL):
                mPatchLogInfo("Infrapatching %s sanity is not run for %s %s in %s."
                              % (TASK_POSTCHECK, aOperationStyle, aOperation, aTarget))
                return True, _status_output
            else:
                _target = aTarget
                # After completion of non-rolling dom0 patching, postchek sanity data is queried with Cell as target
                if aOperationStyle == OP_STYLE_NON_ROLLING and aTarget == PATCH_DOM0:
                    _target = PATCH_CELL

                if not _ignore_dcs_agent_sanity_checks:
                    # Execute POSTCHECK sanity through dcsagent API
                    _dcsagent_infrapatching_sanity_status = mGetInfrapatchingSanityCheckStatus(_target,
                                                                                               TASK_POSTCHECK,
                                                                                               aOperationStyle)
                    if _dcsagent_infrapatching_sanity_status == EXIT_FAILURE:
                        mPatchLogError("Infrapatching %s sanity is failed for %s %s operation in %s."
                                       % (TASK_POSTCHECK, aOperationStyle, aOperation, _target))
                        return False, _status_output
                    else:
                        mPatchLogInfo("Infrapatching %s sanity is successful for %s %s operation in %s."
                                      % (TASK_POSTCHECK, aOperationStyle, aOperation, _target))
                        return True, _status_output
                else:
                    # ignore_dcs_agent_sanity_checks is set to True, so ignoring postcheck sanity result.
                    return True, _status_output
        else:
            mPatchLogError("Infrapatching %s %s operation failed in %s."
                           % (aOperationStyle, aOperation, aTarget))
            return False, _status_output
    else:
        return mExecuteInfraPatchCommandWithRetry(aOperation, aTarget, aOperationStyle)


def mExecuteInfraPatchCommandWithRetry(aOperation, aTarget, aOperationStyle):
    _ret = False
    _status_output = ""
    _max_retry_count = 4
    _cur_iter = 1;
    while _cur_iter < _max_retry_count:
        mPatchLogInfo("Attempt #%d, executing infrapatching %s operatin on %s." % (_cur_iter, aOperation, aTarget))
        _ret, _status_output = mExecuteInfraPatchCommand(aOperation, aTarget, aOperationStyle)

        if not _ret:
            """
            It is seen that sometimes worker thread is not created so infrapatch operation is failing at exacloud side
            and retrying execution of infrapatch operation is going fine.

            Infrapatch operation is retried when it fails and below conditions are met
            1. status is 500 in status call output.
            2. master_request_uuid is present but child_request_uuid is not present in status call output.
            3. when exacloud connectivity errors seen
            """
            if _status_output and "\"status\":500" in _status_output and (
                    ("child_request_uuid" not in _status_output and "master_request_uuid" in _status_output)
                    or
                    ("Failed to get response from exacloud" in _status_output)):
                _cur_iter += 1
                time.sleep(SLEEP_TIME_BETWEEN_PATCH_EXECUTION_RETRIES_IN_SECONDS)
            else:
                # Other failure scenarios
                break
        else:
            # In success scenario, no need to retry
            break
    return _ret, _status_output


def mGetParamValueFromJsonFile(aFile, aKey):
    """
     This method fetches all the configurable parameters
     from the json file
     and returns to the caller.

     Returns :
       Relevant values if present.
       None if empty.
    """

    _infrapatching_config_params = None
    _ret = None
    with open(aFile) as fd:
        _infrapatching_config_params = json.load(fd, object_pairs_hook=OrderedDict)

    if _infrapatching_config_params:
        # Fetch required values from test_infrapatching.conf
        if _infrapatching_config_params[aKey]:
            _ret = _infrapatching_config_params[aKey]
        else:
            mPatchLogError("Configurable parameter : {0} not found or is invalid in {1} file.".format(aKey, aFile))
    else:
        mPatchLogError("{0} file is empty".format(aFile))

    return _ret


def mGetInfraPatchingTestConfigParam(aKey):
    """
    This is method is used to read any parameter value in test_infrapatching.conf file.
    """
    return mGetParamValueFromJsonFile(TEST_INFRAPATCHING_CONFIG_FILE_LOCATION, aKey)


def mGetParamValueFromPayLoadJson(aKey):
    """
    This is method is used to read any parameter value in payload.json file.
    """
    return mGetParamValueFromJsonFile(TEST_PAYLOAD_FILE, aKey)


def mGetDomuClusterDetails(domu_hostname):
    """
     This method reads all the VMs for the key "vm_map" based on the given key (customerHostname or domuNatHostname) from test_infrapatching.conf file
     and returns to the caller the corresponding customerHostname, domuNatHostname and clusterName

     ex:
     1. mGetDomuClusterDetails("sea201323exddu0603.sea2mvm01roce.adminsea2.oraclevcn.com") gives
        [{'clusterName': u'sea2-d3-cl4-c1981188-b4db-4a28-9900-62d6a99d797a-clu03', 'domuNatHostname': u'sea201323exddu0603.sea2mvm01roce.adminsea2.oraclevcn.com', 'customerHostname': u'auto-hbzvr1.client.infrapatchdev.oraclevcn.com'}]

     2. mGetDomuClusterDetails("auto-hbzvr2.client.infrapatchdev.oraclevcn.com") gives
        [{'clusterName': u'sea2-d3-cl4-c1981188-b4db-4a28-9900-62d6a99d797a-clu03', 'domuNatHostname': u'sea201323exddu0703.sea2mvm01roce.adminsea2.oraclevcn.com', 'customerHostname': u'auto-hbzvr2.client.infrapatchdev.oraclevcn.com'}]
    """
    _domu_cluster_details = []
    with open(TEST_INFRAPATCHING_CONFIG_FILE_LOCATION) as file:
        _json_data = json.load(file)

    for _vm_map_key, _vm_map_value in _json_data.get("vm_map", {}).items():
        for _vm in _vm_map_value.get("vms", []):
            if _vm["domuNatHostname"] == domu_hostname or _vm["customerHostname"] == domu_hostname:
                _domu_cluster_details.append({
                    "domuNatHostname": _vm["domuNatHostname"],
                    "customerHostname": _vm["customerHostname"],
                    "clusterName": _vm["clusterName"]
                })

    return _domu_cluster_details


def mRunDbaascliOnDomu(aNode, aDbName):
    """
    This method is used to run dbaascli of a given cdb on a given domu

    Parameters:
    - aNode: domu nathostname where the sql cms needs to be run
    - aDbName: cdb name

    Returns:
    - json o/p of dbaascli cmd of given cdb
      ex:
         {
           "dbSyncTime" : 1721817553315,
           "createTime" : 1712054111000,
           "updateTime" : 0,
           "dbName" : "orclcdb",
           "dbUniqueName" : "orclcdb",
           "dbDomain" : "us.oracle.com",
           "dbId" : 2936289887,
           "cpuCount" : 8,
           "sgaTarget" : "3808MB",
           "pgaAggregateTarget" : "2500MB",
           "dbSize" : "55GB",
           "dbUsedSize" : "12GB",
           "totalFraSize" : "300GB",
           "fraSizeUsed" : "24GB",
           "dbKmsKeyOcid" : null,
           "isCDB" : true,
           "dbRole" : "PRIMARY",
           "dbType" : "RAC",
           "dbClass" : "OLTP",
           "dbEdition" : "EE",
           "dgEnabled" : false,
           "patchVersion" : "19.22.0.0.0",
           "resourceOCIDSettings" : null,
           "tdeDetails" : {
             "tdeKeystoreType" : "FILE",
             "keyVersionOcid" : null,
             "tdeMasterKeyId" : "AYeQbC38VU8dv+HEenGyjWoAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"
           },
           "dbCharacterSet" : {
             "characterSet" : "AL32UTF8",
             "nlsCharacterset" : "AL16UTF16",
             "dbTerritory" : "AMERICA",
             "dbLanguage" : "AMERICAN"
           },
           "dbNodeLevelDetails" : null,
           "pdbs" : {
             "ORCLPDB" : {
               "id" : "151BD2AD919507C9E063A4891F0A5E91",
               "pdbName" : "ORCLPDB",
               "pdbUID" : "408199009",
               "createTime" : 1712054808000,
               "cdbId" : "ca67d685-7ce7-4963-9e98-291fc9b68882",
               "refreshablePDB" : false,
               "refreshMode" : null,
               "refreshIntervalInMinutes" : null,
               "cpuCount" : 8,
               "storageAllocated" : "UNLIMITED",
               "storageUsed" : "8544MB",
               "guid" : "151BD2AD919507C9E063A4891F0A5E91",
               "dbid" : "408199009",
               "conId" : "3",
               "resourceOCIDSettings" : null,
               "tdeDetails" : {
                 "tdeKeystoreType" : "FILE",
                 "keyVersionOcid" : null,
                 "tdeMasterKeyId" : "AQZhYyJrzU9Bv2/9ocXt6jMAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"
               },
               "pdbNodeLevelDetails" : {
                 "scaqag01dv0501m" : {
                   "nodeName" : "scaqag01dv0501m",
                   "openMode" : "READ_WRITE",
                   "restricted" : false
                 }
               },
               "pdbConnectStrings" : [ {
                 "serviceName" : "orclcdb_ORCLPDB.paas.oracle.com",
                 "connectString" : "scaqag0101-scan5:1521/orclcdb_ORCLPDB.paas.oracle.com",
                 "tcpsConnectStrings" : [ "scaqag0101-scan5:2484/orclcdb_ORCLPDB.paas.oracle.com" ]
               } ],
               "pdbSize" : "8GB",
               "pdbUsedSize" : "1GB",
               "pdbSnapshotDetails" : null
             }
           },
           "messages" : [ {
             "locale" : "en",
             "message" : "[FATAL] [DBAAS-60018] Unable to get patch version of Oracle home '/u02/app/oracle/product/19.0.0.0/dbhome_1'.",
             "errorCode" : "DBAAS-60018",
             "cause" : null,
             "action" : "Make sure that home exists and is registered to central inventory.",
             "extraDetails" : null,
             "stackTrace" : null,
             "severityLevel" : "SEVERE",
             "exceptionCause" : {
               "locale" : "en",
               "message" : "[FATAL] [DBAAS-60022] Command '/u02/app/oracle/product/19.0.0.0/dbhome_1/bin/oraversion -compositeVersion' has failed on nodes [scaqag01dv0601m].",
               "errorCode" : "DBAAS-60022",
               "cause" : null,
               "action" : null,
               "extraDetails" : "Result of node:scaqag01dv0601m\n\nExit code of the operation:255",
               "stackTrace" : null,
               "severityLevel" : "SEVERE",
               "exceptionCause" : null
             }
           } ]
         }
    """
    _exacloud_path = mGetExacloudInstallPath()
    _exassh_bin_path = "%s/bin/exassh" % _exacloud_path
    _cmd = "dbaascli database getdetails --dbname %s --reload" % aDbName
    _exassh_cmd = "%s -sl %s -e %s" % (_exassh_bin_path, aNode, _cmd)
    _out, _stat = mExecuteLocal(_exassh_cmd)
    # self.assertEqual(_stat, EXIT_SUCCESS, "ERROR running '%s' on %s" % (_cmd, aNode))
    if _stat == 0:
        _out = re.search(r'\{.*\}', _out, re.DOTALL).group(0)
        _out = json.loads(_out)

    return _out, _stat


def mGetPdbDetails():
    """
    This method is used to get the pdb details

    returns
     1) pdb_name - name of the pdb
     2) oracle_home_path - patch to oracle home
     3) connect_strings - List of db connect strings for each domu except 1st domu
     4) domu_nathostnames - List of domu nat hostnames of each domu except 1st domu

    """
    _test_location = "%s/exabox/infrapatching/test" % mGetExacloudInstallPath()
    _cluster_name = mGetInfraPatchingTestConfigParam('cluster')
    _domu_nathostnames = mGetInfraPatchingTestVms('domuNatHostname', _cluster_name)
    _domu_customer_hostnames = mGetInfraPatchingTestVms('customerHostname', _cluster_name)[1:]
    _stat = 0

    # Get oracle cdb name, user and secret from test config
    _oracle_dbname = mGetInfraPatchingTestConfigParam('oracle_dbname')
    _oracle_dbuser = mGetInfraPatchingTestConfigParam('oracle_dbuser')
    _oracle_dbsecret = mGetInfraPatchingTestConfigParam('oracle_dbsecret')

    # run dbaascli and store the o/p as json
    _output, _stat = mRunDbaascliOnDomu(_domu_nathostnames[0], _oracle_dbname)
    if _stat != 0:
        _stat = 1
        mPatchLogError("Error while running dbaascli cmd on %s" % _domu_nathostnames[0])
        return _stat, '', '', '', ''

    # get the pdb name and connect string
    _pdbs = _output.get("pdbs", {})
    _pdb_key = next(iter(_pdbs))
    _pdb_name = _pdbs[_pdb_key].get("pdbName")
    _connect_string = _pdbs[_pdb_key].get("pdbConnectStrings", [])[0].get("connectString")
    _connect_strings = []

    # create a list of connect string for each domu
    for _domu_customer_hostname in _domu_customer_hostnames:
        _colon_index = _connect_string.find(":")
        _connect_strings.append("%s/%s@%s%s" % (
            _oracle_dbuser, _oracle_dbsecret, _domu_customer_hostname, _connect_string[_colon_index:]))

    # get oracle home location
    _db_node_details = _output.get("dbNodeLevelDetails", {})
    _oracle_home_paths = {node: details.get("homePath") for node, details in _db_node_details.items()}
    _oracle_home_path = _oracle_home_paths[next(iter(_oracle_home_paths))]

    return _stat, _pdb_name, _oracle_home_path, _connect_strings, _domu_nathostnames[1:]


def mRunSqlCommandsOnDomu(aNode, aUser, aFileName):
    """
    This method is used to upload a file having db commands to a given domu and execute it

    Parameters:
    - aNode: domu nathostname where the sql cms needs to be run
    - aUser: user which the files needs to be uploaded and run as
    - aFileName: Name of the file to save the commands.

    Returns:
    - sql command o/p and return status
    """

    _exacloud_path = mGetExacloudInstallPath()
    _exassh_bin_path = "%s/bin/exassh" % _exacloud_path
    _exassh_cmd = "%s -u %s %s -up %s %s" % (_exassh_bin_path, aUser, aNode, aFileName, aFileName)
    _out, _stat = mExecuteLocal(_exassh_cmd)
    if _stat == 0:
        _cmd = "bash %s" % aFileName
        _exassh_cmd = "%s %s -sl -u %s -e %s" % (_exassh_bin_path, aNode, aUser, _cmd)
        mPatchLogInfo("Executing '%s'" % _exassh_cmd)
        _db_out, _stat = mExecuteLocal(_exassh_cmd)
        mPatchLogInfo("_db_out : \n%s" % _db_out)

    return _db_out, _stat

def mUpdatePdb(aFileName, aPdbName, aOracleHomePath, aConnectStrings, aDomuNatHostnames,
               aRestrictedMode=True, aCopyCmdsFile=False):
    """
    This method is used to update/remove pdb to/from restricted mode

    Parameters:
    - aFileName: File to add db cmds and copy to domus
    - aPdbName: pdb name
    - aOracleHomePath: ORACLE_HOME path
    - aConnectStrings: list of connect strings for each domu
    - aDomuNatHostname: list of domu nat hostnames
    - aRestrictMode: True - Will set to resticted mode
                     False - Will remove restricted mode
    - aCopyCmdsFile: True: Will only copy the db cmds file to domu
                     False: Will copy db cmds file and execute the db cmds on domu

    """
    # sql cmds to put the pdb in restricted mode
    if aRestrictedMode:
        _mode = "ALTER PLUGGABLE DATABASE %s OPEN RESTRICTED FORCE;" % aPdbName.upper()
    else:
        _mode = "ALTER PLUGGABLE DATABASE %s OPEN FORCE;" % aPdbName.upper()

    _sql_commands = [
        "show pdbs;",
        "%s" % _mode,
        "show pdbs;"
    ]

    # generate file for running sql cmds and upload to the node
    # sample content of db_cmds:
    # export ORACLE_HOME=/u02/app/oracle/product/19.0.0.0/dbhome_1
    # export PATH=$ORACLE_HOME/bin:$PATH
    # sqlplus -s /nolog <<EOF
    # CONNECT sys/Welcome#123@scaqag01dv0601m.us.oracle.com:1521/orclcdb_ORCLPDB.paas.oracle.com as sysdba
    # WHENEVER SQLERROR EXIT 2;
    # show pdbs;
    # ALTER PLUGGABLE DATABASE ORCLPDB OPEN FORCE;
    # show pdbs;
    # exit;
    # EOF
    _user = "root"
    _exassh_bin_path = "%s/bin/exassh" % mGetExacloudInstallPath()
    _out = ""
    _stat = 0

    # for each domu and connect string from list prepare generate file with list of db cmds to be executed and
    # run them on the domu
    if aCopyCmdsFile:
        # for each domu and connect string from list prepare generate file with list of db cmds to be
        # executed and copy them on the domu
        mGenerateSqlCommands(_sql_commands, aOracleHomePath, aConnectStrings, aFileName, aDomuNatHostnames[0], mGetInfraPatchingTestConfigParam('oracle_dbname') )
        _exassh_cmd = "%s %s -up %s %s" % (_exassh_bin_path, aDomuNatHostnames[0], aFileName, aFileName)
        _out, _stat = mExecuteLocal(_exassh_cmd)
    else:
        for _domunathostname, _connect_string in zip(aDomuNatHostnames, aConnectStrings):
            mPatchLogInfo("Generating sql cmds for %s" % _domunathostname)
            # for each domu and connect string from list prepare generate file with list of db cmds to be
            # executed and run them on the domu
            mGenerateSqlCommands(_sql_commands, aOracleHomePath, [_connect_string], aFileName)
            _out, _stat = mRunSqlCommandsOnDomu(_domunathostname, _user, aFileName)

            if _stat != 0:
                break

    return _stat

def mCreateDbScript(aOracleDbName, aDomuNatHostname, aCmds, aFileName):
    # get customer hostname
    _customer_hostname = mGetCustomerHostname(aDomuNatHostname).split('.')[0]

    # Actual script to keep db in restricted mode
    _script_content = f"""#!/bin/bash

CMD_CHECK_DB="/u01/app/19.0.0.0/grid/bin/crsctl status resource ora.{aOracleDbName}.db -v | grep 'STATE=ONLINE on {_customer_hostname}'"
CMD_CHECK_SVC="/u01/app/19.0.0.0/grid/bin/crsctl status resource ora.{aOracleDbName}.{aOracleDbName}_orclpdb.paas.oracle.com.svc -v | grep 'STATE=ONLINE on {_customer_hostname}'"

# Commands to start db in restrict mode
check_and_stop() {{
    while true; do
        echo "$(date): Checking database status..."
        eval "$CMD_CHECK_DB" > /dev/null 2>&1
        if [ $? -eq 0 ]; then
            echo "$(date): Database is ONLINE. Checking service status..."
            eval "$CMD_CHECK_SVC" > /dev/null 2>&1
            if [ $? -eq 0 ]; then
                echo "$(date): Service is ONLINE. Keeping db in restircted mode..."
                {aCmds}
                echo "$(date): Exiting script."
                exit 0
            else
                echo "$(date): Service is not ONLINE. Retrying in 5 seconds..."
            fi
        else
            echo "$(date): Database is not ONLINE. Retrying in 5 seconds..."
        fi
        sleep 5
    done
}}

check_and_stop
    """

    # Write the content to the file
    with open(aFileName, "w") as file:
        file.write(_script_content)

    mPatchLogInfo("DB script created at: %s" % aFileName)


def mUpdateCdb(aOracleDbName, aDomuNatHostname, aRestrictMode=True, aWriteToFile=False, aFileName=None):
    """
    This method is used to update/remove cdb to/from restricted mode

    Parameters:
    - aOracleDbName: cdb name
    - aDomuNatHostname: domu nat hostname
    - aRestrictMode: True - Will set to resticted mode
                     False - Will remove restricted mode
    - aWriteToFile: True - Will write the cmds to file and copies to give location on domu (used for degradation case)
                    False - Will execute the cmds directly on domu (used for dowtime case)
    - aFileName: File location to which db cmds to be written (used for degradation case). None is default value

    o/p of crsctl once cdb is in restricted mode
    --------------------------------------------------------------------------------
    Name           Target  State        Server                   State details
    --------------------------------------------------------------------------------
    Cluster Resources
    --------------------------------------------------------------------------------
    ora.orclcdb.db
          1        ONLINE  ONLINE       scaqag01dv0501m          Open,HOME=/u02/app/o
                                                                 racle/product/19.0.0
                                                                 .0/dbhome_1,STABLE
          2        ONLINE  INTERMEDIATE scaqag01dv0601m          Restricted Access,HO
                                                                 ME=/u02/app/oracle/p
                                                                 roduct/19.0.0.0/dbho
                                                                 me_1,STABLE
    --------------------------------------------------------------------------------

    """
    _exassh_bin_path = "%s/bin/exassh" % mGetExacloudInstallPath()
    _final_cmds = []
    _stat = 0

    # get instance names
    _cmd = "srvctl config database -d %s | grep \"Database instances: \"" % aOracleDbName
    _exassh_cmd = "%s -sl -u %s %s -e '%s'" % (_exassh_bin_path, "grid", aDomuNatHostname, _cmd)
    _out, _stat = mExecuteLocal(_exassh_cmd)
    if _stat != 0:
        mPatchLogError("Error while executing '%s'" % _exassh_cmd)
        return _stat

    _instances = _out.split("Database instances: ")[1]
    _instances = _instances.split(',')
    if _instances:
        _instances.pop(0)
    else:
        _stat = 1
        mPatchLogError("Error no db instances are running on domus.")
        return _stat

    # For each instance we run the following cmds to keep cdb in restricted mode
    #
    # srvctl stop instance -d orclcdb -i orclcdb2
    # srvctl status instance -d orclcdb -i orclcdb2
    # srvctl start instance -d orclcdb -i orclcdb2 -o restrict
    # srvctl status instance -d orclcdb -i orclcdb2
    # crsctl status resource ora.orclcdb.db -t
    #
    # For each instance we run the following cmds to remove cdb from restricted mode
    #
    # srvctl stop instance -d orclcdb -i orclcdb2
    # srvctl status instance -d orclcdb -i orclcdb2
    # srvctl start instance -d orclcdb -i orclcdb2
    # srvctl status instance -d orclcdb -i orclcdb2
    # crsctl status resource ora.orclcdb.db -t
    for _instance in _instances:
        if aRestrictMode:
            _start_db = "srvctl start instance -d %s -i %s -o restrict" % (aOracleDbName, _instance)
        else:
            _start_db = "srvctl start instance -d %s -i %s" % (aOracleDbName, _instance)

        _cmds = ["srvctl stop instance -d %s -i %s -f" % (aOracleDbName, _instance),
                 "srvctl status instance -d %s -i %s" % (aOracleDbName, _instance),
                 "%s" % _start_db,
                 "srvctl status instance -d %s -i %s" % (aOracleDbName, _instance),
                 "crsctl status resource ora.%s.db -t" % aOracleDbName]

        _final_cmds.extend(_cmds)

    # Write the cmds to file and upload to corresponding domu. This condition is used by cdb degradation scenario
    if aWriteToFile:
        # get customer hostname
        _customer_hostname = mGetCustomerHostname(aDomuNatHostname).split('.')[0]

        # Generate cmds to stop and start db in restricted mode
        _db_cmd_stop_start_restrict_mode = "\n".join([f'eval "{_cmd}"' for _cmd in _final_cmds])
        
        # Generate the script to run domu post reboot
        mCreateDbScript(aOracleDbName, aDomuNatHostname, _db_cmd_stop_start_restrict_mode, aFileName)

        # Upload the generated script to domu that needs to be run post reboot
        _exassh_cmd = "%s -u %s %s -up %s %s" % (_exassh_bin_path, "grid", aDomuNatHostname, aFileName, aFileName)
        mPatchLogInfo("_exassh_cmd : %s" % _exassh_cmd)
        _out, _stat = mExecuteLocal(_exassh_cmd)
        mPatchLogInfo("_out : %s" % _out)
        mPatchLogInfo("_stat : %s" % _stat)
        if _stat != 0:
            mPatchLogError("Error while executing '%s'" % _exassh_cmd)
            return _stat
    else:
        # Directly execute the cmds on corresponding domu. This condition is used by cdb downgrade scenario.
        for _cmd in _final_cmds:
            mPatchLogInfo("Executing : %s" % _cmd)
            _exassh_cmd = "%s -u %s %s -e '%s'" % (_exassh_bin_path, "grid", aDomuNatHostname, _cmd)
            _out, _stat = mExecuteLocal(_exassh_cmd)
            mPatchLogInfo("_out : %s" % _out)
            mPatchLogInfo("_stat : %s" % _stat)
            if _stat != 0:
                mPatchLogError("Error while executing '%s'" % _exassh_cmd)
                return _stat

    return _stat


def mUpdateCrontabEntry(aHostnames, aCrontabEntry, aDeleteCronEntry=False):
    """
    This method is used to add/delete crontrab entry like '@reboot <aFileName>'

    Parameters:
    - aHostnames: natHostnames where crontab entries to be added/deleted
    - aCrontabEntry: crontab entry that needs to be added
    - aDeleteCronEntry: True - Will remove the crontab entry
                     False - Will add the crontab entry

    """
    _exacloud_path = mGetExacloudInstallPath()
    _exassh_bin_path = "%s/bin/exassh" % _exacloud_path
    _stat = 0

    if aDeleteCronEntry:
        _cmd = "(crontab -l | grep -Fv '%s') | crontab -" % aCrontabEntry
    else:
        _cmd = "(crontab -l 2>/dev/null; echo -e '%s') | crontab -" % aCrontabEntry

    for _aHostname in aHostnames:
        _exassh_cmd = "%s %s -e \"%s\"" % (_exassh_bin_path, _aHostname, _cmd)
        _out, _stat = mExecuteLocal(_exassh_cmd)
        if _stat != 0:
            mPatchLogError("ERROR while running : %s" % _exassh_cmd)
            break

    return _stat


def mGenerateSqlCommands(aSqlCommands, aOracleHome, aConnectStrings, aFileName, aDomuNatHostname=None, aOracleDbName=None):
    """
    This method is used to generate a file with given sql commands that needs to be executed on domu

    Parameters:
    - aSqlCommands: List of SQL command strings.
    - aOracleHome: oracle home location
    - aConnectString: connection string
    - aFileName: Name of the file to save the commands.
    - aCmds: list of cmds to be executed

    Ex 1:
    generate_sql_commands(["show pdbs", select * from abc"], "/u02/app/oracle/product/19.0.0.0/dbhome_1", "sys/Welcome#123@auto-hbzvr1.client.infrapatchdev.oraclevcn.com:1521/orclcdb_ORCLPDB.paas.oracle.com", "/tmp/db_cmds", ["ls -l", mkdir /tmp/test])
    """
    _cmd = "rm -f %s" % aFileName
    mExecuteLocal(_cmd)

    for _connect_string in aConnectStrings:
        _pre_sql_cmds = """
export ORACLE_HOME=%s
export PATH=$ORACLE_HOME/bin:$PATH
sqlplus -s /nolog <<EOF
CONNECT %s as sysdba
WHENEVER SQLERROR EXIT 2;""" % (aOracleHome, _connect_string)

        _sql_cmds = ""
        for _command in aSqlCommands:
            _sql_cmds += "%s\n" % _command
        _sql_cmds += "exit;\n"
        _sql_cmds += "EOF\n\n"

        _final_sql_cmds = "%s\n%s" % (_pre_sql_cmds, _sql_cmds)

        if aDomuNatHostname:
            # get customer hostname
            _customer_hostname = mGetCustomerHostname(aDomuNatHostname).split('.')[0]

            # Generate db script that needs to be run on domu
            mCreateDbScript(aOracleDbName, aDomuNatHostname, _final_sql_cmds, aFileName)
        else:
            with open(aFileName, 'a') as file:
                file.write(_final_sql_cmds)

def mShutdownVMsOnDom0(aDom0s):
    _ret = True
    _vm_map = mGetInfraPatchingTestVms("customerHostname", "all")
    for aDom0 in aDom0s:
        _vms = _vm_map[aDom0]
        mPatchLogInfo("vm list on %s is %s " % (aDom0, str(_vms)))
        _shutdown_cmd = ""
        if mGetInfraPatchingTestConfigParam("is_r1_env") == "True":
            _shutdown_cmd = "virsh shutdown "
        else:
            _shutdown_cmd = "xm shutdown "

        for _vm in _vms:
            _cmd = " %s %s " % (_shutdown_cmd, _vm)
            _result, _output_map = mExecuteRemoteExasshCmd(_cmd, [aDom0])
            # Giving sometime for graceful shutdown
            if mGetInfraPatchingTestConfigParam("is_r1_env") == "True":
                time.sleep(60)
            else:
                time.sleep(90)

            if not _result:
                _ret = False
                break
    return _ret


def mStartVMsOnDom0(aDom0s):
    _ret = True
    _vm_map = mGetInfraPatchingTestVms("customerHostname", "all")
    for aDom0 in aDom0s:
        _vms = _vm_map[aDom0]
        mPatchLogInfo("vm list on %s is %s " % (aDom0, str(_vms)))
        for _vm in _vms:
            _start_cmd = ""
            if mGetInfraPatchingTestConfigParam("is_r1_env") == "True":
                _start_cmd = "virsh start "
                _cmd = " %s %s " % (_start_cmd, _vm)
            else:
                _start_cmd = "xm create /EXAVMIMAGES/GuestImages/%s/vm.cfg" % _vm
                _cmd = " %s " % (_start_cmd)

            _result, _output_map = mExecuteRemoteExasshCmd(_cmd, [aDom0])
            # Giving sometime for startup
            time.sleep(10)
            if not _result:
                _ret = False
                break
    return _ret

def mGetCustomerHostname(aDomuNatHostname):
    """
    This method is used to return customer hostname given a nathostname from test_infrapatching.conf

    Ex:
    mGetCustomerHostname("sea201323exddu0601.sea2xx2xx0061qf.adminsea2.oraclevcn.com")
      returns -> pats-6dypb1.client.infrapatchdev.oraclevcn.com
    """
    with open(TEST_INFRAPATCHING_CONFIG_FILE_LOCATION) as file:
        _infrapatching_config_params = json.load(file)

    for key, value in _infrapatching_config_params["vm_map"].items():
        for vm in value["vms"]:
            if vm["domuNatHostname"] == aDomuNatHostname:
                return vm["customerHostname"]
    return None

def mGetInfraPatchingTestVms(aKey, aFilterValue):
    """
     This method reads all the VMs for the key "vm_map" based on the given key (customerHostname or domuNatHostname) from test_infrapatching.conf file
     and returns to the caller.

     Following are the cases covered:

     1. mGetInfraPatchingTestVms("customerHostname", "domu") : prints all domus (based on customerHostname) only of all dom0s
        ex: ['auto-hbzvr1.client.infrapatchdev.oraclevcn.com', 'pats-6dypb1.client.infrapatchdev.oraclevcn.com', 'celts-2ofzi1.client.infrapatchdev.oraclevcn.com', 'eagles-2fdyd1.client.infrapatchdev.oraclevcn.com', 'pats-6dypb2.client.infrapatchdev.oraclevcn.com', 'auto-hbzvr2.client.infrapatchdev.oraclevcn.com', 'auto-hbzvr3.client.infrapatchdev.oraclevcn.com', 'eagles-2fdyd2.client.infrapatchdev.oraclevcn.com', 'pats-6dypb3.client.infrapatchdev.oraclevcn.com', 'pats-6dypb4.client.infrapatchdev.oraclevcn.com', 'celts-2ofzi2.client.infrapatchdev.oraclevcn.com']

     2. mGetInfraPatchingTestVms("customerHostname", "all")  : prints all domu (based on customerHostname) and dom0s
        ex: {'sea201323exdd006.sea2xx2xx0061qf.adminsea2.oraclevcn.com': ['auto-hbzvr1.client.infrapatchdev.oraclevcn.com', 'pats-6dypb1.client.infrapatchdev.oraclevcn.com', 'celts-2ofzi1.client.infrapatchdev.oraclevcn.com'], 'sea201323exdd007.sea2xx2xx0061qf.adminsea2.oraclevcn.com': ['eagles-2fdyd1.client.infrapatchdev.oraclevcn.com', 'pats-6dypb2.client.infrapatchdev.oraclevcn.com', 'auto-hbzvr2.client.infrapatchdev.oraclevcn.com'], 'sea201323exdd008.sea2xx2xx0061qf.adminsea2.oraclevcn.com': ['auto-hbzvr3.client.infrapatchdev.oraclevcn.com', 'eagles-2fdyd2.client.infrapatchdev.oraclevcn.com', 'pats-6dypb3.client.infrapatchdev.oraclevcn.com'], 'sea201323exdd011.sea2xx2xx0061qf.adminsea2.oraclevcn.com': ['pats-6dypb4.client.infrapatchdev.oraclevcn.com', 'celts-2ofzi2.client.infrapatchdev.oraclevcn.com']}

     3. mGetInfraPatchingTestVms("customerHostname", "sea201323exdd006.sea2xx2xx0061qf.adminsea2.oraclevcn.com") : prints only the corresponding domus of given dom0
        ex: ['auto-hbzvr1.client.infrapatchdev.oraclevcn.com', 'pats-6dypb1.client.infrapatchdev.oraclevcn.com', 'celts-2ofzi1.client.infrapatchdev.oraclevcn.com']

     4. mGetInfraPatchingTestVms("customerHostname", "sea2-d3-cl4-c1981188-b4db-4a28-9900-62d6a99d797a-clu03") : prints only the corresponding domus of given cluster
        ex: ['auto-hbzvr1.client.infrapatchdev.oraclevcn.com', 'auto-hbzvr2.client.infrapatchdev.oraclevcn.com', 'auto-hbzvr3.client.infrapatchdev.oraclevcn.com']

     test_infrapatching.conf contains Vms like below
       "vm_map": {
            "sea201323exdd006.sea2xx2xx0061qf.adminsea2.oraclevcn.com": {
                "vms": [
                    {
                        "customerHostname": "auto-hbzvr1.client.infrapatchdev.oraclevcn.com",
                        "domuNatHostname": "sea201323exddu0603.sea2mvm01roce.adminsea2.oraclevcn.com",
                        "clusterName": "sea2-d3-cl4-c1981188-b4db-4a28-9900-62d6a99d797a-clu03"
                    },
                    {
                        "customerHostname": "pats-6dypb1.client.infrapatchdev.oraclevcn.com",
                        "domuNatHostname": "sea201323exddu0601.sea2xx2xx0061qf.adminsea2.oraclevcn.com",
                        "clusterName": "sea2-d3-cl4-c1981188-b4db-4a28-9900-62d6a99d797a-clu01"
                    },
                    {
                        "customerHostname": "celts-2ofzi1.client.infrapatchdev.oraclevcn.com",
                        "domuNatHostname": "sea201323exddu0604.sea2mvm01roce.adminsea2.oraclevcn.com",
                        "clusterName": "sea2-d3-cl4-c1981188-b4db-4a28-9900-62d6a99d797a-clu04"
                    }
                ]
            },
            "sea201323exdd007.sea2xx2xx0061qf.adminsea2.oraclevcn.com": {
                "vms": [
                    {
                        "customerHostname": "eagles-2fdyd1.client.infrapatchdev.oraclevcn.com",
                        "domuNatHostname": "sea201323exddu0702.sea2mvm01roce.adminsea2.oraclevcn.com",
                        "clusterName": "sea2-d3-cl4-c1981188-b4db-4a28-9900-62d6a99d797a-clu02"
                    },
                    {
                        "customerHostname": "pats-6dypb2.client.infrapatchdev.oraclevcn.com",
                        "domuNatHostname": "sea201323exddu0701.sea2xx2xx0061qf.adminsea2.oraclevcn.com",
                        "clusterName": "sea2-d3-cl4-c1981188-b4db-4a28-9900-62d6a99d797a-clu01"
                    },
                    {
                        "customerHostname": "auto-hbzvr2.client.infrapatchdev.oraclevcn.com",
                        "domuNatHostname": "sea201323exddu0703.sea2mvm01roce.adminsea2.oraclevcn.com",
                        "clusterName": "sea2-d3-cl4-c1981188-b4db-4a28-9900-62d6a99d797a-clu03"
                    }
                ]
            },
            "sea201323exdd008.sea2xx2xx0061qf.adminsea2.oraclevcn.com": {
                "vms": [
                    {
                        "customerHostname": "auto-hbzvr3.client.infrapatchdev.oraclevcn.com",
                        "domuNatHostname": "sea201323exddu0803.sea2mvm01roce.adminsea2.oraclevcn.com",
                        "clusterName": "sea2-d3-cl4-c1981188-b4db-4a28-9900-62d6a99d797a-clu03"
                    },
                    {
                        "customerHostname": "eagles-2fdyd2.client.infrapatchdev.oraclevcn.com",
                        "domuNatHostname": "sea201323exddu0802.sea2mvm01roce.adminsea2.oraclevcn.com",
                        "clusterName": "sea2-d3-cl4-c1981188-b4db-4a28-9900-62d6a99d797a-clu02"
                    },
                    {
                        "customerHostname": "pats-6dypb3.client.infrapatchdev.oraclevcn.com",
                        "domuNatHostname": "sea201323exddu0801.sea2xx2xx0061qf.adminsea2.oraclevcn.com",
                        "clusterName": "sea2-d3-cl4-c1981188-b4db-4a28-9900-62d6a99d797a-clu01"
                    }
                ]
            },
            "sea201323exdd011.sea2xx2xx0061qf.adminsea2.oraclevcn.com": {
                "vms": [
                    {
                        "customerHostname": "pats-6dypb4.client.infrapatchdev.oraclevcn.com",
                        "domuNatHostname": "sea201323exddu1101.sea2xx2xx0061qf.adminsea2.oraclevcn.com",
                        "clusterName": "sea2-d3-cl4-c1981188-b4db-4a28-9900-62d6a99d797a-clu01"
                    },
                    {
                        "customerHostname": "celts-2ofzi2.client.infrapatchdev.oraclevcn.com",
                        "domuNatHostname": "sea201323exddu1104.sea2mvm01roce.adminsea2.oraclevcn.com",
                        "clusterName": "sea2-d3-cl4-c1981188-b4db-4a28-9900-62d6a99d797a-clu04"
                    }
                ]
            }
        }
    """

    # Load the JSON data from the file
    with open(TEST_INFRAPATCHING_CONFIG_FILE_LOCATION) as file:
        _infrapatching_config_params = json.load(file)

    # Create an empty list or dictionary to store the results
    _result = []

    # Iterate over the JSON data to find matching values based on aFilterValue
    for _vm_key, _vm_value in _infrapatching_config_params['vm_map'].items():
        for _vm in _vm_value['vms']:
            if aFilterValue == "all" or aFilterValue == _vm_key or aFilterValue == "domu":
                _result.append(_vm[aKey])
            elif 'clusterName' in _vm and aFilterValue == _vm['clusterName']:
                _result.append(_vm[aKey])

    # Return None if there are no matching values for Examples 1 and 3
    if (aFilterValue != "all" and len(_result) == 0) or (
            aFilterValue == "all" and len(_infrapatching_config_params['vm_map']) == 0):
        return None

    if aFilterValue == "all" and len(_result) == 0:
        return {}

    # Create a dictionary with the VM names as keys and the matching domuNatHostname as values for Example 2
    if aFilterValue == "all":
        _output = {}
        for _vm_key, _vm_value in _infrapatching_config_params['vm_map'].items():
            _vm_names = [_vm[aKey] for _vm in _vm_value['vms']]
            _output[_vm_key] = _vm_names
        return _output

    return _result


def mGetInfrapatchingSanityCheckStatus(aTarget, aAction, aOperationStyle):
    """
     This method is used to get overall infrapatching (precheck/postcheck) sanity check status
     by calling dcsagent on all applicable VMs.

     Returns :
       0 if infrapatching sanity check status is successful.
       1 otherwise.

       In summary, this is being done in this method.

       1.Call dcs agent with PRECHECK/POSTCHECK sanity status API from all the applicable VMs with payload as below.
        The following format of curl call is made by providing appropriate idempotency token.
          On success, it returns the jobid for polling.

        curl --silent -X POST -H 'Content-Type: application/json' -H 'Accept: application/json' -d '
        {"action": "PRECHECK", "targetType": "CELL", "operationStyle": "NON-ROLLING", "scope": "CLUSTER", "errorChecks":
         ["CRS_RUNNING", "CRS_RESOURCE", "PING", "REBALANCE", "INCOMPLETE_GI_PATCHINGHUGEPAGES", "QUORAM_DISKS"]} '
         -H 'opc-idempotency-token:1638268749'  http://scaqag01dv0601m.us.oracle.com:7070/infra/patching/sanityCheck

       2. Jobid retrieved from above step is used to poll dcs agent endpoint to get the actual sanity check status
          using below format of curl call.
          curl http://scaqag01dv0601m:7070/infra/patching/sanityCheck?jobId=845bd31d-b351-4157-b58f-e04736e2826e

          Polling dcs agent with jobid returns the following the format of json output.
          {"sanity_precheck_status":0,"sanity_postcheck_status":-1,"error":[],"warning":[]}
       3. Based on the value of sanity_precheck_status/sanity_postcheck_status success or failure is determined.
    """

    def _mPollInfrapatchingSanityCheckJobStatus(aVM, aJobStatusCmd, aActionStatusType):

        """
        This internal method is used to poll sanity check job status.

        Returns :
          if precheck/postcheck sanity is successful
           it returns True along with errors(empty) and warnings list based on sanity check data.
          otherwise
           it returns False along with errors and warnings list based on sanity check data.

        Input parameters:
          aVM : VM hostname
          aJobStatusCmd : curl command to poll
          aActionStatusType :  sanity_precheck_status or sanity_postcheck_status

        Note:
        DCSagent may take 1-2 mins to provide the correct output, so polling is used.

        Initially it may give output like below
          {"code":"NotFound","message":"No infra patching sanity check info found for job id: null"}

        But at the end, the json output comes in the below format(sanity check status is
         reported in "sanity_precheck_status").
           {"sanity_precheck_status":0,"sanity_postcheck_status":-1,"error":[],"warning":[]}

           "sanity_precheck_status" 0 means PRECHECK sanity status successful
                                    1 mean PRECHECK sanity check status failed.

        Same is applicable for POSTCHECK sanity status except that status is reported in "sanity_postcheck_status"

        In case of failure, issues are are reported in either "error" or "warning" depending upon the value of
        "errorChecks" in the input payload.

        If "errorChecks" is empty, all the issues are reported in "warning".
        For the attributes those are marked in "errorChecks", issues get reported in "error".
        """

        # Sample curl command to poll the job status is
        # curl http://scaqag01dv0601m:7070/infra/patching/sanityCheck?jobId=c4ca8dbc-39bf-41b9-89ca-e6c39f599ed4

        mPatchLogInfo("Curl command used to poll %s sanity job status : %s ." % (aAction, aJobStatusCmd))

        _starttime = time.time()
        _elapsed = 0
        if mGetInfraPatchingTestConfigParam("is_r1_env") == "True":
            _timeout = 120  # poll for 2 minutes
        else:
            _timeout = 600  # poll for 10 minutes

        _sanity_status_succeeded = False
        _errors = []
        _warnings = []
        while _elapsed < _timeout:
            _, _output_map = mExecuteDCSAgentCurlCmdOnRemoteNode(aJobStatusCmd, [aVM])
            _job_status_cmd_output = _output_map[aVM]
            mPatchLogInfo("%s sanity job status command output : %s" % (aAction, _job_status_cmd_output))
            if _job_status_cmd_output:
                try:
                    _status_json_out = json.loads(_job_status_cmd_output)
                    """
                     Output is expected in the below format.
                     {"sanity_precheck_status":0,"sanity_postcheck_status":-1,"error":[],"warning":[]}.

                     If it is not th expected format, poll for some more time.                    
                    """
                    if aActionStatusType in _status_json_out:
                        _sanitycheck_status = _status_json_out[aActionStatusType]
                        if _sanitycheck_status == 0:
                            mPatchLogInfo("%s sanity status is successful in %s." % (aAction, aVM))
                            _sanity_status_succeeded = True
                        elif _sanitycheck_status == 1:
                            mPatchLogError("%s sanity status is failed in %s." % (aAction, aVM))
                        else:
                            mPatchLogError("Failed to fetch sanity check job status report."
                                           "Please check the dcs agent status.")

                        if "error" in _status_json_out:
                            _errors = _status_json_out["error"]
                        if "warning" in _status_json_out:
                            _warnings = _status_json_out["warning"]
                        break
                    else:
                        time.sleep(10)  # Sleep for 10 seconds
                        _elapsed = time.time() - _starttime
                        mPatchLogInfo("Polling to retrieve %s sanity job status." % (aAction))
                except Exception as e:
                    mPatchLogError("Exception occured while parsing polling command output to get infrapatching "
                                   "sanity check status.")
                    mPatchLogPrint(traceback.format_exc())
                    break
            else:
                time.sleep(10)  # Sleep for 10 seconds
                _elapsed = time.time() - _starttime
                mPatchLogInfo("Curl command output is empty.Polling to retrieve %s sanity job status." % (aAction))
        return _sanity_status_succeeded, _errors, _warnings
        # end of internal method _mPollInfrapatchingSanityCheckJobStatus

    def _mGetInfrapatchingSanityCheckStatusFromDCSAgent(aVM, aPayload):

        """
        This internal method is used to get infrapatching (precheck/postcheck) sanity check status
        by calling dcsagent on a single VM passed as input parameter.

        Returns :
        if precheck/postcheck sanity is successful
          it returns True along with errors(empty) and warnings list based on sanity check data.
        otherwise
           it returns False along with errors and warnings list based on sanity check data.
        """

        _sanity_check_succeeded = False
        _sanity_check_errors = []
        _sanity_check_warnings = []
        _dbcs_agent_port = str(mGetInfraPatchingTestConfigParam('dbcs_agent_port'))
        _sanity_check_endpoint = "http://%s:%s/infra/patching/sanityCheck" % ("localhost", _dbcs_agent_port)

        """"
         Sample curl command:

         curl --silent -X POST -H 'Content-Type: application/json' -H 'Accept: application/json' 
         -d ' {"action": "PRECHECK", "targetType": "DOM0", "operationStyle": "ROLLING", "scope": "CLUSTER", 
         "errorChecks": ["CRS_RUNNING", "CRS_RESOURCE", "PING", "REBALANCE", "INCOMPLETE_GI_PATCHINGHUGEPAGES", 
         "QUORAM_DISKS"]} ' -H 'opc-idempotency-token:2'  
         http://scaqag01dv0601m.us.oracle.com:7070/infra/patching/sanityCheck  

         Sample output:
           {"updatedTime":1638181696847,"jobId":"3fca442c-a4dc-40a7-926c-ff3a9c7bb771","status":"Success",\
           "message":"Database job: Success. ","logFile":null,"reports":[],"appMessages":null,"exceptionErrorCodes":[],
           "createTimestamp":1638181696845,"resourceList":[],"pct_complete":"100",
           "description":"dbaasapi sanity_check job","infofile":null,"jobSpecificDetailsJson":""}                    
        """

        # Replace escape characters in the payload to pass curl command to exassh command
        aPayload = aPayload.replace('"', '\\\"')

        # To generate random unique idempotency token, time() value is choosen.

        _sanity_check_curl_cmd = "curl --silent -X POST -H \\'Content-Type: application/json\\'" \
                                 " -H \\'Accept: application/json\\' --data \\' %s" \
                                 " \\' -H \\'opc-idempotency-token:%d\\' " \
                                 " %s " % (aPayload, int(time.time()), _sanity_check_endpoint)

        mPatchLogInfo("Command used to invoke %s sanity status : %s " % (aAction, _sanity_check_curl_cmd))
        _, _cmd_output_map = mExecuteDCSAgentCurlCmdOnRemoteNode(_sanity_check_curl_cmd, [aVM])
        _sanity_check_curl_cmd_output = _cmd_output_map[aVM]

        mPatchLogInfo("curl command output for %s sanity status : %s " % (aAction, _sanity_check_curl_cmd_output))

        if _sanity_check_curl_cmd_output:
            try:
                sanity_check_cmd_json_output = json.loads(_sanity_check_curl_cmd_output)
                if "jobId" in sanity_check_cmd_json_output:
                    _jobid = sanity_check_cmd_json_output["jobId"]
                    if aAction == TASK_PRECHECK:
                        aActionStatusType = "sanity_precheck_status"
                    elif aAction == TASK_POSTCHECK:
                        aActionStatusType = "sanity_postcheck_status"

                    _job_status_cmd = "curl %s?jobId=%s" % (_sanity_check_endpoint, _jobid)
                    _sanity_check_succeeded, _sanity_check_errors, _sanity_check_warnings = \
                        _mPollInfrapatchingSanityCheckJobStatus(aVM, _job_status_cmd, aActionStatusType)
                else:
                    mPatchLogError("Job id is not present.Getting %s sanity status for %s failed." % (aAction, aTarget))
            except Exception as e:
                mPatchLogError("Exception occured while parsing curl command output to get infrapatching "
                               "sanity check status.")
                mPatchLogPrint(traceback.format_exc())
        else:
            mPatchLogError("Infrapatching sanity check status command output is empty."
                           "Please check the dcsagent status in %s ." % (aVM))
        return _sanity_check_succeeded, _sanity_check_errors, _sanity_check_warnings
        # end of internal method _mGetInfrapatchingSanityCheckStatusFromDCSAgent

    def _mGetInfrapatchingSanityCheckStatusFromDCSAgentWithRetry(aVM, aPayload):

        """
         This internal method is used to get infrapatching (precheck/postcheck) sanity check status
         by calling dcsagent, with retry mechanism if required.


         When querying for postcheck data, immediatley after patch operation, CRS resources might be in
         intermediate state so postcheck could report failure status.
         To get proper data, retry mechanism implemented to query for a max period.

         Returns :
          1 , if precheck/postcheck sanity is successful
          0, otherwise
         """
        _result = EXIT_FAILURE
        _start_time = time.time()
        _elapsed_time = 0
        _iterations = 0
        _max_poll_period = 600
        _max_poll_period = mGetInfraPatchingTestConfigParam('max_sanity_check_status_poll_time_in_seconds')
        mPatchLogInfo("Max poll time to get the infrapatching sanity check status is %d seconds." % (_max_poll_period))
        while _elapsed_time < _max_poll_period:
            _succeeded, _e, _w = _mGetInfrapatchingSanityCheckStatusFromDCSAgent(aVM, aPayload)
            if not _succeeded and aAction == TASK_POSTCHECK and len(_e) > 0:
                if _iterations % 5 == 0:
                    mPatchLogInfo("Errors found in %s sanity data in %s, so retrying in a loop." % (aAction, aVM))
                _iterations += 1
                time.sleep(10)
                _elapsed_time = time.time() - _start_time
            else:
                if _succeeded:
                    _result = EXIT_SUCCESS
                break

        return _result
        # end of internal method _mGetInfrapatchingSanityCheckStatusFromDCSAgentWithRetry

    _ret = EXIT_SUCCESS
    # PRECHEK/POSTCHECK sanity is applicable only for dom0,cell and domu.
    if aTarget in [PATCH_DOMU, PATCH_DOM0, PATCH_CELL]:
        """"
        The follwoing payload structure is sent to get infrapatching sanity status.
        _payload = {"action": "PRECHECK",
                    "targetType": "DOM0",
                    "operationStyle": "ROLLING",
                    "scope": "CLUSTER",
                    "errorChecks": ["CRS_RUNNING", "CRS_RESOURCE", "PING", "REBALANCE", "INCOMPLETE_GI_PATCHING"
                                                                                        "HUGEPAGES", "QUORAM_DISKS"]}
        """
        _payload = {}

        # dcsagent is called from the VMs in _target_VMs list
        _target_VMs = []
        _features_for_error_checks = ["CRS_RUNNING", "CRS_RESOURCE", "PING", "REBALANCE",
                                      "INCOMPLETE_GI_PATCHING", "HUGEPAGES", "QUORAM_DISKS"]

        _payload["action"] = aAction.upper()
        _payload["targetType"] = aTarget.upper()
        _payload["operationStyle"] = aOperationStyle.upper()
        _payload["errorChecks"] = _features_for_error_checks

        _vms = mGetInfraPatchingTestVms("domuNatHostname", "all")

        """"        
        For Domu and Cell,precheck/postcheck status need to be retrieved from any one of the VM in the cluster
         and from all the clusters.
        So all VMs present from first dom0 are considered.

        In case Dom0, precheck/postcheck status need to retrieved from from domus present in it
        So all VMs are considered for status retrieval.
        """
        if aTarget in [PATCH_DOMU, PATCH_CELL]:
            _payload["scope"] = SANITY_CHECK_SCOPE_CLUSTER
            for _dom0 in _vms:
                _target_VMs = _vms[_dom0]
                break;

        if aTarget in [PATCH_DOM0]:
            _payload["scope"] = SANITY_CHECK_SCOPE_VM
            for _dom0 in _vms:
                for _target_vm in _vms[_dom0]:
                    _target_VMs.append(_target_vm)

        _payload_str = json.dumps(_payload)

        # Infrapatching sanity check status is retrieved through dcsagnet API from all applicable VMs.
        mPatchLogInfo("%s sanity would be run in %s ." % (aAction, str(_target_VMs)))
        '''
        As we don't want to fail in the middle of code coverage we are adding this temporary w/a until below bug is fixed 
        Remove this once "EXACS-114389 - MVM INFRAPATCHING TEST AUTOMATION : domus are in shut off state after non rolling rollback operation" is fixed
        '''
        if mGetInfraPatchingTestConfigParam("is_r1_env") == "True":
            mPatchLogInfo("Verifying if domus are in shutoff state and starting them")
            import subprocess
            output = subprocess.check_output("bash /home/oracle/bin/check_start_domus", shell=True).decode().strip()
            mPatchLogInfo("Script Output:\n" + output)
        '''
        Remove this once "EXACS-114389 - MVM INFRAPATCHING TEST AUTOMATION : domus are in shut off state after non rolling rollback operation" is fixed
        '''
        for _vm in _target_VMs:
            if _mGetInfrapatchingSanityCheckStatusFromDCSAgentWithRetry(_vm, _payload_str) == EXIT_FAILURE:
                _ret = EXIT_FAILURE
    else:
        mPatchLogError("Infrapatching sanity check is not applicable for %s ." % (aTarget))
        _ret = EXIT_FAILURE
    return _ret


def mFetchNodeProgressData(aStatusOutput):
    _node_progress_status_final_str = ""
    _node_progress_status_patttern = "\"node_progressing_status.*status.*status_details.*:.*Succeeded.*\"}]},"

    """
    The mFetchNodeProgressData function filters the patch manager status output and returns the node progress
    date from it   

    Sample input is as follows:

    "node_progressing_status\":{\"sleep_infra_patch\":\"no\",\"infra_patch_start_time\":\"2022-05-25 05:51:39-0700\",
    \"node_patching_progress_data\":[{\"node_name\":\"slcs27celadm05.us.oracle.com\",\"target_type\":\"cell\",
    \"patchmgr_start_time\":\"2022-05-25 05:51:39-0700\",\"last_updated_time\":\"2022-05-25 05:51:39-0700\",\"status\":\"Completed\",
    \"status_details\":\"Succeeded\"},{\"node_name\":\"slcs27celadm04.us.oracle.com\",\"target_type\":\"cell\",
    \"patchmgr_start_time\":\"2022-05-25 05:51:39-0700\",\"last_updated_time\":\"2022-05-25 05:51:39-0700\",\"status\":\"Completed\",
    \"status_details\":\"Succeeded\"}]},
    """
    _node_progress_status_matches = re.findall(_node_progress_status_patttern, aStatusOutput)

    _node_progress_status_str = ""
    if len(_node_progress_status_matches) > 0:
        _node_progress_status_str = _node_progress_status_matches[0]
    else:
        mPatchLogError("Failed to extract node_progress_status from status output.")
        return _node_progress_status_final_str

    # Extract only the value of node_progress_status
    _node_progress_status_str = _node_progress_status_str[_node_progress_status_str.find('{'):-1]

    mPatchLogInfo("Node progress status before filtering the content - %s.\n" % (_node_progress_status_str))

    _node_progress_status_final_str = ""
    # Remove unnecessary \ present in status output
    for i in _node_progress_status_str:
        if i == '\\':
            pass
        else:
            _node_progress_status_final_str += i

    """
    output of node progress status after filtering is of the below format

    {"sleep_infra_patch":"no","infra_patch_start_time":"2022-05-25 06:46:58-0700",
    "node_patching_progress_data":[
    {"node_name":"slcs27celadm04.us.oracle.com","target_type":"cell","patchmgr_start_time":"2022-05-25 06:46:58-0700",
    "last_updated_time":"2022-05-25 06:46:58-0700","status":"Completed","status_details":"Succeeded"},
    {"node_name":"slcs27celadm05.us.oracle.com","target_type":"cell","patchmgr_start_time":"2022-05-25 06:46:58-0700",
    "last_updated_time":"2022-05-25 06:46:58-0700","status":"Completed","status_details":"Succeeded"},
    {"node_name":"slcs27celadm06.us.oracle.com","target_type":"cell","patchmgr_start_time":"2022-05-25 06:46:58-0700",
    "last_updated_time":"2022-05-25 06:46:58-0700","status":"Completed","status_details":"Succeeded"}]}
    """

    mPatchLogInfo("Node progress status after filtering the content - %s.\n" % (_node_progress_status_final_str))
    return _node_progress_status_final_str


def mGetNodesWithPatchOperation(aStatusOutput):
    """
        The mGetNodesWithPatchOperation function returns the list of nodes on which patch operation ran
        (in turn patch_mgr).
        If already upgraded nodes are passed along with nodes which can be patched,
        it would return the nodes which got patched,where patch manager actually ran

        Input:
             Patch status output from patch operation
        Output:
             List of nodes on which patch actually ran

    """

    _nodes = []

    progress_status_str = mFetchNodeProgressData(aStatusOutput)

    _node_progress_status = json.loads(progress_status_str)
    _node_progress_status_data = _node_progress_status["node_patching_progress_data"]
    for _node_data in _node_progress_status_data:
        if "node_name" in _node_data and "patchmgr_start_time" in _node_data \
                and "last_updated_time" in _node_data and "status" in _node_data:
            if _node_data["patchmgr_start_time"] != _node_data["last_updated_time"] \
                    and _node_data["status"] == "Completed":
                _nodes.append(_node_data["node_name"])
    return _nodes


def mCheckNodesPresenceInNodeProgressStatus(aStatusOutput, aExistableNodes, aNonExistableNodes):
    """
    This method is used to extract node_progress_status from status output and checks for required nodes in it.
    node_progress_status should contain only nodes from aExistableNodes and should not contain nodes from
    aNonExistableNodes

    Returns :
       True when both aNonExistableNodes and aNonExistableNodes presence appropriately in node_progress_status.
       False otherwise.
    """

    _node_progress_status_final_str = mFetchNodeProgressData(aStatusOutput)

    mPatchLogInfo("Nodes those should be present in node_progress_status are - %s.\n" % (str(aExistableNodes)))
    mPatchLogInfo(
        "Nodes those should not be present in node_progress_status are - %s.\n" % (str(aNonExistableNodes)))

    _node_progress_status = json.loads(_node_progress_status_final_str)

    # node_progress_status keys would contain all the nodes where patch operation is run.
    _node_progress_status_data = _node_progress_status["node_patching_progress_data"]
    _nodes_in__node_progress_status = set()
    for _node_data in _node_progress_status_data:
        if "node_name" in _node_data:
            _nodes_in__node_progress_status.add(_node_data["node_name"])

    _wrong_nodes = []
    for _node in aExistableNodes:
        if _node not in _nodes_in__node_progress_status:
            _wrong_nodes.append(_node)

    if (len(_wrong_nodes) > 0):
        mPatchLogError(
            "Nodes %s do not exist in node_progress_status that should have existed." % (str(_wrong_nodes)))
        return False

    _wrong_nodes.clear()

    for _node in aNonExistableNodes:
        if _node in _nodes_in__node_progress_status:
            _wrong_nodes.append(_node)

    if (len(_wrong_nodes) > 0):
        mPatchLogError("Nodes %s exist in node_progress_status that should not have existed." % (str(_wrong_nodes)))
        return False

    mPatchLogInfo("Node_progress_status has correct set of nodes.")
    return True

def mStageCustomPluginScripts(aTargetType, aForcePluginFailure = False):
    """
    This method is used to Stage custom script files to dom0/domu nodes.
    Returns :
       True when copying custom script files to dom0/domu nodes is successful
       False otherwise.
    """
    _scripts = []
    if aTargetType in [PATCH_DOM0]:
        _scripts = ["custom_dom0.sh", "custom_dom0_domu.sh"]
    elif aTargetType in [PATCH_DOMU]:
        _scripts = ["custom_domu.sh"]

    # Create custom_*.sh files
    for _script in _scripts:
        _cmd = "echo 'echo \"Automation Verification\"' > %s" % (_script)
        if aForcePluginFailure:
            _cmd = "echo 'exit 1' > %s" % (_script)
        _output, _stat = mExecuteLocal(_cmd)
        if _stat != 0:
            mPatchLogError("Failed to create script file %s." % (_script))
            return False

    def _mCopyScriptsToRemoteNode(aScript, aNodes, aIsDomuPlugin):
        """
        This helper method is used to copy custom_*.sh files to remote dom0/domu nodes.
        """
        _exacloud_path = mGetExacloudInstallPath()
        _user_id = mExecuteCmd("whoami")
        mPatchLogInfo("_user_id = %s " % (_user_id))
        # exassh is used to execute and copy local file to remote node
        _exassh_bin_path = "%s/bin/exassh" % _exacloud_path
        _test_location = "%s/exabox/infrapatching/test" % _exacloud_path
        mPatchLogInfo("Complete path for Exassh is %s. " % (_exassh_bin_path))
        for _node in aNodes:
            # Create /opt/exacloud/customs/plugins/ directory format in the remote node.
            _cmd = "%s %s -e 'mkdir -p /opt/exacloud/customs/plugins/'" \
                   % (_exassh_bin_path, _node)
            _output, _stat = mExecuteLocal(_cmd)
            if _stat != 0:
                mPatchLogError("Error occured while creating plugin directory in %s." % (_node))
                return False
            # Copy the script to /opt/exacloud/customs/plugins/
            _cmd = "%s %s -up  %s/%s /opt/exacloud/customs/plugins/%s" \
                   % (_exassh_bin_path, _node, _test_location, aScript, aScript)
            _output, _stat = mExecuteLocal(_cmd)
            if _stat != 0:
                mPatchLogError("Error occured while copying %s in %s." % (aScript, _node))
                return False

            # Change permission to execute the custom script
            _cmd = "%s %s -e 'chmod +x /opt/exacloud/customs/plugins/%s'" \
                   % (_exassh_bin_path, _node, aScript)
            _output, _stat = mExecuteLocal(_cmd)
            if _stat != 0:
                mPatchLogError("Error occured while changing permissions to %s in %s." % (aScript, _node))
                return False

            # Change ownership of custom script in case of domu
            if aIsDomuPlugin:
                _cmd = "%s %s -e 'chown opc:opc /opt/exacloud/customs/plugins/%s'" \
                       % (_exassh_bin_path, _node, aScript)
                _output, _stat = mExecuteLocal(_cmd)
                if _stat != 0:
                    mPatchLogError("Error occured while changing ownership of %s in %s." % (aScript, _node))
                    return False
            mPatchLogInfo("Custom script %s is staged to the node %s" % (aScript, _node))
        return True

    # End of internal method _mCopyScriptsToRemoteNode

    _domus = mGetInfraPatchingTestVms('domuNatHostname', 'domu')
    _dom0s = mGetInfraPatchingTestConfigParam('dom0s')
    mPatchLogInfo("domus used in automation are : %s " % (str(_domus)))
    mPatchLogInfo("dom0s used in automation are : %s " % (str(_dom0s)))

    if aTargetType in [PATCH_DOM0]:
        _ret = False
        # Copy custom_dom0.sh to dom0 nodes
        _ret = _mCopyScriptsToRemoteNode(_scripts[0], _dom0s, False)
        if _ret:
            # Copy custom_dom0_domu.sh to domu nodes
            _ret = _mCopyScriptsToRemoteNode(_scripts[1], _domus, True)
        return _ret

    if aTargetType in [PATCH_DOMU]:
        # Copy custom_domu.sh to domu nodes
        return _mCopyScriptsToRemoteNode(_scripts[0], _domus, True)

    return True

def mGetSiteCustomizePyLocation():
    _py_version_cmd = "%s/bin/python --version" % mGetExacloudInstallPath()
    _exacloud_py_version = mExecuteCmd(aCmd=_py_version_cmd)
    _exacloud_py_version = ".".join(_exacloud_py_version.split()[1].split('.')[:2])
    if _exacloud_py_version == "3.8":
        _site_customize_path = mGetExacloudInstallPath() + f"/opt/tmp_py38bins/lib/python{_exacloud_py_version}/site-packages/sitecustomize.py"
    else:
        _site_customize_path = mGetExacloudInstallPath() + f"/opt/py3_venv/lib/python{_exacloud_py_version}/site-packages/sitecustomize.py"
    return _site_customize_path

def mGetExacloudInstallPath():
    _ecra_install_base = mGetInfraPatchingTestConfigParam('ecra_install_base')
    _ecra_install_directory = mGetInfraPatchingTestConfigParam('ecra_install_directory')
    _user_id = mExecuteCmd("whoami")
    _exacloud_path = "%s/%s/ecra_installs/%s/mw_home/user_projects/domains/exacloud/" \
                     % (_ecra_install_base, _user_id, _ecra_install_directory)
    if os.path.exists(_exacloud_path):
        mPatchLogInfo("Exacloud install path is %s." % _exacloud_path)
        return _exacloud_path
    else:
        mPatchLogError("Exacloud install path %s does not exist." % _exacloud_path)
        return None


def mGetExacloudOEDARequestsPath(aChildRequestId):
    """
    With this Enh 36601769 - ARCHIVE OEDA REQUEST DIRECTORY AS SOON AS THE REQUEST FINISHES
    oeda requests directory got changed to $EXACLOUD_ROOT/oeda/requests.bak/{exaunit_id}_{operation_uuid} from
     $EXACLOUD_ROOT/oeda/requests/{exaunit_id}_{operation_uuid}
    """
    _exacloud_oeda_requests_folder_path = ""
    _exa_unit_id = "0000-0000-0000-0000"
    # /scratch/sdevasek/ecra_installs/automate/mw_home/user_projects/domains/exacloud/oeda/requests.bak/0000-0000-0000-0000_4fb01508-3103-11ee-8372-00001701a08b/
    _exacloud_oeda_requests_folder_path = "%s/oeda/requests.bak/%s_%s" % (
        mGetExacloudInstallPath(), _exa_unit_id, aChildRequestId)
    return _exacloud_oeda_requests_folder_path


def mExecuteRemoteExasshCmd(aCmd, aNodes=[], ignoreError=None):
    """
    This method is used to execute command on remote nodes using exassh.
    It returns failure even when failure occurs in one of the node to execute the command,
    it returns the command output map with node_name as key and command output as value
    """
    _exited_with_success = True
    _output_map = {}
    _exacloud_path = mGetExacloudInstallPath()
    _exassh_bin_path = "%s/bin/exassh" % _exacloud_path
    for _node in aNodes:
        _exassh_cmd = "%s %s -e %s" % (_exassh_bin_path, _node, aCmd)
        _output, _stat = mExecuteLocal(_exassh_cmd)
        if _stat != 0:
            if ignoreError:
                mPatchLogInfo("Command executed %s on %s and output is %s ." % (
                    _exassh_cmd, _node, _output))
            else:
                mPatchLogError("Error occurred while executing the command %s on %s and output is %s ." % (
                    _exassh_cmd, _node, _output))
                _exited_with_success = False
        else:
            mPatchLogInfo("Output for the command %s on %s is %s ." % (_exassh_cmd, _node, _output))
            """
            Command output for any exassh command contains exassh related meta info so removing first and last line to get the actual command output 
             Sample exassh cmd output:
             ------------------------
             ExaKms: ExaKmsOCI
                {"updatedTime":1670152094148,"jobId":"c28c59d5-9fe0-40e4-b2dc-cb6eb36a07bb","status":"Success",
                "message":"Database job: Success. ","logFile":"/var/opt/oracle/log/grid/dbaasapi/db/sanity_check/dbaasapi_2022-12-04_11:08:10.561271_243122.log","reports":[],"appMessages":null,"exceptionErrorCodes":[],"createTimestamp":1670152094147,"resourceList":[],"pct_complete":"100","description":"dbaasapi sanity_check job","infofile":null,"jobSpecificDetailsJson":""}

                *** Exassh Execution time: 6.634429693222046
            """
            if _output.startswith("ExaKms:"):
                _output_lines = _output.splitlines()
                _output_map[_node] = "".join(_output_lines[1:-2])
            else:
                """
                For some commands output does not start with ExaKms:ExaKmsOCI, so directly capturing the output
                Eg:
                /bin/exassh slcs27dv0305m.us.oracle.com -e cat /etc/oratab|grep grid|grep -v '^#'|grep -i asm|cut -d ':' -f2 on slcs27dv0305m.us.oracle.com is /u01/app/19.0.0.0/grid .

                """
                _output_map[_node] = _output

    return _exited_with_success, _output_map


def mUpdateDBCSconfig(aNodes=[]):
    """
    This method is used to ssl to ssl2 in /opt/oracle/dcs/conf/dcs-agent.json on domus
      1. Take backup of existing dcs-agent.json
      2. Check if ssl2 is already updated in dcs-agent.json if not update it
      3. restart dbcsagent
    """
    _dbcs_config = "/opt/oracle/dcs/conf/dcs-agent.json"

    if mGetInfraPatchingTestConfigParam("is_r1_env") == "True":
        mExecuteRemoteExasshCmd("cp %s %s.bkp.$$" % (_dbcs_config, _dbcs_config), aNodes)
        mExecuteRemoteExasshCmd("grep ssl %s" % _dbcs_config, aNodes)
        mExecuteRemoteExasshCmd("/bin/sed -i \\'s/\\\"ssl\\\"/\\\"ssl2\\\"/\\' %s" % _dbcs_config,
                                aNodes)
        _exited_with_success, _output_map = mExecuteRemoteExasshCmd("grep \"ssl2\" %s" % _dbcs_config, aNodes)
        if _exited_with_success:
            _exited_with_success, _output_map = mExecuteRemoteExasshCmd("systemctl restart dbcsagent", aNodes)
            if not _exited_with_success:
                mPatchLogError("Issue while restarting dbsagent")
                return False
            mPatchLogInfo("Successfully updated dbcs config on all nodes")
            return _exited_with_success
        else:
            mPatchLogError("%s is not updated with ssl2" % _dbcs_config)
            return False
    return True


def mExecuteDCSAgentCurlCmdOnRemoteNode(aCmd, aNodes=[]):
    """
    This method is to execute curl command on guestVM.

    exassh command returns the following output.
    So need to capture the json output from the below command output

    2022-12-05 09:07:45+0000  - PatchTestHandler -INFO- Output for the command /u02/ecra_preprov//oracle/ecra_installs/infrapatch0917/mw_home/user_projects/domains/exacloud//bin/exassh sea201323exddu0701.sea2xx2xx0061qf.adminsea2.oraclevcn.com -e curl http://localhost:7070/infra/patching/sanityCheck?jobId=42060c33-16e5-4fa2-b3b5-b58696976b89 on sea201323exddu0701.sea2xx2xx0061qf.adminsea2.oraclevcn.com is ExaKms: ExaKmsOCI
    {"code":"NotFound","message":"No infra patching sanity check info found for job id: 42060c33-16e5-4fa2-b3b5-b58696976b89"}
    % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                     Dload  Upload   Total   Spent    Left  Speed

      0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
      0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0
    100   122    0   122    0     0     85      0 --:--:--  0:00:01 --:--:--    85

    *** Exassh Execution time: 2.3303062915802

    """

    _exited_with_success, _output_map = mExecuteRemoteExasshCmd(aCmd, aNodes)
    if _exited_with_success:
        for _node in aNodes:
            _output = _output_map[_node]
            _first_bracket_index = _output.find('{')
            _last_bracket_index = _output.rfind('}')
            if _first_bracket_index > -1 and _last_bracket_index > -1:
                _output_map[_node] = _output[_first_bracket_index:_last_bracket_index + 1]

    return _exited_with_success, _output_map


def mCreateOneOffScriptFile(aTaskType):
    """
    This utility method is to create one-off script on the test machine.
    Returns the path for one-off script file upon successful creation otherwise None.
    """
    _script_file_path = None
    if aTaskType == TASK_ONEOFF:
        _script_location = "/tmp/devtest/%s" % aTaskType
        _cmd = "mkdir -p %s" % _script_location
        _output, _stat = mExecuteLocal(_cmd)
        if _stat != 0:
            mPatchLogError("Failed to create directory to stage %s script." % aTaskType)
            return _script_file_path
        _script_file_path = "%s/%s.sh" % (_script_location, aTaskType)
        _default_script_file_path = ""
        if aTaskType == TASK_ONEOFF:
            _default_script_file_path = "%s/exadataPrePostPlugins/oneoff_patch/oneoff_patch.sh" \
                                        "" % (mGetExacloudInstallPath())
        if os.path.exists(_script_file_path):
            os.remove(_script_file_path)
        _cmd = "cp %s %s" % (_default_script_file_path, _script_file_path)
        _output, _stat = mExecuteLocal(_cmd)
        if _stat != 0:
            mPatchLogError("Failed to copy %s to %s ." % (_default_script_file_path, _script_file_path))
            return None
        return _script_file_path
    else:
        return _script_file_path


def mDeletePendingFailedECRARequest():
    """
    This method is used to delete any pending failed ecra request that has rack_patch_update status in ecs_registries table.

    Context:
    --------
    Suppose, while processing the request, exacloud agent gets shut down, then ecra request receives 500 status and ecs_registries table
    would still have rack_patch_update entry for that request. We need to explicitly delete this entry to make sure next request goes through.

    ecra cli output:
    ---------------
    ecra>
    ecra> info
    * Ecracli -22.227
    * host              : http://slc16ofa.us.oracle.com:9001/ecra/endpoint
    * username          : ops
    * password          : ****
    * created exaunits
    *  <id> : <rackname> : <exaname>         : <rackstate> : <ongoing op>      : <workflowId> : <dbSIDs>
    *  61   : slcs27     : slcs27adm0304clu5 : PROVISIONED : rack_patch_update :              :
    * Connecting to endpoint and getting version info...

    cmd output when pending request exists:
    ----------------------------------------
   -bash-4.2$  curl  -k -u "user_name:***" -X DELETE  http://slc16ofa.us.oracle.com:9001/ecra/endpoint/exaunit/61/abort/rack_patch_update?status=500
    {"message":"Successfully aborted requests","status":200,"op":"notset","status-detail":"success"}-bash-4.2$

    cmd output when no pending request exists:
    ------------------------------------------
    -bash-4.2$  curl  -k -u "user_name:***" -X DELETE  http://slc16ofa.us.oracle.com:9001/ecra/endpoint/exaunit/61/abort/rack_patch_update?status=500
    {"status":400,"message":"Could not find request with given parameters","status-detail":"Could not find request with given parameters","op":"notset"}-bash-4.2$


    Note:
    This method would be called at the start of every test to delete any pending failed request.

    Neither the exception is caught nor any error is captured in this method because if issue exists with ecra connectivity, eventually test also fails.
    """
    _username = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrausername')).decode('utf-8')
    _password = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrapassword')).decode('utf-8')
    _ecra_url = mGetInfraPatchingTestConfigParam('ecraurl')
    _rack_exaunit_id = mGetInfraPatchingTestConfigParam('rack_exaunit')
    _ecra_abort_request_cmd = "curl -s -u %s:%s  -k -X DELETE %s/exaunit/%s/abort/rack_patch_update?status=500" % \
                              (_username, _password, _ecra_url, _rack_exaunit_id)
    mPatchLogInfo("Command used to delete pending failed ecra request is - %s " % _ecra_abort_request_cmd)
    _ecra_abort_request_cmd_output = mExecuteCmd(aCmd=_ecra_abort_request_cmd)
    mPatchLogInfo("Curl command output for delete pending failed request is - %s " % _ecra_abort_request_cmd_output)


def mCopyTimeStatsFileForTimeDiffAnalysis(aStatusOutput):
    """
    This method copies time_stats.log file from thread logs directory to a directory where time_stats file for all
    the infrapatch operations of current run.

    Eg:File from ${EXACLOUD_ROOT}/log/threads/0000-0000-0000-0000/132eb072-8036-11ed-8622-fa163e10869c_infrapatch_patch_time_stats.log
    is copied to ${ECRA_BASE}/cur_success_time_stats/cell_rolling_patch_stats.log

    The following steps are followed here
    1. Take back up of previous time stats file ${ECRA_BASE}/prev_success_time_stats/cell_rolling_patch_stats.log as cell_rolling_patch_stats.log_bak
    2. Move current time stats file to previous time stats directory
       i.e Move ${ECRA_BASE}/cur_success_time_stats/cell_rolling_patch_stats.log to ${ECRA_BASE}/prev_success_time_stats/cell_rolling_patch_stats.log
    3. Copy time_stats file from exacloud thread location to current time stats directory
      i.e copy ${EXACLOUD_ROOT}/log/threads/0000-0000-0000-0000/132eb072-8036-11ed-8622-fa163e10869c_infrapatch_patch_time_stats.log to
      ${ECRA_BASE}/cur_success_time_stats/cell_rolling_patch_stats.log
    """
    _ecra_install_base = mGetInfraPatchingTestConfigParam('ecra_install_base')
    _patching_metadata = mCreatePatchOperationMetaDataFromStatusOutput(aStatusOutput)
    _exaclolud_path = mGetExacloudInstallPath()
    _current_infra_patch_time_stats_file = None
    if _patching_metadata:
        _current_infra_patch_time_stats_file = "%s/log/threads/0000-0000-0000-0000/%s_infrapatch_%s_time_stats.log" % (
            _exaclolud_path,
            _patching_metadata.mGetChildRequestUUID(),
            _patching_metadata.mGetOperation())

    if not _current_infra_patch_time_stats_file:
        mPatchLogError("Current infra_patch time_stats file %s does not exist" % _current_infra_patch_time_stats_file)
        return EXIT_FAILURE

    _time_stats_file = "%s_%s_%s_time_stats.log" % (
        _patching_metadata.mGetTargetType(), _patching_metadata.mGetOperationStyle(),
        _patching_metadata.mGetOperation())
    _previous_success_time_stats_dir = "%s/prev_success_time_stats" % _ecra_install_base
    _current_success_time_stats_dir = "%s/cur_success_time_stats" % _ecra_install_base
    if not os.path.exists(_previous_success_time_stats_dir):
        os.makedirs(_previous_success_time_stats_dir)
    if not os.path.exists(_current_success_time_stats_dir):
        os.makedirs(_current_success_time_stats_dir)

    # 1.Take back up of previous time stats file
    _src_file = _previous_success_time_stats_dir + "/" + _time_stats_file
    if os.path.exists(_src_file):
        _src_file = _previous_success_time_stats_dir + "/" + _time_stats_file
        _dest_file = _previous_success_time_stats_dir + "/" + _time_stats_file + "_bak"
        # Remove bak file if exists
        if os.path.exists(_dest_file):
            os.remove(_dest_file)
        _file_move_cmd = "mv %s %s " % (_src_file, _dest_file)
        _output, _stat = mExecuteLocal(_file_move_cmd)
        if _stat != 0:
            mPatchLogError("Unable to take back up for %s." % _src_file)
            return EXIT_FAILURE

    _src_file = _current_success_time_stats_dir + "/" + _time_stats_file
    # Comparing the time diff between the current run result and the previous run result.
    # If the time diff is more than 10 min we are marking the current test as failed.
    _enable_time_diff_analysis = (mGetInfraPatchingTestConfigParam('enable_time_diff_analysis') == 'True')
    if _enable_time_diff_analysis and os.path.isfile(_src_file) and os.path.isfile(
            _current_infra_patch_time_stats_file):
        _pre_run_data = mLoadJson(_src_file)
        _current_run_data = mLoadJson(_current_infra_patch_time_stats_file)
        if type(_pre_run_data) == type(_current_run_data):
            for _current_run_item in _current_run_data.get("node_patching_time_stats"):
                for _pre_run_item in _pre_run_data.get("node_patching_time_stats"):
                    if set(_current_run_item.get("node_names")) == set(_pre_run_item.get("node_names")) \
                            and _current_run_item.get("stage") == _pre_run_item.get("stage"):
                        _time_diff = _current_run_item.get("duration_in_seconds") - _pre_run_item.get(
                            "duration_in_seconds")
                        if _time_diff > TIME_DIFF_THRESHOLD_IN_SECONDS:
                            mPatchLogError("The current test execution has taken more time (%s in seconds) "
                                           "than the previously completed test run (%s in seconds)."
                                           "Marking the current test execution as failed to identify the cause."
                                           "Current run file = %s and previous run file = %s are used to compare the patch stage = %s and nodes = %s."
                                           % (_time_diff, _pre_run_item.get("duration_in_seconds"),
                                              _current_infra_patch_time_stats_file,
                                              _src_file, _current_run_item.get("stage"),
                                              str(_current_run_item.get("node_names"))))
                            return EXIT_FAILURE
        else:
            mPatchLogError(
                "Data type mismatch between previous run time_stats file %s  and current run time_stats file %s"
                % (_src_file, _current_infra_patch_time_stats_file))
            return EXIT_FAILURE

    # 2.Move current time stats file to previous time stats directory
    if os.path.exists(_src_file):
        _dest_file = _previous_success_time_stats_dir + "/" + _time_stats_file
        _file_move_cmd = "mv %s %s " % (_src_file, _dest_file)
        _output, _stat = mExecuteLocal(_file_move_cmd)
        if _stat != 0:
            mPatchLogError("Unable to move current time_stats file %s to previous stats directory" % _src_file)
            return EXIT_FAILURE

    # 3.Copy time_stats file from exacloud thread location to current time stats directory
    _src_file = _current_infra_patch_time_stats_file
    _dest_file = _current_success_time_stats_dir + "/" + _time_stats_file
    _file_move_cmd = "cp -f  %s %s " % (_src_file, _dest_file)
    _output, _stat = mExecuteLocal(_file_move_cmd)
    if _stat != 0:
        mPatchLogError("Unable to copy contents of %s in to current stats directory." % _src_file)
        return EXIT_FAILURE
    mPatchLogInfo("File %s copied to %s for time_profile_diff analysis" % (_src_file, _dest_file))

    return EXIT_SUCCESS


def mPrepareTimeDiffData():
    """
    This method returns the map of infrapatch operation to time profile data with current and previous successful runs duration.

    Time profile data for previous succcessful run is stored in ${ECRA_BASE}/prev_success_time_stats
    Time profile data for latest succcessful run is stored in ${ECRA_BASE}/cur_success_time_stats
    Each operation time stats file name is of the format target_operationstyle_operation_time_stats.log
    Eg: cell_rolling_patch_prereq_check_time_stats.log

    Output for the returning map is as follows
        {'cell_rolling_patch_prereq_check':
                                     [
                                     {
                                        'stage': 'PATCH_MGR',
                                        'node_names': "['slcs27celadm04.us.oracle.com']",
                                        'cur_duration_in_sec': '196.0',
                                        'prev_duration_in_sec': '188.0'},
                                     {
                                        'stage': 'PATCH_MGR',
                                        'node_names': "['slcs27celadm05.us.oracle.com']",
                                        'cur_duration_in_sec': '196.0',
                                        'prev_duration_in_sec': '186.0'},
                                      {
                                        'stage': 'PATCH_MGR',
                                        'node_names': "['slcs27celadm06.us.oracle.com']",
                                        'cur_duration_in_sec': '196.0',
                                        'prev_duration_in_sec': '184.0'},
                                       {
                                         'stage': 'PRE_PATCH',
                                         'node_names': "['slcs27celadm04.us.oracle.com','slcs27celadm05.us.oracle.com', 'slcs27celadm06.us.oracle.com']",
                                         'cur_duration_in_sec': '143',
                                         'prev_duration_in_sec': '216'},
                                       {
                                          'stage': 'POST_PATCH',
                                          'node_names': "['slcs27celadm04.us.oracle.com', 'slcs27celadm05.us.oracle.com', 'slcs27celadm06.us.oracle.com']",
                                          'cur_duration_in_sec': '66',
                                           'prev_duration_in_sec': '72'
                                        }
                                        ]
                                    }
    """
    _env = mGetInfraPatchingTestConfigParam('is_r1_env')
    '''
     We are adding this condition as ibswtich is not applicable in x9m and few tests might differ in future (ex: switch and roce tests)
    '''
    if _env in "True":
        _infrapatch_operations_list = ["cell_rolling_patch_prereq_check", "cell_rolling_patch", "cell_rolling_rollback",
                                       "dom0_rolling_patch_prereq_check", "dom0_rolling_patch", "dom0_rolling_rollback",
                                       "domu_rolling_patch_prereq_check", "domu_rolling_patch", "domu_rolling_rollback",
                                       "cell_non-rolling_patch", "cell_non-rolling_rollback",
                                       "dom0_non-rolling_patch_prereq_check", "dom0_non-rolling_patch",
                                       "dom0_non-rolling_rollback"]
    else:
        _infrapatch_operations_list = ["cell_rolling_patch_prereq_check", "cell_rolling_patch", "cell_rolling_rollback",
                                       "dom0_rolling_patch_prereq_check", "dom0_rolling_patch", "dom0_rolling_rollback",
                                       "domu_rolling_patch_prereq_check", "domu_rolling_patch", "domu_rolling_rollback",
                                       "cell_non-rolling_patch", "cell_non-rolling_rollback",
                                       "dom0_non-rolling_patch_prereq_check", "dom0_non-rolling_patch",
                                       "dom0_non-rolling_rollback",
                                       "ibswitch_rolling_patch_prereq_check", "ibswitch_rolling_patch",
                                       "ibswitch_rolling_rollback"]
    _time_diff_map = {}
    for _infrapatch_operation in _infrapatch_operations_list:
        _time_stats_file = "%s_time_stats.log" % _infrapatch_operation
        _ecra_install_base = mGetInfraPatchingTestConfigParam('ecra_install_base')
        _previous_success_time_stats_file = "%s/prev_success_time_stats/%s" % (_ecra_install_base, _time_stats_file)
        _current_success_time_stats_file = "%s/cur_success_time_stats/%s" % (_ecra_install_base, _time_stats_file)
        _cur_patching_timestats_json = mGetTimeStatsJson(_current_success_time_stats_file)
        _prev_patching_timestats_json = mGetTimeStatsJson(_previous_success_time_stats_file)

        _time_stats_diff_list = []
        _time_stats_diff_entry = {}
        _cur_patching_timestats_json = _cur_patching_timestats_json["node_patching_time_stats"]
        # 1. first get the time profile data for current run with duartions
        for _time_stat in _cur_patching_timestats_json:
            _time_stats_diff_entry = {"stage": _time_stat["stage"], "node_names": _time_stat["node_names"],
                                      "cur_duration_in_sec": str(_time_stat["duration_in_seconds"]),
                                      "prev_duration_in_sec": "NA"}
            _time_stats_diff_list.append(_time_stats_diff_entry)
        # 2. Update the previous run duration for the stages that are captured for current run
        _prev_patching_timestats_json = _prev_patching_timestats_json["node_patching_time_stats"]
        for _time_stat in _prev_patching_timestats_json:
            for _time_stats_diff_list_item in _time_stats_diff_list:
                if _time_stat["stage"] == _time_stats_diff_list_item["stage"] and _time_stat["node_names"] == \
                        _time_stats_diff_list_item["node_names"]:
                    _time_stats_diff_list_item["prev_duration_in_sec"] = str(_time_stat["duration_in_seconds"])

        _time_diff_map[_infrapatch_operation] = _time_stats_diff_list
    return _time_diff_map


def mCreatePatchOperationMetaDataFromStatusOutput(aStatusOutput):
    """
    Fetch infrapatch operation meta data from status call output
    """
    _patching_metadata = None
    _status_json = json.loads(aStatusOutput)
    if _status_json and _status_json["patch_list"]:
        _patch_list = _status_json["patch_list"]
        _patch_list_json = json.loads(_patch_list)
        if _patch_list_json:
            # Get the first key of the dictionary
            _worker_thread_id = next(iter(_patch_list_json))
            if not _worker_thread_id:
                mPatchLogError(
                    "mCreatePatchOperationMetaDataFromStatusOutput : patch_list data from status output does not contain exacloud worker thread details.")

            if _patch_list_json[_worker_thread_id] and _patch_list_json[_worker_thread_id]["report"] and \
                    _patch_list_json[_worker_thread_id]["report"]["data"] and \
                    "operation_type" in _patch_list_json[_worker_thread_id]["report"]["data"] and \
                    "operation_style" in _patch_list_json[_worker_thread_id]["report"]["data"] and \
                    "target_type" in _patch_list_json[_worker_thread_id]["report"]["data"] and \
                    "exa_splice" in _patch_list_json[_worker_thread_id]["report"]["data"]:
                _operation_type = _patch_list_json[_worker_thread_id]["report"]["data"]["operation_type"]
                _operation_style = _patch_list_json[_worker_thread_id]["report"]["data"]["operation_style"]
                _target_type = _patch_list_json[_worker_thread_id]["report"]["data"]["target_type"]
                _target_type = _target_type[0]
                _exa_splice = _patch_list_json[_worker_thread_id]["report"]["data"]["exa_splice"]
                if _exa_splice == "yes":
                    _exa_splice = "Monthly"
                else:
                    _exa_splice = "Quarterly"

                _patching_metadata = InfrapatchOperationMetadata(_worker_thread_id, _target_type,
                                                                 _operation_type, _exa_splice, _operation_style)
            else:
                mPatchLogError(
                    "mCreatePatchOperationMetaDataFromStatusOutput : patch operation meta dat from patch_list data.")

        else:
            mPatchLogError(
                "mCreatePatchOperationMetaDataFromStatusOutput : status output contains empty patch_list data.")
    else:
        mPatchLogError(
            "mCreatePatchOperationMetaDataFromStatusOutput : status output does not contain patch_list data.")
    return _patching_metadata


def mGetTimeStatsJson(aFile):
    _patching_timestats_json = None
    with open(aFile) as fd:
        _patching_timestats_json = json.load(fd, object_pairs_hook=OrderedDict)
    if not _patching_timestats_json:
        mPatchLogError("{0} file is empty".format(aFile))
    return _patching_timestats_json


def GetKeyFromHost(aHost):
    """
    Returns  aHost's ssh public keys found in id_rsa.pub file, if it is not present it would generate and
    returns the key.
    """

    # remove entry using public key
    # Using # as a sed separator in order for the code not to worry about existing slaesh in the key (if any)

    _cmd = 'cat /root/.ssh/id_rsa.pub'
    _result, _output_map = mExecuteRemoteExasshCmd(_cmd, [aHost])
    if not _result:
        _cmd = f"ssh-keygen -q -t rsa -f /root/.ssh/id_rsa -N \\'\\'"
        _result, _output_map = mExecuteRemoteExasshCmd(_cmd, [aHost])
        _cmd = 'cat /root/.ssh/id_rsa.pub'
        _result, _output_map = mExecuteRemoteExasshCmd(_cmd, [aHost])
    return _output_map[aHost]


def RemoveKeyFromHosts(aRemoteHosts, aLaunchNode=None):
    """
    Removes the last ssh public key found in the authorized_keys file on aRemoteHost node.

    It returns:

      zero -> if success
      non-zero -> if failure

    """
    _exassh_bin_path = "%s/bin/exassh" % (mGetExacloudInstallPath())
    _key_file = "/tmp/id_rsa.pub"
    _key_file_content = ""
    if aLaunchNode is not None:
        _exassh_bin_path = "%s/bin/exassh" % (mGetExacloudInstallPath())
        _cmd = "%s %s -dw /root/.ssh/id_rsa.pub %s" \
               % (_exassh_bin_path, aLaunchNode, _key_file)
        mPatchLogInfo("_cmd = %s " % _cmd)
        _output, _status = mExecuteLocal(_cmd)

        if os.path.exists(_key_file):
            with open(_key_file, 'r') as file:
                _key_file_content = file.read().rstrip()

    if _key_file_content:
        _removecmd = 'grep -v "%s" /root/.ssh/authorized_keys > /tmp/remove; cp /tmp/remove /root/.ssh/authorized_keys' % (
            _key_file_content)
    else:
        _removecmd = 'head --lines -1 /root/.ssh/authorized_keys > /tmp/remove; cp /tmp/remove /root/.ssh/authorized_keys'

    for aRemoteHost in aRemoteHosts:
        _cmd = "%s %s -e '%s'" \
               % (_exassh_bin_path, aRemoteHost, _removecmd)
        mPatchLogInfo("_cmd = %s " % _cmd)
        _result, _status = mExecuteLocal(_cmd)
        if _status:
            mPatchLogInfo("Failed to remove the keys")
    return _status


def ConfigureSSHKeys(aLocalHost, aRemoteHosts):
    """
    Configures passwordlesss ssh between alocalHost and aRemoteHost
    """

    cleanupRequired = False
    configureStatus = False

    GetKeyFromHost(aLocalHost)

    _exassh_bin_path = "%s/bin/exassh" % (mGetExacloudInstallPath())
    _cmd = "%s %s -dw /root/.ssh/id_rsa.pub /tmp/id_rsa.pub" \
           % (_exassh_bin_path, aLocalHost)
    mPatchLogInfo("_cmd = %s " % (_cmd))
    _output, _status = mExecuteLocal(_cmd)
    if _status:
        mPatchLogError("%s execution failed" % _cmd)
        return configureStatus, cleanupRequired

    for aRemoteHost in aRemoteHosts:
        _cmd = "%s %s -dw /root/.ssh/authorized_keys /tmp/authorized_keys" \
               % (_exassh_bin_path, aRemoteHost)
        mPatchLogInfo("_cmd = %s " % (_cmd))
        _output, _status = mExecuteLocal(_cmd)
        if _status:
            mPatchLogError("%s execution failed" % _cmd)
            return configureStatus, cleanupRequired

        _cmd = "cat /tmp/id_rsa.pub"
        mPatchLogInfo("_cmd = %s " % (_cmd))
        _output, _status = mExecuteLocal(_cmd)
        if _status:
            mPatchLogError("%s execution failed" % _cmd)
            return configureStatus, cleanupRequired

        _cmd = "cat /tmp/authorized_keys | grep \"%s\"" % _output
        mPatchLogInfo("_cmd = %s " % (_cmd))
        _output, _status = mExecuteLocal(_cmd)

        if not _output:
            _cmd = "%s %s -up /tmp/id_rsa.pub /tmp/keys" \
                   % (_exassh_bin_path, aRemoteHost)
            mPatchLogInfo("_cmd = %s " % (_cmd))
            _output, _status = mExecuteLocal(_cmd)
            if _status:
                mPatchLogError("%s execution failed" % _cmd)
                return configureStatus, cleanupRequired

            # Take backup of authorized_keys file
            _cmd = f'cp /root/.ssh/authorized_keys /root/.ssh/authorized_keys.bk'
            _result, _output_map = mExecuteRemoteExasshCmd(_cmd, [aRemoteHost])

            _cmd = "%s %s -e 'cat /tmp/keys >> /root/.ssh/authorized_keys'" \
                   % (_exassh_bin_path, aRemoteHost)
            mPatchLogInfo("_cmd = %s " % (_cmd))
            mExecuteLocal(_cmd)
            cleanupRequired = True
        else:
            mPatchLogInfo("Authorized key already present")
    configureStatus = True
    return configureStatus, cleanupRequired


def mUpdateCRSonVMs(aAction, aVmList):
    """
     Performs various CRS operation on the DomUs.
     Return
       True - in case CRS operation was successful.
       else - False
    """
    _ret = True
    _output_node_list = {}
    _result = True
    _result, _output_map = mExecuteRemoteExasshCmd("cat /etc/oratab|grep grid|grep -v '^#'|grep -i asm|cut -d ':' -f2",
                                                   [aVmList[0]])
    _crs_bin_path = _output_map[aVmList[0]]
    '''
     aAction can be stop, start, config, check, restart
    '''
    _crs_cmd = "%s/bin/crsctl %s crs" % (_crs_bin_path, aAction)
    mPatchLogInfo("_cmd = %s " % str(_crs_cmd))
    _result, _output_map = mExecuteRemoteExasshCmd("%s" % _crs_cmd, aVmList)

    if not _result:
        _ret = False
        _nodes = _output_map.keys()
        mPatchLogError("CRS operation failed on the nodes : %s" % str(_nodes))
    return _ret

def mRunPatchMgrCmdInBackGroundWithSameLogDirAsInStatusOutput(aStatus, aListWithHostNames, aTargetType, aOperationType,
                                                              aLaunchNode=None):
    """
    This method does the following
    1. Extract patch_mgr_log_dir and other required parameters to run the patch_mgr patch command in the background
    2. Create ssh equvivalance between launchnode and target node
    3. Create patch_mgr_log_dir on the launchnode
    4. Create node_list file on the launchnode
    5. Form patch_mgr command and execute it on the launchnode

    """
    _result = False
    _final_patch_list = ""
    _patch_mgr_log_dir = ""
    _status_json = json.loads(aStatus)
    #  1. Extract patch_mgr_log_dir and other required parameters to run the patch_mgr patch comamnd in the background
    if "patch_list" in _status_json:
        _patch_list = _status_json["patch_list"]
        for i in _patch_list:
            if i == '\\':
                pass
            else:
                _final_patch_list += i
        _patch_list_json = json.loads(_final_patch_list)
        mPatchLogInfo("Final patch_list json is -%s" % str(_patch_list_json))
        _patch_mgr_job_detail = list(_patch_list_json.values())[0]
        if "report" in _patch_mgr_job_detail:
            _report_json = _patch_mgr_job_detail["report"]
            if "data" in _report_json:
                _data_json = _report_json["data"]
                _patch_mgr_log_dir = _data_json["log_dir"]
    else:
        return _result, ""

    """
    log_dir is of the below format
    /u02/dbserver.patch.zip_exadata_ol7_22.1.10.0.0.230422_Linux-x86-64.zip/dbserver_patch_221130/patchmgr_log_b75f885d-74c0-4979-8219-506d909aff6a_slcs27dv0405m
    """
    _temp_list = _patch_mgr_log_dir.split("patchmgr_log_")
    _patch_mgr_log_dir_base_path = _temp_list[0]
    _ecra_request_id = _temp_list[1]
    _ecra_request_id = _ecra_request_id[0:36]
    _patch_mgr_log_dir = "%spatchmgr_log_%s" % (_patch_mgr_log_dir_base_path, _ecra_request_id)

    mPatchLogInfo("patch_mgr_log_dir value is %s" % _patch_mgr_log_dir)
    mPatchLogInfo("ecra_request_id value is %s" % _ecra_request_id)

    if aLaunchNode is None:
        aLaunchNode = aListWithHostNames[1]

    # 2. Setup the ssh equivalance between the launchnode and targetnode
    _ssh_equivalence_configured, _ssh_equivalence_cleanup_required = ConfigureSSHKeys(aLaunchNode,
                                                                                      [aListWithHostNames[0]])

    # add hostname to known_hosts file
    _result, _ = mExecuteRemoteExasshCmd("\'ssh-keyscan -H %s  >> ~/.ssh/known_hosts\'" % (aListWithHostNames[0]),
                                         [aLaunchNode])

    # 3. Create patch_mgr log dir if it does not exist
    result, _ = mExecuteRemoteExasshCmd("mkdir -p %s" % _patch_mgr_log_dir, [aLaunchNode])

    _node_list_file = "%s/node_list" % _patch_mgr_log_dir_base_path
    # 4. Create node_list file on the launchnode
    _node_list_cmd = "\'/usr/bin/echo %s > %s\'" % (aListWithHostNames[0], _node_list_file)
    mPatchLogInfo("cmd used to create node_list file on the launch node %s" % _node_list_cmd)

    result, _ = mExecuteRemoteExasshCmd(_node_list_cmd, [aLaunchNode])

    if aTargetType == PATCH_DOMU or aTargetType == PATCH_DOM0:
        # _patch_mgr_log_dir = '/u02/dbserver.patch.zip_exadata_ol7_22.1.10.0.0.230422_Linux-x86-64.zip/dbserver_patch_221130/'
        # _temp_list will be of this format
        # exadata_ol7_22.1.10.0.0.230422_Linux-x86-64.zip/dbserver_patch_221130/patchmgr_log_b75f885d-74c0-4979-8219-506d909aff6a_slcs27dv0405m
        _temp_list = _patch_mgr_log_dir.split("dbserver.patch.zip_")[1]

        # iso_repo_name will be of this format exadata_ol7_22.1.10.0.0.230422_Linux-x86-64.zip
        _iso_repo_name = _temp_list.split('/')[0]

        # target_version will be of this format 22.1.10.0.0.230422
        _target_version = _iso_repo_name.split('_')[2]

    if ( aTargetType == PATCH_DOM0 or aTargetType == PATCH_DOMU ) and aOperationType == TASK_PATCH:
        # 5. Form patch_mgr command and execute it on the launchnode
        _patch_mgr_cmd = "\'cd %s; nohup ./patchmgr -dbnodes node_list -upgrade -iso_repo %s  -target_version %s  -rolling -log_dir %s  -allow_active_network_mounts </dev/null &> %s/PatchmgrConsole.out &\'" % (
            _patch_mgr_log_dir_base_path, _iso_repo_name, _target_version, _patch_mgr_log_dir, _patch_mgr_log_dir)
    elif ( aTargetType == PATCH_DOM0 or aTargetType == PATCH_DOMU ) and aOperationType == TASK_PREREQ_CHECK:
        _patch_mgr_cmd = "\'cd %s; nohup ./patchmgr -dbnodes node_list --precheck -iso_repo %s  --target_version %s --log_dir %s </dev/null &> %s/PatchmgrConsole.out &\'" % (
            _patch_mgr_log_dir_base_path, _iso_repo_name, _target_version, _patch_mgr_log_dir, _patch_mgr_log_dir)
    elif aTargetType == PATCH_CELL and aOperationType == TASK_PATCH:
        _patch_mgr_cmd = "\'cd %s; nohup ./patchmgr -cells node_list -patch --log_dir %s -rolling </dev/null &> %s/PatchmgrConsole.out &\'" % (
            _patch_mgr_log_dir_base_path, _patch_mgr_log_dir, _patch_mgr_log_dir)
    elif aTargetType == PATCH_IBSWITCH and aOperationType == TASK_PATCH:
        _patch_mgr_cmd = "\'cd %s; nohup ./patchmgr --ibswitches node_list --upgrade -force --log_dir %s </dev/null &> %s/PatchmgrConsole.out &\'" % ( _patch_mgr_log_dir_base_path, _patch_mgr_log_dir, _patch_mgr_log_dir)


    mPatchLogInfo("patch_mgr_cmd used to execute patch_mgr session in the background is %s" % _patch_mgr_cmd)
    result, _ = mExecuteRemoteExasshCmd(_patch_mgr_cmd, [aLaunchNode])

    return result, _ecra_request_id, _ssh_equivalence_cleanup_required



def mUpdateInfrapatchingConfParam(aParam, aValue, aAppendDelimiter=True):
    """
     Update the parameter with value passed in infrapatching.conf
        aParam - parameter name, eg: free_space_check_validation_enabled_on_cell
        aValue - value to be update for the parameter, eg: False
        aAppendDelimiter - This is to determine whether to append "," at the end of the string
                         - Eg:     "free_space_check_validation_enabled_on_cell": "False",
    """
    _ret = True
    _line_separator = ""
    if aAppendDelimiter:
        _line_separator = ","
    _infrapatching_conf_file = "%s/%s" % (mGetExacloudInstallPath(), INFRAPATCHING_CONFIG_FILE_LOCATION)

    _cmd = 'sed -i \'s/"%s":.*/"%s": "%s"%s/\' %s' % (aParam, aParam, aValue, _line_separator, _infrapatching_conf_file)
    mPatchLogInfo("Command executed is %s " % str(_cmd))
    _output, _exit_status = mExecuteLocal(_cmd)
    if _exit_status != 0:
        mPatchLogError("%s execution failed" % _cmd)
        _ret = False
    return _ret

def mUpdateCleanUpPatchingLogsConfig(aUpdateWithEmptyList=False):
    """
    This method reads metadata for cleanup patches on the launchnode for single vm cluster from test_infrapatching.conf and 
    updates it in cleanup_infrapatching_logs.conf
    """
    _src_file = f"{mGetExacloudInstallPath()}/exabox/infrapatching/test/{TEST_INFRAPATCHING_CONFIG_FILE_LOCATION}"
    _dst_file = f"{mGetExacloudInstallPath()}/exabox/infrapatching/config/cleanup_infrapatching_logs.conf"

    with open(_src_file, "r") as _src:
        _src_data = json.load(_src)
        _src_cleanupMetadata = _src_data.get("cleanupMetadata", [])

    with open(_dst_file, "r") as _dst:
        _dst_data = json.load(_dst)

    _default_empty_list = [
        {
            "userName": "",
            "hostName": "",
            "folderToPurge": "",
            "isActive": "no"
        }
    ]

    if aUpdateWithEmptyList:
        _dst_data["cleanupMetadata"] = _default_empty_list
    else:
        _dst_data["cleanupMetadata"] = _src_cleanupMetadata

    with open(_dst_file, "w") as _dst:
        json.dump(_dst_data, _dst, indent=4)

# Mock tool generic apis
def mLogSubprocessOutput(aPipe):
    """
    Helper method to print subprocess console messages to capture into required o/p stream
    """
    _output = []
    for _line in iter(aPipe.readline, b''):  # b'\n'-separated lines
        _each_line = _line.decode('utf-8').strip()
        _output.append(_each_line)
        mPatchLogInfo(_each_line)
    return _output

def mParseStatusCallReportForEluDetails(aOperation, aStatusDetails, aEluoptions):
    """
     Parses the status report for Exadata Live Update (ELU) details.
     For PATCH: Compares elu_type with aEluoptions and checks fields present.
     For PREREQ_CHECK: Validates has_outstanding_work logic; elu_type may not be present.
     For ROLLBACK: Validates ELU fields must not be present.
     Returns True if all validations pass; otherwise, False.
    """
    _ret = True
    try:
        mPatchLogInfo("Parsing status report...")
        _status_detail_dict = json.loads(aStatusDetails)
        patch_list_str = _status_detail_dict.get('patch_list', '{}')
        patch_list = json.loads(patch_list_str)

        first_patch_key = next(iter(patch_list.keys()))
        report = patch_list[first_patch_key].get('report', {})
        report_data = report.get('data', {})

        node_patching = (
            report_data.get('node_progressing_status', {}).get('node_patching_progress_data', [{}])[0])
        elu_type = node_patching.get('exadata_live_update_type')
        has_outstanding_work = node_patching.get('exadata_live_update_has_outstanding_work')

        mPatchLogInfo(f"Operation: {aOperation}")
        mPatchLogInfo(f"Input aEluoptions: {aEluoptions}")
        mPatchLogInfo(f"elu_type from output: {elu_type}")
        mPatchLogInfo(f"has_outstanding_work: {has_outstanding_work}")

        if aOperation == TASK_PATCH:
            # Fields must be present after upgrade
            if elu_type is None or has_outstanding_work is None:
                mPatchLogInfo("Mandatory ELU fields missing after upgrade!")
                mPatchLogError("elu_type and has_outstanding_work must both be present after upgrade (patch operation).")
                _ret = False
            else:
                mPatchLogInfo("Mandatory ELU fields found after upgrade.")

                # Only in PATCH, compare elu_type with aEluoptions
                if aEluoptions in [ "highcvss", "allcvss", "full" ] and aEluoptions != elu_type:
                    mPatchLogError(f"Mismatch: aEluoptions ('{aEluoptions}') is not equal to elu_type ('{elu_type}') in report.")
                    _ret = False
                else:
                    mPatchLogInfo("aEluoptions matches elu_type.")

            if aEluoptions in ['allcvss', 'highcvss', 'full']:
                if has_outstanding_work != 'yes':
                    mPatchLogInfo("has_outstanding_work should be 'yes' after upgrade for highcvss/allcvss/full.")
                    mPatchLogError(f"Error: 'exadata_live_update_has_outstanding_work' is not 'yes' for elu_type '{aEluoptions}'")
                    _ret = False
            elif aEluoptions == 'applypending':
                if has_outstanding_work != 'no exadata live update active':
                    mPatchLogInfo("has_outstanding_work should be 'no exadata live update active' after upgrade for applypending.")
                    mPatchLogError(f"Error: 'exadata_live_update_has_outstanding_work' is not 'no' for elu_type '{aEluoptions}'")
                    _ret = False
            else:
                mPatchLogInfo(f"Invalid elu_type after upgrade: {elu_type}")
                mPatchLogError(f"Invalid elu_type: {elu_type}")
                _ret = False

        elif aOperation == TASK_PREREQ_CHECK:
            mPatchLogInfo("ELU PreCheck operation.")
            # elu_type and has_outstanding_work may or may not be present; logic only checks has_outstanding_work
            if has_outstanding_work is not None:
                if (elu_type in ['allcvss', 'highcvss', 'full'] and
                    has_outstanding_work not in ['no', 'no exadata live update active']):
                    mPatchLogInfo("has_outstanding_work should be 'no' or 'no exadata live update active' for precheck.")
                    mPatchLogError(
                        f"Error: 'exadata_live_update_has_outstanding_work' is not in ['no', 'no exadata live update active'] "
                        f"for elu_type '{elu_type}' in precheck"
                    )
                    _ret = False
                elif elu_type == 'applypending' and has_outstanding_work != 'yes':
                    mPatchLogInfo("has_outstanding_work should be 'yes' for precheck if elu_type is applypending.")
                    mPatchLogError(
                        f"Error: 'exadata_live_update_has_outstanding_work' is not 'yes' for elu_type '{elu_type}' in precheck"
                    )
                    _ret = False
            else:
                mPatchLogInfo("has_outstanding_work not present in precheck, which is allowed.")

        elif aOperation == TASK_ROLLBACK:
            # ELU fields must NOT be present after rollback
            if elu_type is not None or has_outstanding_work is not None:
                mPatchLogInfo("ELU fields should not be present after rollback!")
                mPatchLogError("elu_type and has_outstanding_work must NOT be present after rollback (rollback operation).")
                _ret = False
            else:
                mPatchLogInfo("ELU fields correctly absent after rollback.")

        else:
            mPatchLogInfo(f"Invalid operation_type: {operation_type}")
            mPatchLogError(f"Invalid operation_type: {operation_type}")
            _ret = False

    except (json.JSONDecodeError, KeyError, TypeError) as e:
        mPatchLogInfo(f"Exception during parsing: {e}")
        mPatchLogError(f"Failed to parse report data: {e}")
        _ret = False

    mPatchLogInfo(f"Validation result: {_ret}")
    return _ret

def mRunViaSubprocess(aExecutionType, aExecute, aArg=None):
    """
    Helper method to run cmd/script via subprocess
    """

    # mPatchLogInfo("\nRequest to run: {execute} with arguments: {args} as {executionType}\n".format(execute=aExecute, args=aArg, executionType=aExecutionType))
    if aExecutionType == 'cmd':
        try:
            _process = subprocess.run(aExecute, shell=True, check=True, stdout=subprocess.PIPE)
            _output = _process.stdout.decode('utf-8').strip()
            return _output
        except subprocess.CalledProcessError as e:
            mPatchLogError(e.output)
            raise Exception("Error while running: {cmd}".format(cmd=aExecute))
    elif aExecutionType == 'script':
        if aArg is not None:
            _process = Popen([aExecute, aArg], stdout=PIPE, stderr=STDOUT)
        else:
            _process = Popen(aExecute, stdout=PIPE, stderr=STDOUT)
        with _process.stdout:
            _output = mLogSubprocessOutput(aPipe=_process.stdout)
            return _output
        if _process.wait() != 0:
            raise Exception("Error while running: {cmd}".format(cmd=aExecute))


def mCheckAndStartEcraServices(aEcraInstallFolder):
    """
    Helper method to check ecra services status
    """

    _ecractlScript = "{ecraInstallFolder}/ecractl.sh".format(ecraInstallFolder=aEcraInstallFolder)
    # Before ecra moving to helidon, ecra restart options
    # _options = ['status']
    # With helidon, ecra restart options
    _options = ['status_ecra_server']
    mPatchLogInfo("Checking for ecra services status...\n")
    for _option in _options:
        _process = Popen(["{scriptName}".format(scriptName=_ecractlScript), "{optionName}".format(optionName=_option)],
                         stdout=PIPE, stderr=STDOUT)
    _flag = 0
    with _process.stdout:
        for _line in iter(_process.stdout.readline, b''):  # b'\n'-separated lines
            if "\"result\": \"fail\"" in _line.decode('utf-8').strip():
                _flag = 1
                break;
    if _flag == 1:
        mPatchLogWarn("Ecra services not healthy. Restarting...\n")
        mRestartEcraServices(aEcraInstallFolder)


def mRestartEcraServices(aEcraInstallFolder):
    """
    Helper method to restart ecra services
    """

    _ecractlScript = "{ecraInstallFolder}/ecractl.sh".format(ecraInstallFolder=aEcraInstallFolder)
    # Before ecra moving to helidon, ecra restart options
    # _options = ['stop', 'start', 'status']
    # With helidon, ecra restart options
    _options = ['stop_ecra_server', 'start_ecra_server', 'status_ecra_server']
    for _option in _options:
        # with helidon, ecra services sometime failing with this sort of error msg:
        # Log file /tmp/ecradpy.log2901173840 already exist, use a different path
        fileList = glob.glob('/tmp/ecradpy.log*', recursive=False)
        for file in fileList:
            try:
                os.remove(file)
            except:
                pass
        mRunViaSubprocess(aExecutionType='script', aExecute="{scriptName}".format(scriptName=_ecractlScript),
                          aArg="{optionName}".format(optionName=_option))


def mRestartExacloudService(aEcraInstallFolder):
    """
    Helper method to restart exacloud services
    """

    _exacloudScript = "{ecraInstallFolder}/{domainsFolder}/exacloud/bin/exacloud".format(
        ecraInstallFolder=aEcraInstallFolder, domainsFolder=DOMAINS_FOLDER)
    _options = ['--agent stop', '--agent start --da', '--agent status']
    for _option in _options:
        _output = mRunViaSubprocess(aExecutionType='cmd',
                                    aExecute="{scriptName} {optionName}".format(scriptName=_exacloudScript,
                                                                                optionName=_option))
        mPatchLogInfo(_output)


def mGetDbReleaseFolderNumber(aEcraInstallFolder):
    """
    Helper method to get db release folder name
    """

    _dbRelease = mRunViaSubprocess(aExecutionType='cmd',
                                   aExecute="cd {ecraInstallFolder}/db_home/oracle/product; find . -mindepth 1 -maxdepth 1 -type d -printf '%f\n'".format(
                                       ecraInstallFolder=aEcraInstallFolder))
    if _dbRelease is None:
        raise Exception("Unable to get ECRA DB details!");
    return _dbRelease


def mGetDbHome(aEcraInstallFolder):
    """
    Helper method to get db home
    """

    _dbRelease = mGetDbReleaseFolderNumber(aEcraInstallFolder)
    _dbHome = "{ecraInstallFolder}/db_home/oracle/product/{dbRelease}/dbhome_1".format(
        ecraInstallFolder=aEcraInstallFolder, dbRelease=_dbRelease)
    return _dbHome


def mGetEcraDbConnectionString(aEcraInstallFolder):
    """
    Helper api to prepare db connection string depending on the installed ecra db version
    """

    _dbHome = mGetDbHome(aEcraInstallFolder)
    _ecraDeploymentFile = "{ecraInstallFolder}/{ecraDbDeploymentFile}".format(ecraInstallFolder=aEcraInstallFolder,
                                                                              ecraDbDeploymentFile=ECRADB_DEPLOYMENT_FILE)
    _ecraDbUsername = mRunViaSubprocess(aExecutionType='cmd',
                                        aExecute="grep -w 'ECRA DB user' %s | awk -F ':' '{print $2}' | xargs" % (
                                            _ecraDeploymentFile))
    _ecraDbHostname = mRunViaSubprocess(aExecutionType='cmd', aExecute="hostname -f")
    _ecraDbPort = mRunViaSubprocess(aExecutionType='cmd',
                                    aExecute="grep -w 'DB Port' %s | awk -F ':' '{print $2}' | xargs" % (
                                        _ecraDeploymentFile))
    _ecraDbSID = mRunViaSubprocess(aExecutionType='cmd',
                                   aExecute="grep -w 'DB SID' %s | awk -F ':' '{print $2}' | xargs" % (
                                       _ecraDeploymentFile))
    _ecraDbServiceName = mRunViaSubprocess(aExecutionType='cmd',
                                           aExecute="grep -w 'DB Service' %s | awk -F ':' '{print $2}' | xargs" % (
                                               _ecraDeploymentFile))
    _ecraDbPassword = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrapassword')).decode('utf-8')

    _dbRelease = mGetDbReleaseFolderNumber(aEcraInstallFolder)
    if _dbRelease.startswith('19'):
        _ecraDbConnectionString = "{ORACLE_HOME}/bin/sqlplus {ECRA_DB_USERNAME}/{ECRA_DB_PASSWORD}@{ECRA_DB_HOSTNAME}:{ECRA_DB_PORT}/{ECRA_DB_SERVICE_NAME}".format(
            ORACLE_HOME=_dbHome, ECRA_DB_USERNAME=_ecraDbUsername, ECRA_DB_PASSWORD=_ecraDbPassword,
            ECRA_DB_HOSTNAME=_ecraDbHostname, ECRA_DB_PORT=_ecraDbPort, ECRA_DB_SERVICE_NAME=_ecraDbServiceName)
    else:
        _ecraDbConnectionString = "{ORACLE_HOME}/bin/sqlplus {ECRA_DB_USERNAME}/{ECRA_DB_PASSWORD}".format(
            ORACLE_HOME=_dbHome, ECRA_DB_USERNAME=_ecraDbUsername, ECRA_DB_PASSWORD=_ecraDbPassword)

    return _ecraDbConnectionString


def mExecuteDbCmd(aEcraInstallFolder, aSqlFileToExecute, aOnSqlError=None):
    """
    Helper api to execute db commands in a sql file. This api prepares the
    db connection string and executes the file. Specify what to do when
    there are errors while running the sql files. Supported values from DB documentation are:
    {EXIT [SUCCESS | FAILURE | WARNING | n | variable  | :BindVariable] [COMMIT | ROLLBACK] | CONTINUE [COMMIT | ROLLBACK | NONE]}
    """

    os.environ['DB_HOME'] = mGetDbHome(aEcraInstallFolder)
    os.environ['ECRA_DB_CONNECTION_STRING'] = mGetEcraDbConnectionString(aEcraInstallFolder)
    os.environ['SQL_FILE'] = aSqlFileToExecute
    if aOnSqlError is None:
        os.environ['ON_SQL_ERROR_DO_THIS'] = "NONE"
    else:
        os.environ['ON_SQL_ERROR_DO_THIS'] = aOnSqlError

    # prepare a bash script to run the given sql file
    _tmpShFile = "{homeDir}/tmpSqlFileToExecute.sh".format(homeDir=str(Path.home()))
    with open("{script}".format(script=_tmpShFile), 'w') as rsh:
        rsh.write('''\
#! /bin/bash
export ORACLE_HOME=$DB_HOME
$ECRA_DB_CONNECTION_STRING << EOF
WHENEVER SQLERROR $ON_SQL_ERROR_DO_THIS;
@$SQL_FILE
exit
EOF
''')
    # run the bash script
    os.chmod("{script}".format(script=_tmpShFile), 0o755)
    output = mRunViaSubprocess(aExecutionType='script', aExecute="{script}".format(script=_tmpShFile))
    return output


def mCreateAdeView(aSeries):
    """
    Helper method to create a temp ade view with latest label from the given series
    and stores the view name in ${HOME}/view_dir.txt
    This can be further used to fetch/process files from ade view
    """

    os.environ['SERIES'] = aSeries

    # prepare a bash script to create ade view
    _ade_create_view_script = "{homeDir}/create_ade_view.sh".format(homeDir=str(Path.home()))
    with open("{script}".format(script=_ade_create_view_script), 'w') as rsh:
        rsh.write('''\
#! /bin/bash
ADE_VIEW="temp_view"
ade destroyview ${ADE_VIEW} -force -no_ask
CREATE_VIEW=$(ade createview ${ADE_VIEW} -series ${SERIES} -latest)
export VIEW_DIR=$(echo -e "${CREATE_VIEW}" | grep View | awk '{print $4}')
echo $VIEW_DIR | awk -F' ' '{print $2}' > ${HOME}/view_dir.txt
''')

    # run the bash script
    os.chmod("{script}".format(script=_ade_create_view_script), 0o755)
    mRunViaSubprocess(aExecutionType='script', aExecute="{script}".format(script=_ade_create_view_script))


def mReadMockConfigDetails(aEcraInstallFolder, aKey):
    """
     This method fetches details for a given key from the mock config json
     and returns to the caller.

     Returns :
       Relevant values if present.
       None if empty.
    """
    _mock_config = None
    _ret = None

    # get mock config json from the default mock config from custom_mock_patch.json.
    _file = f"{aEcraInstallFolder}/{DOMAINS_FOLDER}/exacloud/config/{CUSTOM_MOCK_PATCH_FILE_NAME}"
    with open(_file) as fd:
        _mock_config = json.load(fd)
    if _mock_config and aKey in _mock_config and _mock_config[aKey] is not None and len(_mock_config[aKey]) > 0:
        _ret = _mock_config[aKey]
        mPatchLogInfo(f"{aKey} found in {_file} file.")
    else:
        mPatchLogInfo(f"{aKey} either not found in {_file} file or its value is empty!")

    return _ret


def mGetMockRackDetailsForTargetType(aEcraInstallFolder, aTargetType):
    """
     Return mock rack details for node type
    """
    # read mock rack details from mock config
    _mock_rack_details = mReadMockConfigDetails(aEcraInstallFolder=aEcraInstallFolder, aKey="mock_rack_details")

    # read mock rack details for target type from _mock_rack_details
    _mock_rack_details_for_target_type = []
    if _mock_rack_details and aTargetType in _mock_rack_details:
        _mock_rack_details_for_target_type = _mock_rack_details[aTargetType]
        mPatchLogInfo(f"Mock rack details for target type: {aTargetType} found in mock config: {_mock_rack_details_for_target_type}")
    else:
        mPatchLogInfo(f"Mock rack details for target type: {aTargetType} not found in mock config!")

    return _mock_rack_details_for_target_type


def mGetMockResponseDetailsForTargetInTaskType(aEcraInstallFolder, aTaskType, aTargetType):
    """
     Return mock response details for task type
    """
    _mock_response_details = mReadMockConfigDetails(aEcraInstallFolder=aEcraInstallFolder, aKey="mock_response_details")

    # read mock response details for task type from _mock_response_details
    _mock_response_details_for_target_in_task_type = []
    if _mock_response_details and aTaskType in _mock_response_details:
        _mock_response_details_for_task_type_dict = _mock_response_details[aTaskType]
        for _mock_response_details_for_task_type_dict_elem in _mock_response_details_for_task_type_dict:
            if 'target_type' in _mock_response_details_for_task_type_dict_elem.keys() and aTargetType == _mock_response_details_for_task_type_dict_elem['target_type']:
                _mock_response_details_for_target_in_task_type = _mock_response_details_for_task_type_dict_elem
    if len(_mock_response_details_for_target_in_task_type) > 0:
        mPatchLogInfo(f"Mock response details for target type: {aTargetType} in task type: {aTaskType} found in mock config: {_mock_response_details_for_target_in_task_type}")
    else:
        mPatchLogInfo(f"Mock response details for target type: {aTargetType} in task type: {aTaskType} not found in mock config")

    return _mock_response_details_for_target_in_task_type

def mExecuteSqlonEcraDb(sqlStr):

    _ecra_install_folder = os.path.join(
        mGetInfraPatchingTestConfigParam('ecra_install_base'),
        os.getlogin(),
        "ecra_installs",
        mGetInfraPatchingTestConfigParam('ecra_install_directory')
    )
    _user_home = str(Path.home())

    _sql_file = f"{_user_home}/update_db.sql"

    # create a file with db commands to update error_code and error_message in the ecra db
    with open(_sql_file, 'w') as f:
        f.write(
            f"{sqlStr}"
        )

    output = mExecuteDbCmd(aEcraInstallFolder=_ecra_install_folder, aSqlFileToExecute=_sql_file, aOnSqlError="CONTINUE")
    return output



def mRunPatchRetryScenario(aPatchType, aPatchTask, aPatchNodeList, aFailureCmd, aRestoreCmd, aErrorCode, aErrorMsg, aLaunchNode=None):
    """
      The following things are done in this method
      1. Execute patch and make it to fail at patch_mgr side
      2. ecradb error code is changed to SUCCESS so that retry continutes and latches on the existing patchmgr session
      3. Run patch_mgr cmd explicitly in the background
      4. After certain time limit retry the workflow
      5. Start monitoring status of the above request
    """
    _result, _ = mExecuteRemoteExasshCmd(aFailureCmd, aPatchNodeList)
    if not _result:
        return False, "Failed to update sshd_config."

    mSetECRAProperty("ABORT_INFRAPATCH_WORKFLOWS_ON_FAILURE", "DISABLED")

    _result, _status_output = mExecuteInfraPatchCommandWithRetry(aPatchTask, aPatchType, OP_STYLE_ROLLING)
    #if not _result:
    #    return False, "Failed to initiate patch command."

    _status_json = json.loads(_status_output)
    _wf_uuid = _status_json.get("wf_uuid")
    if not _wf_uuid:
        return False, "Workflow UUID not found in status output."

    _result, _ = mExecuteRemoteExasshCmd(aRestoreCmd, aPatchNodeList)
    if not _result:
        return False, "Failed to update sshd_config"

    _ecra_install_folder = os.path.join(
        mGetInfraPatchingTestConfigParam('ecra_install_base'),
        os.getlogin(),
        "ecra_installs",
        mGetInfraPatchingTestConfigParam('ecra_install_directory')
    )
    _user_home = str(Path.home())

    _patch_list_json = json.loads(_status_json["patch_list"])
    _log_dir = next(iter(_patch_list_json.values()))["report"]["data"].get("log_dir", "")
    match = re.search(r'patchmgr_log_([0-9a-fA-F\-]{36})', _log_dir)
    if not match:
        return False, "Could not extract request ID from log_dir."
    _request_id = match.group(1)

    _sql_file = f"{_user_home}/update_db.sql"

    # create a file with db commands to update error_code and error_message in the ecra db
    with open(_sql_file, 'w') as f:
        f.write(
            f"\nUPDATE ecs_requests SET details = REPLACE(REPLACE(details, '\\\"error_code\\\":\\\"{aErrorCode}\\\"', '\\\"error_code\\\":\\\"0x00000000\\\"'), '\\\"error_message\\\":\\\"{aErrorMsg}\\\"', '\\\"error_message\\\":\\\"\\\"') WHERE id = '{_request_id}';"
        )
        f.write("\ncommit;")
        f.write("\nSET LONG 100000\nSET LONGCHUNKSIZE 100000\nSET LINESIZE 32767\nSET PAGESIZE 0;")
        f.write(f"\nSELECT details FROM ecs_requests WHERE id = '{_request_id}';")

    mExecuteDbCmd(aEcraInstallFolder=_ecra_install_folder, aSqlFileToExecute=_sql_file, aOnSqlError="CONTINUE")

    _result, _ecra_request_id, _ssh_equiv_cleanup_needed = mRunPatchMgrCmdInBackGroundWithSameLogDirAsInStatusOutput(
        _status_output, aPatchNodeList, aPatchType, aPatchTask, aLaunchNode
    )
    if not _result:
        return False, "Failed to run patch_mgr in background."

    time.sleep(120)

    mInvokeWorkflowRetry(_wf_uuid)

    ecra_url = mGetInfraPatchingTestConfigParam('ecraurl')
    status_url = f"{ecra_url}/statuses/{_ecra_request_id}"

    time.sleep(120)
    _result, _status_output = mCheckPatchOperationStatus(status_url)
    if not _result:
        return False, "Retry patch operation failed."

    if aLaunchNode is None:
        aLaunchNode = [aPatchNodeList[1]]

    if _ssh_equiv_cleanup_needed:
        _status = RemoveKeyFromHosts([aPatchNodeList[0]], aLaunchNode)

    mSetECRAProperty("ABORT_INFRAPATCH_WORKFLOWS_ON_FAILURE", "ENABLED")

    return True, ""

def mValidatePatchPrereqCheckRetryFailure(aNodeList, aTargetName, aFailureCmd, aRestoreCmd, aErrorCode):
    """
    This test is to simulate patch prereq check retry failure scenario

    The following things are done in this test
    1. Execute patch and make it to fail at patch_mgr side
    2. After certain time limit retry the workflow
    3. Start monitoring status of the above request
    4. Verify if retry failed with expected error message 
    """
    __operation = TASK_PREREQ_CHECK
    __operation_style = OP_STYLE_ROLLING
    __patch_node_list = aNodeList

    _result, _ = mExecuteRemoteExasshCmd(aFailureCmd, __patch_node_list)
    if not _result:
        return _result, "Retry patch operation failed."

    # Patch is going to fail so making the workflow to get into failed state instead of aborted.
    # When workflow is in failed state, workflow retry can be triggered
    mSetECRAProperty("ABORT_INFRAPATCH_WORKFLOWS_ON_FAILURE", "DISABLED")
    _result, _status_output = mExecuteInfraPatchCommandWithRetry(__operation, aTargetName,
                                                                 __operation_style)
    _status_json = json.loads(_status_output)
    mPatchLogInfo("STATUS_JSON : %s" % _status_json)
    _wf_uuid = None
    if "wf_uuid" in _status_json:
        _wf_uuid = _status_json["wf_uuid"]

    _result, _ = mExecuteRemoteExasshCmd(aRestoreCmd, __patch_node_list)
    if not _result:
        return _result, "Could not execute restore command."

    if _status_output:
        __text = json.dumps(_status_json)
        # Regex to extract the UUID from "patchmgr_log_<uuid>"
        _patchmgr_log_file = re.search(r'patchmgr_log_([a-f0-9\-]{36})', __text)

        if _patchmgr_log_file:
            aRequestId = _patchmgr_log_file.group(1)
            mPatchLogInfo("aRequestId : %s" % aRequestId)
        else:
            mPatchLogError("UUID not found.")
        # Step 2: workflow retry
        _workflow_cmd_output = mInvokeWorkflowRetry(_wf_uuid)
        _ecra_url = mGetInfraPatchingTestConfigParam('ecraurl')
        _status_fetching_url = "%s/statuses/%s" % (_ecra_url, aRequestId)
        mPatchLogInfo("Command used to fetch status output is %s" % _status_fetching_url)
        time.sleep(120)
        # Step 3 : start monitoring status output from retry request
        _result, _status_output = mCheckPatchOperationStatus(_status_fetching_url)
        _json_data = json.loads(_status_output)
        _error_code = _json_data.get("errorCode") == aErrorCode
        mPatchLogInfo("++++++++++ -> : %s" %_error_code)
        if not _error_code:
            return False, "Retry error code didn't match."
        # Step 4 : wf should fail with below
        _ecra_error_pattern = rf"Retry ECRA Request ID .* has already failed before ECRA Upgrade with error code {aErrorCode}\. Failing infrapatching on Retry after ECRA upgrade"
        _ecra_error = _json_data.get("ecra_error", "")
        _match_ecra_error = bool(re.fullmatch(_ecra_error_pattern, _ecra_error))
        if not _error_code:
            return False, "ecra_error didn't match."
    else:
        return False, "status call output is empty for the patch request."

    mSetECRAProperty("ABORT_INFRAPATCH_WORKFLOWS_ON_FAILURE", "ENABLED")
    return True, ""


class PluginScriptExecutionValidator:

    def __init__(self, aTargetName, aOperation, aOperationStyle=OP_STYLE_AUTO):
        self.__target_name = aTargetName
        self.__operation = aOperation
        self.__operation_style = aOperationStyle

    def mGetTargetName(self):
        return self.__target_name

    def mGetOperation(self):
        return self.__operation

    def mGetOperationStyle(self):
        return self.__operation_style

    @abc.abstractmethod
    def mValidate(self, aStatusOutput, aExclusiveNodesListToCheckPluginConsoleLogs=[]):
        pass

    def mGetChildRequestId(self, aStatusOutput):
        if aStatusOutput:
            _child_request_id_regex_pattern = r'patch_list\":\"{\\"[0-9a-fA-F]{8}\-[0-9a-fA-F]{4}\-[0-9a-fA-F]{4}\-' \
                                              r'[0-9a-fA-F]{4}\-[0-9a-fA-F]{12}'
            _child_request_id_matches = re.findall(_child_request_id_regex_pattern, aStatusOutput)

            # Find child_request_id from status
            _child_request_id_str = ""
            if len(_child_request_id_matches) > 0:
                _child_request_id_str = _child_request_id_matches[0]
                # Extract only guid
                _child_request_id_str = _child_request_id_str[-36:]
                mPatchLogInfo("child_request_id value from status output is %s " % (_child_request_id_str))
                return _child_request_id_str
            else:
                raise mPatchLogError("child_request_uuid is not found in status output.")
        else:
            raise mPatchLogError("status call output is empty.")


class OneOffPluginScriptExecutionValidator(PluginScriptExecutionValidator):

    def __init__(self, aTarget_name, aOperation, aOperationStyle=OP_STYLE_AUTO):
        super().__init__(aTarget_name, aOperation, aOperationStyle)

    def mValidate(self, aStatusOutput, aExclusiveNodesListToCheckPluginConsoleLogs=[]):
        _ret = True
        _child_request_id_str = self.mGetChildRequestId(aStatusOutput)

        if len(aExclusiveNodesListToCheckPluginConsoleLogs) > 0:
            _nodes = aExclusiveNodesListToCheckPluginConsoleLogs
        else:
            _target_type = self.mGetTargetName()
            if _target_type == PATCH_DOM0:
                _nodes = mGetInfraPatchingTestConfigParam('dom0s')
            elif _target_type == PATCH_DOMU:
                _cluster = mGetInfraPatchingTestConfigParam("cluster")
                _nodes = mGetInfraPatchingTestVms('customerHostname', _cluster)
            elif _target_type == PATCH_CELL:
                _nodes = mGetInfraPatchingTestConfigParam('cells')
            elif _target_type in [ PATCH_IBSWITCH, PATCH_SWITCH ]:
                _nodes = mGetInfraPatchingTestConfigParam('switches')

        # Check for presence of script execution console log files from patchmgr log location appropriatley.
        _patch_logs_path_from_exacloud_requests_dir = "%s/log/patch_logs" % (
            mGetExacloudOEDARequestsPath(_child_request_id_str))

        for _node in _nodes:
            _script_console_log_path = ""
            if self.mGetOperation() == TASK_ONEOFFV2:
                # log is of the format oneoffv2.slcs27adm03.us.oracle.com_syslens.log (syslens is the script alias name)
                # here in the automation alias is of the format  script_dom0_oneff
                _script_console_log_path = "%s/%s.%s_script_%s_oneoff.log" % (
                    _patch_logs_path_from_exacloud_requests_dir, self.mGetOperation().lower(), _node, self.mGetTargetName())
            else:
                _script_console_log_path = "%s/%s.%s.log" % (_patch_logs_path_from_exacloud_requests_dir,
                                                             self.mGetOperation().lower(), _node)
            if not os.path.exists(_script_console_log_path):
                mPatchLogError("%s script console log file path %s does not exist." % (self.mGetOperation(),
                                                                                       _script_console_log_path))
                _ret = False
            else:
                mPatchLogInfo("%s script console log file path %s exists." % (self.mGetOperation(),
                                                                              _script_console_log_path))
        if _ret:
            mPatchLogInfo("Verification of %s script console logs completed successfully." % self.mGetOperation())
        else:
            mPatchLogError("Verification of %s script console logs failed." % self.mGetOperation())
        return _ret


class ExacloudPluginScriptExecutionValidator(PluginScriptExecutionValidator):

    def __init__(self, aTarget_name, aOperation, aOperationStyle=OP_STYLE_AUTO):
        super().__init__(aTarget_name, aOperation, aOperationStyle)

    def mValidate(self, aStatusOutput, aExclusiveNodesListToCheckPluginConsoleLogs=[]):
        """
        This method is used to check presence of plugin console logs in patchmgr log location in exacloud directory.
        Returns :
           True all plugin console logs are present in patchmgr log location
           False otherwise.
        """
        _ret = True
        _child_request_id_str = self.mGetChildRequestId(aStatusOutput)

        # Fetch dom0 and domu nodes where plugin console logs need to checked.
        _domus = []
        _dom0s = []
        _cells = []
        _target_type = self.mGetTargetName()
        # When exclusive list is provided, check plugin console logs only in respective domus and domos only.
        if (len(aExclusiveNodesListToCheckPluginConsoleLogs) > 0):
            mPatchLogInfo("Exclusive Node list to check plugin console logs is not opted.")
            if _target_type in [PATCH_DOM0]:
                _dom0s = aExclusiveNodesListToCheckPluginConsoleLogs
                _vm_map = mGetInfraPatchingTestVms("domuNatHostname", "all")
                for _included_dom0 in aExclusiveNodesListToCheckPluginConsoleLogs:
                    if _included_dom0 in _vm_map:
                        _domus = _domus + _vm_map[_included_dom0]
            elif _target_type in [PATCH_DOMU]:
                _domus = aExclusiveNodesListToCheckPluginConsoleLogs
            elif _target_type in [PATCH_CELL]:
                _cells = aExclusiveNodesListToCheckPluginConsoleLogs

        # Check for presence of plugin console log files from pathmgr log location appropriatley.
        _nodes = []
        _plugin_types = mGetParamValueFromPayLoadJson("PluginTypes")
        _patch_stages = ["Pre", "Post"]

        # In case of dom0+dom0domu plugin, plugin console log files need to be looked in both dom0 and domu nodes.
        if _plugin_types == "dom0+dom0domu":
            _nodes = _dom0s
            _nodes = _dom0s
            mPatchLogInfo("dom0 nodes to check for plugin console logs are %s." % (str(_dom0s)))

            # For non-rolling style, dom0domu plugin is not run so no need to check for plugin console logs
            if self.mGetOperationStyle() != OP_STYLE_NON_ROLLING:
                _nodes = _nodes + _domus
                mPatchLogInfo("domu nodes to check for plugin console logs are %s." % (str(_domus)))
        elif _plugin_types == "domu":
            mPatchLogInfo("domu nodes to check for plugin console logs are %s." % (str(_domus)))
            _nodes = _domus
        elif _plugin_types == "cell":
            mPatchLogInfo("cell nodes to check for plugin console logs are %s." % (str(_cells)))
            _nodes = _cells

        _patch_mgr_logs_path_from_exacloud_requests_dir = "%s/log/patchmgr_logs" % (
            mGetExacloudOEDARequestsPath(_child_request_id_str))

        # In each node, need to check for both pre and post stage log files.
        for _node in _nodes:
            for _stage in _patch_stages:
                _plugin_console_log_path = "%s/plugin_%s_patch_%s_console.out" % (
                    _patch_mgr_logs_path_from_exacloud_requests_dir, _stage.lower(), _node)
                if not os.path.exists(_plugin_console_log_path):
                    mPatchLogError("Plugin console log file path %s does not exist." % (_plugin_console_log_path))
                    _ret = False
                else:
                    mPatchLogInfo("Plugin console log file path %s exists." % (_plugin_console_log_path))

        """
        In each node, need to check for exacloud plugin logs(These plugin scripts are registered with ecra)        
        As part of the test framework, we are registering only Pre stage for exacloud plugins so checking for pre stage only
        Format of the logs is pre_patch_slcs27adm04.us.oracle.com_syslens.log

        """
        if self.mGetOperation() == TASK_PATCH:
            for _node in _dom0s:
                for _stage in _patch_stages:
                    _plugin_console_log_path = "%s/%s_patch_%s_script_%s_exacloud_%s.log" % (
                        _patch_mgr_logs_path_from_exacloud_requests_dir, _stage, _node, _target_type, _stage.lower())
                    if not os.path.exists(_plugin_console_log_path):
                        mPatchLogError("Plugin console log file path %s does not exist." % (_plugin_console_log_path))
                        _ret = False
                    else:
                        mPatchLogInfo("Plugin console log file path %s exists." % _plugin_console_log_path)

        if _ret:
            mPatchLogInfo("Verification of plugin console logs completed successfully.")
        else:
            mPatchLogError("Verification of plugin console logs failed.")
        return _ret


class TimeProfileDataValidator:

    def __init__(self, aTargetName, aOperation, aOperationStyle):
        self.__target_name = aTargetName
        self.__operation = aOperation
        self.__operation_style = aOperationStyle

    def mGetTargetName(self):
        return self.__target_name

    def mGetOperation(self):
        return self.__operation

    def mGetOperationStyle(self):
        return self.__operation_style

    def mValidate(self, aStatusOutput, aExistableNodes=[], aNonExistableNodes=[]):
        """
        This method is used to extract time_profile_data from status output and checks for required nodes in it.
        time_profile_data should contain only nodes from aExistableNodes and should not contain nodes from
        aNonExistableNodes

        Returns :
           True when both aNonExistableNodes and aNonExistableNodes presence appropriately in node_progress_status.
           False otherwise.
        """
        _ret = True
        _disable_time_profile_data_validation = (
                mGetInfraPatchingTestConfigParam('disable_time_profile_data_validation') == 'True')
        if not _disable_time_profile_data_validation:
            _time_profile_data = None
            _status_json = json.loads(aStatusOutput)
            if _status_json and _status_json["patch_list"]:
                _patch_list = _status_json["patch_list"]
                _patch_list_json = json.loads(_patch_list)
                _jobid = None
                if _patch_list_json:
                    # Get the first key of the dictionary
                    _jobid = next(iter(_patch_list_json))
                    if not _jobid:
                        mPatchLogError(
                            "patch_list data from status output does not contain exacloud worker thread details.")
                else:
                    mPatchLogError("status output contains empty patch_list data.")
                    return False
                if _patch_list_json[_jobid] and _patch_list_json[_jobid]["report"] and \
                        _patch_list_json[_jobid]["report"]["data"] \
                        and "time_profile_data" in _patch_list_json[_jobid]["report"]["data"]:
                    _time_profile_data = _patch_list_json[_jobid]["report"]["data"]["time_profile_data"]
            else:
                mPatchLogError("status output does not contain patch_list data.")
                return False

            """
            output of time_profile_data after filtering is of the below format
                {
                  "node_patching_time_stats": [
                    {
                      "node_names": "['slcs27celadm04.us.oracle.com']",
                      "stage": "PATCH_MGR",
                      "sub_stage": "",
                      "start_time": "2022-08-11 09:45:49+0000",
                      "end_time": "2022-08-11 09:48:51+0000",
                      "duration_in_seconds": 182
                    },
                    {
                      "node_names": "['slcs27celadm05.us.oracle.com']",
                      "stage": "PATCH_MGR",
                      "sub_stage": "",
                      "start_time": "2022-08-11 09:45:50+0000",
                      "end_time": "2022-08-11 09:48:51+0000",
                      "duration_in_seconds": 181
                    },
                    {
                      "node_names": "['slcs27celadm06.us.oracle.com']",
                      "stage": "PATCH_MGR",
                      "sub_stage": "",
                      "start_time": "2022-08-11 09:45:50+0000",
                      "end_time": "2022-08-11 09:48:51+0000",
                      "duration_in_seconds": 181
                    }
                  ],
                  "exacloud_start_time": "2022-08-11 02:43:10-0700",
                  "exacloud_end_time": "2022-08-11 02:50:15-0700"
                }
            """

            # time_profile_data does not contain patch_mgr time_stats for non-rolling
            if self.mGetOperationStyle() == OP_STYLE_NON_ROLLING and _time_profile_data and "node_patching_time_stats" in _time_profile_data:
                mPatchLogError("time_profile_data for non-rolling should not present in status output.")
                return False
            # time_profile_data captured currently only for rolling case
            if self.mGetOperationStyle() == OP_STYLE_ROLLING:
                """
                Validate for all the attributes(exacloud_start_time,exacloud_end_time and node_patching_time_stats) to be present in time_profile_data
                """
                _expected_time_profile_data_keys = set()
                if _time_profile_data:
                    mPatchLogInfo("time_profile_data from status output is %s" % _time_profile_data)
                    _expected_time_profile_data_keys.add("exacloud_start_time")
                    _expected_time_profile_data_keys.add("exacloud_end_time")

                    # When all the nodes are up-to-date, no patch_mgr is run and so node_patching_time_stats key does not exist in time_profile_data
                    if len(aExistableNodes) > 0:
                        _expected_time_profile_data_keys.add("node_patching_time_stats")

                    if len(_expected_time_profile_data_keys) != len(_time_profile_data.keys()):
                        mPatchLogError("time_profile_data from status output does not contain all the attributes.")
                        return False

                    for _key in _time_profile_data.keys():
                        if _key not in _expected_time_profile_data_keys:
                            mPatchLogError(
                                "time_profile_data from status output contains different attribute- %s." % _key)
                            return False
                else:
                    mPatchLogError("time_profile_data from status output is empty.")
                    return False

                """
                Validate for attributes in patch_mgr_time_stats and validate for nodes in it.
                time_profile_data keys would contain all the nodes where patch operation is run.    
                """
                if len(aExistableNodes) == 0:
                    if "node_patching_time_stats" in _time_profile_data:
                        mPatchLogError("time_profile_data should not contain node_patching_time_stats key "
                                       "when all the nodes are up-to-date.")
                        return False
                else:
                    _expected_time_profile_data_keys.clear()
                    _expected_time_profile_data_keys.add("node_names")
                    _expected_time_profile_data_keys.add("start_time")
                    _expected_time_profile_data_keys.add("end_time")
                    _expected_time_profile_data_keys.add("duration_in_seconds")
                    _expected_time_profile_data_keys.add("stage")
                    _expected_time_profile_data_keys.add("sub_stage")
                    _patch_mgr_time_stats_array = _time_profile_data["node_patching_time_stats"]
                    _nodes_in_patch_mgr_time_stats = set()
                    # loop through all the entries of patch_mgr stats to get node name details and validate for the attributes
                    for _node_wise_patch_mgr_time_stats in _patch_mgr_time_stats_array:
                        if len(_expected_time_profile_data_keys) != len(_node_wise_patch_mgr_time_stats.keys()):
                            mPatchLogError(
                                "patch_mgr_time_stats from time_profile_data does not contain all the attributes.")
                            return False
                        for _key in _node_wise_patch_mgr_time_stats.keys():
                            if _key not in _expected_time_profile_data_keys:
                                mPatchLogError(
                                    "patch_mgr_time_stats from time_profile_data contains different attribute- %s." % _key)
                                return False
                        _nodes_in_patch_mgr_time_stats.add(_node_wise_patch_mgr_time_stats["node_names"][2:-2])
                    mPatchLogInfo(
                        "Nodes those should be present in time_profile_data are - %s.\n" % (str(aExistableNodes)))
                    mPatchLogInfo(
                        "Nodes those should not be present in time_profile_data are - %s.\n" % (
                            str(aNonExistableNodes)))
                    _wrong_nodes = []
                    for _node in aExistableNodes:
                        if str(_nodes_in_patch_mgr_time_stats).find(_node) < 0:
                            _wrong_nodes.append(_node)

                    if len(_wrong_nodes) > 0:
                        mPatchLogError(
                            "Nodes %s do not exist in time_profile_data that should have existed." % (
                                str(_wrong_nodes)))
                        return False

                    _wrong_nodes.clear()

                    for _node in aNonExistableNodes:
                        if _node in _nodes_in_patch_mgr_time_stats:
                            _wrong_nodes.append(_node)

                    if len(_wrong_nodes) > 0:
                        mPatchLogError(
                            "Nodes %s exist in time_profile_data that should not have existed." % (str(_wrong_nodes)))
                        return False

                    mPatchLogInfo("time_profile_data has correct set of attributes.")
                return _ret
            else:
                # For non-rolling no need to check for time_profile_data
                return _ret
        else:
            return True


# class to hold infrapatch operation metadata
class InfrapatchOperationMetadata(object):

    def __init__(self, aChildRequestUUID, aTargetType, aOperation, aPatchType,
                 aOperationStyle):
        self.__child_request_uuid = aChildRequestUUID
        self.__target_type = aTargetType
        self.__operation = aOperation
        self.__patch_type = aPatchType
        self.__operation_style = aOperationStyle

    def mGetChildRequestUUID(self):
        return self.__child_request_uuid

    def mGetTargetType(self):
        return self.__target_type

    def mGetOperation(self):
        return self.__operation

    def mGetPatchType(self):
        return self.__patch_type

    def mGetOperationStyle(self):
        return self.__operation_style


class CurlRequest(object):
    """
    Wrapper to execute curl commands for all HTTP CRUD operations
    """

    def __init__(self, aBaseUrl=None, aUser=None, aPassword=None):
        self.__base_url = aBaseUrl
        self.__usr = aUser
        self.__passwd = aPassword
        mPatchLogInfo("CurlRequest")

    def mExecute(self, aEndpointURI, aMethod, aData={}, additionalHeaders=None):
        """
        :param aEndpointURI: endpoint URI on which curl command would get executed eg: infrapatchpluginscripts/registration,  workflows/retry etc
        :param aMethod: HTTP methods liek GET,POST,PUT and DELETE
        :param aData: HTTP Request data that need to be sent
        :param additionalHeaders: Addition headers that need to be appended to curl command. By default, Content-Type and Accept are passed
        :return: If it received 200 HTTP Response code, it returns 0 otherwise 1
        """
        _ret = 1
        _username = None
        _password = None

        # if no baseurl is provided use ecra url from test folder
        if not self.__base_url:
            self.__base_url = mGetInfraPatchingTestConfigParam('ecraurl')
            self.__usr = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrausername')).decode('utf-8')
            self.__passwd = base64.b64decode(mGetInfraPatchingTestConfigParam('ecrapassword')).decode('utf-8')

        _curl_cmd = "curl --silent  -u %s:%s -i -k -X  %s -H 'Content-Type: application/json' -H 'Accept: " \
                    "application/json' %s -d '%s'  '%s/%s'" % (
                        self.__usr, self.__passwd, aMethod,
                        ("-H '%s' " % additionalHeaders if additionalHeaders is not None else ""), str(aData),
                        self.__base_url, aEndpointURI)
        mPatchLogInfo("CurlRequest cmd is : %s " % _curl_cmd)
        _cmd_out = mExecuteCmd(aCmd=_curl_cmd)
        mPatchLogInfo("CurlRequest cmd output is : %s " % _cmd_out)

        if _cmd_out.startswith(HTTP_200_RESPONSE_CODE):
            _ret = 0

        mPatchLogInfo("CurlRequest mExecute method return value is : %s " % str(_ret))
        return _ret, _cmd_out

class LaunchNodeRegistrationHandler(object):
    """
    Wrapper to run CRUD operations wrt infrapatch launch node
    """

    def __init__(self):
        mPatchLogInfo("InfrapatchLaunchNode")
        self.__curl_executor = CurlRequest()

    def mRegisterInfrapatchLaunchNode(self, aJson):
        mPatchLogInfo("mRegisterInfrapatchLaunchNode : data for the  operation is %s " % str(aJson))
        _ret, _output = self.__curl_executor.mExecute("infrapatch/launchNode/register", "POST", aJson)
        if _ret == 0:
            _output = _output.strip().splitlines()[-1]
            mPatchLogInfo(
                "mRegisterInfrapatchLaunchNode : launch node registered successfully: %s" % (
                    _output.strip().splitlines()[-1]))
        return _ret, _output

    def mDeregisterInfrapatchLaunchNode(self, infraType, infraName=None):
        mPatchLogInfo("mDeregisterInfrapatchLaunchNode : infraType is %s " % str(infraType))
        tmp_url = 'infrapatch/launchNode/%s' % (infraType)
        if infraName:
            url = '%s?infraName=%s' % (tmp_url,infraName)
        else:
            url = tmp_url

        _ret, _output = self.__curl_executor.mExecute(url, "DELETE")
        if _ret == 0:
            mPatchLogInfo(
                "mDeregisterInfrapatchLaunchNode was successfull for: %s" % (infraType))
        return _ret, _output



class InfrapatchPluginMetadataHandler(object):
    """
    Wrapper to run CRUD operations wrt infrapatch plugin metadata
    """

    def __init__(self):
        mPatchLogInfo("InfrapatchPluginMetadataValidator")
        self.__curl_executor = CurlRequest()

    def mRegisterInfrapatchPluginMetadataScripts(self, aPluginTarget, aPluginType, aPhase=""):
        """
        This method does the following
        1. Creates a script inside exacloud/exadataPrePostPlugins based on plugin type passed as input to the method.
        2. Register the plugin metadata script with ECRA
        3. returns the registered plugin metadata
        :param aPluginTarget: PluginTarget, it can be dom0,domu,cell, dom0domu
        :param aPluginType: PluginType, it can be exacloud, oneoff and dbnu
        :param aPhase: Phase value in case of exacloud plugin
        :return: Upon success return 0 along with registered plugin metadata otherwise return 1 along with script_alias
        """

        # script name is of the format script_dom0_oneff.sh and for exacloud plugins phase gets appened at the end
        # script_dom0_exacloud_pre.sh
        _file_name = "script_%s_%s%s" % (aPluginTarget, aPluginType, ("_%s" % aPhase.lower()) if aPhase else "")

        _plugin_type_directory_mapping = {"oneoff": "oneoff_patch", "exacloud": "exacloud_plugins",
                                          "dbnu": "dbnu_plugins"}

        _script_file_path = "%s/exadataPrePostPlugins/%s" \
                            "" % (
                                mGetExacloudInstallPath(), _plugin_type_directory_mapping.get(aPluginType))

        _file_name_sh = "%s.sh" % _file_name
        _script_file_abs_path = f"{_script_file_path}/{_file_name_sh}"
        if os.path.exists(_script_file_abs_path):
            os.remove(_script_file_abs_path)

        # Prepare the content of the script file, here script_dom0_oneff.sh from above example will have the below
        # content and script exit code 1
        _cmd = "echo 'echo \"Infrapatching automation script for the verification of plugin target %s and plugintype " \
               "%s \" && exit 1' > %s/%s " % (
                   aPluginTarget, aPluginType, _script_file_path,_file_name_sh)
        _cmd_out = mExecuteCmd(aCmd=_cmd)

        # Create a tar.gz file
        # For oneoff plugin: <exacloud_install_path>/exadataPrePostPlugins/oneoff_patch/script_dom0_oneoff.tar.gz
        # For exacloud plugin: <exacloud_install_path>/exadataPrePostPlugins/exacloud_plugins/script_dom0_exacloud_pre.tar.gz

        _tar_file = "%s/%s.tar.gz" % (_script_file_path, _file_name)
        _cmd = "tar -czf %s -C %s %s" % (_tar_file, _script_file_path, _file_name_sh)
        mPatchLogInfo("InfrapatchPluginMetadataValidator : Executing the command %s " % _cmd)
        _output, _stat = mExecuteLocal(_cmd)

        # Calculate SHA256 sum
        _cmd = "sha256sum %s | awk '{print $1}'" % _tar_file
        mPatchLogInfo("InfrapatchPluginMetadataValidator : Executing the command %s " % _cmd)
        _sha256_sum, _stat = mExecuteLocal(_cmd)
        _sha256_sum = _sha256_sum.strip()

        # Register the plugin metadata


        _data = '{"ScriptName": "%s", "ScriptAlias": "%s", "ScriptBundleHash": "%s", "ChangeRequestID": "CRID-4567", "Description": ' \
                '"sample", "PluginType": "%s", "PluginTarget": "%s", "Phase": "%s"}' % (_file_name_sh,
                                                                                        _file_name,_sha256_sum, aPluginType,
                                                                                        aPluginTarget, aPhase)
        mPatchLogInfo("mRegisterInfrapatchPluginMetadataScripts : data for the update operation is %s " % str(_data))
        _ret, _output = self.__curl_executor.mExecute("infrapatchpluginscripts/registration", "POST", _data)
        if _ret == 0:
            _output = _output.strip().splitlines()[-1]
            mPatchLogInfo(
                "mRegisterInfrapatchPluginMetadataScripts : plugin metadata %s registered successfully " % (
                    _output.strip().splitlines()[-1]))

        return _ret, _output

    def mRegisterInfrapatchPluginMetadataScriptsWithJsonInput(self, aData):
        """
        This method to register the metadata passed as argument
        :param aData: json string of plugin metadata to register
        :return:  Upon success returns 0 along with plugin script metadata otherwise returns 1
        """
        mPatchLogInfo(
            "mRegisterInfrapatchPluginMetadataScriptsWithJsonInput : data for the update operation is %s " % str(aData))
        _ret, _output = self.__curl_executor.mExecute("infrapatchpluginscripts/registration", "POST", aData)
        if _ret == 0:
            _output = _output.strip().splitlines()[-1]
            mPatchLogInfo(
                "mRegisterInfrapatchPluginMetadataScriptsWithJsonInput : plugin metadata %s registered successfully " % (
                    _output.strip().splitlines()[-1]))
        else:
            mPatchLogError(
                "mRegisterInfrapatchPluginMetadataScriptsWithJsonInput : plugin metadata registration failed")

        return _ret, _output.strip().splitlines()[-1]

    def mGetRegisteredInfrapatchPlugineMetadataScripts(self, aScriptAlias="", aPluginTarget="", aPluginType=""):
        """
        :param aScriptAlias: script_alias of the script
        :param aPluginTarget: PluginTarget, it can be dom0,domu,cell, dom0domu
        :param aPluginType: PluginType, it can be exacloud, oneoff and dbnu
        :return: Upon success returns 0 along with plugin script metadata otherwise returns 1
        """
        _data = '{ "ScriptAlias": "%s",  "PluginType": "%s", "PluginTarget": "%s"}' % (
            aScriptAlias, aPluginType, aPluginTarget)

        mPatchLogInfo("mGetRegisteredInfrapatchPlugineMetadataScripts : data for the get operation is %s " % str(_data))
        _ret, _output = self.__curl_executor.mExecute("infrapatchpluginscripts/registration", "GET", _data)

        if _ret == 0:
            _output = _output.strip().splitlines()[-1]

        return _ret, _output

    def mUpdateRegisteredInfrapatchPlugineMetadataScripts(self, aScriptAlias, aPluginTarget, aPluginType,
                                                          aIsEnabled="Yes"):
        """
        :param aScriptAlias: script_alias of the script
        :param aPluginTarget: PluginTarget, it can be dom0,domu,cell, dom0domu
        :param aPluginType: PluginType, it can be exacloud, oneoff and dbnu
        :param aIsEnabled:  Whether to enable or disable the execution of the script
        :return: Upon success returns 0 along with plugin script metadata otherwise returns 1
        """
        curl_req = CurlRequest()
        _data = '{"ScriptAlias": "%s", "PluginType": "%s", "PluginTarget": "%s", "IsEnabled": "%s"}' % (
            aScriptAlias, aPluginTarget, aPluginType, aIsEnabled)
        mPatchLogInfo(
            "mUpdateRegisteredInfrapatchPlugineMetadataScripts : data for the update operation is %s " % str(_data))
        _ret, _output = self.__curl_executor.mExecute("infrapatchpluginscripts/registration", "PUT", _data)
        if _ret == 0:
            _output = _output.strip().splitlines()[-1]

        return _ret, _output

    def mDeleteRegisteredInfrapatchPlugineMetadataScripts(self, aScriptAlias, aPluginTarget, aPluginType):
        """
        :param aScriptAlias: script_alias of the script
        :param aPluginTarget: PluginTarget, it can be dom0,domu,cell, dom0domu
        :param aPluginType: PluginType, it can be exacloud, oneoff and dbnu
        :return: Upon success returns 0 along success response otherwise returns 1 along with failure response
        """
        _plugin_type_directory_mapping = {"oneoff": "oneoff_patch", "exacloud": "exacloud_plugins",
                                          "dbnu": "dbnu_plugins"}
        _data = '{"PluginType": "%s", "PluginTarget": "%s"}' % (aPluginType, aPluginTarget)
        _ret, _output = self.__curl_executor.mExecute("infrapatchpluginscripts/registration/%s" % aScriptAlias,
                                                      "DELETE", _data)

        _output = _output.strip().splitlines()[-1]
        mPatchLogInfo("mDeleteRegisteredInfrapatchPlugineMetadataScripts : command response is %s " % _output)
        if _ret == 0:
            _output = _output.strip().splitlines()[-1]

            _cmd = "rm -rf %s/exadataPrePostPlugins/%s/%s" % (
                mGetExacloudInstallPath(), _plugin_type_directory_mapping.get(aPluginType), "script*")
            mPatchLogInfo("mDeleteRegisteredInfrapatchPlugineMetadataScripts : Executing the command %s " % _cmd)
            _, _stat = mExecuteLocal(_cmd)

        return _ret, _output


