# pylint: disable=invalid-name, line-too-long, C0103, C0301
"""
$Header: 

 Copyright (c) 2014, 2025, Oracle and/or its affiliates.

NAME:
    OVM - Basic functionality

FUNCTION:
    Provide basic/core API for managing OVM Cluster (Cluster Lifecycle,...)
    History comments from 2015-2024 are moved to 
    exacloud/documents/history_comments_exacloud/exabox/ovm/clucontrol.txt
    (Move last year's comments to the same file after year completion to keep this
    file lean)

NOTE:
    Please if you are adding change to payloads/apis then update this confluence page:
    https://confluence.oraclecorp.com/confluence/display/EDCS/API+Payloads+from+ECRA+to+ExaCloud

History:

       MODIFIED (MM/DD/YY)
       prsshukl  12/01/25 - 38711578 - [EXACC EXACLOUD]: REMOVE
                            ENABLE_CA_SIGNED_CERTS FLAG FROM EXABOX.CONF
       ririgoye  11/26/25 - Bug 38636333 - EXACLOUD PYTHON:ADD INSTANTCLIENT TO
                            LD_LIBRARY_PATH
       jesandov  11/21/25 - 38677852: Remove log message on duplicated nft rules
       prsshukl  11/20/25 - Enh 38394526 - EXACC: SUPPORT PER DOMU DOMUCLIENT
                            CERTIFICATE DURING CREATE SERVICE, ADD NODE
       ririgoye  11/20/25 - Bug 38667586 - EXACS: MAIN: PYTHON3.11: SUPRASS ALL
                            PYTHON WARNINGS
       prsshukl  11/20/25 - Bug 38675257 - EXADBXS PROVISIONING FAILING IN
                            FETCHUPDATEDXMLFROMEXACLOUD
       joysjose  11/18/25 - Bug 38665213 - EXACS: NEW NAMING FOR 23.26.0.0.0
                            CAUSING FAILURE IN POSTVMINSTALL STEP WHILE PARSING
                            DATE
       prsshukl  11/16/25 - Bug 36981808 - UT for ADBS PHASE 2 -> ELASTIC
       scoral    11/14/25 - Bug 38635624 - Use SCANPORT for IPv6 SCAN if available.
       remamid   11/12/25 - Add an option to copy different dbaasapi payload to
                            each domu bug 38581933
       mpedapro  11/12/25 - Enh::38235082 Xml patching changes for sriov
       abysebas  11/10/25 - ER 38299823 - PHASE 1: CPU CORE POWER SAVING -
                            EXACLOUD CHANGES
       rajsag    11/10/25 - bug 38631571 - exacc gen-2 - exascale testing:
                            config exascle is failing where asm cluster does
                            not have active db
       joysjose  11/04/25 - Bug 38599605 modify GI bits naming for n-3
       jesandov  11/03/25 - 38589333: Add fetch of imagebase host from payload
       jesandov  10/30/25 - 38554948: Add force shutdown and force bounce
       scoral    10/28/25 - Enh 38452359: Support separate "admin" network
                            section in payload.
       jfsaldan  10/28/25 - Bug 38550997 - THE SHRINK BACK TO ORIGINAL SIZE
                            STEP WILL FAIL WHILE NO MORE THAN 15% FREESPACE |
                            EXACLOUD TO SUPPORT NEW FLAG IN ASM RESHAPE TO
                            CALCULATE NEW TARGET SIZE IF ORIGINAL SIZE CANNOT
                            BE SATISFIED W/CURRENT USED SPACE
       jfsaldan  10/20/25 - Bug 38518160 - EXADB-XS Y25W41/41 | EXACLOUD CREATE
                            USERS STEP TAKES 1.5 MINUTES TO FINISH MAKING ECRA
                            TO LAST 2 MINUTES | EXPLORE OPTIONS TO REDUCE IT TO
                            BE 1 MINUTE AT MOS
       nelango   10/15/25 - Bug 38102710: Handle empty bond slave output
       abysebas  10/07/25 - Enh 38299822 - PHASE 1: FREE NODE POWER SAVING -
                            EXACLOUD CHANGES
       jesandov  10/07/25 - 38479752: Add instance principal to exagip
       prsshukl  10/06/25 - Bug 38453754 - Force RTG image copy if all the
                            dom0s are rtg compatible
       scoral    09/26/25 - Bug 38474589: Change aForce default value to False
                            in mReadBaseDBdom0domU.
       siyarlag  09/25/25 - 38471354: stage.sh not to emit error messages
       scoral    09/22/25 - Bug 38446950: Make sure mSetupNatNfTablesOnDom0v2
                            add the rules in the correct order.
       aararora  09/22/25 - Bug 38434589: Increase the timeout for vm bounce
                            operation and make it configurable
       scoral    09/19/25 - Bug 38448719: Implement aForce flag for mReadBaseDBdom0domU.
       rajsag    09/17/25 - enh 38389132 - exacloud: autoencryption support for
                            exascale configuration
       rajsag    09/15/25 - Enh 38336329 - exacs: during vm reshape, make sure
                            to wait only for those dbs which are auto restart
                            enabled
       remamid   09/15/25 - Remove griddisk check from memreshape bug 38317248
       bhpati    09/15/25 - Bug 38247504 - Adding diagnostics to capture
                            database status during updatecloudvmclustermemory
                            operation
       bhpati    09/11/25 - Bug 38276080 - delete-service is not removing vm's
                            from the host if hypervisor service is down
       aararora  09/05/25 - Bug 38391988: Disable tcps port config for atp
                            environment
       pbellary  09/05/25 - 38393496 - EXACLOUD SHOULD UPDATE NETWORK INTERFACES IN XML BY ENABLING SKIPPASSPROPERTY 
       jfsaldan  09/02/25 - Bug 38244250 - EXACS: PROVISIONING FAILED WITH
                            EXACLOUD ERROR CODE: 1859 EXACLOUD : CELL
                            CONSISTENCY/SEMANTIC CHECK FAILED. | CELL
                            CONSISTENCY CHECK SHOULD NOT RAISE AN EXCEPTION IN
                            CREATE SERVICE
       rajsag    08/29/25 - 38360364 EXASCALE: EXACLOUD FAILURE IN MGETRACKSIZE
                            DURING XSCONFIG FLOW ON ECS MAIN LABELS 38363129
                            OCIEXACC: EXASCALE: EXACLOUD SHOULD CHECK TO SEE IF
                            AT LEAST 1 EXASCALECLUSTER IS DEFINED IN ES.XML TO
                            DETERMINE IF XML IS EXASCALE OR NOT
       avimonda  09/04/24 - Bug 38179586 - OCI: VMLOCALSTORAGE OPERATION FAILED
                            DUE TO RAC ONE DATABASE
       shapatna  08/28/25 - Bug 38149723 - EXACC GEN2: VM CLUSTER CREATION
       jfsaldan  08/26/25 - Enh 37999800 - EXACLOUD: EXASCALE CONFIG FLOW TO
       bhpati    08/21/25  - Bug 38240277 - memory reshape precheck failing on retry
                            when one node is already reshaped
       ajayasin  08/19/25 - 38288530 : dead code removal
       nelango   08/11/25 - Bug 38257756: Lists failed DBs after reboot,
                            without Bug 38019309 fix return value issue
       gparada   08/11/25 - 38253988 Dynamic Storage for data reco sparse in CS flow
       jreyesm   08/10/25 - Bug 38296814. Increase total clusters to 190
       avimonda  08/07/25 - Bug 38184216 - OCI: EXACS | SCALE MEMORY STRUCK AT
                            AWT_TMP_SSH_KEY_ASYN STEP
       akkar     08/07/25 - Bug 38289656 : N-3 Addnode xml patching fix for 23c
       rajsag    08/06/25 - Enh 38208138 exascale phase 2: fetch stre0 and
                            stre1 ips from exacloud for add node operation
       rajsag    08/06/25 - bug 38277430 - exacc gen2 : memoy reshape :
                            exacloud calling memory reshape precheck during
                            retry, causing failure
       bhpati    08/04/25 - Bug 38204376 - Addvmcloudvmcluster Failed Missing
                            Nftables
       akkar     08/03/25 - Bug 38257130: Fix add node flow for multigi bundle
       scoral    07/31/25 - Enh 38190209: Support EDV resize during mHandlerAddVmExtraSize.
       aararora  07/30/25 - ER 38132942: Single stack support for ipv6
       avimonda  07/22/25 - Bug 38226294 - OCI: UNITED AIR: ESCALATION:
                            SOFTRESETEXACCVMNODE WORKFLOW COMPLETED IN 3
                            MINUTES WITHOUT ACTUALLY STOPPING OR STARTING THE
                            VM
       nelango   07/21/25 - Bug 38215648: backout txn of 38019309
       bhpati    07/18/25 - Bug 38133410 - Memory reshape workflow fails to set
                            hugepages
       hcheon    07/17/25 - 37805224 Added VM force shutdown command
       akkar     30/06/25 - Bug 38116134: Dont add private key to response during tmp key operation
       prsshukl  07/15/25 - Bug 38187973 - OCI: EXACC GEN2 - in the domU, create the group
                            before adding the user to the group
       luimendo  07/14/25 - 35779785 - Add spine switch endpoint
       scoral    07/09/25 - Bug 38171717: Patch guestLocalDiskSize XML field with /u01 size from Payload.
       nelango   07/08/25 - Bug 38019309: Enhance VM Memory Update to report
                            databases not started post-reboot
       akkar     18/06/25 - Bug 38078405: Remove raising unnecessary expection for 100gb suppport
       dekuckre  07/01/25 - 37842347: Use query params for baseDB info
       dekuckre  06/30/25 - 38122510: support memory scale for basedb.
       prsshukl  06/25/25 - Bug 38114303 - EXADB-XS- VM MOVEPREPARE FAILED IN
                            R1, LABEL ECS_25.2.1.0.0_LINUX.X64_250623.1401
       vikasras  06/24/25 - Bug 37712234 - EXACS: EXACLOUD PROVISIONING 
                            FAILURE OCDE STEP: NID CONFIGURATION ERROR DUE
                            TO INSUFFICIENT /VAR SPACE
       aararora  06/20/25 - Bug 38030822: Move history comments upto 2024 to
                            exacloud/documents/history_comments_exacloud/exabox/ovm/clucontrol.txt
       prsshukl  06/20/25 - Bug 38099255 - EXADB-XS: 23AI VM MOVE PREPARE
       jfsaldan  06/19/25 - Bug 37945713 - ADBD: OBSERVING WRONG PERMISSIONS IN
       jfsaldan  06/19/25 - Bug 37945713 - ADBD: OBSERVING WRONG PERMISSIONS IN
       bhpati    06/18/25 - Bug 37935269 - New error code for VM serial console
                            workflows for dom0 connection failures
       prsshukl  06/17/25 - Enh 37747083 - EXADB-XS -> New endpoint to validate volumes 
                            attached to the vm and verify with the volumes present in the xml
       abflores  06/09/25 - Bug 37508725 - IMPROVE PORT SCAN
       ajayasin  06/05/25 - 37982865 : clucontrol refactor : move handler
                            functions
       rajsag    06/04/25 - Enhancement Request 38022921 support additional
                            response fields in exacloud status response for
                            reshape steps
       aararora  06/04/25 - Bug 37999466: Handle down interfaces for DR Network
       bhpati    06/03/25 - Bug 37906334 - Add option for force shutdown of
                            domU in parallel
       gparada   06/02/25 - 37963204 Fix for System Image in hybrid infra
       dekuckre  05/05/25 - 38038870: Skip memory reshape precheck for ExaDBXS and BaseDB
       scoral    05/29/25 - Bug 37989871 - Always update bondmonitor RPM for MVM migration.
       prsshukl  05/26/25 - Bug 37984118 - BaseDB CS changes
       rajsag    05/20/25 - ..
       pbellary  05/15/25 - Enh 37698277 - EXASCALE - CREATE SERVICE FLOW TO SUPPORT VM STORAGE ON EDV OF IMAGE VAULT 
       pbellary  05/15/25 - Bug 37951223 - EXASCALE: MISMATCH /U01 SIZE IN VM - HIGHER VALUE IN VM CREATION (150GB)
       naps      05/15/25 - Bug 37942960 - clusterless patching fix for 
       dekuckre  05/14/25 - 37851186: Add adbs_insert_key endpoint
       ajayasin  05/13/25 - 37673251 log optimization
       naps      05/13/25 - Bug 37811674 - Perform cavium checks in parallel.
       prsshukl  05/12/25 - Bug 37916548 - EXADB-XS-19C: THE COMMAND OF EXAUNIT
                            INFO FAILED TO GET INFORMATION FROM ECRA VM
       araghave  05/07/25 - Enh 37892080 - TO IMPLEMENT NEWER PATCHSWITCHTYPE
                            CHANGES APPLICABLE TO ALL SWITCH TARGET TYPES AND
                            PATCH COMBINATIONS
       ririgoye  05/05/25 - Bug 37802755 - NFT DUPLICATE RULE DELETION FAILED
       avimonda  05/05/25 - Bug 37878113 - DOMU OS PRECHECK FAILING WITH SSH
                            CONNECTIVITY ISSUE
       rajsag    05/02/25 - 37744919 - memory reshape wf fails to set hugepages
                            on domu
       abflores  04/30/25 - Bug 37862473: EXACLOUD SECRET CLEANER ISSUE |
                            CLUCONTROL CONTAINS AUTHORIZATION TOKENS
       jfsaldan  04/28/25 - Bug 37698419 - EXACLOUD EXACOMPUTE | LOGIC IN
                            MGETMAJORITYHOSTVERSION() RUNS SEQUENTIALLY | TO
                            ADD PARALLELISM TO IMPROVE TIME
       akkar     04/25/25 - Bug 37833446: Check GI support in OEDA
       aararora  04/22/25 - Bug 37813165: For x8 and below, error information
                            is not propagated for the network api.
       akkar     04/17/25   Bug 37840677  Make MultiGi logic flag independent
       naps      04/17/25 - Bug 37487009 - Run srvctl config in parallel.
       prsshukl  04/17/25 - Bug 37835498 - EXADB-XS 19C: PROVISIONING FAILED IN 
                            DEV2 WITH ERROR "OEDA TRANSLATION TABLE NOT AVAILABLE"
       gojoseph  04/16/25 - Bug 37794590 Delete deprecated iptables chain
       pbellary  04/16/25 - Bug 37778364: CONFIGURE EXASCALE IS FAILING IN X11 ENV FOR EXTREME FLASH STORAGE TYPES
       abflores  04/14/25 - Bug 37473868: Fix marker files logging
       aararora  04/09/25 - Bug 37804078: Reset network information before
                            checking for a DOM0 network info
       akkar     04/06/25 - 37641178: 100gbs client network support
       jfsaldan  04/03/25 - Bug 37783368 - EXACC FS ENCRYPTION | DURING ADD
                            NODE EXACLOUD IS NOT CREATING A NEW ENTRY IN WALLET
                            FOR NEW GUEST
       remamid   04/01/25 - Remove host keys based on IP address in
                            mConfigurePasswordLessDomU bug 37693300
       prsshukl  03/27/25 - Enh 37740750 - EXADB-XS 19C :CS AND ADD COMPUTE
                            ENDPOINT UPDATE-> ATTACH DBVOLUME VOLUMES TO GUEST
                            VMS
       dekuckre  03/27/25 - 37740767: Add xs_vol_attach, xs_vol_detach, xs_vol_resize
       jfsaldan  03/24/25 - Bug 37578751 - EXACLOUD VM_CMD BOUNCE | SOFTRESET
                            WONT WORK IF DOMU IS HUNG STATE/NOT PINGABLE BUT
                            RUNNING IN VIRSH/LIBVIRT LAYER
       akkar     03/20/25 - 37727813: MAA transaction for multiprocessing mupdatevmetrics
       pbellary  03/17/25 - Bug 37713924 - EXASCALE: ADD CELL OPERATION DELETING CLUSTERS DIRECTORY 
       naps      03/12/25 - Bug 37486891 - dom0 access inside
                            mCheckDom0Resources need to made in parallel.
       jfsaldan  03/10/25 - Bug 37687032 - EXADB-XS EXACLOUD | DURING CREATE
                            EXACOMPUTE CLUSTER EXACLOUD SHOULD NOT CREATE OEDA
                            WORKDIR GRID-KLONE SYMLINKS
       aararora  03/10/25 - Bug 37672091: Set global properties in
                            es.properties oeda file
       dekuckre  03/10/25 - 37492060: update mConfigureSyslogIlomHost
       bhpati    03/06/25 - Bug 37653016 - MEMORY SCALE UP STUCK
       aararora  03/04/25 - Bug 37651686: Drop PMEM cache and logs before
                            secureerase
       jfsaldan  02/28/25 - Bug 37651494 - EXACLOUD ADD NODE FAILS POSTGINID ||
                            OCDE INIT RUNS AFTER SRVCTL SETUP DUE TO 37459561
       aararora  02/25/25 - Bug 37513962: Raise exception if there is an issue
                            during storage resize
       abflores  02/24/25 - Bug 37487904: Fix typo in clucontrol
       pbellary  02/20/25 - Bug 37612971 - EXACC: UNABLE TO COPY CONFIGURATION FILES TO CLUSTERSJSON DIRECTORY 
       naps      02/18/25 - Bug 37556553 - skip node access for large cluster
                            provisioning in base config logic.
       jfsaldan  02/12/25 - Bug 37570873 - EXADB-D|XS -- EXACLOUD |
                            PROVISIONING | REVIEW AND ORGANIZE PREVM_CHECKS AND
                            PREVM_SETUP STEPS
       gparada   02/11/25 37573276 Fix mExecuteCmdParallel output
       gparada   02/10/25 Bug 37569998 Fix cellcli output in mUpdateCelldiskSize
       nelango   02/12/24 - Bug 37266604: Handle cell services down in Validate
                            GDs
       scoral    02/06/25 - Bug 37559593: Made mRemoveRootAccess more resilient.
       naps      02/05/25 - Bug 37492116 - Perform u02 creation and shm
                            modification in parallel.
       rajsag    02/05/25 - Enh 37487814 - exacloud | postgiinstall oci
                            hardening takes 4 minutes in large cluster, but 11
                            seconds in small config | improve large cluster
                            provisioning
       joysjose  02/05/25 - 37353081: Ignore M2_SYS_0 or M2_SYS_1 errors in
                            provisioning and elastic flows
       aararora  02/03/25 - Bug 37495011: Exclude lines beginning with comment
       jfsaldan  01/29/25 - Bug 37459561 - REVIEW ECRA STEP OCDE NID
       ririgoye  01/28/25 - Enh 35314599 - EXACLOUD: CREATE AN API IN ECRA FOR
                            MARS TEAM TO PUSH THE VMCORE TO MOS
       nelango   02/12/24 - Bug 37266604: Handle cell services down in Validate
                            GDs
       aararora  01/28/25 - Bug 37526644: Skip ntp/dns patching for exacc
       piyushsi  01/27/25 - BUG 37379443 Fleet Injection API
       akkar     01/27/25 - Bug 37270396: Exacloud key rotation include list
       gparada   01/27/25 - Bug 37450961 Fallback to IMG file if RGT not present
       aararora  01/27/25 - Bug 37504796: Ignore DOCKER-USER rule when deleting
                            iptables existing rules
       rajsag    01/23/25 - Enh 37474509 - exacloud | perform estp_createuser
                            opc creation and ssh key sync in parallel | improve
                            large cluster provisioning time
       pbellary  01/23/25 - Bug 37506231 - EXACALE:CLUSTER PROVISIONING FAILING DUE TO EDV SERVICES STARTUP FAILURE
       ririgoye  01/23/25 - Bug 37461853 - Setting cavium reset system binary
                            paths according to the version
       aararora  01/23/25 - Bug 37510360: Add ntp and dns information from
                            original xml
       rajsag    01/21/25 - 37477389 - exadb-d | exacloud prevm_checks
                            mcheckdiskresources() verifies cells services
                            sequentially instead of parallel | improve large
                            cluster provisionig time
       naps      01/21/25 - Bug 37481917 - make mCheckCellsStatus to run in
                            parallel.
       naps      01/21/25 - Bug 37474863 - Run mPostVMCellPatching in parallel.
       pbellary  01/21/25 - Bug 37501771 - EXASCALE: ASM PROVISIOING FAILED DUE TO EXISTING HCPOOL GRIDDISKS
       jesandov  01/15/25 - Bug 37480102 - Execute latest change from
                            jesandov_bug-37474804 only for EXADB-XS
       aararora  01/08/25 - Bug 37419029: Set DISABLEVALIDATEDGSPACEFOR37371565
                            to true.
       prsshukl  10/24/24 - ER 36981808 - EXACS | ADBS | ELASTIC COMPUTE AND
                            CELL OPERATION ENHANCEMENTS -> IMPLEMENT PHASE 2
       mirivier 01/20/15 - Create file (Check for history comments from 2015-2024 in 
                           exacloud/documents/history_comments_exacloud/exabox/ovm/clucontrol.txt)
"""

import sys
import itertools
import signal
import socket
import six
import base64

from exabox.ovm.cluvmrecoveryutils import NodeRecovery

try:
    from collections import defaultdict
except ImportError:
    from collections.abc import defaultdict

from functools import partial
from exabox.utils.common import check_string_base64, version_compare
from exabox.utils import common
from exabox.tools.AttributeWrapper import wrapStrBytesFunctions
import paramiko
from paramiko.ssh_exception import SSHException
from exabox.infrapatching.core.infrapatcherror import PATCHING_NODE_SSH_CHECK_FAILED, ebPatchFormatBuildError 
from exabox.core.Error import ebError, ExacloudRuntimeError, gReshapeError, gPartialError, gProvError, gNodeElasticError
from exabox.core.Node import exaBoxNode, exaBoxNodePool
from exabox.log.LogMgr import ebLogError, ebLogInfo, ebLogWarn, ebLogDebug, ebLogVerbose, ebLogJson, ebLogCritical, ebLogTrace, gLogMgrDirectory
from exabox.ovm.vmconfig import exaBoxClusterConfig
import os, sys, subprocess, uuid, os.path, shlex, select, pwd, math, pty, stat
from ast import literal_eval
import psutil
from time import strftime
from subprocess import PIPE
import threading
import xml.etree.cElementTree as etree
from exabox.core.Context import get_gcontext
from exabox.tools.ebTree.ebTree import ebTree
from exabox.infrapatching.core.cluinfrapatch import ebCluInfraPatch
from exabox.core.Core import exaBoxCoreInit
import exabox.ovm.vmcontrol as vmcontrol
from exabox.ovm.vmcontrol import exaBoxOVMCtrl, ebVgLifeCycle
from exabox.ovm.sysimghandler import (copyVMImageVersionToDom0IfMissing, 
                                      formatVMImageBaseName,
                                      hasDomUCustomOS, 
                                      mGetImageFromDom0ToLocal,
                                      mGetImageFromOSSToLocal,
                                      mVerifyImagesMultiProc,
                                      mSearchImgInDom0s,
                                      mBuildAndAttachU02DiskKVM,
                                      mAttachVDiskToKVMGuest,
                                      mBuildVDisk,
                                      mCopySystemImgLocalToDOM0,
                                      mIsRtgImg,
                                      mIsRtgImgPresent)
from exabox.ovm.clumisc import (ebCluPreChecks, ebCluFetchSshKeys, 
                                ebCluScheduleManager, ebCluCellValidate, 
                                ebCluServerSshConnectionCheck, 
                                ebCluReshapePrecheck, ebCluNodeSubsetPrecheck,
                                mGetDom0sImagesListSorted,
                                ebCopyDBCSAgentpfxFile, ebCluEthernetConfig, mPatchPrivNetworks, ebCluSshSetup, mGetGridListSupportedByOeda)
from exabox.ovm.clupowermanagement import ebCluStartStopHostFromIlom
from exabox.ovm.clumisc import ebSubnetSet, ebCluPostComputeValidate, ebMiscFx, ebCluFaultInjection, ebMigrateUsersUtil
from exabox.ovm.cluinfradelete import ebCluInfraDelete
from exabox.ovm.clubackup import backupCreateVMLogs
import exabox.ovm.clusshkey as clusshkey
from tempfile import NamedTemporaryFile, TemporaryDirectory
import time
import datetime
from base64 import b64decode, b64encode

try:
    from base64 import decodestring
except ImportError:
    from base64 import b64decode as decodestring

from struct import unpack
import hashlib
import fnmatch, re
from ipaddress import IPv4Address, IPv4Network, IPv6Address, ip_interface, ip_address, IPv6Network
import string
import json, copy, crypt
import glob
import traceback
import operator
import getpass
import fcntl
import shutil
import copy
from six.moves import urllib
from typing import Any, Dict, List, Mapping, Sequence, Optional, Tuple, Set
from exabox.network.NfTables import NfTables

urlopen = urllib.request.urlopen
Request = urllib.request.Request
URLError = urllib.error.URLError
HTTPError = urllib.error.HTTPError
from exabox.core.CrashDump import CrashDump
from exabox.tools.scripts import ebScriptsEngineFetch
from exabox.core.DBStore import ebGetDefaultDB
from multiprocessing import Process, Manager
from exabox.ovm.monitor import ebClusterNode
from exabox.ovm.cluhealth import ebCluHealthCheck
from exabox.healthcheck.cluhealthcheck import ebCluHealth
from exabox.ovm.cluresmgr import ebCluResManager
from exabox.ovm.cludgmgr import ebCluDataguardManager
from exabox.ovm.clusparse import ebCluSparseClone
from exabox.ovm.cludiskgroups import ebCluManageDiskgroup
from exabox.ovm.cludbaas import ebCluDbaas
from exabox.ovm.cludiag import exaBoxDiagCtrl
from exabox.ovm.clustorage import ebCluStorageConfig, ebCluManageStorage
from exabox.ovm.vmbackup import ebCluManageVMBackup
from exabox.ovm.cludomupartitions import ebCluManageDomUPartition
from exabox.ovm.atp import AtpAddRoutes2DomU, ebCluATPConfig, AtpSetupSecondListener, AtpAddiptables2Dom0, AtpCreateAtpIni, AtpAddScanname2EtcHosts, AtpSetupNamespace, AtpSetupASMListener
from exabox.ovm.AtpUtils import ebAtpUtils
from exabox.ovm.atpaddroutes import ATPAddBackupRoutes
from exabox.core.Mask import umask, umaskSensitiveData, maskSensitiveData
from exabox.ovm.cluelastic import ebCluElastic, getGridHome
from exabox.ovm.bmc import V1OedaXMLRebuilder, XMLProcessor
from exabox.ovm.cluelasticcells import ebCluElasticCellManager
from exabox.tools.ebOedacli.ebExportExacloud import ebExportExacloud
from exabox.tools.ebOedacli.ebCommandGenerator import ebCommandGenerator
from exabox.tools.ebOedacli.ebOedacli import ebOedacli
from exabox.tools.ebNoSql.ebNoSqlInstaller import ebNoSqlInstaller
from exabox.tools.oedacli import OedacliCmdMgr
from exabox.ovm.cluelasticcompute import ebCluReshapeCompute
from exabox.ovm.clusteps import *
from exabox.ovm.csstep.cs_driver import csDriver
from exabox.network.dns.DNSConfig import ebDNSConfig
from exabox.ovm.cluexaccib import ExaCCIB_CPS, ExaCCIB_DomU
from exabox.ovm.cluexaccroce import ExaCCRoCE_CPS
from exabox.ovm.cluexaccsecrets import ebExaCCSecrets
from exabox.ovm.exawatcher import exaBoxExaWatcher, cleanupExaWatcherLogs
from exabox.BaseServer.AsyncProcessing import ProcessManager, ProcessStructure, TimeoutBehavior, ExitCodeBehavior
from exabox.ovm.hypervisorutils import getHVInstance, ebVgCompRegistry
from exabox.ovm.kvmcpumgr import exaBoxKvmCpuMgr
from exabox.ovm.cluexaccatp import ebExaCCAtpPatchXML,ebExaCCAtpEtcHostsNATVIPs, ebExaCCAtpListener
from exabox.ovm.cluexaccatp_filtering import ebExaCCAtpFiltering
from exabox.ovm.cluexaccutils import ebOCPSJSONReader
from exabox.ovm.cluiptablesroce import ebIpTablesRoCE
from exabox.config.Config import ebCluCmdCheckOptions, ebCsSubCmdCheckOptions, ebVmCmdCheckOptions
import exabox.ovm.clujumboframes as clujumboframes
from exabox.core.DBLockTableUtils import ebDBLockTypes, sDBLockCleanLockOnHost
from exabox.ovm.coredump import ebCoredumpUtil, setKvmOnCrash
from exabox.ovm.kvmdiskmgr import exaBoxKvmDiskMgr
from exabox.ovm.reconfig.clupreprov_reconfig_factory import ebCluPreprovReconfigFactory
from exabox.ovm.cluexaccmigration import migrateExaCCGen1ClusterToGen2
import exabox.ovm.clubonding as clubonding
import exabox.ovm.clunetupdate as clunetupdate
from exabox.infrapatching.handlers.handlertypes import getInfraPatchingTaskHandlerInstance
from exabox.recordreplay.record_replay import ebRecordReplay
from exabox.ovm.clunetworkdetect import ebDiscoverOEDANetwork
from exabox.ovm.cluhealthpostprov import executeHealthPostProv
from exabox.ovm.cluzdlra import exaBoxZdlra
from exabox.agent.Client import ebExaClient, ebGetClientConfig
from exabox.agent.ebJobRequest import ebJobRequest, nsOpt
from exabox.ovm.csstep.cs_util import csUtil
from exabox.ovm.clubasedb import exaBoxBaseDB
from exabox.exakms.ExaKmsEntry import ExaKmsEntry, ExaKmsHostType
from exabox.exakms.ExaKmsSingleton import ExaKmsSingleton
from exabox.utils.common import mCompareModel
from exabox.utils.node import (connect_to_host, node_connect_to_host, node_exec_cmd,
                               node_exec_cmd_check, node_update_key_val_file,
                               node_cmd_abs_path_check,
                               node_write_text_file, node_read_text_file, node_replace_file)
from exabox.ovm.cluconfig import ebCluHeaderConfig, ebCluMachinesConfig, \
ebCluClustersConfig, ebCluClusterScansConfig, ebCluDatabaseHomesConfig, \
ebCluDabasesConfig, ebCluNetworkConfig, ebCluNetworksConfig, ebCluVMSizesConfig, \
ebCluSwitchesConfig, ebCluEsRacksConfig, ebCluUsersConfig, ebCluGroupsConfig,\
ebCluIlomsConfig, ebCluStorageDesc, ebCluDRVipConfig, ebCluDRScanConfig
from exabox.ovm.remotelock import RemoteLock
from exabox.ovm.cludomufilesystems import expand_domu_filesystem, ebDomUFilesystem, parse_size, ebDomUFSResizeMode, ebDiskImageInfo, attach_dom0_disk_image, create_new_lvm_disk_image, fill_disk_with_lvm_partition, get_max_domu_filesystem_sizes, shutdown_domu, start_domu, GIB, TIB
from exabox.tools.profiling.profiler import measure_exec_time
from exabox.tools.profiling.stepwise import steal_step_substep
from exabox.globalcache.GlobalCacheFactory import GlobalCacheFactory
from exabox.ovm.clunetworkvalidations import NetworkValidations
from exabox.ovm.cluencryption import (isEncryptionRequested,
    encryptionSetupDomU, patchEncryptedKVMGuest, patchXMLForEncryption,
    addEncryptionProperties, executeLuksOperation, mSetLuksPassphraseOnDom0Exacc,
    luksCharchannelExistsInDom0, mSetLuksChannelOnDom0Exacc,
    exacc_get_fsencryption_passphrase, exacc_save_fsencryption_passphrase,
    exacc_fsencryption_requested, exacc_del_fsencryption_passphrase,
    resizeOEDAEncryptedFS, getMountPointInfo, validateMinImgEncryptionSupport)
from exabox.exakms.ExaKmsEndpoint import ExaKmsEndpoint
from exabox.exakms.ExaKms import exakms_enable_fetch_clustername_decorator
from exabox.ovm.cluexascale import ebCluExaScale
from exabox.ovm.csstep.exascale.escli_util import ebEscliUtils
from exabox.ovm.csstep.exascale.exascaleutils import ebExascaleUtils
from exabox.ovm.clurevertnetworkreconfig import ebCluRevertNetworkReconfig
from exabox.ovm.clunetworkbonding import ebCluNetworkBonding
from exabox.ovm.cluserialconsole import serialConsole
from exabox.ovm.clunetwork import CLIENT, BACKUP, DR
from exabox.ovm.cluvmconsole_deploy import VMConsoleDeploy
from exabox.exadbxs.edv import get_hosts_edv_from_cs_payload, get_hosts_edv_state, EDVState, EDVInfo, get_guest_edvs_from_cluster_xml, get_edvs_from_cluster_xml
from exabox.ovm.cluvmreconfig import mVMReconfig
from exabox.ovm.vmboss import ebCluVmbackupObjectStore
from exabox.ovm.userutils import ebUserUtils
from exabox.ovm.utils.clu_utils import ebCluUtils
from exabox.network.NetworkUtils import NetworkUtils
from exabox.ovm.clumisc import mWaitForSystemBoot, ebCluStorageReshapePrecheck
from exabox.ovm.secureerase.secureerase import SecureErase
from exabox.infrapatching.core.cluinfrasinglereqpatch import ebCluInfraSingeRequestPatch
from exabox.ovm.bom_manager import ImageBOM
from exabox.ovm.clucommandhandler import CommandHandler
from exabox.ovm.clucontrol_deprecated import ebCluControlDeprecated
from exabox.ovm.cluacceleratednetwork import ebCluAcceleratedNetwork
from exabox.ovm.adbs_elastic_service import exaBoxAdbs


DEVNULL = open(os.devnull, 'wb')

OSTP_VALIDATE_CNF = 1
OSTP_CREATE_VM    = 2
OSTP_CREATE_USER  = 3
OSTP_SETUP_CELL   = 4
OSTP_CREATE_CELL  = 5
OSTP_CREATE_GDISK = 6
OSTP_INSTALL_CLUSTER = 7
OSTP_INIT_CLUSTER = 8
OSTP_INSTALL_DB   = 9
OSTP_RELINK_DB    = 10
OSTP_CREATE_ASM   = 11
OSTP_CREATE_DB    = 12
OSTP_CREATE_PDB   = 13
OSTP_APPLY_FIX    = 14
OSTP_INSTALL_EXCHK = 15
OSTP_CREATE_SUMMARY = 16
OSTP_RESECURE_MAC = 17

OSTP_PRE_INSTALL    = 127
OSTP_PREVM_INSTALL  = 128
OSTP_PREGI_INSTALL  = 129
OSTP_POSTVM_INSTALL = 130
OSTP_POSTGI_INSTALL = 131
OSTP_POST_INSTALL   = 132
OSTP_PREDB_INSTALL  = 133
OSTP_POSTDB_INSTALL = 134

OSTP_PREGI_DELETE   = 135
OSTP_POSTGI_DELETE  = 136
OSTP_PREVM_DELETE   = 137
OSTP_POSTVM_DELETE  = 138
OSTP_PREDB_DELETE   = 139
OSTP_POSTDB_DELETE  = 140

OSTP_POSTGI_NID     = 141
OSTP_DBNID_INSTALL  = 142
OSTP_APPLY_FIX_NID  = 143
OSTP_DG_CONFIG      = 144

OSTP_END_INSTALL    = 255


OSTP_SKIP_LIST = [OSTP_PRE_INSTALL, OSTP_PREVM_INSTALL, OSTP_PREGI_INSTALL, OSTP_POSTVM_INSTALL,
                  OSTP_POSTGI_INSTALL, OSTP_POST_INSTALL, OSTP_PREDB_INSTALL, OSTP_POSTDB_INSTALL,
                  OSTP_END_INSTALL, OSTP_PREGI_DELETE, OSTP_POSTGI_DELETE, OSTP_PREVM_DELETE, OSTP_POSTVM_DELETE]
VM_MODE   = 1
GI_MODE   = 2
VMGI_MODE = 3

#SELINUX Update return codes
SELINUX_UPDATE_SUCCESS = 0

CUSTOMUSERS = 'users_with_custom_id'
CUSTOMGROUPS = 'groups_with_custom_id'

class exaBoxCluCtrl(object):

    def __init__(self, aCtx, aNode=None, aOptions=None):

        self.__ctx = aCtx
        self.__node = aNode
        self.__config    = None
        self.__patchconfig = None
        self.__updatedns = False
        self.__machines  = None
        self.__clusters  = None
        self.__databases = None
        self.__storage   = None
        self.__configPath = None
        self.__networks   = None
        self.__switches   = None
        self.__users      = None
        self.__groups     = None
        self.__iloms      = None
        self.__esracks    = None
        self.__options    = aOptions
        self.__oeda_step  = self.mGetArgsOptions().oeda_step
        self.__conf_files = {}
        self.__base_path = None
        self.__oeda_path  = None
        self.__oeda_req_path = None
        self.__scriptname = None
        self.__remoteconfig = None
        self.__oeda_stable = None
        self.__cluster_path = None
        self.__cmd          = None
        self.__header            = None
        self.__skip_xml_checks = False
        self.__cell_type = {}
        self.__rack_size = None
        self.__fedramp = False
        self.__casignedcerts = False
        self.__ut = False
        self.__exadata_images_map = {}
        self.__dyndepfiles = self.mReadDynDepConfig() #read only once, clone it if needed
        self.__exaunit_id = "0000-0000-0000-0000"

        self.__mock_mode = str(self.mCheckConfigOption("mock_mode")).upper()

        # This is set only in Agent mode for now and it allows us to update the status of a given
        # request while it is being processed (e.g. statusinfo) so that we don't always return
        # 'pending' as the the reason why the request is still processing.
        self.__requestobj  = None

        self.__tools_key_public  = None
        self.__tools_key_private = None

        # OCDE Specific fields
        self.__dbname = None
        self.__dbname_cfg = None
        self.__dbpatch = None
        self.__standbydb = None
        self.__debug = self.mGetArgsOptions().debug
        self.__verbose = self.mGetArgsOptions().verbose

        self.__key   = None
        self.__hosts_list = []

        self.__tmp_buffer = None
        self.__tmp_status = 0
        self.__uuid = None

        self.__parsexml = False

        self.__CDBEnabled = False

        self.__timeZone = None

        self.__shared_env:bool = None
        self.__cluster_name = None

        self.remote_lock = RemoteLock(self)
        self.__dr_net_present = False
        self.__drVips = {}
        self.__drScans = None

        self.__ipv6_dual_stack_present = False
        self.__ipv6_single_stack_present = False

        #nid starter db related parameter
        #__enable_nid_starterdb -> parameter used for checking if starter db needs to be created using nid or oeda
        #__create_nid_starterdb -> parameter passed to ocde for it to detect if it is run for starter db or additional db. During starter db ocde flow scan is not completely setup
        self.__enable_nid_starterdb = self.mCheckConfigOption('enable_nid_starterdb', 'True')
        self.__create_nid_starterdb = None
        self.__nid_backup_files = {}

        # DBaaS API integration for ords/cns
        # if the dbaas payload is found then exacloud
        # will invoke the dbaas script and copy the config
        # to the domU, ords/cns
        self.__dbaas_api_payload = {}

        _pvl = self.mCheckConfigOption('timeout_ecops')
        if _pvl is not None:
            self.__timeout_ecops = int(_pvl)
        else:
            self.__timeout_ecops = 300

        # Flag for identifying if current environment is KVM/ROCE.
        self.__kvm_enabled = None

        # TODO: xxx/MR: For KVM always -egil to pick up the latest GI version
        self.__enable_gilatest = self.mGetArgsOptions().enablegilatest

        self.__enable_quorum = self.mCheckConfigOption('enable_quorum','True')
        self.__enable_asmss = None
        self.__enable_recreate_svc = self.mCheckConfigOption('enable_recreate_svc','True')

        # 15 minutes default timeout for VM CPU resize
        _pvl = self.mCheckConfigOption('timeout_vmcpu_resize')
        if _pvl is not None:
            self.__timeout_vmcpu_resize = int(_pvl)
        else:
            self.__timeout_vmcpu_resize = 900

        self.__images_version = None
        self.__skip_xml_patching = False

        self.__db_version = None
        self.__isNoOeda = False
        self._dom0U_list = []

        self.__compat = {
            '181': ('181', '122', '121', '112'),
            '122': ('122', '121', '112'),
            '121': ('121', '112')
        }

        # allow different set of GI for ATP
        self.__dbgi_config = {}
        self.__repo_download_location = None
        self.__repo_inventory = {}
        self.__GiMultiImageSupport = False

        # Can be overwritten if we detect vnuma on a compute node
        self.__disable_vcpus_pinning = self.mCheckConfigOption('disable_vcpus_pinning', 'True')

        # Will probably be replaced by dynamic VM detection
        self.__ol7 = None
        self.__ol8 = None
        self.__ol8domU = None
        self.__preprovisioning = False
        self.__19cVMGI = False
        self.__over18cSupported = None
        self.__over23cSupported = None

        # ATP Support
        self.__ATP = ebCluATPConfig(aOptions)
        self.__isATP = self.__ATP.isATP()

        self.__exacm = self.mCheckConfigOption("exacm", "True")
        self.__exabm = self.mCheckConfigOption("exabm", "True")
        # count of physical disks attached to dom0
        self.__dom0_disks = None

        self.__fiber_backup  = False
        self.__copper_client = False
        self.__fortpond_net  = False
        
        # XML options
        self.__extraXmlPatchingCommands = []
        self.__extraXmlPrePatchingCommands = []
        self.__isXmlElasticShape = None

        self.__network_type  = self.mCheckConfigOption("network_interface_type")
        if self.__network_type == 'copper':
            self.__copper_client = True
        elif self.__network_type == 'fiber':
            self.__fiber_backup  = True
        elif self.__network_type == 'fortpond':
            self.__fortpond_net  = True
        elif self.__network_type != 'auto':
            #if none of network options are manually set, use AUTO mode by default
            if self.mCheckConfigOption("backup_network_link") is None and \
               self.mCheckConfigOption("client_network_link") is None and \
               self.mCheckConfigOption("enable_fortpond_card")is None:
                self.__network_type  = 'auto'
            else: # copper/fiber/auto mode not set and legacy parameter used
                self.__fiber_backup  = self.mCheckConfigOption("backup_network_link","fiber")
                self.__copper_client = self.mCheckConfigOption("client_network_link","copper")
                self.__fortpond_net = self.mCheckConfigOption("enable_fortpond_card", "True")

        self.__network_discovered = {}

        self.__amos  = self.mCheckConfigOption("amos", "True")
        self.__bmc_cluster = None

        self.__dyndep_version = "0.0"
        self.__dyndep_update  = True
        self.__domus_dom0s = {}
        self.__domus_dom0s_nat = None
        self.__disable_dom0_cell_lockdown = self.mCheckConfigOption("disable_dom0_cell_lockdown")

        self._hash_file_cache = {}

        self.__ohsize = None
        self.__dbstorage = None
        self.__u02size = None
        self.__racknameEcra = None
        self.__exascale = None
        self.__vmClusterType = None

        self.__database_mapping = {}

        self.__timeout_vm_boot = 600

        if self.mCheckConfigOption('db_timeout') is not None:
            self.__db_timeout = int(self.mCheckConfigOption('db_timeout'))
        else:
            self.__db_timeout = 1800

        self.__ui_oedaxml = self.mCheckConfigOption('ui_oedaxml', 'True')
        if self.__ui_oedaxml:
            ebLogInfo('*** init: ui_oedaxml enabled')
        else:
            ebLogInfo('*** init: ui_oedaxml disable')

        self.__ociexacc = self.mCheckConfigOption('ociexacc', 'True')
        self.__disable_keys_db_sync = self.mCheckConfigOption('disable_keys_db_sync', 'True')
        self.__short_cluster_folder = {}
        self.__cluster_lock_files = []
        self.__netDetectError = {}

        # Please access through mGetOciExaCCServicesSetup which will generate and cache value
        self.__exaccoci_services = None

        self.__run_all_undo_steps = self.mCheckConfigOption('run_all_undo_steps','True')

        self._additional_disks = []
        self.__elastic_old_dom0_domU_pair = None

        self.__clu_verify = self.mCheckConfigOption("enable_cluster_verify", "True")
        self.__CompRegistry = ebVgCompRegistry()

        # Reconfig utility
        self.__factoryPreprovReconfig = ebCluPreprovReconfigFactory(self)
        self.__proverr  = ""
        self.__basedb = exaBoxBaseDB(self)
        self.__ZDLRA = exaBoxZdlra(self)
        self.__zdlra_config_val = self.__ZDLRA.ZdlraProvVal(aOptions)
        self.__zdlra_hthread_val = self.__ZDLRA.ZdlraHThreadVal(aOptions)
        self.__zdlra_dbhome = ""
        # Adbs object
        self.__ADBSobj = exaBoxAdbs(self)
        #OEDA version in YYMMDD format
        self.__oeda_version_yymmdd = ""

        #Node Recovery
        self.__skip_dom0_validation = False
        self.__delete_node_name = []
        self.__fxSharedInfo = {}

        #eth0 removal
        self.__interfaceMapping = {
            "default": "vmeth0",
            "eth0_removal": "vmbondeth0"
        }

        #Ethernet Configuration Utility
        self.__ethConfig = ebCluEthernetConfig(self, aOptions)

        #Exascale service for exacs & exacc
        self.__XS = False
        self.__xsUtils = ebExascaleUtils(self)

        self.__adbs = False

        #Initializing default storage type as ASM
        self.__storageType = "ASM"

        #Create instance of clucontrol deprecated  object. This code will be removed soon.
        self.__cluCtrlDeprecatedObj = ebCluControlDeprecated(self)  

        self.__command_handler = CommandHandler(self)
        self.__cellInfo = {}

    def mGetCluCtrlDeprecateObj(self):
        return self.__cluCtrlDeprecatedObj
 
    def mGetRunAllUndoSteps(self):
        return self.__run_all_undo_steps

    def mGetCommandHandler(self):
        return self.__command_handler

    def mGetDebug(self):
        return self.__debug

    def mSetNoOeda(self, aValue):
        ebLogInfo(f"Set NO-OEDA to: {aValue}")
        self.__isNoOeda = aValue

    def mCalculateNoOeda(self, aCmd):

        _substepNoOeda = True
        _endpointNoOeda = ebCluCmdCheckOptions(aCmd, ['nooeda'])

        if self.mGetOptions() and self.mGetOptions().steplist:
            for _step in self.mGetOptions().steplist.split(","):
                _substepNoOeda = _substepNoOeda and ebCsSubCmdCheckOptions(_step.strip(), ["nooeda"])

        else:
            _substepNoOeda = False


        # OEDA on specific cases
        try:
            if self.mGetOptions() and self.mGetOptions().steplist:
                for _step in self.mGetOptions().steplist.split(","):
                    if ebCsSubCmdCheckOptions(_step.strip(), ["oeda_on_singlevm"]):
                        if not self.mGetSharedEnv():
                            _endpointNoOeda = False
                            _substepNoOeda = False
                            break
        except:
            pass

        self.mSetNoOeda(_endpointNoOeda or _substepNoOeda)

    def mIsNoOeda(self):
        return self.__isNoOeda

    def mGetDbaasApiPayload(self):
        return self.__dbaas_api_payload

    def mSetDbaasApiPayload(self, aVal):
        self.__dbaas_api_payload = aVal

    def mGetCurrentMasterInterface(self, aEth0Present=False, aDom0=None):
        _dom0 = aDom0
        _eth0present = aEth0Present
        _master = ""
        if _eth0present or not ebMiscFx.mIsEth0Removed(self.__options.jsonconf, _dom0):
            _master = self.__interfaceMapping.get("default", "vmeth0")
        else:
            _master = self.__interfaceMapping.get("eth0_removal", "vmbondeth0")
        return _master

    def mSetU02Size(self, aSize):
        self.__u02size = aSize

    def mGetRemoteLock(self):
        return self.remote_lock

    def mGetOedaVersYYMMDD(self):
        return self.__oeda_version_yymmdd

    def mIsUt(self):
        return self.__ut

    def mSetUt(self, aUt):
        self.__ut = aUt

    def mGetTimeZone(self):
        return self.__timeZone

    def mSetTimeZone(self, aTimeZone):
        self.__timeZone = aTimeZone

    def mGetExtrXmlPatchingCmds(self):
        return self.__extraXmlPatchingCommands

    def mSetOedaVersYYMMDD(self,aOedaVersion):
        self.__oeda_version_yymmdd = aOedaVersion
        
    def mGetNetworkDiscovered(self):
        return self.__network_discovered

    def mSetNetworkDiscovered(self, aAdminNet=None, aClientNet=None, aBackupNet=None, aDRNetInterfaces=None)->None:
        """
            Represents an OEDA network spec, for instance

            :param str aAdminNet: 'vmeth0::eth0'
            :param str aClientNet: 'vmbondeth1:eth3,eth4:bondeth1'
            :param str aBackupNet: 'vmbondeth0:eth1,eth2:bondeth0'
        """
        _oeda_network = self.mGetNetworkDiscovered()

        def __extract_net_info(aNetInfo: str)->dict:
            _bridge, _bond_slaves, _bond_master = aNetInfo.split(":")
            return {
                'bridge': _bridge,
                'bond_master': _bond_master,
                'bond_slaves': _bond_slaves.replace(',', ' ')
            }

        if aAdminNet:
            _oeda_network['admin_net'] = __extract_net_info(aAdminNet)

        if aClientNet:
            _oeda_network['client_net'] = __extract_net_info(aClientNet)

        if aBackupNet:
            _oeda_network['backup_net'] = __extract_net_info(aBackupNet)

        if aDRNetInterfaces:
            _oeda_network['dr_net'] = __extract_net_info(aDRNetInterfaces)

        self.__network_discovered = _oeda_network

    def mGetProvErr(self):
        return self.__proverr

    def mSetProvErr(self, err):
        self.__proverr = err

    def IsZdlraProv(self):
        return self.__zdlra_config_val

    def mSetZdlraProv(self, aZDLRA):
        self.__zdlra_config_val = aZDLRA

    def IsZdlraHThread(self):
        return self.__zdlra_hthread_val

    def mGetZDLRA(self):
        return self.__ZDLRA
    
    def mSetZdlraHThread(self, aValue):
        self.__zdlra_hthread_val = aValue
        
    def mSetCellInfo(self, aValue):
        if not isinstance(aValue, dict):
            raise ValueError("cellInfo must be a dictionary")
        self.__cellInfo = aValue
        
    def mGetCellInfo(self):
        return self.__cellInfo
        
    def mGetZdlraDbhome(self):
        return self.__zdlra_dbhome

    def mSetZdlraDbhome(self, aDbhome):
        self.__zdlra_dbhome = aDbhome

    def mGetBaseDB(self):
        return self.__basedb

    def mGetADBS(self):
        return self.__ADBSobj

    # Function needs to be called after ociexcc flag is set
    def mGetOciExaCCServicesSetup(self):
        # Perf and caching ({} is not None)
        if self.__exaccoci_services is not None:
            return self.__exaccoci_services

        if not self.__ociexacc:
            self.__exaccoci_services = {}
            return {}

        # from cluexaccutils
        self.__exaccoci_services = ebOCPSJSONReader().mGetServices()

        return self.__exaccoci_services

    def mIsExaCCNonProdSharedRack(self):
        """
        :returns bool:
            True:
                -- The CPS has 'scaqa' in it's name
                -- There is 1 XML only in config-bundle area on cps:
                    /opt/oci/config-bundle/<id> area"
            False:
                If any of the above conditions is not met
        """
        # First discard if env is not ExaCC
        if not self.mIsOciEXACC():
            return False

        _rc, _, _o, _ = self.mExecuteLocal("/bin/hostname")
        _cps_hostname = _o.strip()
        
        # Discard immediately if CPS hostname does not contain "scaqa"
        if not self.mEnvTarget():
            ebLogInfo(f"\'scaqa\' not present in CPS hostname: {_cps_hostname}")
            return False

        # Read OCPS json file, discard if file does not exist or filename does not point to a file
        _ocps_path = self.mCheckConfigOption("ocps_jsonpath")
        if not (_ocps_path and os.path.isfile(_ocps_path)):
            ebLogError(f"{_ocps_path} does not exist or is not a file.")
            return False

        _ocps_dict = None
        try:
            with open(_ocps_path, 'r') as _json:
                _ocps_dict = json.load(_json)
        except Exception as e:
            _err = f"Could not read {_ocps_path} due to exception: {e}"
            ebLogError(_err)
            raise ExacloudRuntimeError(0x0750, 0xA, _err) from e

        # Get info from json file
        _rackname = _ocps_dict.get("rackName")
        if not _rackname:
            raise ExacloudRuntimeError(0x0750, 0xA, "ExaCC OCPS json file missing rackname")
        
        _rackbasename = _ocps_dict.get("rackBaseName")
        if not _rackbasename:
            raise ExacloudRuntimeError(0x0750, 0xA, "ExaCC OCPS json file missing rackbasename")

        # Discard if xml list does not contain more than one file. Example:
        # >>> glob.glob("/opt/......scaqan03XXX-clu*.xml")
        # ['/opt/oci/config_bundle/...-scaqan03XXX-clu01.xml']
        _dir_rack = os.path.join(_ocps_path, _rackname)
        _rack_xml_list = glob.glob(f"{_dir_rack}-clu*.xml")

        ebLogTrace("Detected the XML list of: '{_rack_xml_list}'")

        if len(_rack_xml_list) > 1:
            ebLogTrace(f"ExaCC directory '{_dir_rack}' needs to contain only one XML file.")
            return False

        # If we reach this line, it means this is a shared rack from ExaCC
        return True

    def mGetToolsKey(self):
        return self.__tools_key_public

    def mSetToolsKey(self, aVal):
        self.__tools_key_public = aVal

    def mGetToolsKeyPrivate(self):
        return self.__tools_key_private

    def mSetToolsKeyPrivate(self, aVal):
        self.__tools_key_private = aVal

    def mIsKVM(self,aHostname=None):

        if self.__kvm_enabled == None and aHostname is None:
            _dpairs = self.mReturnDom0DomUPair()
            if not _dpairs and self.mIsClusterLessXML():
                #Some of the xmls for kvm dont hvae dom0-domu pair.
                self.__kvm_enabled = True
                get_gcontext().mSetConfigOption('enable_kvm', 'True') # hook to access KVM value in sshgen.py
                return self.__kvm_enabled
            _dom0, _ = _dpairs[0]
            _targetConfig  = self.__machines.mGetMachineConfig(_dom0)
            _machineHVType = _targetConfig.mGetMacOsType()
            if _machineHVType == 'LinuxKVM':
                self.__kvm_enabled = True
                get_gcontext().mSetConfigOption('enable_kvm', 'True') # hook to access KVM value in sshgen.py
            else:
                self.__kvm_enabled = False
                get_gcontext().mSetConfigOption('enable_kvm', 'False')
            ebLogDebug("KVM discovery return isKVM: %s" % str(self.__kvm_enabled))

        # TODO: xxx/MR: Add suport for remote check of target
        if self.__kvm_enabled == None and aHostname:
            raise NotImplementedError

        return self.__kvm_enabled

    def mIsAdbs(self):
        self.__adbs = False
        aOptions = self.mGetArgsOptions()
        if aOptions is not None:
            _jconf = aOptions.jsonconf
        else:
            _jconf = None

        if _jconf and 'adb_s' in _jconf :
            if _jconf.get("adb_s").upper() == "TRUE":
                ebLogTrace('Active adb_s flag in payload')
                self.__adbs = True
        elif _jconf and 'AdditionalOptions' in _jconf:
            for _dict in _jconf['AdditionalOptions']:
                if _dict.get('serviceType') == "ADBS":
                    ebLogTrace('Using ADBS service type from payload')
                    self.__adbs = True

        return self.__adbs

    def mIsXS(self):

        if self.mIsExaScale():
            ebLogInfo("*** EXADB-XS Enable, Disabling flag of StorageType=XS")
            self.__XS = False 
            return False

        if self.__XS:
            return self.__XS

        aOptions = self.mGetArgsOptions()
        if aOptions is not None:
            _jconf = aOptions.jsonconf
        else:
            _jconf = None

        if _jconf is not None and "rack" in list(_jconf.keys()) \
            and "storageType" in list(_jconf["rack"].keys()) and _jconf["rack"]["storageType"].upper() == "XS":
            self.__XS = True
        else:
            _utils = self.mGetExascaleUtils()
            _xs_cluster = _utils.mCheckVaultTag()
            if _xs_cluster:
                self.__XS = True
                ebLogInfo("*** XS(EXASCALE) ENABLE")
            else:
                self.__XS = False

        return self.__XS

    def mGetXS(self):
        return self.__XS

    def mSetXS(self, aXS):
        self.__XS = aXS

    def mGetExascaleUtils(self):
        return self.__xsUtils

    # Utility method to identify a RoCE QinQ setup
    def mIsRoCEQinQ(self):
        _dpairs = self.mReturnDom0DomUPair()
        if len(_dpairs) < 1 or len(_dpairs[0]) < 2:
            if self.__debug:
                ebLogDebug("Not enough information to identify RoCE QinQ setup.")
            return False

        # Take just the first domU as reference
        _domU = _dpairs[0][1]
        if self.__debug:
            ebLogDebug("Using %s domU to identify RoCE QinQ" % _domU)
        _domU_mac = self.__machines.mGetMachineConfig(_domU)
        _domU_net_list = _domU_mac.mGetMacNetworks()
        for _net_id in _domU_net_list:
            _net_conf = self.__networks.mGetNetworkConfig(_net_id)
            if self.__debug:
                ebLogDebug("Verifying %s network for RoCE QinQ" % _net_conf.mGetNetName())
            # If we find a private network with no vLAN ID, this is not RoCE QinQ
            if _net_conf.mGetNetType() == 'private' and \
                    _net_conf.mGetNetVlanId() == 'UNDEFINED':
                if self.__debug:
                    ebLogDebug("%s network is private and has no vlanId, so this is no RoCE QinQ setup" % _net_conf.mGetNetName())
                return False

        if self.__debug:
            ebLogDebug("RoCE QinQ setup identified")
        return True


    # Verify QinQ is properly setup in the CPS.
    def mCheckCPSQinQSetup(self, aMode=True):
        cells_roce_ips = list()
        # Extract a list of the cells IPs attached to RoCE QinQ interfaces
        for _cell_name in self.mReturnCellNodes().keys():
            _cell_mac = self.__machines.mGetMachineConfig(_cell_name)
            _net_list = _cell_mac.mGetMacNetworks()
            for _net_id in _net_list:
                _net_conf = self.__networks.mGetNetworkConfig(_net_id)
                # Process all QinQ networks to obtain their associated ip
                # These networks are the private with vlan ID ones (on the cells)
                if _net_conf.mGetNetType() == 'private' and _net_conf.mGetNetVlanId() != 'UNDEFINED':
                    cells_roce_ips.append(_net_conf.mGetNetIpAddr())
                    if self.__debug:
                        ebLogDebug('Cell ip %s in %s associated to RoCE QinQ interface' % (_net_conf.mGetNetIpAddr(), _cell_name))

        # At this point, we have isolated all RoCE QinQ ips from the cells.
        # Delegate the CPS are properly setup
        if not cells_roce_ips:
            ebLogError('::mCheckCPSQinQSetup No RoCE QinQ IP identified in the cells.')
            raise ExacloudRuntimeError(0x0120, 0x0A, 'No RoCE QinQ IP identified in the cells.')

        _exacc_roce_cps = ExaCCRoCE_CPS(self.__debug, aMode)
        verify_return = _exacc_roce_cps.mVerifyCPSRoCEQinQSetup(cells_roce_ips)

        if self.__debug:
            ebLogDebug('Successful CPS RoCE QinQ setup validation.')

        return verify_return


    def mIsDisableDom0CellLockdown(self):
        if (self.__disable_dom0_cell_lockdown is not None) and (self.__disable_dom0_cell_lockdown == "True") :
           return True
        else:
           return False

    def mIsFedramp(self, aOptions=None):
        self.__fedramp = False
        if aOptions is not None and aOptions.jsonconf is not None and 'fedramp' in list(aOptions.jsonconf.keys()) and  str(aOptions.jsonconf['fedramp']).upper() == 'Y':
            ebLogInfo("*** FEDRAMP FEATURE ENABLE")
            self.__fedramp = True
        elif self.mIsOciEXACC():
            _ocps_jsonpath = self.mCheckConfigOption('ocps_jsonpath')
            if _ocps_jsonpath and os.path.exists(_ocps_jsonpath):
                with open(_ocps_jsonpath, 'r') as fd:
                    _ocps_json = json.load(fd)
                if 'fedramp' in _ocps_json and _ocps_json['fedramp'] == "ENABLED":
                    ebLogInfo("*** FEDRAMP FEATURE ENABLE")
                    self.__fedramp = True
        return self.__fedramp

    def mIsCaSignedCerts(self, aOptions=None):
        self.__casignedcerts = False
        if not aOptions:
            aOptions = self.mGetArgsOptions()
        if self.mIsOciEXACC() and aOptions is not None and aOptions.jsonconf is not None and 'caSignedCerts' in list(aOptions.jsonconf.keys()) and aOptions.jsonconf['caSignedCerts']:
            ebLogInfo("*** caSignedCerts FEATURE ENABLE")
            self.__casignedcerts = True
        return self.__casignedcerts

    def mIs19cVMGI(self):
        return self.__19cVMGI

    def mSetOL8(self, aValue):
        self.__ol8 = aValue

    def mGetATP(self):
        return self.__ATP

    def mSetATP(self, aConf):
        self.__ATP = aConf

    def mIsXmlElasticShape(self):
        return self.__isXmlElasticShape

    def mSetIsXmlElasticShape(self, aValue):
        self.__isXmlElasticShape = aValue

    def mGetSharedEnv(self) -> bool:
        return self.__shared_env

    def mSetSharedEnv(self, aValue:bool):
        self.__shared_env = aValue

    def mGetDyndepFiles(self):
        return self.__dyndepfiles

    def mSetDyndepFiles(self, aValue):
        self.__dyndepfiles = aValue

    def mGetImageFiles(self):
        return self.__imagefiles

    def mSetImageFiles(self, aImages):
        self.__imagefiles = aImages

    def mGetCompat(self):
        return self.__compat

    def mGetEnableGILatest(self):
        return self.__enable_gilatest

    def mSetEnableGILatest(self, aGiLatest):
        self.__enable_gilatest = aGiLatest

    def mGetBasePath(self):
        if not self.__base_path:
            self.__base_path = self.__ctx.mGetBasePath()
        return self.__base_path

    def mGetOedaPath(self):
        return self.__oeda_path

    def mSetOedaPath(self, aPath):
        self.__oeda_path = aPath

    def mSetOEDARequestsPath(self, aRequestsPath):
        self.__oeda_req_path = aRequestsPath

    def mGetOEDARequestsPath(self):
        return self.__oeda_req_path

    def mCheckNIDStarterDB(self):
        return self.__enable_nid_starterdb

    def mGetRemoteConfig(self):
        return self.__remoteconfig

    def mSetRemoteConfig(self, aRemoteConfig):
        self.__remoteconfig = aRemoteConfig

    def mGetDomUsDom0s(self):
        return self.__domus_dom0s

    def mSetDomUsDom0s(self, aClusterID, aDom0DomUPair):
        self.__domus_dom0s[aClusterID] = aDom0DomUPair

    def mGetElasticOldDom0DomUPair(self):
        return self.__elastic_old_dom0_domU_pair

    def mSetElasticOldDom0DomUPair(self, aValue):
        self.__elastic_old_dom0_domU_pair = aValue

    def mGetUiOedaXml(self):
        return self.__ui_oedaxml

    def mSetUiOedaXml(self, aConf):
        self.__ui_oedaxml = aConf

    def SharedEnv(self) -> bool:
        return self.__shared_env

    def isATP(self):
        return self.__isATP

    def mSetIsATP(self, aBool):
        self.__isATP = aBool

    def isBaseDB(self):
        
        aOptions = self.mGetArgsOptions()
        if aOptions is not None:
            _jconf = aOptions.jsonconf
        else:
            _jconf = None

        if self.mIsExaScale() and _jconf and 'clusterType' in _jconf and 'serviceSubType' in _jconf:
            if _jconf.get("clusterType").lower() == "blockstorage" and _jconf.get("serviceSubType").lower() == "basedb":
                ebLogTrace('BaseDB detected in payload.')
                return True

        if aOptions is not None and hasattr(aOptions, 'clusterType') and \
           hasattr(aOptions, 'serviceSubType')  and  hasattr(aOptions, 'storageType')  and \
           str(aOptions.clusterType).lower() == "blockstorage" and \
           str(aOptions.serviceSubType).lower() == "basedb" and \
           str(aOptions.storageType).lower() == "exascale":
            ebLogTrace('BaseDB detected in query params.')
            return True

        return False

    def isExacomputeVM(self):

        aOptions = self.mGetArgsOptions()
        if aOptions is not None:
            _jconf = aOptions.jsonconf
        else:
            _jconf = None

        if self.mIsExaScale() and _jconf and 'clusterType' in _jconf and 'serviceSubType' in _jconf:
            if _jconf.get("clusterType").lower() == "blockstorage" and _jconf.get("serviceSubType").lower() == "exacompute":
                ebLogTrace('Exacompute VM as serviceSubType detected in payload.')
                return True

        if aOptions is not None and hasattr(aOptions, 'clusterType') and \
           hasattr(aOptions, 'serviceSubType')  and  hasattr(aOptions, 'storageType')  and \
           str(aOptions.clusterType).lower() == "blockstorage" and \
           str(aOptions.serviceSubType).lower() == "exacompute" and \
           str(aOptions.storageType).lower() == "exascale":
            ebLogTrace('Exacompute VM detected in query params.')
            return True

        return False


    def isDBonVolumes(self):
        
        aOptions = self.mGetArgsOptions()
        if aOptions is not None:
            _jconf = aOptions.jsonconf
        else:
            _jconf = None

        if self.mIsExaScale() and _jconf and 'clusterType' in _jconf and 'serviceSubType' in _jconf:
            if _jconf.get("clusterType").lower() == "blockstorage" and _jconf.get("serviceSubType").lower() == "exadbxs":
                ebLogTrace('ExaDbXS config for 19C detected in payload.')
                return True

        return False

    def mGetEnableKVM(self):
        return self.__kvm_enabled

    def mSetEnableKVM(self, aValue):
        self.__kvm_enabled = aValue

    def mGetLocalNode(self):
        return self.__node

    def mSetLocalNode(self, aValue):
        self.__node = aValue

    def mGetStorageType(self):
        return self.__storageType

    def mSetStorageType(self, aType):
        self.__storageType = aType

    def mGetExadataImagesMap(self):
        return self.__exadata_images_map

    def mSetExadataImagesMap(self, aValue):
        self.__exadata_images_map = aValue

    def mGetExadataImageFromMap(self, aHostname):

        #Get the name of the nat
        _host = aHostname

        _ctx = get_gcontext()
        if _ctx.mCheckRegEntry('_natHN_' + _host):
            _host = _ctx.mGetRegEntry('_natHN_' + _host)

        self.mGenerateExadataImageMapEntry(_host)
        return self.mGetExadataImagesMap()[_host]

    def mGetOracleLinuxVersion(self, aHostname):

        _exadataImage = self.mGetExadataImageFromMap(aHostname)
        if not _exadataImage:
            return None

        _, _domUs, _, _ = self.mReturnAllClusterHosts()

        # Get nat and non-nat domU names in one single list
        _domUsExtendedList = []
        _ctx = get_gcontext()

        for _host in _domUs:
            if _ctx.mCheckRegEntry('_natHN_' + _host):
                _nat_host = _ctx.mGetRegEntry('_natHN_' + _host)
                _domUsExtendedList.append(_nat_host)
            _domUsExtendedList.append(_host)

        # TODO Include OL9 in the future
        _olV = ""

        if aHostname not in _domUsExtendedList:

            if self.mIsKVM():

                if version_compare(_exadataImage, "23.1.0") >= 0:
                    _olV = "OL8"

                elif version_compare(_exadataImage, "19.2.0") >= 0:
                    _olV = "OL7"

                else:
                    _olV = "OL6"

            else: # Xen

                if version_compare(_exadataImage, "23.1.0") >= 0:
                    _olV = "OL7"

                elif version_compare(_exadataImage, "22.1.0") >= 0:
                    _olV = "OL7"

                else:
                    _olV = "OL6"

        else:

            if version_compare(_exadataImage, "23.1.0") >= 0:
                _olV = "OL8"

            elif version_compare(_exadataImage, "19.2.0") >= 0:
                _olV = "OL7"

            else:
                _olV = "OL6"

        ebLogInfo(f"Detection of {aHostname} with Image '{_exadataImage}' to '{_olV}'")
        return _olV



    def mGetMajorityHostVersion(self, aHostType):

        def wrappermGetOracleLinuxVersion(aHost, aProxyList):
            """
            Helper to call mGetOracleLinuxVersion and store result
            in Proxy Object to use in parallel calls
            """
            try:
                _ol_version = self.mGetOracleLinuxVersion(aHost)
                aProxyList.append(_ol_version)
            except ValueError as e:
                ebLogInfo(f'A ValueError exception "{e}" encountered while obtaining oracle linux version on {aHost}')
                raise ExacloudRuntimeError(0x0119, 0xA, f'Image version information not found for {aHost}')
            except Exception as e:
                ebLogInfo(f"An exception {e} encountered while obtaining oracle linux version on {aHost}")
                raise e 

        _dom0s, _domUs, _cells, _switches = self.mReturnAllClusterHosts()
        if aHostType == ExaKmsHostType.DOM0:
            _hosts = _dom0s

        elif aHostType == ExaKmsHostType.DOMU:
            _hosts = _domUs

        elif aHostType == ExaKmsHostType.CELL:
            _hosts = _cells

        elif aHostType == ExaKmsHostType.SWITCH:
            _hosts = _switches

        _plist = ProcessManager()
        _versionList = _plist.mGetManager().list()

        for _host in _hosts:

            _p = ProcessStructure(wrappermGetOracleLinuxVersion, [_host, _versionList], _host)
            _p.mSetMaxExecutionTime(5*60) # 5 minutes
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()

        return max(set(_versionList), key = _versionList.count)


    def mGetHostsByTypeAndOLVersion(self, aHostType, aVersions):

        _dom0s, _domUs, _cells, _switches = self.mReturnAllClusterHosts()
        _hostList = []

        if aHostType == ExaKmsHostType.DOM0:
            _hostList = _dom0s

        elif aHostType == ExaKmsHostType.DOMU:
            _hostList = _domUs

        elif aHostType == ExaKmsHostType.CELL:
            _hostList = _cells

        elif aHostType == ExaKmsHostType.SWITCH:
            _hostList = _switches

        _filterList = []

        for _host in _hostList:
            for _version in aVersions:
                if self.mGetOracleLinuxVersion(_host) == _version:
                    _filterList.append(_host)
                    break

        return _filterList


    def mIsHostOL8(self, aHostname):
        _oracleLinux = self.mGetOracleLinuxVersion(aHostname)
        return _oracleLinux == "OL8"

    def mIsHostOL7(self, aHostname):
        _oracleLinux = self.mGetOracleLinuxVersion(aHostname)
        return _oracleLinux == "OL7"

    def mIsHostOL6(self, aHostname):
        _oracleLinux = self.mGetOracleLinuxVersion(aHostname)
        return _oracleLinux == "OL6"

    def mGetArgsOptions(self):

        if self.__options is None:
            return self.__ctx.mGetArgsOptions()
        else:
            return self.__options

    def mGetOptions(self):
        return self.__options

    def mSetOptions(self, aOptions):
        self.__options = aOptions

    def mIsSkipXmlPatching(self):
        return self.__skip_xml_patching

    def mSetSkipXmlPatching(self, aValue):
        self.__skip_xml_patching = aValue

    def mCalculateSkipXmlPatching(self):

        # Skip XML Patching not allowed for ExaCC
        if not self.mIsOciEXACC():
            if ebCluCmdCheckOptions(self.mGetCmd(), ['skip_xml_patching']):
                self.mSetSkipXmlPatching(True)

        # Enable XML Patching in case of missing vmSizes
        try:
            self.__vmsizes.mGetVMSize("Large").mGetVMSizeAttr('cpuCount')
        except:
            self.mSetSkipXmlPatching(False)

        ebLogInfo(f"Skip XML Patching Flag set to: {self.__skip_xml_patching}")


    def mGetEbtableSetup(self):
        if self.__exabm:
            return self.mCheckSubConfigOption('ebtables_setup', 'exabm', 'True')
        elif self.__exacm:
            return self.mCheckSubConfigOption('ebtables_setup', 'exacm', 'True')
        elif self.__ociexacc:
            return self.mCheckSubConfigOption('ebtables_setup', 'ociexacc', 'True')
        else:
            return self.mCheckSubConfigOption('ebtables_setup', 'default', 'True')

    def mGetX8u02Config(self):
        if self.__ociexacc:
            return self.mCheckSubConfigOption('x8_u02_size', 'ociexacc')
        elif self.__exacm:
            return self.mCheckSubConfigOption('x8_u02_size', 'exacm')
        else:
            return self.mCheckSubConfigOption('x8_u02_size', 'default')


    def isBM(self):
        _url = 'http://169.254.169.254/opc/v2/instance/region'
        _header = {'Authorization': 'Bearer Oracle'}
        _resp = None
        _tries, _delay, _backoff = 4,3,2
        for _ in range(_tries):
            try:
                _resp =  urlopen(Request(_url, None, _header)).read()
                break
            except:
                ebLogWarn('*** Retrying...Unable to fetch region information from instance metadata')
                time.sleep(_delay)
                _delay *= _backoff

        if _resp is None:
           return False

        return True


    def mCheckSubConfigOption(self, aOption, aSuboption, aValue=None):
        if aValue is None:
            if aOption in list(get_gcontext().mGetConfigOptions().keys()):
                if aSuboption in list(get_gcontext().mGetConfigOptions()[aOption].keys()):
                    return get_gcontext().mGetConfigOptions()[aOption][aSuboption]
                else:
                    return None
            else:
                return None

        if aOption in list(get_gcontext().mGetConfigOptions().keys()):
            if aSuboption in list(get_gcontext().mGetConfigOptions()[ aOption ].keys()):
                if get_gcontext().mGetConfigOptions()[ aOption ][ aSuboption ] == aValue:
                    return True
                else:
                    return False
            else:
                return False
        else:
            return False

    def mCheckConfigOption(self, aOption, aValue=None):
        """
        This function is really two in one. See below overall behavior according to parameters passed.
            1 - Check if parameter aOption is present in the exabox.conf and return it's value.
            2 - Test if parameter aOption is present in exabox.conf and compare it's value to one provided
        :param aOption: Option name
        :param aValue: Value to compare with the current Option name value
        :return:
            if aValue is specified (e.g not None) then return True if aValue == Option.value False otherwise
            return None if aValue is not provided and aOption name does not exist
            return cValue or current value if aValue is not provided and aOption name exist
        """

        if aValue is None:
            if aOption in list(get_gcontext().mGetConfigOptions().keys()):
                return get_gcontext().mGetConfigOptions()[aOption]
            else:
                return None

        if aOption in list(get_gcontext().mGetConfigOptions().keys()):
            if get_gcontext().mGetConfigOptions()[ aOption ] == aValue:
                return True
            else:
                return False
        else:
            return False

    def mIsCdbPdb(self):
        return self.__CDBEnabled

    def mSetCdbPdb(self):
        self.__CDBEnabled = True

    def mGetRequestObj(self):
        return self.__requestobj

    def mSetRequestObj(self, aReqObject):
        self.__requestobj = aReqObject

    def mGetPatchConfig(self):
        return self.__patchconfig

    def mSetPatchConfig(self, aPathConfig):
        self.__patchconfig = aPathConfig

    def mGetCtx(self):
        return self.__ctx

    def mGetOrigDom0sDomUs(self):
        return self._dom0U_list

    def mSetOrigDom0sDomUs(self, aDom0DomUList):
        self._dom0U_list = aDom0DomUList

    def mSetConfig(self, aConfig):
        self.__config = aConfig

    def mGetConfigPath(self):
        return self.__configPath

    def mSetConfigPath(self,aConfigPath):
        self.__configPath = aConfigPath

    def mGetNetDetectError(self):
        return self.__netDetectError

    def mSetNetDetectError(self, aKey, aValue):
        self.__netDetectError[aKey] = aValue

    def mGetCmd(self):
        return self.__cmd

    def mSetCmd(self, aCmd):
        self.__cmd = aCmd

    def mGetHeaders(self):
        return self.__header
    
    def mSetHeaders(self, aHeaders):
        self.__header = aHeaders

    def mGetStorage(self):
        return self.__storage
    
    def mSetStorage(self, aStorage):
        self.__storage = aStorage

    def mGetConfig(self):
        return self.__config

    def mGetNetworks(self):
        return self.__networks

    def mSetNetworks(self, aNetworks):
        self.__networks = aNetworks

    def mGetVMNetConfigs(
            self,
            aVmName: str) -> Mapping[str, ebCluNetworkConfig]:
        """Get network configurations of the given VM.

        Returns a Mapping from network type (e.g. 'client', 'backup', 'admin',
        etc) to the network config (ebCluNetworkConfig) of the networks of the
        given VM (non-NAT name) in the cluster.

        NOTE: the net configs returned are references rather than copies, thus
              any change made to them will be reflected in the exaBoxCluCtrl
              object itself.

        :parm aVmName: non-NAT name of the VM.
        :returns: network configurations.
        :raises Exeception: if something went wrong.
        """
        vm_machine_config = self.mGetMachines().mGetMachineConfig(aVmName)
        vm_net_ids = vm_machine_config.mGetMacNetworks()
        networks = self.mGetNetworks()
        vm_nets = (networks.mGetNetworkConfig(nid) for nid in vm_net_ids)
        _net_configs = {}
        for config in vm_nets:
            if ':' in config.mGetNetIpAddr():
                _key = f'{config.mGetNetType()}_ipv6'
            else:
                _key = config.mGetNetType()
            _net_configs[_key] = config
        _net_types = list(_net_configs.keys())
        for _type in _net_types:
            # Example: For ipv6 only network - with the above iteration - say for a
            # client network, client_ipv6 key is added but client key will not exist
            # Here, we are ensuring atleast 1 'client' key exists
            if '_ipv6' in _type and _type[:-5] not in _net_types:
                _net_configs[_type[:-5]] = _net_configs[_type]
                del _net_configs[_type]
        return _net_configs

    def mGetVMSizesConfig(self):
        return self.__vmsizes

    def mSetVMSizesConfig(self, aVMSizesConfig):
        self.__vmsizes = aVMSizesConfig

    def mGetClusters(self):
        return self.__clusters

    def mSetClusters(self, aClusters):
        self.__clusters = aClusters

    def mGetShortClusterPath(self, aKey):
        return self.__short_cluster_folder.get(aKey)

    def mSetShortClusterPath(self, aKey, aShortClusterPath):
        self.__short_cluster_folder[aKey] = aShortClusterPath

    def mGetMachines(self):
        return self.__machines

    def mSetMachines(self, aMachines):
        self.__machines = aMachines

    def mGetDatabases(self):
        return self.__databases

    def mSetDatabases(self, aDatabases):
        self.__databases = aDatabases

    def mGetDBHomes(self):
        return self.__dbhomes
    
    def mSetDBHomes(self, aDBHomes):
        self.__dbhomes = aDBHomes

    def mGetScans(self):
        return self.__scans
    
    def mSetScans(self, aScans):
        self.__scans = aScans

    def mIsDRNetPresent(self):
        return self.__dr_net_present

    def mSetDRNetPresent(self, aDrNetPresent):
        self.__dr_net_present = aDrNetPresent

    def mGetDRScans(self):
        return self.__drScans
    
    def mSetDRScans(self, drScans):
        self.__drScans = drScans

    def mSetDRVips(self, aDrVips):
        return self.__drVips.update(aDrVips)

    def mGetDRVips(self):
        return self.__drVips

    def mIsIPv6DualStackPresent(self):
        return self.__ipv6_dual_stack_present

    def mSetIPv6DualStackPresent(self, aIPv6Present):
        self.__ipv6_dual_stack_present = aIPv6Present

    def mIsIPv6SingleStackPresent(self):
        return self.__ipv6_single_stack_present

    def mSetIPv6SingleStackPresent(self, aIPv6Present):
        self.__ipv6_single_stack_present = aIPv6Present

    def mGetComponentRegistry(self):
        return self.__CompRegistry

    def mGetSwitches(self):
        return self.__switches

    def mSetSwitches(self, aSwitches):
        self.__switches = aSwitches

    def mGetUsers(self):
        return self.__users

    def mSetUsers(self, aUsers):
        self.__users = aUsers

    def mGetGroups(self):
        return self.__groups

    def mSetGroups(self, aGroups):
        self.__groups = aGroups

    def mGetIloms(self):
        return self.__iloms

    def mSetIloms(self, aIloms):
        self.__iloms = aIloms

    def mGetVerbose(self):
        return self.__verbose

    def mGenerateUUID(self):
        return str(uuid.uuid1())

    def mSetClusterLockFile(self, aFile):
        self.__cluster_lock_files.append(aFile)

    def mGetClusterLockFiles(self):
        return self.__cluster_lock_files

    def mGetUUID(self):
        return self.__uuid

    def mSetUUID(self, aUUID):
        self.__uuid = aUUID
        
    def mSetExaunitID(self,exaunitID):
        self.__exaunit_id = exaunitID
    
    def mGetExaunitID(self):
        return self.__exaunit_id

    def mGetKey(self):
        return self.__key

    def mSetKey(self, aKey):
        # Check that key isn't too long
        if len(aKey) >= 255:
            aKey = aKey[:250] # Saving 5 characters for file extensions
        self.__key = aKey

    def mGetClusterPath(self):
        return self.__cluster_path

    def mSetClusterPath(self, aPath):
        self.__cluster_path = aPath

    def mSetEnableQuorum(self, aFlag):
        self.__enable_quorum = aFlag

    def mSetEnableAsmss(self, aValue):
        self.__enable_asmss = aValue

    def mGetEnableQuorum(self):
        return self.__enable_quorum

    def mGetEnableAsmss(self):
        return self.__enable_asmss

    def mIsExabm(self):
        return self.__exabm

    def mSetExabm(self, aBool):
        self.__exabm = aBool

    def mGetTimeoutEcops(self):
        return self.__timeout_ecops

    def mSetTimeoutEcops(self, aValue):
        self.__timeout_ecops = aValue

    def mIsExaCCMasterCPS(self):

        if os.path.exists("/etc/keepalived/MASTER"):
            return True
        else:
            return False

    def mIsOciEXACC(self):
        return self.__ociexacc

    def mHasNatAndCustomerNet(self):
        return self.__exabm or self.__ociexacc

    def mGetRackNameEcra(self):
        return self.__racknameEcra

    def mSetRackNameEcra(self, aName):
        self.__racknameEcra = aName

    def mGetVmClusterType(self):
        return self.__vmClusterType

    def mSetVmClusterType(self, aVal):
        self.__vmClusterType = aVal

    def mGetEsracks(self):
        return self.__esracks

    def mSetEsracks(self, aEsracks):
        self.__esracks = aEsracks

    def mGetStorageDesc(self):
        return self.__storagedesc

    def mSetStorageDesc(self, aStorageDesc):
        self.__storagedesc = aStorageDesc

    def mGetDbStorage(self):
        return self.__dbstorage

    def mSetDbStorage(self, aVal):
        self.__dbstorage = aVal

    def mSetDebug(self, aValue):
        self.__debug = aValue

    def mIsDebug(self):
        return self.__debug

    def mIsExacm(self):
        return self.__exacm

    def mGetOciExacc(self):
        return self.__ociexacc

    def mSetOciExacc(self, aValue):
        self.__ociexacc = aValue

        if get_gcontext().mCheckRegEntry("ENV_EXACC"):
            get_gcontext().mDelRegEntry("ENV_EXACC")

        get_gcontext().mSetRegEntry('ENV_EXACC', aValue)

    def mIsExaScale(self):
        return self.__exascale

    def mSetExaScale(self, aValue):
        self.__exascale = aValue

        if get_gcontext().mCheckRegEntry("ENV_EXADBXS"):
            get_gcontext().mDelRegEntry("ENV_EXADBXS")

        get_gcontext().mSetRegEntry('ENV_EXADBXS', aValue)

    def mGetFactoryPreprovReconfig(self):
        return self.__factoryPreprovReconfig

    def mBuildClusterId(self, aClusterId=None, aForce=False):

        _mode = self.mCheckConfigOption('legacy_key_mode', 'True')
        if self.__exabm or self.__ociexacc:
            _ddp = self.mReturnDom0DomUNATPair(aForce)
            if _ddp:
                _,_domu = _ddp[0]
                if _domu.lower() == 'undefined':
                    _ddp = self.mReturnDom0DomUPair(aForce, aClusterId)
                    ebLogWarn('*** XXX/MR: FIX ME (CC) ROCE/KVM - Fall back for non NAT compliant system mBuildClusterId()')
        else:
            _ddp = self.mReturnDom0DomUPair(aForce, aClusterId)

        if self.mIsClusterLessXML():
            _dom0s, _, _cells, _ = self.mReturnAllClusterHosts()
            _host_list=''
            if _dom0s:
                _host_list = _dom0s[0].split('.')[0] + _dom0s[-1].split('.')[0]
            elif _cells:
                _host_list = _cells[0].split('.')[0] + _cells[-1].split('.')[0]
            # Check that file name is not too long
            _str_host_list = ''.join(_host_list)
            self.mSetKey(_str_host_list)

            return self.__key

        if not _ddp:
            self.mSetKey('')
            ebLogError("Failed to get list of dom0s and domUs to build cluster dir key")
            return self.__key

        # Only do compressed ClusterID on KVM+OCI for now as ExaCC-OCI uses for
        # X8M the full clusterID in the keys.db
        if self.mIsKVM() and self.__exabm:
            #FirstDomULastDomU
            _host_list = _ddp[0][1].split('.')[0] + _ddp[-1][1].split('.')[0]
            self.mSetKey(_host_list)

        else:
            _length = self.mCheckConfigOption('cluster_length')
            if _length is not None:
                _cluster_length = int(_length)
            else:
                _cluster_length = 245

            _key  = ''
            _key2 = ''
            for _dom0, _domU in _ddp:
                    _key = _key + _dom0.split('.')[0] + _domU.split('.')[0]
                    _key2 = _key2 + _domU.split('.')[0]
            if len(_key) >= _cluster_length:
                # Fix for cluster id greater than 245 characters (e.g. SABRE)
                self.mSetKey(_key2[:_cluster_length])
            else:
                self.mSetKey(_key)

        return self.__key

    def mAppendToHostList(self, aHost):
        _host_list = self.mGetHostList()
        if aHost not in _host_list:
            self.__hosts_list.append(aHost)
        ebLogInfo('*** mAppendToHostList host : ' + aHost + ' Hosts_List : ' + ','.join(self.__hosts_list))

    def mGetHostList(self, aForce=False):
        if not aForce and self.__hosts_list:
            ebLogTrace('*** mGetHostList hosts (non empty self.__hosts_list) : ' +','.join(self.__hosts_list))
            return self.__hosts_list

        _domUs_filtered = []
        if not self.mIsKVM():
            _dom0s, _domUs, _cells, _switches = self.mReturnAllClusterHosts()
        else:
            _dom0s, _domUs, _cells, _ = self.mReturnAllClusterHosts()
            _switches = []

        for _host in _domUs:
            _ctx = get_gcontext()
            if _ctx.mCheckRegEntry('_natHN_' + _host):
                _host = _ctx.mGetRegEntry('_natHN_' + _host)
            _domUs_filtered.append(_host)

        self.__hosts_list = _dom0s + _domUs_filtered + _cells + _switches
        ebLogTrace('*** mGetHostList hosts : ' +','.join(self.__hosts_list))

        return self.__hosts_list

    def mGetClusterName(self):
        return self.__cluster_name

    def mSetClusterName(self, aClusterName):
        self.__cluster_name = aClusterName

    def mParseXMLConfig(self, aOptions, aMinimalParse=False):

        ebLogVerbose('mParseXMLConfig: Parse XML configuration file.')

        self.__parsexml = False
        #
        # Note: __configPath can be set in case we are not using -cf option the XML file path
        #
        if not aOptions.configpath and self.__configPath is None:
            if aOptions.id:
                _cfpath = None
                _cfid   = aOptions.id
                if os.path.exists('clusters/cluster-'+_cfid):
                    _cfpath = 'clusters/cluster-'+_cfid+'/config/'+_cfid+'.xml'
                    if os.path.exists(_cfpath):
                        self.__configPath = _cfpath
                    else:
                        _cfpath = None
                if _cfpath is None:
                    ebLogError('::mCreateCluster Cluster configuration required none provided')
                    raise ExacloudRuntimeError(0x0200, 0x0A, "Cluster configuration was not provided")
                else:
                    if self.__verbose:
                        ebLogInfo('*** CF: %s' % (_cfpath))
            else:
                ebLogError('::mCreateCluster Cluster configuration required none provided')
                raise ExacloudRuntimeError(0x0200, 0x0A, "Cluster configuration was not provided")

        _configdata = None
        if self.__configPath is not None:
            _configpath = self.__configPath
        else:
            _configpath = aOptions.configpath

        try:
            self.__config = exaBoxClusterConfig(self.__ctx, _configpath)
            self.__configPath = os.path.abspath(self.__config.mGetConfigPath())
        except Exception as e:
            ebLogError('::mCreateCluster Fatal Error could not access/process configuration file: '+_configpath)
            ebLogError('>>> '+str(e))
            raise

        if not self.__config.mConfigRoot():
            ebLogError('::mCreateCluster Fatal Error could not access configuration file')
            raise ExacloudRuntimeError(0x0200, 0x0A, "Cluster configuration could not be accessed")

        # Header
        self.__header = ebCluHeaderConfig(self.__config)

        # Machines
        self.__machines = ebCluMachinesConfig(self.__config)

        # Clusters
        self.__clusters = ebCluClustersConfig(self.__config, aOptions)

        # Scans Clusters Config
        self.__scans    = ebCluClusterScansConfig(self.__config)

        # DatabaseHomes
        self.__dbhomes   = ebCluDatabaseHomesConfig(self.__config)

        # Databases
        self.__databases = ebCluDabasesConfig(self.__config)

        # Storage
        self.__storage = ebCluStorageConfig(self, self.__config)

        # Networks
        self.__networks = ebCluNetworksConfig(self.__config)

        # VM Size
        self.__vmsizes = ebCluVMSizesConfig(self.__config)

        # Switches
        self.__switches = ebCluSwitchesConfig(self.__config)

        # EsRacks
        self.__esracks = ebCluEsRacksConfig(self.__config)

        # Users
        self.__users = ebCluUsersConfig(self.__config)

        # groups
        self.__groups = ebCluGroupsConfig(self.__config)

        # iloms
        self.__iloms = ebCluIlomsConfig(self.__config)

        # Populate quick access field
        self.__cluster_name = self.__header.mGetHeaderCustomerName()

        # Storage Desc.
        self.__storagedesc = ebCluStorageDesc(self.__config)

        if aMinimalParse:
            return

        # Detect Elastic Shape
        _elasticShapeText = self.__header.mGetXmlElasticShape()
        if _elasticShapeText.upper() == "TRUE":
            self.mSetIsXmlElasticShape(True)
        else:
            self.mSetIsXmlElasticShape(False)

        # Refresh dom0 domU pair affected by the flag of Elastic Shape
        self.mReturnDom0DomUPair(aForce=True)
        self.mReturnDom0DomUNATPair(aForce=True)

        # Signature only on Dev/QA
        if not self.mEnvTarget():
            self.__header.mSetSkipXmlSignature(not self.mCheckConfigOption('skip_xml_signature', "False"))
            self.__header.mGenerateSignature(self.__cmd)

        # Compute Cluster signature (self.__key)
        self.mBuildClusterId()

        #
        # BM XML Support (if customer field in json then triggers and set bmmode
        #
        _jconf = aOptions.jsonconf
        if _jconf is not None:
            _jconf_keys = list(_jconf.keys())
        else:
            _jconf_keys = None

        #if something fails on patch, it would change the params
        try:
            if _jconf_keys is not None:
                if 'nodes' in _jconf.get('customer_network', {}):
                    _netinfo = _jconf['customer_network']
                elif 'network' in _jconf_keys:
                    _netinfo = _jconf['network']
                else:
                    _netinfo = None
                # dev_run is passed as value for customer_network for ETF runs with Gen2 create service operation.
                if _netinfo == "dev_run":
                    _netinfo = None
                if _netinfo:
                    # For OCICC, we rely on parameter to be more deterministic
                    if not _netinfo['nodes'][0]['client'].get("mac"):
                        ebLogInfo('*** OCICC CUSTOMER NETWORK INFO IN PAYLOAD')
                        if not self.__ociexacc:
                            raise Exception('OCI CC Payload detected while OCICC exacloud parameter is not set')
                        else:
                            self.mCustomerNetworkXMLUpdate(aOptions)
                    else:
                        ebLogInfo('*** BM MODE ENABLED BASED ON CUSTOMER NETWORK INFO IN PAYLOAD')
                        self.__exabm = True
                        if self.isBaseDB() or self.isExacomputeVM():
                            self.mGetBaseDB().mCustomerNetworkXMLUpdateBaseDB(aOptions)
                        else:
                            self.mCustomerNetworkXMLUpdate(aOptions)

                if 'kvmroce' in _jconf_keys:
                    if not ebCluCmdCheckOptions(self.__cmd, ['skip_kvm_xml_patch']):
                        ebLogInfo('*** KVM-ROCE MODE ENABLED BASED ON INFO IN PAYLOAD')
                        self.mKvmRoceXMLUpdate(aOptions)
                    else:
                        ebLogInfo('*** KVM-ROCE MODE ENABLED BASED ON INFO IN PAYLOAD')
                        ebLogInfo('*** Skip Patch of KVM')

        except Exception as e:
            _reqobj = self.mGetRequestObj()
            if _reqobj is not None and isinstance(_reqobj.mGetParams(), dict) and 'jsonconf' in _reqobj.mGetParams():
                #Update the params
                _params = _reqobj.mGetParams()
                _params['error_jsonconf'] = _params['jsonconf']
                _params['jsonconf'] = ""
                _reqobj.mSetParams(_params)

                #Save on the DB
                _db = ebGetDefaultDB()
                _db.mUpdateParams(_reqobj)

            #print the exception and the stacktrace
            _error_str = f'Error on Patch XML: "{e}"'
            ebLogError(f'{_error_str}: Traceback "{traceback.format_exc()}"')
            raise

        if not (self.__exabm or self.__ociexacc):
            for _, _domu in self.mReturnDom0DomUPair():
                _domu_conf = self.__machines.mGetMachineConfig(_domu)
                _domu_conf_net = _domu_conf.mGetMacNetworks()
                for _net_id in _domu_conf_net:
                    _net_conf = self.__networks.mGetNetworkConfig(_net_id)
                    # Check first HiggsExtraVif as bm has NAT IP
                    if _net_conf.mGetNetVswitchNetworkParams() == 'UNDEFINED' and _net_conf.mGetNetNatHostName(aFallBack=False) is not None:
                        # This path will be reached only if ociexacc is False
                        ebLogInfo('*** BM MODE ENABLED BASED ON XML NAT PRESENT IN DOMU CONFIG')
                        self.__exabm = True
                        break
                if self.__exabm:
                    break

        self.__parsexml = True
        
        # Below code is using the clusters detail in xml -> which is not present in case of ClusterlessXML and BaseDB xml
        if not self.mIsClusterLessXML() and not self.isBaseDB() and not self.isExacomputeVM():
            # Enabled ASM Scope security if xml asmScopeSecurity tag contains true
            _asmss = self.mGetClusters().mGetCluster().mGetCluAsmScopedSecurity()  
            self.mSetEnableAsmss(_asmss)
            ebLogInfo(f"ASM Scope Security set to {_asmss}")

        ebLogVerbose('mParseXMLConfig: Parsing of XML configuration file completed.')

        if self.mIsOciEXACC():
            _cell_list = self.mReturnCellNodes()
            if not _cell_list and not self.mIsXS():
                ebLogInfo("ExaCC: No cell configured in cluster, activating ExaScale flag")
                self.mSetExaScale(True)

        return 0


    def mApplyCommandsOedacli(self, aSuffix="", aOptions=None):
        # For Exascale clusters we prefer to patch the XML for each run in
        # order to preserve the <exascale> section in case the XML comes from
        # a different ECRA stack.
        if ebCluCmdCheckOptions(self.__cmd, ['no_apply_oedacli']) and not self.mIsExaScale():
            ebLogInfo(f"*** Operation is {self.__cmd} - not running apply oedacli")
            return 0

        _options = self.mGetArgsOptions()
        if _options and self.__cmd == "vm_cmd" and _options.vmcmd in ['prepare_move'] and ebVmCmdCheckOptions(_options.vmcmd, ['no_apply_oedacli']) and self.mIsExaScale():
            ebLogInfo(f"*** Operation is {_options.vmcmd} - not running apply oedacli")
            return 0

        # Write new XML of OEDA
        if self.mGetUiOedaXml() and not self.mIsClusterLessXML():

            ebLogInfo("Apply oedacli commands to XML")

            _localprfx = 'log/ui_oedaxml_{0}'.format(self.__uuid)

            _oeda     = "{0}/oeda{1}.xml".format(_localprfx, self.__ui_oedaxml_timestamp)
            _exacloud = "{0}/exacloud{1}.xml".format(_localprfx, self.__ui_oedaxml_timestamp)
            _patched  = "{0}/patched{1}{2}.xml".format(_localprfx, self.__ui_oedaxml_timestamp, aSuffix)
            _final    = "{0}/final{1}{2}.xml".format(_localprfx, self.__ui_oedaxml_timestamp, aSuffix)

            self.__config.mWriteConfig(_patched)

            _oedacli_bin = self.__oeda_path + '/oedacli'
            _oedacli = ebOedacli(_oedacli_bin, _localprfx, aLogFile="oedacli_ui_oedaxml.log")

            _commandGenerator = ebCommandGenerator(_exacloud, _patched, _oeda, _oedacli)
            _commandGenerator.mSetExtraCommands(self.__extraXmlPatchingCommands)
            _commandGenerator.mSetPreExtraCommands(self.__extraXmlPrePatchingCommands)
            _commandGenerator.mPatchOriginalOeda()

            #Update patch config with new one
            _commandGenerator.mGetOedaFinalTree().mExportXml(_final)
            _commandGenerator.mGetOedaFinalTree().mExportXml(self.__patchconfig)

            if self.mIsExaScale() or \
               (self.__cmd == "elastic_cell_update" and \
               aOptions.steplist is not None and \
               "CONFIG_CELL" in str(aOptions.steplist)) or \
               (self.__cmd == "storage_resize"):

                self.mSetConfigPath(self.__patchconfig)
                self.mParseXMLConfig(aOptions)

            if self.mIsIPv6DualStackPresent():
                # Update in memory network object with ipv6 network info
                self.mUpdateInMemoryXmlConfig(self.__patchconfig, aOptions)

    def mUIOedaCliXmlCleanUp(self):
        if self.mGetUiOedaXml():
            if not self.__debug:
                _localprfx = 'log/ui_oedaxml_{0}'.format(self.__uuid)
                self.mExecuteLocal("/bin/rm -rf {0}".format(_localprfx), aCurrDir=self.mGetBasePath())
                ebLogInfo("*** Deleting UI_OEDACLI files under: {0}".format(_localprfx))

    def mGenerateExacloudXML(self, aOptions):

        if self.mGetUiOedaXml() and \
           not self.mIsClusterLessXML() and \
           not ebCluCmdCheckOptions(self.mGetCmd(), ['skip_uioeda_xml']) and \
           not ebCluCmdCheckOptions(self.mGetCmd(), ['instant_commands']):
            # skip generate xml for collect log flow to avoid multiple worker process

            if self.mIsSkipXmlPatching():
                ebLogInfo("Skip mGenerateExacloudXML because XML patching is disabled")
                return
            
            #Patch oeda with exascale properties for 'prepare_move' command
            if aOptions.vmcmd and self.mGetCmd() == "vm_cmd" and aOptions.vmcmd == "prepare_move":
                _exascale = ebCluExaScale(self)
                _exascale.mCreateOedaProperties()
                return

            _oconfigpath= self.__configPath
            self.__extraXmlPatchingCommands = []
            self.__extraXmlPrePatchingCommands = []

            if _oconfigpath is None:
                _oconfigpath = aOptions.configpath

            if _oconfigpath is None and os.path.exists('clusters/cluster-'+aOptions.id):
                _oconfigpath = 'clusters/cluster-'+aOptions.id

            if _oconfigpath is None:
                ebLogError('::mCreateCluster Cluster configuration required none provided')
                return -1

            #Generate the exacloud elementaltree
            _localprfx = os.path.abspath('log/ui_oedaxml_{0}'.format(self.__uuid))

            self.__ui_oedaxml_timestamp = str(time.time()).replace(".", "")

            self.mExecuteLocal("/bin/mkdir -p {0}".format(_localprfx), aCurrDir=self.mGetBasePath())
            self.mExecuteLocal("/bin/cp {0} {1}/oeda{2}.xml".format(_oconfigpath, _localprfx, self.__ui_oedaxml_timestamp), aCurrDir=self.mGetBasePath())

            _exportExacloud = ebExportExacloud(_oconfigpath,aDebug=self.__debug)
            _exacloudTree   = _exportExacloud.mOedaToExacloud()

            self.__configPath = "{0}/exacloud{1}.xml".format(_localprfx, self.__ui_oedaxml_timestamp)
            self.mSetPatchConfig(self.__configPath)

            _exacloudTree.mExportXml(self.__configPath)
            ebLogInfo("Generate Exacloud XML on: {0}".format(self.__configPath))


    def mCreateCluster(self, aOptions):

        _rc = 0
        _jconf = aOptions.jsonconf
        if _jconf is not None:
            _jconf_keys = list(_jconf.keys())
        else:
            _jconf_keys = None
        #
        # Skip mCreateCluster if no XML configuration is provided - this happens for example w/ aCmd=version
        #
        if self.__skip_xml_checks:
            return 0
        if not self.__parsexml:
            if aOptions.debug:
                ebLogInfo('*** Skipping ::mCreateCluster no Cluster XML configuration provided (e.g. aCmd=version?)')
            return -1

        if aOptions.debug:
            ebLogInfo('mCreateCluster: Starting Cluster creation')
            ebLogInfo('Using Cluster Configuration: ' + self.__configPath)

        # Parameter/JSON override (ddbv)
        _dbd_version = self.mCheckConfigOption('default_db_version')
        if _dbd_version is not None:
            if _jconf_keys is not None and 'dbParams' in _jconf_keys: # Fix jcnf iter
                _jconf2_keys = _jconf['dbParams']
            else:
                _jconf2_keys = None
            if _jconf2_keys is not None and ('version' in _jconf2_keys or 'dbVersion' in _jconf2_keys):
                _jconf['version'] = _dbd_version
                _jconf['dbVersion'] = _dbd_version
                ebLogWarn('*** DB Version override enabled - version set to : %s' % (_dbd_version))

        if ebCluCmdCheckOptions(self.mGetCmd(), ['instant_commands']):
            return _rc

        if self.isBaseDB() or self.isExacomputeVM():
            # Patch XM Cluster Configuration (note: this also write/create the new __patchconfig file)
            self.mGetBaseDB().mPatchClusterConfig(aOptions)
            _db = ebGetDefaultDB()
            _db.import_file(self.__patchconfig)
            self.__remoteconfig = self.__oeda_path+'/exacloud.conf/'+os.path.basename(self.__patchconfig)
            self.mExecuteCmd('/bin/mkdir -p '+self.__oeda_path+'/exacloud.conf')
            self.mCopyFile(self.__patchconfig, self.__remoteconfig)
            ebLogTrace('mCreateCluster: Completed for BaseDB.')
            return _rc

        # Patch XML Cluster Configuration (note: this also write/create the new __patchconfig file)
        self.mPatchClusterConfig(aOptions)
        _db = ebGetDefaultDB()
        _db.import_file(self.__patchconfig)

        #
        # !!! WARNING !!! Past this _point_ the XML Cluster Configuration has been saved anything
        # below should _NOT_ change the XML Cluster Configuration file.
        #
        self.__remoteconfig = self.__oeda_path+'/exacloud.conf/'+os.path.basename(self.__patchconfig)
        self.mExecuteCmd('/bin/mkdir -p '+self.__oeda_path+'/exacloud.conf')
        self.mCopyFile(self.__patchconfig, self.__remoteconfig)

        _jconf = aOptions.jsonconf
        if _jconf and 'dbParams' in list(_jconf.keys()) and 'dbname' in list(_jconf['dbParams'].keys()):
            if 'db_unique_name' in list(_jconf['dbParams'].keys()):
                self.__dbname = _jconf['dbParams']['db_unique_name']
            else:
                self.__dbname = _jconf['dbParams']['dbname']
        else:
            self.__dbname = 'undefined'
        #
        # Update ExaCS subtype for OCDE/DBAAS/..
        #
        if _jconf is not None and 'dbParams' in list(_jconf.keys()):
            if self.__exabm:
                _subtype = 'exabm'
            elif self.__exacm:
                _subtype = 'exacm'
            else:
                _subtype = 'exaopc'
            _jconf['dbParams']['envsubtype'] = _subtype
            if self.__debug:
                ebLogDebug('*** ENV_SUBTYPE SET TO: %s' % (_subtype))

        ebLogVerbose('mCreateCluster: Completed.')

        return _rc

    def mReadDynDepConfig(self):

        #
        # Read configurarion file for dyn. dependencies file
        #
        # NOTE: THIS FILE CAN NOT BE CACHED !!!
        #
        _d = None
        try:
            with open('config/dyndep.conf') as _f:
                _d = json.loads(_f.read())
        except:
            ebLogError('*** Can not access/read dyndep.conf file')
            return {}

        # GlobalCache change
        _global_cache = self.mCheckConfigOption("global_cache_dom0_folder")

        if _global_cache:

            _strd = json.dumps(_d)
            _strd = _strd.replace("<GlobalCache>", _global_cache.rstrip("/"))
            _d = json.loads(_strd)

        return _d

    def mSetupBDCSTree(self):
        for _, _domu in self.mReturnDom0DomUPair():

            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domu)
            _node.mExecuteCmdLog('chmod +x /u02/opt/bdcs/bdsql-edcs-install.sh')
            _node.mExecuteCmdLog('chmod +x /u02/opt/bdcs/json-select')
            _node.mExecuteCmdLog('mkdir -p /u02/opt/bdcs/bin')
            _node.mExecuteCmdLog('mkdir -p /u02/opt/bdcs/spec')
            _node.mExecuteCmdLog('cp /u02/opt/bdcs/json-select /u02/opt/bdcs/bin/json-select')
            _node.mDisconnect()

    def mExtraRPMsConfig(self,aOptions, aUndo=False):

        if self.mCheckConfigOption('extra_rpms_config','False'):
            ebLogWarn('*** !!! EXTRA RPMS CONFIG DISABLED !!! ***')
            return

        _rpm_list = self.mDynDepNonImageList(['rpm'])

        if aUndo:
            _cmd = 'rpm -e '
        else:
            _cmd = 'rpm --force -Uhv '

        for _, _domu in self.mReturnDom0DomUPair():

            if aUndo and not self.mPingHost(_domu):
                ebLogWarn("*** Node:{0} is not pingable, skipping RPM removal process ***".format(_domu))
                continue

            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domu)

            _rpmfile = []
            for _file in _rpm_list:
                if 'bdcs' in list(_file.keys()):
                    if _file['bdcs'] == 'True':
                        ebLogInfo('*** mExtraRPMsConfig: Installing BDCS specific RPM: %s' % (_file['dom0'].split('/')[-1]))
                        _rpmfile.append('/u02/opt/bdcs/'+_file['dom0'].split('/')[-1])
                    else:
                        continue
                elif 'managed' in list(_file.keys()):
                    if _file['managed'] == 'True':
                        ebLogInfo('*** mExtraRPMsConfig: Extracting Managed specific RPMs from: %s' % (_file['dom0'].split('/')[-1]))
                        _node.mExecuteCmdLog('tar -xzvf {0}/{1} -C {0}'.format('/u02/opt/dbaas_images/managed', _file['dom0'].split('/')[-1] ))
                        _node.mExecuteCmdLog('rm -f /u02/opt/dbaas_images/managed/'+_file['dom0'].split('/')[-1])
                        _rpmfile.append('/u02/opt/dbaas_images/managed/*.rpm')
                    else:
                        continue
                elif 'service' in list(_file.keys()):
                    if 'ExaCC' in _file['service'] and self.mIsOciEXACC():
                        ebLogInfo('*** mExtraRPMsConfig: Installing ExaCC specific RPM: %s' % (_file['dom0'].split('/')[-1]))
                        _rpmfile.append('/u02/opt/dbaas_images/'+_file['dom0'].split('/')[-1])
                    elif 'ATP' in _file['service'] and self.isATP():
                        ebLogInfo('*** mExtraRPMsConfig: Installing ADB/ATP specific RPM: %s' % (_file['dom0'].split('/')[-1]))
                        _rpmfile.append('/u02/opt/dbaas_images/'+_file['dom0'].split('/')[-1])
                    else:
                        ebLogInfo('*** mExtraRPMsConfig: Skip specific RPM: %s' % (_file['dom0'].split('/')[-1]))
                        continue
                else:
                    _rpmfile.append('/u02/opt/dbaas_images/'+_file['dom0'].split('/')[-1])

            for _rpm in _rpmfile:
                _cmd_str = _cmd + _rpm
                _node.mExecuteCmdLog(_cmd_str)

            _node.mDisconnect()


    def mFirewallAgentRunning(self):

        for _dom0, _ in self.mReturnDom0DomUPair():
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            _cmdstr = '/etc/init.d/agent_init status| grep running'

            _, _o, _e = _node.mExecuteCmd(_cmdstr)
            if _node.mGetCmdExitStatus():
                ebLogInfo('Firewall cmd %s returned %s, error %s' % (_cmdstr, str(_o.readlines()), str(_e.readlines())))
                ebLogError('Firewall agent install check Failed Node: %s , cmd %s' % (_node.mGetHostname(), _cmdstr))
                _node.mDisconnect()
                raise ExacloudRuntimeError(0x0110, 0xA, 'Firewall agent not running')

            ebLogInfo('Firewall agent running on %s. cmdout:%s' % (_node.mGetHostname(), _o.readlines()))
            _node.mDisconnect()
        return 'FirewallPresent'

    """
        Classify the received image as "nid" or "klone".

        _image is supposed to have the following attributes:
            filename, shortversion, longversion, map, dom0, local, atp and cdb

        If no matching is possible, return None
    """
    def mGetImageGroup (self, _image):
        # This is not an image entry, return None
        if not 'filename' in _image or not _image['filename']:
            return None

        return 'klone' if 'klone' in _image['filename'] else None

    """
        Simple check for service type being provisioned,
        returns a string consistent with service type
        (supplied per-image) in inventory.json.

        Possible return values so far: "ATP" & "EXACS",
        can include more service types in the future if needed.
    """
    def mGetServiceType(self):
        return "ATP" if self.isATP() else "EXACS"
    #mGetServiceType

    """
        Builds a list of image artifacts based off the current setup

        _required_groups: list of groups/tags to be included as result of this method.
            Valid values are:
                - nid
                - klone
    """
    def mDynDepImageList (self, _required_groups):
        _img_files = list(self.__imagefiles)

        _file_list = []
        for _img_file in _img_files:
            _img_group = self.mGetImageGroup(_img_file)
            # If current image group is not required, skip the entry
            if _img_group not in _required_groups:
                ebLogDebug(f"*** Image group: {_img_group} is not required. Skipping file {_img_file['filename']}")
                continue

            # Check if image is compatible with service.
            if ('service' in _img_file.keys()) and (self.mGetServiceType() not in _img_file['service']):
                continue
            #if

            # Check if GI/DB version is supported in cluster.
            # Converting to int, if this is already an int, this is a no-op
            if (not self.__over18cSupported) and (int(_img_file['shortversion']) > 181):
                ebLogDebug('*** Image short version ' + str(_img_file['shortversion']) +
                           ' is higher than supported version (18.1). ' + 'Skipping file ' + _img_file['filename'])
                continue
            #if

            # Reaching this point means the file should be added to the list
            _file_list.append(_img_file)
        # end for

        return _file_list

    """
        Classify the received dependency artifact on a group.

        Most of the classification will happen through the _dependency_key (coming from dyndep.conf)

        Known return values are:
            - db_templates
            - ocde_bits
            - db_homes
            - gi_homes
            - security_rpms
            - rpm
            - bdcs
            - version

        If no matching is possible, return None
    """
    def mGetDependencyGroup(self, _dependency_key):
        # These are the potential groups. We will look in the _dependency_key
        # for a substring match to identify the group this artifact belongs
        _groups = ['db_templates', 'ocde_bits', 'db_homes', 'gi_homes',
                   'security_rpms', 'rpm', 'bdcs', 'version']

        for _group in _groups:
            if _group in _dependency_key:
                return _group

        return None

    # Similar as mDynDepImageList, but in this case we do not care about versioning, ATP distinction and we also handle
    # special code cases as the RPM ones
    """
        _required_groups: list of groups to be included as result of this method.
            Valid values would be:
                - db_templates
                - ocde_bits
                - db_homes
                - gi_homes
                - security_rpms
                - rpm
                - bdcs
                - version
    """
    def mDynDepNonImageList (self, _required_groups):
        _dep_files = dict(self.__dyndepfiles)
        if _dep_files is None or _dep_files == {}:
            ebLogWarn('*** Dyndep configuration file not found ***')
            return

        _file_list = []
        for _dep_file_key, _dep_file in list(_dep_files.items()):
            _dependency_group = self.mGetDependencyGroup(_dep_file_key)
            # If current image group is not required, skip the entry
            if _dependency_group not in _required_groups:
                continue

            # Verify whether internally to this dependency there is an ol6/ol7 distinction, otherwise go with the
            # entry as-is
            # Some old/out-of-support RPMs have CVEs, hence, removed from list
            if any(k in _dep_file for k in ('ol6', 'ol7', 'ol8')):
                _majorityhostVersion = self.mGetMajorityHostVersion(ExaKmsHostType.DOMU)
                if not _majorityhostVersion:
                    raise ExacloudRuntimeError(0x0119, 0xA, 'VMCluster image not specified or default_domu_img_version_last_res not set')
                _majorityVersion = _majorityhostVersion.lower()
                if _majorityVersion in _dep_file and _dep_file[_majorityVersion]:
                    _file_list.append(_dep_file[_majorityVersion])

            else:
                _file_list.append(_dep_file)
        # end for

        return _file_list

    def mDyndepFilesList(self):
        _image_list = self.mDynDepImageList(['klone'])
        if not _image_list:
            ebLogWarn('*** Image list is empty. Skipping rest of dependency processing ***')
            return None, {}
        # Remove klones version 18c. They were never part of this list
        _image_list = [_file for _file in _image_list if self.mGetImageGroup(_file) != 'klone' or int(_file['shortversion']) != 181]

        _non_image_list = self.mDynDepNonImageList(['db_templates', 'bdcs', 'rpm', 'ocde_bits'])

        return self.mDynDepNonImageList(['version'])[0], _image_list + _non_image_list

    def mUpdateDepFiles(self):

        if self.mCheckConfigOption('skip_dyn_dep','True'):
            ebLogWarn('*** !!! DEP FILES HAS BEEN DISABLED !!! ***')
            return

        _dyndep_version, _file_list = self.mDyndepFilesList()
        self.__dyndep_version = _dyndep_version

        if _file_list is None or _file_list == {}:
            ebLogWarn('*** Image configuration not found - skipping images/bits update and copy')
            return

        #
        # Parallel file upload
        #
        self.mParallelFileLoad()
        ebLogInfo('*** Update of dyndep components file to remote nodes completed')

    #
    # mParallelFileLoad:
    # Copy images to Dom0 with Manager and Child Process
    #
    def mParallelFileLoad(self):

        self.mAcquireRemoteLock()
        _globalCacheFactory = GlobalCacheFactory(self)

        try:

            _globalCacheFactory.mCreatePasswordless()
            _globalCacheFactory.mValidateImageInventory()
            _globalCacheFactory.mDoParallelCopy()

        finally:
            _globalCacheFactory.mCleanPassordless()
            self.mReleaseRemoteLock()


    def mCreateImage(self, aNode, aPath, aSize):
        '''Create vDisk

        Default format is raw new 12.2 dom0 system include new qemu-img not
        supporting old -F raw format remove -F format to make it work on 12.1
        and 12.2 dom0 sys img.
        '''
        _cmd = 'qemu-img create ' + aPath + ' ' + aSize
        _, _o, _e = aNode.mExecuteCmd(_cmd)
        _rc = aNode.mGetCmdExitStatus()
        if not _rc:
            _, _o, _e = aNode.mExecuteCmd('mkfs.ext4 -F ' + aPath + ' 2>&1 > /dev/null')
            _rc = aNode.mGetCmdExitStatus()
            if _rc:
                _, _o, _e = aNode.mExecuteCmd('umount --force ' + aPath)
                _, _o, _e = aNode.mExecuteCmd('mkfs.ext4 -F ' + aPath + ' 2>&1 > /dev/null')
                _rc = aNode.mGetCmdExitStatus()
        if not _rc:
            _, _o, _e = aNode.mExecuteCmd('fsck.ext4 -p -v ' + aPath)
            _rc = aNode.mGetCmdExitStatus()
        if _rc:
            _error_str = '*** vDisk image creation fail (mkimg): ' + str(_o.readlines()) + ' ' + str(_e.readlines())
            ebLogError(_error_str)
            raise ExacloudRuntimeError(0x0204, 0x0A, _error_str)

    def mFreeDevs(self, disk_config):
        '''Generator returning the free devices to use due a given disk
        configuration (from Xen vm file)'''
        _existing = set()
        for disk in disk_config:
            _, device, _ = disk.split(',')
            _existing.add(device)
        for l in string.ascii_lowercase:
            dev = 'xvd' + l
            if dev not in _existing:
                try:
                    yield dev
                except StopIteration:
                    return

    def mSaveVMCfg(self, aNode, aDomU, aRawConfig):
        '''Save the modified vm.cfg and push it to remote dom0/host'''

        _ntmp = NamedTemporaryFile(delete=False)
        _ntmp.file.write(six.ensure_binary(aRawConfig))
        _ntmp.file.close()
        _vmPath = '/EXAVMIMAGES/GuestImages/' + aDomU
        _vmCfgPath = _vmPath + '/vm.cfg'
        _vmCfg_Backup = _vmPath + '/vm.cfg.prev' + self.__uuid
        aNode.mExecuteCmd('cp ' + _vmCfgPath + ' ' + _vmCfg_Backup)
        if self.mIsDebug():
            ebLogDebug('cp ' + _vmCfgPath + ' ' + _vmCfg_Backup)
        aNode.mCopyFile(_ntmp.name, _vmCfgPath)
        os.unlink(_ntmp.name)
        assert(not os.path.exists(_ntmp.name))

        if self.mCheckConfigOption('vm_cfg_prev_limit') not in ["0", ""]:
            try:
                _find_cmd = node_cmd_abs_path_check(node=aNode, cmd="find")
                _xargs_cmd = node_cmd_abs_path_check(node=aNode, cmd="xargs")
                _ls_cmd = node_cmd_abs_path_check(node=aNode, cmd="ls")
                # Find all vm.cfg.prev* files in reverse order of their creation time (Recent to Oldest)
                _cmd_str = f'{_find_cmd} {_vmPath} -name \"vm.cfg.prev*\" | {_xargs_cmd} {_ls_cmd} -t'

                _i, _o, _e = aNode.mExecuteCmd(_cmd_str)
                _vm_cfg_prev_files = [_line.strip() for _line in _o.readlines()]
                _vm_cfg_prev_limit = int(self.mCheckConfigOption('vm_cfg_prev_limit'))
                if len(_vm_cfg_prev_files) > _vm_cfg_prev_limit:
                    ebLogInfo(f"*** The number of vm.cfg.prev files present in {aNode} {len(_vm_cfg_prev_files)} are greater than the maximum permissible limit of {_vm_cfg_prev_limit} . Hence, Cleaning the older vm.cfg.prev files.")
                    for _vm_cfg_prev_file in _vm_cfg_prev_files[_vm_cfg_prev_limit:]:
                        _rm_cmd = node_cmd_abs_path_check(node=aNode, cmd="rm")
                        aNode.mExecuteCmd(f"{_rm_cmd} -f {_vm_cfg_prev_file}")
                    ebLogInfo(f"*** Cleanup of the older vm.cfg.prev files is Successful in {aNode} ***")
            except Exception as e:
                ebLogError(f"*** Error in removing older vm.cfg.prev files in {aNode}: {str(e)}")
        else:
            ebLogInfo(f"*** Skipping vm.cfg.prev file cleanup ***")

    def mCreateExtraImageTemplate(self, aNode, aDomU):
        '''Create a new template image using LVM, ext4 and labels, ment to be
        used in discrete partitions

        Note:
            This function will restart the given domU, be sure to reconnect
            your `Node` instances after calling this method
        '''
        _img_name = 'discrete_partitions.img'
        _tmpl_name = _img_name + '.tmpl'
        _guestimg = '/EXAVMIMAGES/GuestImages/'
        _tmpl_path = os.path.join(_guestimg, _tmpl_name)
        _ximg_path = os.path.join(_guestimg, aDomU, _img_name)
        if aNode.mFileExists(_tmpl_path):
            aNode.mExecuteCmdLog("rm " + _tmpl_path)
            if not (aNode.mGetCmdExitStatus()==0):
                raise ExacloudRuntimeError(0x0400, 0xA, "Could not delete file " + _tmpl_path)
        if not aNode.mFileExists(_tmpl_path):
            _total_size = 0
            for _, _size in self._additional_disks:
                _total_size += int(_size[:-2])
            self.mCreateImage(aNode, _tmpl_path, str(_total_size) + 'G')

            _i, _o, _e = aNode.mExecuteCmd('losetup -f')
            _loop_dev = _o.readlines()[0].strip()
            aNode.mExecuteCmdLog('losetup {} {}'.format(_loop_dev, _tmpl_path))
            aNode.mExecuteCmdLog('pvcreate {}'.format(_loop_dev))
            _vg_name = 'VGExaDbExternal'
            aNode.mExecuteCmdLog('vgcreate {} {}'.format(_vg_name, _loop_dev))
            for _mp, _size in self._additional_disks[:-1]:
                _mp_name = _mp.title().replace('/','')
                _lg_name = 'LVExternal{}'.format(_mp_name)
                aNode.mExecuteCmdLog('lvcreate -L {} -n {} {}'.format(_size, _lg_name, _vg_name))
                aNode.mExecuteCmdLog('mkfs.ext4 /dev/mapper/{}-{}'.format(_vg_name, _lg_name))

            # Last LV should be 100%FREE istead of fixed number
            _mp, _size = self._additional_disks[-1]
            _mp_name = _mp.title().replace('/','')
            _lg_name = 'LVExternal{}'.format(_mp_name)
            aNode.mExecuteCmdLog('lvcreate -l 100%FREE -n {} {}'.format(_lg_name, _vg_name))
            aNode.mExecuteCmdLog('mkfs.ext4 /dev/mapper/{}-{}'.format(_vg_name, _lg_name))

            # Deactivate the new disk
            aNode.mExecuteCmdLog('vgchange -an VGExaDbExternal')
            aNode.mExecuteCmdLog('losetup -d {}'.format(_loop_dev))

        aNode.mExecuteCmdLog('reflink {} {}'.format(_tmpl_path, _ximg_path))
        return _ximg_path

    ## remove disk entries from vm.cfg disk value
    def mRemoveDBHomesFromVM(self, aCfg, aNode, aDomU):
        
        def remove_vm_disk(aFile, dData, aCfg=None):
            '''
            returns the disk data with the entry corresponding to the file removed
            '''
            _disk_data = [x for x in dData if not aFile in x]
            if aCfg:
                aCfg.mSetValue('disk', str(_disk_data))
            return _disk_data

        def remove_fstab_entry(aDevice, aDomU):
            '''
            removes a device entry from the /etc/fstab file of VM
            '''
            _vmnode = exaBoxNode(get_gcontext())
            _vmnode.mConnect(aDomU)
            ebLogInfo("Remove %s from /etc/fstab on %s" % (aDevice, aDomU))
            _vmnode.mExecuteCmdLog('[ ! -f /etc/fstabremoveOH.bak ] && cp -a /etc/fstab /etc/fstab.removeOH.bak; cat /etc/fstab | grep -v %s > /etc/fstab.orig; cp /etc/fstab.orig /etc/fstab' % (aDevice))
            _vmnode.mDisconnect()

        def remove_dbhome_vdisks(aDiskData, aNodeContext, aDomU):
            '''
            returns the updated disk data after removing virtual disk images 
            that match the pattern db\d\d.*.img
            '''
            _rtn_ddata = aDiskData
            for _disk in aDiskData:
                _d_attr = _disk.split(",")
                _img_file = _d_attr[0].split(":")[1]
                ebLogDebug("Checking :" + _img_file)
                _fin, _fout, _ferr = aNodeContext.mExecuteCmd("readlink -f " + _img_file)
                _out = _fout.readlines()
                _err = _ferr.readlines()
                if _err:
                    ebLogError("err: " + _img_file + " - " + _err)
                elif len(_out)==0:
                    ebLogError(f"Disk image-{_img_file} not existent, nothing to delete")
                else:
                    ebLogInfo("[ " + _d_attr[1] + " ]: \n        " + _img_file + "\n                  -> " + _out[0][:-1])
                    # search for db[n][n].x.x.x.img
                    if re.search(r'^db\d\d.*.img',  _out[0].split("/")[-1]):
                        ebLogInfo("Removing " + _d_attr[1]) ## + " : " + _img_file)
                        _rtn_ddata = remove_vm_disk(_img_file, _rtn_ddata)
                        remove_fstab_entry(_d_attr[1], aDomU)
                    else:
                        ebLogInfo("Keeping " + _d_attr[1])
            return _rtn_ddata

        _cfg = aCfg
        _node = aNode
        _ddata = literal_eval(_cfg.mGetValue('disk'))
        ebLogInfo("Original disk value: ")
        ebLogInfo(_ddata)
        _ddata = remove_dbhome_vdisks(_ddata, _node, aDomU)
        ebLogInfo("Set new disk value to: ")
        ebLogInfo(_ddata)
        _cfg.mSetValue('disk', str(_ddata))

    def mMakeFipsCompliant(self, aOptions, aHost=None, aSshConfOnly=False):
        """
        Return (rc, msg) about the fips results where rc=0 is success
        """

        _enablefips = self.mGetEnableFipsPayload(aOptions)

        if not _enablefips:
            return -1, "ok"

        # Rotate ExaKms Key
        _migrateKey = False
        _dom0s, _domUs, _, _ = self.mReturnAllClusterHosts()

        if self.mIsHostOL8(aHost):
            _migrateKey = True

        if _migrateKey:
            ebLogInfo(f"Migrating key in {aHost} to ECDSA")
            _exakmsEndpoint = ExaKmsEndpoint(None)
            _exakmsEndpoint.mSingleRotateKey(aHost, aClassName="ECDSA")

        _cmd_chk_fips_proc = "/bin/cat /proc/sys/crypto/fips_enabled"
        _cmd_chk_fips_hac = "/opt/oracle.cellos/host_access_control fips-mode --status"
        _cmd_enable_fips = "/opt/oracle.cellos/host_access_control fips-mode --enable"
        _cmd_chk_ciphers = "/bin/grep '^Ciphers aes128-ctr,aes192-ctr,aes256-ctr' "
        _cmd_chk_macs = "/opt/oracle.cellos/host_access_control ssh-macs -s"
        _cmd_replace_ciphers = "/bin/sed -i 's/^Ciphers.*/Ciphers aes128-ctr,aes192-ctr,aes256-ctr/' "
        _cmd_replace_macs = "/opt/oracle.cellos/host_access_control ssh-macs --both --enable --macs hmac-sha2-256,hmac-sha2-512"
        _cmd_restart_sshd = "/sbin/service sshd restart"
        _cmd_chk_grub_fips_0 = "/bin/grep 'fips=0' /etc/default/grub"
        _cmd_chk_grub_fips_1 = "/bin/grep 'fips=1' /etc/default/grub"
        _cmd_fetch_boot_uuid = "/bin/findmnt --output=UUID --noheadings --target=/boot"
        _cmd_chk_efi = "/bin/mount | /bin/grep /boot/efi"
        _cmd_fetch_boot_efi_uuid = "/bin/findmnt --output=UUID --noheadings --target=/boot/efi"
        _cmd_make_grub = "/usr/sbin/grub2-mkconfig -o /boot/grub2/grub.cfg"
        _cmd_make_rh_grub = "/usr/sbin/grub2-mkconfig -o /boot/efi/EFI/redhat/grub.cfg"
        _cmd_make_xen_grub = "/usr/sbin/grub2-mkconfig -o /boot/efi/EFI/XEN/xen.cfg"
        _f_rh_grub = "/boot/efi/EFI/redhat/grub.cfg"
        _f_xen_grub = "/boot/efi/EFI/XEN/xen.cfg"


        _cmd_chk_dracut_rpm = "/bin/rpm -q dracut"
        _cmd_chk_dracut_fips_rpm = "/bin/rpm -q dracut-fips"
        _cmd_install_hmaccalc_rpm_ol6 = "/bin/rpm -i /tmp/hmaccalc-0.9.12-2.el6.x86_64.rpm"
        _cmd_install_hmaccalc_rpm_ol7 = "/bin/rpm -i /tmp/hmaccalc-0.9.13-4.el7.x86_64.rpm"
        _cmd_dracut_f = "/sbin/dracut -f"
        _cmd_chk_kex = "/bin/grep '^KexAlgorithms.*curve25519-sha256' "
        _cmd_rm_kex_crv_sha256 = "/bin/sed -i 's/curve25519-sha256 *,//' "
        _cmd_rm_kex_crv_sha256_org = "/bin/sed -i 's/curve25519-sha256@libssh.org *,//' "

        _f_ssh = ["/etc/ssh/sshd_config", "/etc/ssh/ssh_config"]

        _host = aHost
        _reboot_required = False
        _is_fips_enabled = False

        _node = exaBoxNode(get_gcontext())
        _node.mConnect(aHost=_host)
        _, _o, _ = _node.mExecuteCmd(_cmd_chk_fips_proc)
        if _o:
            _out = _o.readlines()
            if _out and _out[0].strip() == "1":
                    ebLogInfo('*** mMakeFipsCompliant: fips-mode already enabled in host ' + _host)
                    _is_fips_enabled = True

        if _is_fips_enabled == False and aSshConfOnly != True:
            _node.mExecuteCmd(_cmd_chk_dracut_fips_rpm)
            _rc = _node.mGetCmdExitStatus()
            if _rc != 0:
                ebLogInfo('*** mMakeFipsCompliant: dracut-fips is not installed on the node.')
                _, _o, _ = _node.mExecuteCmd(_cmd_chk_dracut_rpm)
                if _o:
                    _out = _o.readlines()
                    _out = _out[0].strip()
                    _fips_drc_rpm = "dracut-fips-" + _out.split('dracut-')[1] + ".rpm"
                    _rc, _, _, _ = self.mExecuteLocal("/bin/ls  misc/fips/" + _fips_drc_rpm)
                    if _rc != 0:
                        ebLogError('*** mMakeFipsCompliant: rpm ' + _fips_drc_rpm + ' is not available on the exacloud box!')
                    else:
                        ebLogInfo('*** mMakeFipsCompliant: rpm ' + _fips_drc_rpm + ' is available on the exacloud box!')
                        _node.mCopyFile("misc/fips/" + _fips_drc_rpm, "/tmp/" + _fips_drc_rpm)
                        if "el6" in _fips_drc_rpm:
                            _node.mCopyFile("misc/fips/hmaccalc-0.9.12-2.el6.x86_64.rpm", "/tmp/hmaccalc-0.9.12-2.el6.x86_64.rpm")
                            _node.mExecuteCmd(_cmd_install_hmaccalc_rpm_ol6)
                        elif "el7" in _fips_drc_rpm:
                            _node.mCopyFile("misc/fips/hmaccalc-0.9.13-4.el7.x86_64.rpm", "/tmp/hmaccalc-0.9.13-4.el7.x86_64.rpm")
                            _node.mExecuteCmd(_cmd_install_hmaccalc_rpm_ol7)
                        _, _o, _ = _node.mExecuteCmd("/bin/rpm -i " + "/tmp/" + _fips_drc_rpm)
                        ebLogInfo('*** mMakeFipsCompliant: dracut-fips and dependent rpms successfully installed!')

            ebLogInfo('*** mMakeFipsCompliant: making host ' +_host + ' fips compliant')
            _in, _out, _err = _node.mExecuteCmd(_cmd_chk_fips_hac)
            if _out:
                _out = _out.readlines()
                if _out:
                    _out = _out[0].strip()
                    ebLogInfo('*** mMakeFipsCompliant: host_access_control status output : ' +_out)
                if "FIPS mode is configured and active" in _out:
                    ebLogInfo('*** mMakeFipsCompliant: host_access_control fips-mode already enabled in host ' + _host)
                    _is_fips_enabled = True
                elif "FIPS mode is disabled" in _out:
                    ebLogInfo('*** mMakeFipsCompliant: host_access_control has fips-mode disabled in host ' + _host + '. Will enable it now.')
                    _in, _out, _err = _node.mExecuteCmd(_cmd_enable_fips)
                    _reboot_required = True
                    _is_fips_enabled = True
                elif "FIPS mode is configured but not activated" in _out:
                    ebLogInfo('*** mMakeFipsCompliant: host_access_control has fips-mode enabled in host ' + _host + '. But reboot is pending!')
                    _in, _out, _err = _node.mExecuteCmd(_cmd_enable_fips)
                    _reboot_required = True
                    _is_fips_enabled = True

        if _is_fips_enabled == False and aSshConfOnly != True:
            #Lets try the manual way. This applies to 19.* version images
            _efi_uuid = ""
            ebLogInfo('*** mMakeFipsCompliant: making manual changes to enable fips on host ' + _host)
            _node.mExecuteCmd(_cmd_chk_efi)
            _rc = _node.mGetCmdExitStatus()
            if _rc == 0:
                ebLogInfo('*** mMakeFipsCompliant: EFI present')
                _, _out, _ = _node.mExecuteCmd(_cmd_fetch_boot_efi_uuid)
                if _out:
                    _efi_uuid = _out.readlines()[0].strip()
                    ebLogInfo('*** mMakeFipsCompliant: boot efi partition uuid is : ' +_efi_uuid)

            _, _out, _ = _node.mExecuteCmd(_cmd_fetch_boot_uuid)
            if _out:
                _out = _out.readlines()[0].strip()
                ebLogInfo('*** mMakeFipsCompliant: boot partition uuid is : ' +_out)
                _node.mExecuteCmd(_cmd_chk_grub_fips_1)
                _rc = _node.mGetCmdExitStatus()
                if _rc == 0:
                    ebLogInfo('*** mMakeFipsCompliant: fips already enabled in kernel. But host needs a reboot !')
                else:
                    _node.mExecuteCmd(_cmd_chk_grub_fips_0)
                    _rc = _node.mGetCmdExitStatus()
                    if _rc == 0:
                        if _efi_uuid:
                            _cmd_mod_grub = "/bin/sed -i 's/fips=0/fips=1 boot=UUID=%s root=UUID=%s/g' /etc/default/grub" %(_out, _efi_uuid)
                        else:
                            _cmd_mod_grub = "/bin/sed -i 's/fips=0/fips=1 boot=UUID=%s/g' /etc/default/grub" %(_out)
                    else:
                        if _efi_uuid:
                            _cmd_mod_grub = "/bin/sed -i '/^GRUB_CMDLINE/ s/\"$/  fips=1 boot=UUID=%s root=UUID=%s\"/' /etc/default/grub" %(_out, _efi_uuid)
                        else:
                            _cmd_mod_grub = "/bin/sed -i '/^GRUB_CMDLINE/ s/\"$/  fips=1 boot=UUID=%s\"/' /etc/default/grub" %(_out)

                    _in, _out, _err = _node.mExecuteCmd(_cmd_mod_grub)
                    _in, _out, _err = _node.mExecuteCmd(_cmd_make_grub)

                    if _node.mFileExists(_f_rh_grub):
                        _in, _out, _err = _node.mExecuteCmd(_cmd_make_rh_grub)

                    if _node.mFileExists(_f_xen_grub):
                        _in, _out, _err = _node.mExecuteCmd(_cmd_make_xen_grub)
          
                    _dracut_path  = node_cmd_abs_path_check(_node, "dracut", sbin=True)
                    _cmd_dracut_f = f"{_dracut_path} -f"
                    _node.mExecuteCmd(_cmd_dracut_f)

                    _reboot_required = True

        _restart_sshd = False

        for _fname in _f_ssh:
            ebLogInfo('*** mMakeFipsCompliant: checking file ' + _fname)
            ebLogInfo('*** mMakeFipsCompliant: checking ciphers !')
            _in, _out, _err = _node.mExecuteCmd(_cmd_chk_ciphers + _fname)
            _rc = _node.mGetCmdExitStatus()
            if _rc:
                _restart_sshd = True
                ebLogInfo('*** mMakeFipsCompliant: modifying ciphers !')
                _in, _out, _err = _node.mExecuteCmd(_cmd_replace_ciphers + _fname)

            ebLogInfo('*** mMakeFipsCompliant: checking macs !')
            _replace_macs = False
            _in, _out, _err = _node.mExecuteCmd(_cmd_chk_macs)
            if _out:
                _output = _out.readlines()
                for _line in _output:
                    if "hmac-sha2-256" not in _line or "hmac-sha2-512" not in _line:
                        _replace_macs = True

            if _replace_macs:
                ebLogInfo('*** mMakeFipsCompliant: modifying macs !')
                _in, _out, _err = _node.mExecuteCmd(_cmd_replace_macs)

            ebLogInfo('*** mMakeFipsCompliant: checking Kex algo !')
            _in, _out, _err = _node.mExecuteCmd(_cmd_chk_kex + _fname)
            _rc = _node.mGetCmdExitStatus()
            if _rc == 0:
                _restart_sshd = True
                ebLogInfo('*** mMakeFipsCompliant: modifying kex !')
                _node.mExecuteCmd(_cmd_rm_kex_crv_sha256_org + _fname)
                _node.mExecuteCmd(_cmd_rm_kex_crv_sha256 + _fname)

            if _restart_sshd:
                if _reboot_required:
                    ebLogInfo('*** mMakeFipsCompliant skipping sshd restart, since machine will be rebooted.')
                else:
                    ebLogInfo('*** mMakeFipsCompliant: restarting sshd !')
                    _in, _out, _err = _node.mExecuteCmd(_cmd_restart_sshd)

        _node.mDisconnect()
        if _reboot_required:
            return 0, "reboot_host"
        else:
            return 0, "ok"

    def mPatchVMCfg(self, aOptions=None, aGIHome=None, aDom0DomUPair=None):
        _ddpair = aDom0DomUPair

        with self.remote_lock():
            self.mPatchVMCfgBeforeBoot(aOptions=aOptions, aGIHome=aGIHome, aDom0DomUPair=_ddpair)
            self.mParallelDomUShutdown(aDom0DomUPair=_ddpair, force_on_timeout=True)
            self.mPatchVMCfgOnShutdown(aOptions=aOptions, aGIHome=aGIHome, aDom0DomUPair=_ddpair)
            self.mParallelDomUStart(aDom0DomUPair=_ddpair)
            self.mPatchVMCfgAfterBoot(aOptions=aOptions, aGIHome=aGIHome, aDom0DomUPair=_ddpair)


    def mPatchVMCfgBeforeBoot(self, aOptions=None, aGIHome=None, aDom0DomUPair=None):

        ebLogVerbose('mPatchVMCfg: Started.')

        # KVM TODO: Is required to make an abstaction of editing config file since
        # in KVM this is not needed any more, abtraction would be, setPersistentConf
        # KVM could be done by commands while in Xen could be done in the traditional
        # way.

        _dir = "u01"
        _gridhome = aGIHome
        if _gridhome:
            _dir = _gridhome.split('/', 2)[1]

        if aDom0DomUPair:
            _ddpair = aDom0DomUPair
        else:
            _ddpair = self.mReturnDom0DomUPair()

        if self.mIsKVM():

            #
            # If encryption is requested and system is KVM, create /u02
            # encrypted
            # We check if all the nodes meet the image minimum requirements
            #
            if isEncryptionRequested(aOptions, 'domU') or exacc_fsencryption_requested(aOptions):


                if not validateMinImgEncryptionSupport(
                    self,  self.mReturnDom0DomUPair()):

                    _err_msg = (f"Some nodes failed the "
                        f"minimum image version requirements for Encryption. ")
                    _action_msg = ("Disable encryption on the input payload or upgrade "
                        "the nodes and undo/retry")
                    ebLogCritical(_err_msg, _action_msg)
                    raise ExacloudRuntimeError(0x96, 0x0A, _err_msg)

                # If we're in ExaCC, we must create a socket to use
                # for the fs encryption passphrase communication
                if self.mIsOciEXACC():
                    for _dom0, _domU in _ddpair:
                        mSetLuksChannelOnDom0Exacc(self, _dom0, _domU)

                return patchEncryptedKVMGuest(self, aOptions, aDom0DomUPair=_ddpair)
            else:
                return self.patchKVMGuestCfg(aOptions, aGIHome=_gridhome, aDom0DomUPair=_ddpair)

        _subfactor = 1
        _cores = 0
        _num_computes = len(self._dom0U_list)

        if not aOptions:
            aOptions = self.mGetArgsOptions()

        if aOptions is not None:
            _jconf = aOptions.jsonconf

        if 'subfactor' in list(_jconf.keys()):
            _subfactor = int(_jconf['subfactor'])

        # Disable Pinning if VNUMA is set in xen configuration
        _disable_pinning = False

        if _jconf and "vm" in _jconf and "cloud_vnuma" in _jconf['vm']:
            _vnumaMode = _jconf['vm']['cloud_vnuma']
            if _vnumaMode in ["enabled_without_dom0_overlap", "enabled_with_dom0_overlap"]:
                _disable_pinning = True

        if _disable_pinning:
            self.mCheckDisablePinningComputeImage()


        # Bug 28308976
        # Below block is failing create_service. Turning it off if
        # over-subscription is OFF. Whenever cpu-over-subscription gets supported
        # by upper layer, 'poolsize' needs to be a mandatory param.
        # Also, changing ebError below to an exception since caller doesn't check it.
        if _subfactor > 1:
            _ratio = self.mCheckConfigOption('core_to_vcpu_ratio')
            if _ratio is None:
                _ratio = 2
            else:
                _ratio = int(_ratio)

            if 'poolsize' in list(_jconf.keys()):
               _cores = str(_jconf['poolsize'])
               _cores = str(int(_cores) * int(_ratio))  # xen vcpus across all computes
               _cores = str((int(_cores))/_num_computes)    # xen vcpus per compute : use this for range calculation
            else:
                _error_str = "*** json payload for CPU oversubscription command: poolsize not provided"
                ebLogError(_error_str)
                raise ExacloudRuntimeError(0x0433, 0x0A, _error_str)

        if aDom0DomUPair:
            _ddp = aDom0DomUPair
        else:
            _ddp = self.mReturnDom0DomUPair()

        for _dom0, _domU in _ddp:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            _vmhandle = exaBoxOVMCtrl(aCtx=get_gcontext(), aNode=_node)
            _vmhandle.mReadRemoteCfg(_domU)
            _cfg = _vmhandle.mGetOVSVMConfig(_domU)
            _cfg.mSetValue('on_crash', "'coredump-restart'")
            assert(_cfg is not None)
            #
            # Add a VM disk to vm.cfg
            #
            _free_devs = self.mFreeDevs(literal_eval(_cfg.mGetValue('disk')))

            def _add_extra_vm_disk(aFile):
                _vdisk = next(_free_devs)
                _ddata = literal_eval(_cfg.mGetValue('disk'))
                _ddata.append(','.join(('file://' + aFile, _vdisk, 'w')))
                _cfg.mSetValue('disk', str(_ddata))
                return _vdisk

            # Patch vm.cfg file to use _vif_bridge_file instead of vif-bridge
            def _set_vif_bridge_file(aFile):
                _vif_data = literal_eval(_cfg.mGetValue('vif'))
                _new_vif_data = []
                for _element in _vif_data:
                    if "script" not in _element:
                        _element += f",script={aFile}"
                    _new_vif_data.append(_element)
                _cfg.mSetValue('vif', str(_new_vif_data))

            if not self.__ociexacc:
                _vif_bridge_file = "vif-bridge.EBT"
                _set_vif_bridge_file(_vif_bridge_file)

            #
            # Check if force extra img is set
            #
            if self.mCheckConfigOption('extra_img_force', 'True'):
                _img_force = True
            else:
                _img_force = False
            #
            # Plumb additional vDisk and set/update vm.cfg parameters
            #
            _extra_disk = f"/EXAVMIMAGES/GuestImages/{_domU}/{self.mCheckConfigOption('u02_name') if self.mCheckConfigOption('u02_name') else 'u02_extra'}.img"

            mBuildVDisk(self, _node, _extra_disk)

            #
            # Note: xvdf can be changed in _add_extra_vm_disk if already used. _vdisk returned is the new vdisk name
            #
            _vdisk = _add_extra_vm_disk(_extra_disk)
            #
            # Add max_cpu to vm.cfg
            #
            _in, _out, _err = _node.mExecuteCmd('xm info | grep nr_cpus')
            if _out:
                _out = _out.readlines()
                _dat = _out[0].split(':')[1].strip()

                # Remove Dom0 vcpu from count of vm vcpus
                _dom0vcpus = _node.mSingleLineOutput("xm li | grep Domain-0 | awk '{ print $4 }'")
                if _dom0vcpus:
                    _dat = str(int(_dat) - int(_dom0vcpus))

                ebLogInfo('*** MAX_VCPUS on dom0 %s : %s' % (_dom0, _dat))
                _cfg.mSetValue('maxvcpus', _dat)
            else:
                ebLogError('*** MAX_VCPUS can not be computed !')
            #
            # Pinning support for single-vm configuration
            #
            if not self.__shared_env:
                _dom0vcpus = 0
                _domUvcpus = int(_cfg.mGetValue('vcpus'))

                ebLogInfo('*** Current domU: %s vCPUs allocation: %s' % (_domU, _domUvcpus))

                _cmd = "xm li | grep Domain-0 | awk '{ print $4 }'"
                _, _out, _ = _node.mExecuteCmd(_cmd)
                if _out:
                    _out = _out.readlines()
                    _dom0vcpus = int(_out[0])
                    ebLogInfo('*** DOM0 %s current vCPUS allocation: %d' % (_dom0, _dom0vcpus))
                else:
                    ebLogError('*** Can not retrieve DOM0 vCPUS allocation')

                _pin_range_start = _dom0vcpus
                _pin_range_end = _pin_range_start + _domUvcpus - 1

                if self.__disable_vcpus_pinning:
                    ebLogInfo('*** vCPUs pinning disabled')
                else:
                    ebLogInfo('*** Pinning range for %s set to %d-%d' % (_domU, _pin_range_start, _pin_range_end))
                    _cfg.mSetValue('cpus', "'%d-%d'" % (_pin_range_start, _pin_range_end))
            #
            # Pinning support for mulit-vm configuration and CPU oversubscription is enabled
            #
            if self.__shared_env and _subfactor > 1:
                _dom0vcpus = 0
                _cmd = "xm li | grep Domain-0 | awk '{ print $4 }'"
                _, _out, _ = _node.mExecuteCmd(_cmd)
                if _out:
                    _out = _out.readlines()
                    _dom0vcpus = int(_out[0])
                    ebLogInfo('*** DOM0 %s current VCPUS allocation: %d' % (_dom0, _dom0vcpus))
                else:
                    _node.mDisconnect()
                    _error_str = '*** Can not retrieve DOM0 vCPUS allocation'
                    ebLogError(_error_str)
                    raise ExacloudRuntimeError(0x0780, 0xA, _error_str)

                _pin_range_start = _dom0vcpus
                _pin_range_end = _pin_range_start + int(_cores) - 1

                ebLogInfo('*** Pinning range for %s set to %d-%d' % (_domU, _pin_range_start, _pin_range_end))
                _cfg.mSetValue('cpus', "'%d-%d'" % (_pin_range_start, _pin_range_end))

            #
            # Pinning support for multi-vm configuration and CPU oversubscription is not enabled
            # For now, the same logic as in the COS case is used.
            #
            elif self.__shared_env:
                ebLogInfo('*** Attempting vCPU pinning for multi-VM non-COS system')

                # mModifyService() now handles setting cpus in the cfg files
                # and also does vcpu-pin as necessary. Hence we use the same
                # over here.

                # At this stage, the 'vcpus' key in vm.cfg already has the correct
                # values. We only need to update the 'cpus' key. Also, if other
                # VMs' pinning range may be affected.
                # mModifyService() handles all of these.
                # Note: In this case, poolsize is really not used by mModifyService
                _poolsize = int(_cfg.mGetValue('vcpus'))

                _force_pinning = not self.__disable_vcpus_pinning
                self.mModifyService(aOptions, _subfactor, _poolsize, _domU, _force_pinning)

                # Set the 'cpus' field of current domU once again, since the
                # vm.cfg file written in mModifyService() gets over-written
                # subsequently
                if not self.__disable_vcpus_pinning:
                    _cmd = "xm vcpu-list | grep -v Domain-0 | grep -v ^Name | grep "+_domU+" |awk '{print $7}' |sort | uniq"
                    _, _out, _ = _node.mExecuteCmd(_cmd)
                    if _out:
                        _out = _out.readlines()
                        if (len(_out) ==1):
                            _cfg.mSetValue('cpus', "'%s'" % (_out[0].strip()))
                        else:
                            _node.mDisconnect()
                            _error_str = '*** Can not retrieve domU pinning range'
                            ebLogError(_error_str)
                            raise ExacloudRuntimeError(0x0780, 0xA, _error_str)
                    else:
                        _node.mDisconnect()
                        _error_str = '*** Can not retrieve domU pinning range'
                        ebLogError(_error_str)
                        raise ExacloudRuntimeError(0x0780, 0xA, _error_str)

            #
            # Extend /u01 partition and add /u02 to fstab
            #
            _vmnode = exaBoxNode(get_gcontext())
            _vmnode.mConnect(_domU)
            _vmnode.mExecuteCmdLog('lvresize -r /dev/mapper/VGExaDb-LVDbOra1 -L +90GB 2>&1 &> /dev/null')
            _vmnode.mExecuteCmdLog('mkdir -p /u02')
            _vmnode.mExecuteCmdLog('cat /etc/fstab | grep -v %s > /etc/fstab.orig; cp /etc/fstab.orig /etc/fstab' % (_vdisk))
            _vmnode.mExecuteCmdLog('echo "/dev/%s           /u02    ext4    defaults        1 1" >> /etc/fstab' % (_vdisk))

            if _gridhome:
               _dir = _gridhome.split('/', 2)[1]
               if _dir == "u02":
                   _cmd = f"cp -a /etc/fstab /etc/fstab.gridHome.bak; cat /etc/fstab | grep -v {_gridhome} > /etc/fstab.orig; cp /etc/fstab.orig /etc/fstab"
                   _vmnode.mExecuteCmdLog(_cmd)

            #
            # Additional disks
            #
            self.mSaveVMCfg(_node, _domU, _cfg.mRawConfig())
            #Checking for additional disks in configuration file
            _tmp_additional_disks = copy.deepcopy(self.mCheckConfigOption('additional_disks'))
            if _tmp_additional_disks is not None:
                for itr in range(0,len(_tmp_additional_disks)):
                    _lower_priority = False
                    for itr_main in range(0,len(self._additional_disks)):
                        if _tmp_additional_disks[itr][0].startswith(self._additional_disks[itr_main][0]) or self._additional_disks[itr_main][0].startswith(_tmp_additional_disks[itr][0]):
                            _lower_priority = True
                            break
                    if not _lower_priority:
                        self._additional_disks.append(_tmp_additional_disks[itr])

            if self._additional_disks is None or len(self._additional_disks)==0:
                self._additional_disks = []
            else:
                _vmcfg_dev = next(_free_devs)

                _ximg_path = self.mCreateExtraImageTemplate(_node, _domU)
                _vmcfg_dev = _add_extra_vm_disk(_ximg_path)
                _vmnode.mExecuteCmdLog('cat /etc/fstab | grep -v %s > /etc/fstab.orig; '
                                    'cp /etc/fstab.orig /etc/fstab' % (_vmcfg_dev))
                _i, _o, _e = _node.mExecuteCmd('losetup -f')
                _loop_dev = _o.readlines()[0].strip()
                self.__fxSharedInfo[f"mPatchVMCfg-{_domU}-loop"] = _loop_dev
                _node.mExecuteCmdLog('losetup {} {}'.format(_loop_dev, _ximg_path))
                _node.mExecuteCmdLog('vgchange -ay VGExaDbExternal')
                for _mp, size in self._additional_disks:
                    _mp_name = '/dev/mapper/VGExaDbExternal-LVExternal' + _mp.title().replace('/','')
                    _vmnode.mExecuteCmdLog('echo "%s           %s    ext4    defaults        1 1" >> /etc/fstab' % (_mp_name, _mp))
                _vmnode.mExecuteCmdLog('cat /etc/fstab')

            _vmnode.mDisconnect()

            #
            # Bug 27509094 nchan
            # Remove unused Oraclehome disks from the VM to prevent unexpected disk usage.
            # if "preserve_dbhomes" is not set to True
            #
            if not self.mCheckConfigOption('preserve_dbhomes','True'):
                ebLogInfo('*** Removing obsolated DBhome disks ' + _domU)
                self.mRemoveDBHomesFromVM(_cfg, _node, _domU)

            # Update latest cfg config
            self.mSaveVMCfg(_node, _domU, _cfg.mRawConfig())
            _node.mDisconnect()


    def mPatchVMCfgOnShutdown(self, aOptions=None, aGIHome=None, aDom0DomUPair=None):

        if aDom0DomUPair:
            _ddp = aDom0DomUPair
        else:
            _ddp = self.mReturnDom0DomUPair()

        for _dom0, _domU in _ddp:

            with connect_to_host(_dom0, get_gcontext()) as _node:

                #
                # Inject vmimg.sh
                #
                _script = 'scripts/images/vmimg.sh'
                _path = '/opt/exacloud/bin'
                _cmd  = 'mkdir -p /opt/exacloud/bin'
                _node.mExecuteCmdLog(_cmd)
                _node.mCopyFile(_script, _path + '/vmimg.sh')
                _cmd = 'chmod u+x /opt/exacloud/bin/vmimg.sh'
                _node.mExecuteCmdLog(_cmd)

                # Transfer mount points
                _rsync = 'rsync --exclude=/var/log/lastlog --exclude=/var/log/faillog --sparse --delete --archive --hard-links --one-file-system'
                if self._additional_disks:
                    _cmd = '/opt/exacloud/bin/vmimg.sh mount extra ' + _domU
                    _node.mExecuteCmdLog(_cmd)
                    _tmp_transpath = '/mnt/vmfs_vm1extra_' + _domU
                    _node.mExecuteCmdLog('mkdir -p ' + _tmp_transpath)
                    for _mountp, _ in self._additional_disks:
                        _mountname = _mountp[1:]

                        if _node.mFileExists('/mnt/vmfs_vm0extra/'+_mountname):
                            _node.mExecuteCmdLog('mount /dev/mapper/VGExaDbExternal-LVExternal{} {}'.format(_mountname.title().replace('/', ''), _tmp_transpath))
                            _node.mExecuteCmdLog('chmod  --reference=/mnt/vmfs_vm0extra/{} {}'.format(_mountname, _tmp_transpath))
                            _node.mExecuteCmdLog('{} /mnt/vmfs_vm0extra/{}/* {}'.format(_rsync, _mountname, _tmp_transpath))
                            _node.mExecuteCmdLog('umount ' + _tmp_transpath)
                            ebLogInfo('*** Completed sync for:  ' + _mountp)
                        else:
                            _node.mExecuteCmdLog('mkdir -p /mnt/vmfs_vm0extra/{}'.format(_mountname))
                            ebLogInfo('*** Creating mount point for: ' + _mountp)

                    _node.mExecuteCmdLog('vgchange -an VGExaDbExternal')
                    _node.mExecuteCmdLog('rm -rf ' + _tmp_transpath)
                    _node.mExecuteCmdLog('losetup -d {}'.format(self.__fxSharedInfo[f"mPatchVMCfg-{_domU}-loop"]))
                    _cmd = '/opt/exacloud/bin/vmimg.sh umount extra ' + _domU
                    _node.mExecuteCmdLog(_cmd)

    def mPatchVMCfgAfterBoot(self, aOptions=None, aGIHome=None, aDom0DomUPair=None):

        _dir = "u01"
        _gridhome = aGIHome
        if _gridhome:
            _dir = _gridhome.split('/', 2)[1]

        if aDom0DomUPair:
            _ddp = aDom0DomUPair
        else:
            _ddp = self.mReturnDom0DomUPair()

        for _dom0, _domU in _ddp:

            with connect_to_host(_domU, get_gcontext()) as _vmnode:

                if _dir == "u01":
                    _vmnode.mExecuteCmdLog('chown -fR oracle.oinstall /u02')    # Assume this is run after create user
                #
                # Handle TZ
                #
                if self.__timeZone:
                    _cmd_str = """cp /etc/sysconfig/clock /etc/sysconfig/clock.org ;
                                   echo 'ZONE="%s"' > /etc/sysconfig/clock ;
                                   ln -sf /usr/share/zoneinfo/%s /etc/localtime""" % (self.__timeZone, self.__timeZone)
                    ebLogInfo('*** TZ: ' + _cmd_str)
                    _vmnode.mExecuteCmdLog(_cmd_str)


        ### We will not enable this by default in the 1st phase
        ### Disable kdump and setup dom0 for coredump capture
        #coreDumpUtil = ebCoredumpUtil(doms=self.mReturnDom0DomUPair(), payload=aOptions.jsonconf)
        #coreDumpUtil.mRunCoredumpUtil()


    def mSetSysCtlConfigValue(self, aNode, aConfigName, aConfigValue, aRaiseException=True, aInstantApply=True, aValidate=True):

        _file, _sysctlvalue = self.mGetSysCtlConfigValue(aNode, aConfigName)

        # Create backup
        if not _file:
            _file = "/etc/sysctl.conf"

        _cmd = f"/bin/cp {_file} {_file}.`date +%y.%j.%H.%m.%s`"
        node_exec_cmd_check(aNode, _cmd)

        if _sysctlvalue is not None:
            ebLogInfo(f"Changing sysctl value '{aConfigName} = {aConfigValue}' in file '{_file}', host {aNode.mGetHostname()}")

            # Update value
            _cmd = f'/bin/sed -i  --follow-symlinks "s/^{aConfigName}[^$]*/{aConfigName} = {aConfigValue}/g" {_file}'
            node_exec_cmd_check(aNode, _cmd)

        else:

            _file = '/etc/sysctl.conf'
            ebLogInfo(f"Missing value of {aConfigValue} in sysctl config of host {aNode.mGetHostname()}")
            ebLogInfo(f"Adding sysctl value '{aConfigName} = {aConfigValue}' in file '{_file}', host {aNode.mGetHostname()}")

            # Add new value in default configuration file
            _cmd = f'/bin/echo "{aConfigName} = {aConfigValue}" >> {_file}'
            node_exec_cmd_check(aNode, _cmd)
        
        # Refresh the sysctl
        if aInstantApply:
            _cmd = f"/usr/sbin/sysctl -p {_file}"
            aNode.mExecuteCmd(_cmd, aTimeout=300)
            if aNode.mGetCmdExitStatus():
                ebLogError(f'{_cmd} returned error')
                return False

        # Get the current value after the refresh
        if aValidate:
            _file, _sysctlvalue = self.mGetSysCtlConfigValue(aNode, aConfigName)

            if str(_sysctlvalue) != str(aConfigValue):

                _msg = f"Mismatch between expected value: {aConfigValue} and current value {_sysctlvalue}"
                _msg = f"{_msg}. Error while setting sysctl value '{aConfigName}={aConfigValue}' in node: {aNode.mGetHostname()}"
 
                ebLogWarn(_msg)
                if aRaiseException:
                    raise ExacloudRuntimeError(0x0743, 0xA, _msg)

                return False

        return True



    def mGetSysCtlConfigValue(self, aNode, aConfigName, aSkipValidation = False):

        def mGetSysCtrlValueFromFile(aNode, aFilename, aConfigName):

            _configValue = None

            # find the value in /etc/sysctl.conf
            _, _o, _ = aNode.mExecuteCmd(f"/bin/cat {aFilename} | grep '{aConfigName}'")
            if aNode.mGetCmdExitStatus() == 0:
                for _line in _o.readlines():
                    if _line and "=" in _line and not _line.strip().startswith('#'):
                        _configValue = _line.split("=")[1].strip()

            return _configValue

        _sysctlFile = "/etc/sysctl.conf"
        _configValue = None
        _runtimeValue = None
        _filename = None

        # Search the config 
        # Get the current value after the refresh
        _, _out, _err = aNode.mExecuteCmd(f"/usr/sbin/sysctl -n {aConfigName}")
        if aNode.mGetCmdExitStatus() != 0:
            ebLogWarn(f"Failed to get the value of the sysctl parameter from runtime: {aConfigName}")
            ebLogWarn(_err.read())
        else:
            _runtimeValue = _out.readlines()[0].strip()

        # Search the value in /etc/sysctl.conf
        _configValue = mGetSysCtrlValueFromFile(aNode, _sysctlFile, aConfigName)
        if _configValue is not None:
            _filename = _sysctlFile

        else:

            # Search the value in other files of sysctl.d
            _folder = "/etc/sysctl.d/"
            _, _o, _ = aNode.mExecuteCmd(f"/bin/ls {_folder}")
            if aNode.mGetCmdExitStatus() == 0:

                _files = _o.readlines()

                for _file in _files:

                    _filename = f"{_folder}/{_file}".strip()
                    _configValue = mGetSysCtrlValueFromFile(aNode, _filename, aConfigName)

                    if _configValue is not None:
                        break

        if _configValue is None:
            ebLogTrace(f"Missing sysctl value '{aConfigName}' in host {aNode.mGetHostname()}")
            return None, None

        if aSkipValidation:
            return _filename, str(_configValue)

        if str(_runtimeValue) == str(_configValue):
            ebLogTrace(f"Found sysctl value '{aConfigName} = {_configValue}' in file '{_filename}', host {aNode.mGetHostname()}")
            return _filename, str(_configValue)
        else:
            ebLogTrace(f"Sysctl runtime values '{_runtimeValue}' not match with '{_filename}' value in file {_configValue}")
            return None, None


    def mConfigureShmAll(self, aDomU=None):
        _shmall_file = "/proc/sys/kernel/shmall"

        # FIX_ADD_NODE_MR
        if aDomU is not None:
            _ddp = [ [None, aDomU] ]
        else:
            _ddp = self.mReturnDom0DomUPair()

        def _mUpdateDomuShm(aDomu):
            _domU = aDomu

            with connect_to_host(_domU, get_gcontext()) as _node:

                ebLogInfo(f"*** Getting shmmax value from node: {_domU} ***")
                _, _value = self.mGetSysCtlConfigValue(_node, "kernel.shmmax")

                if _value is None:

                    ebLogError(f"*** Could not get shmmax value from node: {_domU}***".format(_domU))
                    raise ExacloudRuntimeError(0x0743, 0xA, f"Error while getting shmmax settings from node: {_domU}")

                _shmmax_value = int(_value)

                ebLogInfo("*** Current shmmax value:{} ***".format(_shmmax_value))
                ebLogInfo("*** Getting PAGE_SIZE ***")

                _i,_o,_e = _node.mExecuteCmd('getconf PAGE_SIZE')
                _rc = _node.mGetCmdExitStatus()

                if _rc != 0:
                    ebLogError("*** Could not get PAGE_SIZE by running:getconf PAGE_SIZE, node:{}***".format(_domU))
                    raise ExacloudRuntimeError(0x0743, 0xA, "Error while getting PAGE_SIZE, node:{} ".format(_domU))

                _page_size = int(_o.readline().split()[0])

                ebLogInfo("*** Current PAGE_SIZE value:{} ***".format(_page_size))
                ebLogInfo("*** Calculating shmall value ***")

                _shmall_value = int(_shmmax_value/_page_size)

                ebLogInfo("*** New shmmax value:{} ***".format(_shmmax_value))
                self.mSetSysCtlConfigValue(_node, "kernel.shmall", _shmall_value)

                _i,_o,_e = _node.mExecuteCmd('/bin/echo {} > {}'.format(_shmall_value,_shmall_file))
                _rc = _node.mGetCmdExitStatus()

                if _rc != 0:
                    ebLogError("*** Could not update shmall value to:{} in file:{} ***".format(_shmall_value,_shmall_file))
                    raise ExacloudRuntimeError(0x0743, 0xA, "Error while updating shmall value to:{} in file:{}".format(_shmall_value,_shmall_file))

        _plist = ProcessManager()
        for _, _domU in _ddp:
            _p = ProcessStructure(_mUpdateDomuShm, [_domU])
            _p.mSetMaxExecutionTime(60*60)
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()


    def mSetCachingPolicyRecoGD(self, aCellList: list, aOptions:dict)->None:
        """
        This method is to set/change CACHINGPOLICY on RECO GridDisks
        :param aCellList: a list of cells on which cachingPolicy should be updated
        :param aOptions: Options context
        :raises ExacloudRuintimeError: If unable to change cachingPolicy
        :returns: None
        """

        # This caching policy change should be enforced only on ADB-S
        if aOptions.jsonconf.get("adb_s", "False").upper() != "TRUE":
            ebLogInfo("This operation is only for ADB-S, this is nop")
            return

        # According to cellcli documentation, we must cancel FLUSH of FLASH CACHE
        # before setting CACHINGPOLICY to something different to 'none', from docs:
        # ...
        # The cachingPolicy attribute is used to change the flash caching policy
        # of a grid disk. Before changing the caching policy from default to none,
        # ensure there is no cached data in flash cache for the grid disk by using
        # the ALTER GRIDDISK ... FLUSH command.
        # Flash cache is not used with data files placed in a disk group composed
        # of grid disks with their cachingPolicy set to none.
        # To re-enable caching on the grid disk, we must cancel the
        # FLUSH operation
        # ...
        _cmd_flush = "cellcli -e 'ALTER GRIDDISK WHERE NAME LIKE \"RECO.*\" CANCEL FLUSH'"
        _cmd_policy = (f"cellcli -e 'ALTER GRIDDISK CACHINGPOLICY=\"default\" "
                        "WHERE NAME LIKE \"RECO.*\"'")
        _err_msg = ("An error occured while attempting to run cmd: {0}, "
                    "stdout: {1.stdout}, stderr: {1.stderr}, exit-code: {1.exit_code}")

        for _cell_name in aCellList:

            with connect_to_host(_cell_name, get_gcontext()) as _node:

                ebLogInfo(f"Attempting to set cachingPolicy to 'default' on {_cell_name}")

                # Cancel flush as specified in docs to setup 'non-none cachingPolicy'
                _flush_op = node_exec_cmd(_node, _cmd_flush)
                if _flush_op.exit_code:
                    ebLogError(_err_msg.format(_cmd_flush, _flush_op))
                    raise ExacloudRuntimeError(0x802, 0xA,_err_msg.format(
                                                    _cmd_flush, _flush_op))

                # Setup cachingpolicy to "default"
                _policy_op = node_exec_cmd(_node, _cmd_policy)
                if _policy_op.exit_code:
                    ebLogError(_err_msg.format(_cmd_policy, _policy_op))
                    raise ExacloudRuntimeError(0x802, 0xA,_err_msg.format(
                                                    _cmd_flush, _flush_op))

                ebLogInfo(f"Succesfully set cachingPolicy to 'default' on {_cell_name}")


    def mHandlerResizeVMCpuCount(self, aOptions=None):
        # Legacy code

        if not aOptions: 
            aOptions = self.mGetArgsOptions()

        _domU = None

        def _patch_function(aCfg, aOptions):
            _ratio = self.mCheckConfigOption('core_to_vcpu_ratio')
            if _ratio is None:
                _ratio = 2
            else:
                _ratio = int(_ratio)
            _cores = str(aOptions.jsonconf['vm']['cores'])
            _cores = str(int(_cores) * _ratio)
            aCfg.mSetValue('vcpus', _cores)
            ebLogInfo('*** Resizing VM (%s) to vCPUs : %s' % (_domU, _cores))
            return _cores

        # Fetch cpuCount from json file
        if 'vm' in list(aOptions.jsonconf.keys()) and \
            'cores' in list(aOptions.jsonconf['vm'].keys()):

            _ddp = self.mReturnDom0DomUPair()
            for _dom0, _domU in _ddp:
                self.mPatchVMCfgVcpuCount(_dom0, _domU, _patch_function, aOptions)

    def mResetClusterSSHKeys(self,aOptions):

        _d0, _du, _cl, _sw = self.mReturnAllClusterHosts()
        _host_list = _d0 + _du + _cl + _sw

        # Add NAT Hosts
        for _, _domU in self.mReturnDom0DomUPair():

            _host = _domU 
            if get_gcontext().mCheckRegEntry('_natHN_' + _host):
                _host = get_gcontext().mGetRegEntry('_natHN_' + _host)

            _host_list.append(_host)

        for _host in _host_list:
            _cmd = "/bin/ssh-keygen -R {0}".format(_host)
            if self.__debug:
                ebLogInfo('*** Reset SSH Host Key for: %s' % (_host))
            self.mExecuteLocal(_cmd, aStdOut=DEVNULL, aStdErr=DEVNULL)

    def mPatchVMCfgVcpuCount(self, aDom0, aDomU, aPatchFunction, aOptions):

        if self.mIsKVM():
            _exacpueobj = exaBoxKvmCpuMgr(self)
            _exacpueobj.mPatchVMCfgVcpuCountKvm(aDom0, aDomU, aOptions)
            return

        _dom0 = aDom0
        _domU = aDomU

        _node = exaBoxNode(get_gcontext())
        _node.mConnect(aHost=_dom0)
        _vmhandle = exaBoxOVMCtrl(aCtx=get_gcontext(), aNode=_node)

        # Read VM cfg file
        _vmhandle.mReadRemoteCfg(_domU)
        _cfg = _vmhandle.mGetOVSVMConfig(_domU)
        assert(_cfg is not None)

        ebLogInfo('mPatchVMCfgVcpuCount: Current vcpu value (before update) for VM ' + _domU + ' is '+_cfg.mGetValue('vcpus')+' in vm.cfg')
        # Invoke patching function
        _cores = aPatchFunction(_cfg, aOptions)

        # Make a Backup of the vm.cfg
        self.mSaveVMCfg(_node, _domU, _cfg.mRawConfig())
        ebLogInfo('mPatchVMCfgVcpuCount: Updated vcpu value for VM ' + _domU + ' is '+_cfg.mGetValue('vcpus')+' in vm.cfg')

        # Dynamic vcpu update
        ebLogInfo('mPatchVMCfgVcpuCount: Dynamic vcpu update for VM ' + _domU + ' with  '+str(_cores)+' vcpus')

        # Retry logic
        _maxRetries = 3
        _retries = 0
        _updateSuccess = False

        while _retries < _maxRetries:

            _cmd = f'/opt/exadata_ovm/exadata.img.domu_maker vcpu-set {_domU} {_cores}'

            if _retries != 0:
                _cmd = f"{_cmd} --force"

            _node.mExecuteCmdLog(_cmd)

            if _node.mGetCmdExitStatus() == 0:
                _updateSuccess = True
                break
            else:
                time.sleep(5)

            _retries += 1

        _node.mDisconnect()

        if not _updateSuccess:
            _msg = f"VCPU-SET not applied in {_domU} with {_cores}"
            raise ExacloudRuntimeError(0x0436, 0xA, _msg)

    def mRebootNode(self,aHost,aWait=False,aForce=False,aSkipShutdown=False):
        try:
            _rforce = False
            if aWait is False:
                with connect_to_host(aHost, get_gcontext()) as _vmnode:
                    if not aSkipShutdown:
                        _vmnode.mExecuteCmdLog('reboot')
                    else:
                        ebLogWarn('*** Reboot w/ Force option for: %s' % (aHost))
                        _vmnode.mExecuteCmdLog('reboot -f')
                        ebLogWarn('*** Reboot successfully completed')
                        _rforce = True
                    time.sleep(self.__timeout_ecops)
            else:
                time.sleep(10)
        except (IOError, BrokenPipeError) as e:
            ebLogError(f"Broken pipe error during node reboot: {e}")

        _rc = self.mPingHost(aHost)
        if _rc is True and not _rforce:
            ebLogWarn('Not expecting :'+aHost+' to be alive.')
            if aForce is True:
                _count = 60
                while not self.mPingHost(aHost) and _count:
                    time.sleep(5)
                    _count -= 1
                if not _count:
                    ebLogError('*** %s did not go shutdown/reboot during grace period' % (aHost))
                else:
                    ebLogWarn('*** %s finally went down - wait for node to come up' % (aHost))
                    time.sleep(self.__timeout_ecops)

        _connected = 5
        while _connected:
            try:
                # xxx/MR: At this stage the clu/storage pkey should be up
                with connect_to_host(aHost, get_gcontext()) as _vmnode:
                    _vmnode.mExecuteCmdLog("ip addr show | grep 'ib0\|ib1\|inet '")
                    break
            except:
                _connected = _connected - 1
                ebLogWarn('Waiting for VM: '+aHost+' to come up.')
                time.sleep(self.__timeout_ecops)
        if _connected == 0:
            ebLogError('Host '+aHost+' is not accessible.')
        else:
            ebLogInfo('Host '+aHost+' is now up and running.')

    """
    Cluster Name patching
    """
    def mPatchClusterName(self, aRackName=None):

        if self.__cmd in ['iorm']: 
            ebLogInfo("skip patching of clustername in xml for iorm operations")
            return
        elif self.mIsXS() and self.mGetCmd() in ["exascale_acfs_cmd", "deleteservice"]:
            ebLogInfo("EXASCALE: Skip patching of clustername in xml for deleteservice command")
            return

        #
        # if aRackName is None fetch RN from XML (assuming JSon payload is not provided)
        #
        if aRackName is None:
            _rack_name = self.__clusters.mGetCluster().mGetCluName()
        else:
            _rack_name = aRackName
        #
        # Fetch PKEY values
        #
        _skm, _ckm = self.mGetPkeysConfig()
        _pkv = int(_ckm,16) & int(0xFFF)
        _pkvy = '%03x' % (_pkv)
        #
        # Check if _rack_name has already been patched
        #
        try:
            _check = _rack_name[-4:][0]
            if _check == '-':
                _pkvx  = _rack_name[-3:]
            else:
                _pkvx = None
                _check = None
        except:
            _check = None
            _pkvx  = None

        #
        # if RN is not patched endure lentgh is no greater than 11 chars (leaving -XYZ as suffix)
        #
        if _check is None and len(_rack_name) > 11:
            ebLogError('*** Invalid CN length detected (%s/%d)' % (_rack_name, len(_rack_name)))
            if self.__ociexacc:
                _rack_name = _rack_name[:9] + _rack_name[-5:]
            else:
                _rack_name = _rack_name[:11]

            # Fix trailing '-' in rack name after trim (fix for bug 28285342 / 28278784)
            _rack_name = _rack_name.strip('-')
            if not ebCluCmdCheckOptions(self.__cmd, ['dont_check_trunc_name']) and not self.mCheckConfigOption('allow_trunc_cname', 'True'):
                raise ExacloudRuntimeError(0x0742, 0xA, 'Invalid clustername provided length greater than 11 characters')

        if aRackName is None and _check is not None and _pkvx != _pkvy:
            ebLogWarn('*** Invalid CN PKV Signature detected please review XML')
        #
        # if aRN provided always assumed JSon payload RN w/o patch
        #
        if aRackName is not None:

            if self.__shared_env and not self.__ociexacc:
                self.__clusters.mGetCluster().mSetCluName(aRackName, aPrefix="")

            else:

                if self.__ociexacc:
                    #_rack_name = _rack_name + _pkvy
                    #_rack_name = re.sub("[^a-zA-Z0-9\-]", "-", _rack_name)
                    self.__clusters.mGetCluster().mSetCluName(_rack_name, aForce=True)
                else:
                    _rack_name = _rack_name + '-' + _pkvy
                    _rack_name = re.sub("[^a-zA-Z0-9\-]", "-", _rack_name)
                    self.__clusters.mGetCluster().mSetCluName(_rack_name)
        else:
            #
            # Already Patched nothing to be done
            #
            if _check is not None and _pkvx == _pkvy:
                ebLogWarn('*** Valid CN PKV Signature detected in XML provided')
            #
            # if RN not patched use XML CN and append suffix
            #
            elif _check is None:
                if self.__ociexacc:
                    #_rack_name = _rack_name + _pkvy
                    #_rack_name = re.sub("[^a-zA-Z0-9\-]", "-", _rack_name)
                    self.__clusters.mGetCluster().mSetCluName(_rack_name, aForce=True)
                else:
                    _rack_name = _rack_name + '-' + _pkvy
                    _rack_name = re.sub("[^a-zA-Z0-9\-]", "-", _rack_name)
                    self.__clusters.mGetCluster().mSetCluName(_rack_name)
            else:
                pass        # This is the error case - keep existing CN

        ebLogInfo('*** ClusterName updated to: %s' % (self.__clusters.mGetCluster().mGetCluName()))

    #Method returns cluster name configured in source domU
    def mFetchClusterName(self, aDomU):
        _srcdomU = aDomU
        _clusterName = ""

        try:
            _gridhome, _, _ = self.mGetOracleBaseDirectories(aDomU = _srcdomU)

            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost = _srcdomU)

            _cmd = _gridhome + "/bin/crsctl get cluster name | /bin/awk '{print $NF}' "
            _in, _out, _err = _node.mExecuteCmd(_cmd)
            if not _out:
                ebLogError(f"Failed to get the Cluster name on source domU:{_srcdomU}")
                _node.mDisconnect()
                return ""

            _clusterName = literal_eval(_out.readlines()[0].strip())
            ebLogInfo(f"Cluster name on source domU:{_srcdomU}: {_clusterName}")
        except:
            pass
        finally:
            _node.mDisconnect()

        return _clusterName

    def mUpdateClusterName(self, aSrcDomU):
        _clusterName = self.mFetchClusterName(aSrcDomU)
        if _clusterName:
            self.__clusters.mGetCluster().mSetCluName(_clusterName, aForce=True)
            ebLogInfo(f'*** ClusterName updated to: {self.__clusters.mGetCluster().mGetCluName()}')
        else:
            _detail_error = f"Cannot Proceed with Patching of XML..Failed to retrieve clusterName from DomU {aSrcDomU}"
            # Bug 36324332 - Some ExaDB-XS evs do not have CRS.
            if self.mIsExaScale():
                ebLogWarn(_detail_error)
                return
            ebLogError(_detail_error)
            self.mUpdateErrorObject(gNodeElasticError['FAILED_DOMU_XML_PATCHING'], _detail_error)
            raise ExacloudRuntimeError(0x0623, 0xA, _detail_error)
        self.mSaveXMLClusterConfiguration()
        ebLogInfo('ebCluCtrl: Saved patched Cluster Config: ' + self.mGetPatchConfig())

    def mGetExadataCellModel(self) -> Optional[str]:
        """ Connect to first Cell and get model (X5/X6/X7 etc) """
        if self.__mock_mode == 'TRUE':
            try:
                ebLogInfo('*** MOCK Get exadata model information')
                model = re.findall('X[0-9]+-[28]',
                                   self.mGetEsracks().mDumpEsRackDesc())
                return re.sub('-.*', '', model[0])
            except Exception as e:
                ebLogInfo('*** MOCK error, get Cell Model, using X5:' + str(e))
                return 'X5'

        _cell_list = self.mReturnCellNodes()
        _cell_name = list(_cell_list.keys())[0]
        if _cell_name in self.__cell_type:
            return self.__cell_type[_cell_name]

        ebLogInfo('*** Getting Cell Exadata model')
        _exa_model = self.mGetNodeModel(aHostName=_cell_name)
        ebLogInfo(f'*** Cell Exadata model: {_exa_model}')

        self.__cell_type[_cell_name] = _exa_model
        return _exa_model


    """
    DO not rely on xml string
    """
    def mUpdateCelldiskSize(self):
        try:
            if self.mIsExaScale() or self.mIsXS():
                return
            _size_list = []
            _cell_list = self.mReturnCellNodes()

            _cmd = (
                "cellcli -e list physicaldisk `cellcli -e list physicaldisk | head -1 | grep -v FLASH | "
                "awk '{print $1}'` detail | grep physicalSize | awk '{print $2}' | cut -f 1 -d 'T' | tr -d 'G'"
            )

            # First round to fetch the physicalSize
            _toExecute = {}
            for _cell in _cell_list:
                _toExecute[_cell] = [_cmd]
            _result = self.mExecuteCmdParallel(_toExecute)

            _toCheckCells = []
            for _host, _status in _result.items():
                _o = _status[0]["stdout"].strip()
                if _o == "":
                    _toCheckCells.append(_host)
                else:
                    _o = _o.splitlines()[0]
                    _size_list.append(_o)
                    ebLogTrace(f"mUpdateCelldiskSize: Cell {_host} has size {_o}")

            # In case of any cell without pyshicalSize, bring up the cell service
            if _toCheckCells:
                if self.mCheckCellsServicesUp(aCellList=_toCheckCells):

                    _toExecute = {}
                    for _cell in _toCheckCells:
                        _toExecute[_cell] = [_cmd]
                    _result = self.mExecuteCmdParallel(_toExecute)

                    # Second round to fetch the physicalSize
                    for _host, _status in _result.items():
                        _o = _status[0]["stdout"].strip()
                        if _o == "":
                            ebLogError(f'***Cell services are up , Could not retrive Cell Disk Size in {_host}***')
                            raise ExacloudRuntimeError(0x0825, 0xA, "Error retriving Cell Disk Size, invalid output", Cluctrl=self)
                        else:
                            _o = _o.splitlines()[0]
                            _size_list.append(_o)
                            ebLogTrace(f"mUpdateCelldiskSize: Cell {_host} has size {_o}")

                else:
                    ebLogError(f'*** Cell services are down ***')
                    raise ExacloudRuntimeError(0x0825, 0xA, "Error retriving Cell Disk Size, Cell services down!", Cluctrl=self)

            if not _size_list:
                raise ExacloudRuntimeError(0x0825, 0xB, "No valid disk sizes found", Cluctrl=self)

            _minsize = min(_size_list, key=lambda x: float(x))
            _o = str(_minsize)
            ebLogInfo(f"mUpdateCelldiskSize: minimum size for cells is: {_o}")

            # update the celldisk size
            if "8.9" in _o[:3]:
                _o = "10" # ceiling won't work here as we will end up with 9TB
            elif _o.startswith('12.47'):
                _o = '14'
            elif _o.startswith('16.03'):
                _o = '18'
            elif _o.startswith('20.00'):
                _o = '22'
            if "X7-2 Eighth" in self.__esracks.mDumpEsRackDesc():
                _o = "8"  # edge case where the X7 base system uses 8TB disks & not 10TB disks
            if "X8-2 Eighth" in self.__esracks.mDumpEsRackDesc():
                if self.__ociexacc or self.__exacm:
                    _o = "14"
                else:
                    _o = "8"  # edge case where the X8 base system uses 8TB disks & not 10TB disks
            if "X9M-2 Eighth" in self.__esracks.mDumpEsRackDesc():
                if self.__ociexacc or self.__exacm:
                    _o = "14" # FORCE 14TB for Base X9
            if "X10M-2 Eighth" in self.__esracks.mDumpEsRackDesc():
                if self.__ociexacc or self.__exacm:
                    _o = "14" # FORCE 14TB for Base X10
            if "X11M Eighth" in self.__esracks.mDumpEsRackDesc():
                if self.__ociexacc or self.__exacm:
                    _o = "14" # FORCE 14TB for Base X11

            if _o.endswith('G'):
                _o = _o[:-1] # avoid casting to float a value that ends with 'G'
            self.__esracks.mSetDiskSize(int(math.ceil(float(_o))))
            
        except Exception as e:
            ebLogError(f'*** Error while updating cell disk size , error : {str(e)}')
            raise ExacloudRuntimeError(0x0825, 0xA, f"Could not update Cell Disk Size", Cluctrl=self)

    """
    BUG 27104007
    Identify rack size by computing the (number of cells * number of disks per cell)
    We cannot rely on <rackDescription> entry in the xml
    """
    def mGetRackSize(self):
        if self.__rack_size:
            return self.__rack_size
        _cell_list = self.mReturnCellNodes()
        _cell_name = list(_cell_list.keys())[0]
        _mac_cfg   = self.__machines.mGetMachineConfig(_cell_name)
        _ldisk_cnt = _mac_cfg.mGetLocaldisksCount()

        _num_computes = len(self._dom0U_list)
        _num_cells = len(list(_cell_list.keys()))
        ebLogInfo('*** num_computes: {}, num_cells : {}, disk_cnt per cell : {}'.format(_num_computes, _num_cells, _ldisk_cnt))

        if _num_cells < 5 and _ldisk_cnt <= 6:
            self.__rack_size = "eighth"
        elif _num_cells <= 5:
            self.__rack_size = "quarter"
        elif _num_cells >= 6 and _num_cells < 12:
            self.__rack_size = "half"
        elif _num_cells >= 12 and _num_cells < 24:
            self.__rack_size = "full"
        elif _num_cells >= 24:
            self.__rack_size = "double"

        ebLogInfo('*** '+self.__rack_size)
        return self.__rack_size

    """
    Patch the XML cluster configuration file using the json input passed by either SDI/SM
    """
    def mPatchClusterDB(self, aOptions=None):

        if not ebCluCmdCheckOptions(self.mGetCmd(), ['dont_update_cell_disk_size']):
            self.mUpdateCelldiskSize()

        if self.__cmd == 'vmgi_reshape' and self.mIsExaScale():

            # Delete compute
            if 'reshaped_node_subset' in list(aOptions.jsonconf.keys()) and \
               'removed_computes' in list(aOptions.jsonconf['reshaped_node_subset'].keys()) and \
               aOptions.jsonconf['reshaped_node_subset']['removed_computes']:

                ebLogInfo("Skip mPatchClusterDB on cmd {0} in ExaScale".format(self.__cmd))
                return

        #
        # Similar to trim scan ip - the core to vcpu ratio is only applied for non 'info' request
        # In other words for the 'info' request we return in the XML cores instead of vCPUs !
        #
        _rqdb_json_conf = None
        if aOptions is not None:
            aOptions.jsonconf = umaskSensitiveData(aOptions.jsonconf)
            _jconf = aOptions.jsonconf
        else:
            _jconf = None

        # VCPU ratio calculation
        _ratio = self.mCheckConfigOption('core_to_vcpu_ratio')

        if _ratio is None:
            _ratio = 2
        else:
            _ratio = int(_ratio)

        if self.mIsKVM() and self.IsZdlraProv():
            if self.mCheckConfigOption('zdlra_core_to_vcpu_ratio') is not None:
                _ratio = int(self.mCheckConfigOption('zdlra_core_to_vcpu_ratio'))
            else:
                _ratio = 1

        ebLogInfo('*** mPatchClusterDB: _ratio is : %d' %(_ratio))

        # Retrieve DBHome and DBConfig for the cluster.
        _mac_list = self.__clusters.mGetClusterMachines()
        _dbhome_list = {}
        for _mac in _mac_list:
            # Find the DBHome with the corresponding _mac IDs
            _dbhL = self.__dbhomes.mGetDBHomeConfigs()
            _dbhR = []
            for _dbh in _dbhL:
                if _mac in _dbh.mGetDBHomeMacs():
                    _dbhome_list[_dbh.mGetDBHomeId()] = _dbh
                else:
                    _dbhR.append(_dbh)
            #
            # Remove DBHome not used by this cluster ! Currently not an issue (no additional homes)
            #
            for _dbh in _dbhR:
                self.__dbhomes.mRemoveDBHomeConfig(_dbh.mGetDBHomeConfig_ptr(), _dbh)
        #
        # Remove all databases/database Entries not used by our cluster (required for CDB/PDB)
        #
        _dbconfig_list = {}
        _dbcL = self.__databases.mGetDBconfigs()
        _dbcI = []
        for _dbhid in list(_dbhome_list.keys()):
            for _dc in _dbcL:
                if _dbhid == _dbcL[_dc].mGetDBHome():
                    _dbconfig_list[_dbcL[_dc].mGetDBId()] = _dbcL[_dc]
                    _dbcI.append(_dc)
        _dbcX = []
        _dbcL = self.__databases.mGetDBconfigs()
        for _dc in _dbcL:
            if not _dc in _dbcI:
                _dbcX.append(_dc)
        for _dc in _dbcX:
            self.__databases.mRemoveDatabaseConfig(_dc)
        #
        # Remove un-necessary DG
        #
        _cluster = self.__clusters.mGetCluster()
        _cludgroups = _cluster.mGetCluDiskGroups()
        if self.__debug:
            ebLogInfo('*** Cluster DG: '+str(_cludgroups))
        _dgc_x = []
        for _dgcid in self.__storage.mGetDiskGroupConfigList():
            if _dgcid not in _cludgroups:
                _dgc_x.append(_dgcid)
                if self.__debug:
                    ebLogInfo('*** Removing DG:'+str(_dgcid))
        for _dgcid in _dgc_x:
            self.__storage.mRemoveDiskGroupConfig(_dgcid)
        #
        # Cleanup XML to reduce size
        #
        if self.mCheckConfigOption('remove_configkeys','True'):
            _xmlr = self.__config.mConfigRoot()
            _xmlo = _xmlr.find('configKeys')
            _xmlr.remove(_xmlo)
        #
        # Do not proceed further if no json is provided TODO: move at the top of method
        #
        if _jconf is None and _rqdb_json_conf is None:
            if aOptions and aOptions.debug:
                ebLogWarn('No Json configuration file/parameter provided')
            #
            # Handle scenario where we have multiple DB being plumbed 12c and 11g and CDB/PDB enabled w/
            # wrong DBHome (e.g. 11g for CDB/PDB needs to be switch to 12c home)
            #
            ebLogInfo('*** No DB PARAMS available')

            _dbhomes_version = {}
            for _entry in self.__dbhomes.mGetDBHomeConfigs():
                _vhome = _entry.mGetDBHomeVersion().split('.')[0]
                _dbhomes_version[_vhome] = _entry
            ebLogInfo('*** DB_HOME(s) FOUND: %s' % (str(len(list(_dbhomes_version.keys())))))
            for _db in list(_dbhomes_version.keys()):
                _dbhv  = _dbhomes_version[_db].mGetDBHomeVersion()
                _dbhp  = _dbhomes_version[_db].mGetDBHomeLocation()
                _dbhid = _dbhomes_version[_db].mGetDBHomeId()
                ebLogInfo('*** [ [%s] %s %s %s' % (_db, _dbhv, _dbhp, _dbhid))
            #
            # Do a sanity checks of all db config
            #
            for _key in list(_dbconfig_list.keys()):
                _dbc = _dbconfig_list[_key]
                _dbc_type = _dbc.mGetDBType()

                if _dbc_type in ['cdb', 'pdb']:

                    _dbhid = _dbc.mGetDBHome()
                    _dbh   = self.__dbhomes.mGetDBHomeConfig(_dbhid)
                    _dbver = _dbh.mGetDBHomeVersion()
                    #
                    # In some scenarios - we may end up with the wrong DataBaseHome in the DBConfig
                    #
                    if _dbver[0:2] != '12':
                        ebLogWarn('*** Pacthing DBConfig to use 12c for PDB/CDB deployment')
                        #
                        # Set DBConfig DBHome to 12c DBHome
                        #
                        if "12" in _dbhomes_version:
                            _dbc.mSetDBHome(_dbhomes_version['12'].mGetDBHomeId())
            return
        #
        # xxx/MR: Handle DB json payload
        #
        if _rqdb_json_conf is not None:
            _jconf = _rqdb_json_conf
        #
        # Patch DB configuration with correct SID, DB VERSION/HOME
        #
        # Assuming 1 DBHome and 1 DBConfig per cluster the SID and Version

        #_sid = _dbconfig_list[_dbconfig_list.keys()[0]].mGetDBSid()
        #_version = _dbhome_list[_dbhome_list.keys()[0]].mGetDBHomeVersion()
        #_dbpath  = _dbhome_list[_dbhome_list.keys()[0]].mGetDBHomeLocation()

        if 'dbParams' in list(_jconf.keys()) \
                and 'dbname' in list(_jconf['dbParams'].keys()):
            _sid         = _jconf['dbParams']['dbname']
            _uniqueName  = _jconf['dbParams'].get('db_unique_name')

            # Only consider uniqueName if different from SID
            if _sid == _uniqueName:
                _uniqueName = None

            #
            # Fetch DB_NAME
            #
            if 'pdb_name' in list(_jconf['dbParams'].keys()):
                _pdb_name = _jconf['dbParams']['pdb_name']
            else:
                _pdb_name = None
            #
            # Fetch DB_VERSION with 12.2 support needs to rely on 3 digit version: 112, 121, 122
            #
            if 'dbVersion' in list(_jconf.keys()):
                _version = _jconf['dbVersion'][0:3]
            elif 'version' in list(_jconf['dbParams'].keys()):
                _version = _jconf['dbParams']['version'][0:3]

            if _version.startswith('18'):
                self.__db_version = _version = '181'
            else:
                self.__db_version = _version

            if (int(self.__db_version[:2]) >= 19) and not(self.__over18cSupported):
                ebLogError("*** DB/GI VERSION {0} IS NOT SUPPORTED ON THIS CLUSTER".format(self.__db_version))
                raise ExacloudRuntimeError(0x0720, 0xA, "DB/GI VERSION IS NOT SUPPORTED ON THIS CLUSTER")

        #
        # Updating dbparams with the configuration of oneoff_oss_store
        # -> adding oneoff_oss_store from config file to dbParams
        #    to be used by ocde
        #
        if 'dbParams' in list(_jconf.keys()):
            _oneoff_oss_store = self.mCheckConfigOption("oneoff_oss_store")
            if _oneoff_oss_store is None:
                if self.mCheckConfigOption('target_env', 'prod'):
                    _oneoff_oss_store = 'prod'
                if _oneoff_oss_store is None and self.mEnvTarget() is True:
                    _oneoff_oss_store = 'prod'
                else:
                    _oneoff_oss_store = 'dev'
            if _oneoff_oss_store is None:
                _oneoff_oss_store = "prod"
            else:
                if _oneoff_oss_store not in ["dev", "dbqa", "cqa", "prod", "none"]:
                    ebLogWarn('Incorrect value for oneoff_oss_store  provided: ' + str(_oneoff_oss_store))
                    # in case of incorrect value, lets set the value to prod
                    _oneoff_oss_store = "prod"
            _jconf['dbParams']['oneoff_oss_store'] = _oneoff_oss_store
            _disksize=self.__esracks.mGetDiskSize()
            if not self.mIsExaScale():
                _cell_model = self.__cellInfo['exadata_model']
                _jconf['dbParams']['cell_model'] = _cell_model
                _jconf['dbParams']['cell_disk_size'] = str(_disksize)+'TB'
                _exadata_model_lt_X7 = False
                _compare_exadata = self.mCompareExadataModel(_cell_model, 'X7')
                if _compare_exadata < 0:
                    _exadata_model_lt_X7 = True
                if _exadata_model_lt_X7 is True:
                    _jconf['dbParams']['acfs_install_type'] = 'V1'
                else:
                    _jconf['dbParams']['acfs_install_type'] = 'V2'

        if 'dbParams' in list(_jconf.keys()):
            #
            # If DBNID is enable for the starterdb
            # we must turn on this parameter for OCDE
            #
            # add "createservice" along with db_install because with
            # Create service split, starter DB is one of the last steps
            if ebCluCmdCheckOptions(self.__cmd, ['add_nid_flag_to_ocde_payload']) and self.__enable_nid_starterdb:
                _jconf['dbParams']['nid'] = 'yes'
                ebLogInfo("*** Adding NID flag to ocde payload {0} ".format(_jconf['dbParams']['nid']))

            _dict = _jconf['dbParams']

            if ('mdbcs_sla' in list(_dict.keys()) and self.__cmd in ['db_install', 'createservice']):
                # Override sga_target for autonomous pods
                _sga_size = '5800'

                # 19c ATP image have bigger requirement:
                # ORA-00821: .. sga_target 5808M is too small, needs to be at least 6640M
                if 'version' in list(_dict.keys()) and _dict['version'][:2] == '19':
                    _sga_size = '7000'

                _jconf['dbParams']['sga_memory_size'] = _sga_size

            if (self.__cmd in ['db_install']):
                ebLogInfo('*** Fetching ExadataModel')
                _exadata_model = self.mGetExadataDom0Model()
                _isIntel = self.mIsIntelX9MDom0(self.mReturnDom0DomUPair()[0][0])
                #TODO Need to do this for x10m too
                if not _isIntel and mCompareModel(_exadata_model, 'X9') >= 0:
                    _dpairs = self.mReturnDom0DomUPair()
                    _dom0 = _dpairs[0][0]
                    _domU = _dpairs[0][1]

                    _node = exaBoxNode(get_gcontext())
                    _node.mConnect(aHost=_dom0)

                    _cores = 0
                    _in, _out, _err = _node.mExecuteCmd("/usr/sbin/vm_maker --list --vcpu --domain %s | /bin/awk -F: '{print $4}'" % (_domU))
                    if _out:
                        _out = _out.readlines()
                        _cores = int(_out[0].strip())
                        ebLogInfo('*** CPU count for domain %s : %d' % (_domU, _cores))
                    _node.mDisconnect()

                    if _cores >= 128:
                        ebLogInfo('*** Changing default values for sga and pga !')
                        _sga_size = "14000"
                        _jconf['dbParams']['pga_memory_size'] = "10000"
                        _jconf['dbParams']['sga_memory_size'] = _sga_size

            _dir = 'clusters/'+self.__key+'/config'
            try:
                os.stat(_dir)
            except:
                os.mkdir('clusters/'+self.__key)
                os.mkdir(_dir)

            #
            # Build ocde config file dbname.cfg under /var/opt/oracle/ocde/
            #
            _oracle_home, _dbagent_wallet_loc = self.mGetWalletInfo()
            _key_store = _oracle_home + '/bin/mkstore'
            _config_name= _dir+'/'+str(uuid.uuid1())
            f=open(_config_name,'w')
            ebLogInfo('*** JSON Payload dbParams saved as: '+_config_name)
            for _k in list(_dict.keys()):
                if not self.IsZdlraProv() and "passwd" in _k and _dbagent_wallet_loc and _dict[_k]:
                    _wallet_key = self.mAddWalletEntry(_key_store, _dbagent_wallet_loc, _k, _dict[_k])
                    _password = self.mGetWalletViewEntry(_wallet_key)
                    if _password == _dict[_k]:
                        ebLogInfo('*** Add Wallet Entry success')
                    else:
                        ebLogError('*** Add Wallet Entry Failed')
                    _dict[_k] = _wallet_key

                _v = _dict[_k]
                f.write(_k+'='+str(six.ensure_str(_v.encode('unicode_escape')))+'\n')
                if _k == 'sga_memory_size':
                    f.write('sga_target='+str(_v)+'\n')
                if _k == 'pga_memory_size':
                    f.write('pga_target='+str(_v)+'\n')
                if _k == 'version' and _v == '18100':
                    f.write('version=18000\n')
            f.close()
            self.__dbname_cfg = _config_name
            #
            # Save dbname.cfg in the clusterconfiguration in Dom0
            #
            # xxx/MR: Remove in RC5 for security reason
            # self.mCopyFileToClusterConfiguration(self.__dbname_cfg, 'dbname.cfg')
        #
        # Handle ClusterName update based on rack : name
        #
        _rack_name = ""

        if ebCluCmdCheckOptions(self.__cmd, ['elastic_op']):
            if 'name' in list(_jconf.keys()):
                _rack_name = _jconf['name']
        else:
            if 'rack' in list(_jconf.keys()) \
               and 'name' in list(_jconf['rack'].keys()):
                _rack_name = _jconf['rack']['name']

        if _rack_name:
            self.mPatchClusterName(_rack_name)
        else:

            # Avoid random generation of cluname in MVM
            if not self.__shared_env:
                self.mPatchClusterName()
        #
        # "backupDest":"DISK"
        #
        _backup_disk = False        # This is default
        _skip_backup_disk = False
        if 'rack' in list(_jconf.keys()) \
            and 'backup_disk' in list(_jconf['rack'].keys()):

            _backup_disk = _jconf['rack']['backup_disk']

        _asmss = self.__clusters.mGetCluster().mGetCluAsmScopedSecurity()  
        if 'rack' in list(_jconf.keys()) and 'asmss' in list(_jconf['rack'].keys()):
            _asmss = str(_jconf['rack']['asmss'])
        self.mSetEnableAsmss(_asmss)
        ebLogInfo(f"ASM Scope Security set to {_asmss}")

        # Check if sparse diskgroup is to be created
        _doFlags = ['TRUE', 'Y', 'YES']
        _create_sparse = False
        if 'rack' in list(_jconf.keys()) \
            and 'create_sparse' in list(_jconf['rack'].keys()) \
                and _jconf['rack']['create_sparse'].upper() in _doFlags:
            _create_sparse = True

        # Check if ASM diskgroup storage size has been passed
        if 'rack' in list(_jconf.keys()) \
            and 'gb_storage' in list(_jconf['rack'].keys()):
            self.__dbstorage = str(_jconf['rack']['gb_storage'])+'G'

        if 'exaunitAllocations' in list(_jconf.keys()) \
            and 'storageTb' in list(_jconf['exaunitAllocations'].keys()):
            self.__dbstorage = str(_jconf['exaunitAllocations']['storageTb']*1024)+'G'

        if ( 'rack' in _jconf ) \
            and ( 'ecra_db_rack_name' in _jconf['rack'] ):
            self.__racknameEcra = str(_jconf['rack']['ecra_db_rack_name'])

        if ( 'rack' in _jconf ) \
            and ( 'vmclustertype' in _jconf['rack'] ):
            self.__vmClusterType = str(_jconf['rack']['vmclustertype'])

        # Nid enable based on create service payload
        if 'enable_nid_starterdb' in list(_jconf.keys()):
            ebLogInfo("*** enable_nid_starterdb parameter from create service payload:")
            # handling just in case the user provides another type
            try:
                if _jconf['enable_nid_starterdb'].upper() in _doFlags:
                    self.__enable_nid_starterdb = True
                else:
                    self.__enable_nid_starterdb = False
            except Exception as nidParseError:
                ebLogInfo("*** invalid input from user : {0}".format(nidParseError))


            ebLogInfo("*** NID Support enabled: {0}".format(self.__enable_nid_starterdb))

        if not (self.__cmd == 'vmgi_install' or self.__cmd == 'vmgi_preprov') and not self.IsZdlraProv() and not self.mIsExaScale() and not self.mIsXS():
            if 'rack' not in _jconf.keys():
                _jconf['rack'] = {}
            for _dgid in _cludgroups:
                if _dgid.find('sparsedg') != -1:
                    _create_sparse = True
                    _jconf['rack']['create_sparse'] = "true"
            if 'backup_disk' not in _jconf['rack'].keys():
                _data_size = 0
                _reco_size = 0
                for _dgid in _cludgroups:
                    _dgConfig = self.__storage.mGetDiskGroupConfig(_dgid)
                    _dgName = _dgConfig.mGetDgName().lower()
                    _dgroup_sz = _dgConfig.mGetDiskGroupSize()
                    if _dgName.find('data') != -1:
                        _data_size = int(_dgroup_sz[:-1])
                    elif _dgName.find('reco') != -1:
                        _reco_size = int(_dgroup_sz[:-1])
                if _data_size > _reco_size:
                   _backup_disk = False
                   _jconf['rack']['backup_disk'] = "false"
                else:
                   _backup_disk = True
                   _jconf['rack']['backup_disk'] = "true"

        # If create_sparse is false, remove the DG entry
        if not _create_sparse and (self.__cmd == 'vmgi_install' or self.__cmd == 'vmgi_preprov' or self.__cmd == 'vmgi_reshape' or self.__cmd == 'elastic_cell_update'):
            for _dgid in _cludgroups:
                _dgConfig = self.__storage.mGetDiskGroupConfig(_dgid)
                _dgName = _dgConfig.mGetDgName().lower()
                if _dgName.find('sparse') != -1:
                    self.__storage.mRemoveDiskGroupConfig(_dgid)
                    self.__clusters.mGetCluster().mRemoveCluDiskGroupConfig(_dgid)

        # _jconf['rack']['storage_distribution'] = "59.50:20.50:20"
        _storage_distrib = ""  # Default, as it may not come in prev version
        # DATA:RECO:SPARSE distribution
        if 'rack' in list(_jconf.keys()) \
            and 'storage_distribution' in list(_jconf['rack'].keys()):
            _storage_distrib = _jconf['rack']['storage_distribution']
            ebLogInfo(f'DATA:RECO:SPARSE Distribution : {_storage_distrib}')

        if not self.mIsExaScale() and not self.mIsXS():

            if self.IsZdlraProv():
                self.__ZDLRA.mPatchClusterZdlraDisks(self.__storage, aOptions)
            elif not _skip_backup_disk:

                # Patch the XML cluster configuration file with diskgroup
                # information.
                if ebCluCmdCheckOptions(self.__cmd, ['skip_cluster_config_patching']):
                    ebLogInfo('*** Patch cluster config skipped')
                else:
                    self.__storage.mPatchClusterDiskgroup(_create_sparse, _backup_disk, _storage_distrib, aOptions)

        #
        # Handle SSH Key
        #
        _ssh_public_key = ""
        if 'tools_ssh' in list(_jconf.keys()):
            self.__tools_key_public  = _jconf['tools_ssh']['ssh_public_key'].rstrip()
            self.__tools_key_private = _jconf['tools_ssh']['ssh_private_key']

        # Search for ssh public key in payload under 'vm' section
        # This should come in base64 encoding
        elif 'vm' in list(_jconf.keys()):
            _ssh_public_key = _jconf.get("vm", {}).get("sshkey", False)
            if _ssh_public_key:
                self.__tools_key_public = _ssh_public_key.rstrip()
                self.__tools_key_private = None

                # Verify sshkey is not empty and is base64 encoded
                if not self.__tools_key_public or not check_string_base64(self.__tools_key_public):
                    _err = ("sshkey given on payload is empty or invalid (base64 encoding)")
                    ebLogError(_err)
                    raise ExacloudRuntimeError(0x0823, 0xA, _err)
                ebLogTrace("Exacloud using sshkey on payload under 'vm' field")
                self.__tools_key_public = base64.b64decode(
                        self.__tools_key_public).decode()

        # Check if present in legacy 'sshkey' section if we didn't find it
        # already under vm section
        if 'sshkey' in list(_jconf.keys()) and not _ssh_public_key:
            if _jconf['sshkey'] is not None:
                self.__tools_key_public = _jconf['sshkey'].rstrip()
                self.__tools_key_private = None
            else:
                ebLogError ("*** Error *** SSH Key not provided or incorrect")
                raise ExacloudRuntimeError(0x0406, 0xA, "SSH Key not provided or incorrect", aStackTrace=False)

        if aOptions.debug and self.__tools_key_public:
            ebLogInfo('*** Tools keys found')
        #
        # Compute and set accordingly in the XML values for cores, memory and disk
        #
        # JSON payload fields: vm.size and vm.cores/gb_memory/gb_disk
        # Default configuration is XML : Large
        #
        # Priority (where the values are taken from first):
        #   1. JSON Payload
        #   2. exabox.conf (see mPatchClusterConfig - for more information)
        #   3. XML - default set is Large
        #
        _default_size = 'Large'
        _cores  = ""
        _memory = ""
        _disk   = ""

        # Priority for add node use case (where the values are taken from first):
        #   1. JSON Payload
        #   2. Pick the values from source domU
        #   3. exabox.conf (see mPatchClusterConfig - for more information)
        #   4. XML - default set is Large
        #

        _cores  = self.__vmsizes.mGetVMSize(_default_size).mGetVMSizeAttr('cpuCount')
        _memory = self.__vmsizes.mGetVMSize(_default_size).mGetVMSizeAttr('MemSize').upper()
        _disk   = self.__vmsizes.mGetVMSize(_default_size).mGetVMSizeAttr('DiskSize').upper()

        #Patch the XML with memory which was set in the domU
        if self.__cmd == 'vmgi_reshape':
            _dpairs = self.mReturnDom0DomUPair()
            ebLogTrace(f'***mPatchClusterDB: dom0 and domU pairs: {_dpairs}')
            #The first pair can sometimes be the target node itself.. which is not created yet!
            #All the existing nodes are expected to have same memory
            #Hence, lets loop through, until we get the value
            for _dom0, _domU in _dpairs:
                _hv = getHVInstance(_dom0)
                _domains = _hv.mGetDomains()
                _currvmem = 0

                ebLogTrace(f'***mPatchClusterDB: Existing domains in {_dom0}: {_domains}')
                if _domU in _domains:
                    _currvmem = _hv.mGetVMMemory(_domU, 'CUR_MEM')
                else:
                    _currvmem = _hv.mGetVMMemoryFromConfig(_domU)

                if _currvmem:
                    _memoryGB = int(_currvmem) / 1024
                    if _memoryGB:
                        _memory = str(int(_memoryGB))+'GB'
                        ebLogTrace(f'***mPatchClusterDB: Using memory value: {_memory}')
                        break
        #
        # Fetch VM size from payload if present else default is Large
        #
        if 'vm' in list(_jconf.keys()) and \
            'size' in list(_jconf['vm'].keys()):
            _default_size = str(_jconf['vm']['size'])

        # MR/CODE_CHANGE - Overlay VM RSCS with Elastic Payload if present
        _acc0_vm_size = None
        _acc0_vm_cores = None
        _acc0_vm_ohsize = None
        if 'reshaped_node_subset' in list(_jconf.keys()) and \
            'added_computes' in list(_jconf['reshaped_node_subset'].keys()) and \
            len(_jconf['reshaped_node_subset']['added_computes']) > 0:
            _added_compute_conf0 = _jconf['reshaped_node_subset']['added_computes'][0]

            if 'virtual_compute_info' in list(_added_compute_conf0.keys()) and \
                'vm' in list(_added_compute_conf0['virtual_compute_info'].keys()):

                _acc0_vm = _added_compute_conf0['virtual_compute_info']['vm']
                if 'size' in list(_added_compute_conf0['virtual_compute_info']['vm'].keys()):
                    _acc0_vm_size  = _acc0_vm['size']
                if 'cores' in list(_added_compute_conf0['virtual_compute_info']['vm'].keys()):
                    _acc0_vm_cores = str(_acc0_vm['cores'])
                if 'gb_memory' in list(_added_compute_conf0['virtual_compute_info']['vm'].keys()):
                    _memory  = str(_acc0_vm['gb_memory'])+'GB'
                if 'gb_ohsize' in list(_added_compute_conf0['virtual_compute_info']['vm'].keys()) and _acc0_vm['gb_ohsize']:
                    _acc0_vm_ohsize = _acc0_vm['gb_ohsize']
                    self.__ohsize = str(_acc0_vm_ohsize)+'G'
                ebLogInfo('*** elastic payload with added compute has overwrite vm.size/cores: %s/%s' % (_acc0_vm_size, _acc0_vm_cores))
                # Overwrite default size accordingly
                _default_size = _acc0_vm_size
            else:
                ebLogWarn('*** elastic payload with added_compute does not have VM RSCS section (vm.size/cores/memory/ohsize missing)')

        # MR/CODE_CHANGE - Overlay VM RSCS with Elastic Payload if present
        if _acc0_vm_cores is not None:
            _cores = str(int(_acc0_vm_cores) * _ratio)

        if 'vm' in list(_jconf.keys()):
            if 'cores' in list(_jconf['vm'].keys()):
                _cores = str(_jconf['vm']['cores'])
                #
                # _cores needs to translated to vCPUs.
                # For example on X5 the ratio is usually 2 (e.g. 1 core equals 2 vCPUs)
                #
                # Note: The value returned/stashed in the XML is vCPUs not cores except for info request.
                #       The info request is used to return the patched XML to SDI which only manages cores.
                #
                _cores = str(int(_cores) * _ratio)
            if 'gb_memory' in list(_jconf['vm'].keys()) and not self.mCheckConfigOption('ignore_memory_payload','True'):
                _memory = str(_jconf['vm']['gb_memory'])+'GB'
            if 'gb_disk' in list(_jconf['vm'].keys()):
                _disk = str(_jconf['vm']['gb_disk'])+'GB'
            # Parse OH size over here
            if 'gb_ohsize' in list(_jconf['vm'].keys()) and _jconf['vm']['gb_ohsize']:
                self.__ohsize = str(_jconf['vm']['gb_ohsize'])+'G'
            if 'gb_tmpsize' in list(_jconf['vm'].keys()):
                self._additional_disks.append(("/tmp",str(_jconf['vm']['gb_tmpsize'])+'GB'))
            if 'gb_logsize' in list(_jconf['vm'].keys()):
                self._additional_disks.append(("/var/opt/oracle/logs",str(_jconf['vm']['gb_logsize'])+'GB'))

        if 'exaunitAllocations' in list(_jconf.keys()):
            if 'memoryGb' in list(_jconf['exaunitAllocations'].keys()): 
                _memory = str(_jconf['exaunitAllocations']['memoryGb'])+'GB'
            if 'cores' in list(_jconf['exaunitAllocations'].keys()):
                _cores = str(_jconf['exaunitAllocations']['cores'])
                _cores = str(int(_cores) * _ratio)
            if 'ohomeSizeGb' in list(_jconf['exaunitAllocations'].keys()):
                self.__ohsize = str(_jconf['exaunitAllocations']['ohomeSizeGb'])+'G'

        _u01Disksize = self.mCheckConfigOption('force_vm_u01_disksize')
        if not self.__shared_env and _u01Disksize is None:
            ebLogInfo("DEDICATED ENV DETECTED & force_vm_u01_disksize flag was not defined in exabox.conf")
            _u01Disksize = '150GB'

        if 'force_vm_u01_disksize' in list(_jconf.keys()):
            ebLogInfo("force_vm_u01_disksize flag defined in input payload")
            _u01Disksize = _jconf['force_vm_u01_disksize']

        _fs_sizes = get_max_domu_filesystem_sizes(self)
        if ebDomUFilesystem.U01 in _fs_sizes:
            _u01Disksize = f"{math.ceil(_fs_sizes[ebDomUFilesystem.U01] / GIB)}GB"
            ebLogInfo(f"DomUs /u01 size defined in filesystems/mountpoints field in payload to be {_u01Disksize}")

        if _u01Disksize is not None:
            ebLogInfo(f"*** u01 disksize: {_u01Disksize}")

            _newDisksize = _u01Disksize
            _gbPos = _newDisksize.find("GB")

            if _gbPos == -1:
                _error_str = "Mismatch configuration on 'force_vm_u01_disksize', missing GB at end"
                ebLogError(_error_str)
                raise ExacloudRuntimeError(0x0741, 0xA, _error_str, aStackTrace=False)

            try:
                _newDisksize = int(_newDisksize[0:_gbPos]) - 10 #substract 10 since 10GB is the base size of the VM

                if _newDisksize < 0:
                    _error_str = "Mismatch configuration on 'force_vm_u01_disksize', value must be greater than 10GB"
                    ebLogError(_error_str)
                    raise ExacloudRuntimeError(0x0741, 0xA, _error_str, aStackTrace=False)

            except ValueError as _v:
                _error_str = "Mismatch configuration on 'force_vm_u01_disksize', missing integer before GB"
                ebLogError(_error_str)
                raise ExacloudRuntimeError(0x0741, 0xA, _error_str, aStackTrace=False)

            _oedaReqPath = self.mGetOEDARequestsPath()
            for _pname in ('s_Linux','s_LinuxXen','s_LinuxKvm'):
                _propertyFile = "{0}/properties/{1}.properties".format(_oedaReqPath, _pname)
                ebLogInfo("Change u01 disksize to: 10+{0} on {1}".format(_newDisksize, _propertyFile))
                self.mExecuteLocal("/bin/sed -i 's/VGEXTRASPACE=.*/VGEXTRASPACE={0}/g' {1}".format(_newDisksize, _propertyFile))

        #
        # For now overwrite all size with the default one
        #
        _key   = 'cpuCount'
        self.__vmsizes.mGetVMSize('Large').mSetVMSizeAttr( _key, _cores)
        self.__vmsizes.mGetVMSize('Medium').mSetVMSizeAttr(_key, _cores)
        self.__vmsizes.mGetVMSize('Small').mSetVMSizeAttr( _key, _cores)
        _key   = 'MemSize'
        self.__vmsizes.mGetVMSize('Large').mSetVMSizeAttr( _key, _memory)
        self.__vmsizes.mGetVMSize('Medium').mSetVMSizeAttr(_key, _memory)
        self.__vmsizes.mGetVMSize('Small').mSetVMSizeAttr( _key, _memory)
        _key   = 'DiskSize'
        self.__vmsizes.mGetVMSize('Large').mSetVMSizeAttr( _key, _disk)
        self.__vmsizes.mGetVMSize('Medium').mSetVMSizeAttr(_key, _disk)
        self.__vmsizes.mGetVMSize('Small').mSetVMSizeAttr( _key, _disk)

        #33456175 - UI_OEDAXML IS UPDATING INCORRECTLY IF ONE OF THE VM HAS GUESTCORES
        #self.__guestCores, self.__guestMemory, self.__guestLocalDiskSize should have null check
        #before it access the variables,when self.mGetUiOedaXml() is TRUE & only if one of the domU nodes has Mac cores set.
        if self.mIsKVM() or self.mGetUiOedaXml():
            for _, _domU in self.mReturnDom0DomUPair():
                _domU_mac = self.__machines.mGetMachineConfig(_domU)
                ebLogInfo('Setting domU ' + _domU + " with cores = " +str(_cores))
                _domU_mac.mSetMacCores(_cores)

                ebLogInfo('Setting domU ' + _domU + " with memory = " +str(_memory))
                _domU_mac.mSetMacMemory(_memory)

                ebLogInfo('Setting domU ' + _domU + " with disk = " +str(_disk))
                _domU_mac.mSetMacDisk(_disk)

        if aOptions.debug:
            if _ratio == 1:
                ebLogInfo('*** cpuCount patching done (CORES): '+str(_cores))
            else:
                ebLogInfo('*** cpuCount patching done (VCPUS): '+str(_cores))
            ebLogInfo('*** MemSize  patching done: '+str(_memory))
            ebLogInfo('*** DiskSize patching done: '+str(_disk))
        #
        # Handle TZ (before GI install)
        #
        if self.mIsDebug():
            ebLogInfo('*** TimeZone current GS: %s' % (str(self.__timeZone)))
        if 'rack' in list(_jconf.keys()) \
                and 'timezone' in list(_jconf['rack'].keys()):
            self.__timeZone = _jconf['rack']['timezone']
        elif 'customer' in list(_jconf.keys()) \
                and 'timezone' in list(_jconf['customer'].keys()):
            self.__timeZone = _jconf['customer']['timezone']
        elif 'dbParams' in list(_jconf.keys()):
            if 'timezone' in _jconf['dbParams']:
                _timezone = _jconf['dbParams']['timezone']
                self.__timeZone = _timezone

        _timeZone = 'UNDEFINED'
        if self.__timeZone is not None:
            for _, _domU in self.mReturnDom0DomUPair():
                _domU_mac = self.__machines.mGetMachineConfig(_domU)
                _domU_mac.mSetMacTimeZone(self.__timeZone)
                _timeZone = self.__timeZone
        else:
            for _, _domU in self.mReturnDom0DomUPair():
                _domU_mac = self.__machines.mGetMachineConfig(_domU)
                _timeZone = _domU_mac.mGetMacTimeZone()
                break

        if self.__timeZone is None:
            ebLogInfo('*** TimeZone not specified in payload assuming default XML TZ : %s' % (_timeZone))
        else:
            ebLogInfo('*** TimeZone included in JSon payload XML updated with TZ : %s' % (_timeZone))

        #
        # dbaas api params is optional
        #

        if 'dbaas_api' in list(_jconf.keys()):
            self.__dbaas_api_payload = _jconf['dbaas_api']
        #
        # Update StorageDesc entry
        #
        if not self.mCheckConfigOption('disable_storage_desc','True') and not self.mIsExaScale():
            _tss = 0
            for _dgcid in self.__storage.mGetDiskGroupConfigList():
                _dgc = self.__storage.mGetDiskGroupConfig(_dgcid)
                _dgnm = _dgc.mGetDgName()
                _dgsz = _dgc.mGetDiskGroupSize()
                if _dgsz[-2:] in ['gb', 'GB']:
                    _size = int(_dgsz[:-2])
                    _tss = _tss + _size
                elif _dgsz[-2:] in ['tb', 'TB']:
                    _size = int(_dgsz[:-2]) * 1024
                    _tss = _tss + _size
                elif _dgsz[-1:] in ['g', 'G']:
                    _size = int(_dgsz[:-1])
                    _tss = _tss + _size
                elif _dgsz[-1:] in ['t', 'T']:
                    _size = int(_dgsz[:-1]) * 1024
                    _tss = _tss + _size
                else:
                    ebLogError('*** Invalid format detected in DG size (%s/%s) can not compute TSS' % (_dgnm,_dgsz))
                    _tss = 0
                    break
            if _tss == 0:
                ebLogError('*** TSS not set in XML (default to None)')
                self.__storagedesc.mSetStorageDesc('TotalStorageSize', 'None')
            else:
                _tss = str(_tss)+'G'
                ebLogInfo('*** TSS set in XML to: %s' % (_tss))
                self.__storagedesc.mSetStorageDesc('TotalStorageSize', _tss)

    def mBaseSystemConfiguration(self, aOptions):

        _exa_base_v = None
        if aOptions is not None:
            _jconf = aOptions.jsonconf
            if _jconf is not None and 'rack' in list(_jconf.keys()) and 'size' in list(_jconf['rack'].keys()) and _jconf['rack']['size'] == "BASE-RACK":
                ebLogInfo("Base System Configuration detected")

                _exadata_model = self.mGetExadataDom0Model()
                if self.mCompareExadataModel(_exadata_model, 'X10') >= 0:
                    _exa_base_v = {'BASEVERSION': '3', 'CLOUDBASE': 'TRUE'}
                else:
                    _exa_base_v = {'BASEVERSION': '2'}

        _appliedCommand = False

        if aOptions is not None:
            _jconf = aOptions.jsonconf

            if _jconf is not None and "kvmroce" in _jconf:
                _kvmroce_conf = _jconf['kvmroce']
                if 'storageNetwork' in _jconf['kvmroce']:
                    _storageNetwork = _jconf['kvmroce']['storageNetwork']
                    for _cell, _details in _jconf['kvmroce']['storageNetwork'].items():
                        if "celltype" in _details:

                            _appliedCommand = True

                            if _details["celltype"] == "X-Z":
                                _exa_base_v = {'BASEVERSION': '4'}
                                _cmd = ["ALTER MACHINE", {"type": "X8MHC"}, {"hostname": _cell}]
                                self.__extraXmlPrePatchingCommands.append(_cmd)
                            else:
                                _cmd = ["ALTER MACHINE", {"type": _details["celltype"]}, {"hostname": _cell}]
                                self.__extraXmlPrePatchingCommands.append(_cmd)

        if not _appliedCommand:
            def _mFetchCellPrePatchingCmd(aCell, aXmlPatchingCommand):
                _cell = aCell
                _xmlpatching_command = aXmlPatchingCommand

                _node = exaBoxNode(get_gcontext())
                if _node.mIsConnectable(aHost=_cell):

                    _exadata_model = self.mGetNodeModel(_cell)

                    if mCompareModel(_exadata_model, 'X11') >= 0:
                        return

                    elif mCompareModel(_exadata_model, 'X10') >= 0:
                        _cmd = ["ALTER MACHINE", {"type": "X10MHC"}, {"hostname": _cell}]
                        _xmlpatching_command.append(_cmd)

                    elif mCompareModel(_exadata_model, 'X9') >= 0:
                        _cmd = ["ALTER MACHINE", {"type": "X9MHC"}, {"hostname": _cell}]
                        _xmlpatching_command.append(_cmd)

                    elif mCompareModel(_exadata_model, 'X8') >= 0:
                        _cmd = ["ALTER MACHINE", {"type": "X8MHC"}, {"hostname": _cell}]
                        _xmlpatching_command.append(_cmd)

            _plist = ProcessManager()
            _xmlpatching_commands = _plist.mGetManager().list()

            for _cell in list(self.mReturnCellNodes().keys()):
                _p = ProcessStructure(_mFetchCellPrePatchingCmd, [_cell, _xmlpatching_commands], _cell)
                _p.mSetMaxExecutionTime(60*60)
                _p.mSetJoinTimeout(5)
                _p.mSetLogTimeoutFx(ebLogWarn)
                _plist.mStartAppend(_p)
            _plist.mJoinProcess()

            self.__extraXmlPrePatchingCommands.extend(_xmlpatching_commands)

        if _exa_base_v:
            _cmd = ["ALTER ES", _exa_base_v, {}]
            self.__extraXmlPrePatchingCommands.append(_cmd)

        _cell_list = ",".join(list(self.mReturnCellNodes().keys())).strip()
        _cluster = self.mGetClusters().mGetCluster()

        if _cell_list and _cluster:

            _cluster_groups = _cluster.mGetCluDiskGroups()
            for _dgid in _cluster_groups:
                _dgname = self.mGetStorage().mGetDiskGroupConfig(_dgid).mGetDgName()
                _cmd = ["ALTER DISKGROUP", {"CELLLIST": _cell_list}, {"DISKGROUPNAME": _dgname,"CLUSTERNUMBER": "1"}]
                self.__extraXmlPatchingCommands.append(_cmd)


    """
    ::mReturnCellNodes
        Return a dictionary structure where each entry corresponds to a cell of the cluster.
        Each entry contains all the interfaces/ip/dns name for a given cell.
            Example:
            [['scas07celadm12', 'admin', 'eth0', '10.128.76.118'],
            ['scas07celadm12-priv1', 'private', 'stpkeyib0', '201.168.76.137'],
            ['scas07celadm12-priv2', 'private', 'stpkeyib1', '201.168.76.138']]
    """
    def mReturnCellNodes(self, aNetMask=False, aIsClusterLessXML=False, aIsXS=False):

        if self.mIsExaScale():
            return {}

        if aIsClusterLessXML:
            _excluded_node_list = self.mGetExcludedList()
            _data={}
            for _cell in self.mReadCellMachines():
                if _cell not in _excluded_node_list:
                    _data[_cell]=[]
            return _data

        _cell_dict = {}
        _clu = self.__clusters.mGetCluster()
        if not _clu:
            return {}

        if self.mIsXS() or aIsXS:
           _sp_list = self.__storage.mGetStoragePoolConfigList()
           for _spname in _sp_list:
                _sp = self.__storage.mGetStoragePoolConfig(_spname)
                _mac_list = _sp.mGetStoragePoolMachines()
                for _mac in _mac_list:
                    _cell_dict[_mac] = None
        else:
            _dg_list = self.__clusters.mGetCluster().mGetCluDiskGroups()
            for _dgname in _dg_list:
                _dg = self.__storage.mGetDiskGroupConfig(_dgname)
                _mac_list = _dg.mGetDiskGroupMachines()
                for _mac in _mac_list:
                    _cell_dict[_mac] = None

        _cell_data = {}
        for _cellname in list(_cell_dict.keys()):
            _cell_mac = self.__machines.mGetMachineConfig(_cellname)

            _net_list = _cell_mac.mGetMacNetworks()
            _cell_name = None
            _cell_def   = []

            for _net in _net_list:
                _priv = self.__networks.mGetNetworkConfig(_net)
                if _priv.mGetNetType() == 'admin':
                    _cell_name = _priv.mGetNetHostName()+'.'+_priv.mGetNetDomainName()

                if aNetMask:
                    _cell_def.append([_cell_name,_priv.mGetNetType(), _priv.mGetNetMaster(), _priv.mGetNetIpAddr(), _priv.mGetNetMask()])
                else:
                    _cell_def.append([_cell_name,_priv.mGetNetType(), _priv.mGetNetMaster(), _priv.mGetNetIpAddr()])

            _cell_data[_cell_name] = _cell_def

        # Validate if exist duplicate IPs in cells
        _duplicate_ip_cells = self.mDetectDuplicatedIPCells(_cell_data)
        if _duplicate_ip_cells:
            ebLogError(f'*** Duplicate IPs detected in cells: {_duplicate_ip_cells}')
            raise ExacloudRuntimeError(0x0826, 0xA, "Duplicate IPs in cells")

        return _cell_data

    def mDetectDuplicatedIPCells(self, aCellNodesDictionary):
        """
            mDetectDuplicatedIPCells
            Return a list of the duplicated cell nodes
            Example:

            :param dict aCellNodesDictionary: {
                'cell_name':
                    [
                        ['scas07celadm12', 'admin', 'eth0', '10.128.76.118'],
                        ['scas07celadm12-priv1', 'private', 'stpkeyib0', '201.168.76.137'],
                        ['scas07celadm12-priv2', 'private', 'stpkeyib1', '201.168.76.137']
                    ]
                }

            :return list _duplicated_ips_cells_list: [
                        ['scas07celadm12-priv1', 'private', 'stpkeyib0', '201.168.76.137'],
                        ['scas07celadm12-priv2', 'private', 'stpkeyib1', '201.168.76.137']
                    ]
        """
        _duplicate_ips_cells_list = []
        _duplicate_ips_list = self.mReturnDuplicateCellIPs(aCellNodesDictionary)
        if _duplicate_ips_list:
            for cell_name in aCellNodesDictionary:
                for cell in aCellNodesDictionary[cell_name]:
                    if cell[3] in _duplicate_ips_list:
                        _duplicate_ips_cells_list.append(cell)

        return _duplicate_ips_cells_list

    def mExaccCopyCustomExadataDomuImageToDom0(self, aImageVersion):
        """
            mExaccCopyCustomExadataDomuImageToDom0
            Return None.
            
            :param aImgeVersion. The desired ExadataImageVersion in oedacli format (e.g 21.2.13.0.0.220602).

            :Exceptions: It raises an exception if the System.first.boot image is not properly set in dom0s
                to be consumed by Exdata at Create VM step.

            
            Description:
            This method is going to copy the Exadata System.first.boot image from exacloud machine to
                the dom0s.

            Note: 
               -For making the new exabox.conf parameter 'exadata_custom_domu_version' work it is required to place the 
                System.first.boot image of the required version either, in the dom0s at /EXAVMIMAGES/System.first.boot.<version>.img
                or in the //u01/downloads/exadata/images/System.first.boot.<version>.img.bz2 location.
                If the flag 'exadata_custom_domu_version' is passed, it is mandatory that image exists
                in those mentioned location, otherwise an exception will be raised.

        """
        ebLogTrace("Running mExaccCopyCustomExadataDomuImageToDom0")
        ebLogTrace("Custom Image: {}".format(aImageVersion))
        
        def _mExecuteCopy(aDom0, _rc_status):
            _dom0 = aDom0
            _rc_status[_dom0] = 0
            _imgrev = self.mCheckConfigOption('exadata_custom_domu_version')
            _repo_download_location = self.mCheckConfigOption('ociexacc_exadata_patch_download_loc')

            _succ = False
            try:
                _remoteImgFound, _, _imgCopied = \
                  copyVMImageVersionToDom0IfMissing(_dom0, _imgrev,
                                                    self.mIsKVM(),
                                                    _repo_download_location)

                _succ = _remoteImgFound or _imgCopied
            except Exception as copyError:
                ebLogError("Error {0}".format(copyError))
            finally:
                if not _succ:
                    _rc_status[_dom0] = 0x0730

        _plist = ProcessManager()
        _rc_status = _plist.mGetManager().dict()

        # Parallelize execution on dom0s
        _dpairs = self.mReturnDom0DomUPair()

        for _dom0, _ in _dpairs:
            _p = ProcessStructure(_mExecuteCopy, [_dom0, _rc_status], _dom0)
            _p.mSetMaxExecutionTime(60*60) # 30 minutes
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()

        # validate the return codes
        _rc_all = 0

        for _dom0, _ in _dpairs:
            if _rc_status[_dom0]:
                raise Exception("Error: mExaccCopyCustomExadataDomuImageToDom0 failed at dom0: {}".format(_dom0))
        ebLogTrace("Custom DomU imge is OK in all Dom0s!")                 

    def mAddPatchCustomDomuImageOedacliCmd(self, aImageVersion):
        """
            mAddPatchCustomDomuImageOedacliCmd:
            Return None.

            :param aImgeVersion. The desired ExadataImageVersion in oedacli format (e.g 21.2.13.0.0.220622).

            Description: This method adds the proper oedacli command to the cmd list to be executed in the
                oedaxml patching. Note that, in ExaCC there is only one cluster in the xml used by OEDA,
                hence clusternumber is hardcoded to 1.
        """
        ebLogTrace("Running mAddPatchCustomDomuImageOedacliCmd")
        _cmd = ["ALTER MACHINES",
                {"imageversion": aImageVersion},
                {"clusternumber": "1"}]
        
        ebLogTrace("Added Command: {}".format(_cmd))

        self.__extraXmlPatchingCommands.append(_cmd)
        
    def mReturnDuplicateCellIPs(self, aCellNodesDictionary):
        """
            mReturnDuplicateCellIPs
            Return a list of the duplicate ips in a cell nodes dictionary
            Example:

            :param dict aCellNodesDictionary: {
                'cell_name':
                    [
                        ['scas07celadm12', 'admin', 'eth0', '10.128.76.118'],
                        ['scas07celadm12-priv1', 'private', 'stpkeyib0', '201.168.76.138'],
                        ['scas07celadm12-priv2', 'private', 'stpkeyib1', '201.168.76.138']
                    ]
                }

            :return list _ips_list: ['201.168.76.138']
        """
        _ips_list = []
        for cell_name in aCellNodesDictionary:
            _ips_list.extend([x[3] for x in aCellNodesDictionary[cell_name]])
        _duplicated_ips_list = [x for i, x in enumerate(_ips_list) if i != _ips_list.index(x)]
        return _duplicated_ips_list

    def mReturnSwitches(self,aMode, aRoceQinQ=False, aEnableSpineSwitchPatching=False):

        def mCalculateSwitchesExtra(aClubox, aFilter):

            _switches = []
            _list = aClubox.mGetSwitches().mGetSwitchesDict(aFilter)
            for _s in _list:
                _neto = self.mGetNetworks().mGetNetworkConfig(_s["swnetid"])
                _s["fqdn"] = _neto.mGetNetHostName()+'.'+_neto.mGetNetDomainName()
                _switches.append(_s)

            return _switches

        def mCalculateSwitches(aClubox, aFilter):
            _switches = mCalculateSwitchesExtra(aClubox, aFilter)
            return list(map(lambda x: x["fqdn"], _switches))

        # Create a registry of the switches only for KVM-Roce
        if (self.__ociexacc or self.__exacm) and self.mIsKVM():

            if not get_gcontext().mCheckRegEntry("ROCE_SWITCHES"):

                # Filter only roce switch
                _switches = mCalculateSwitchesExtra(self, False)
                _roceSwitches = []
                _roce_spine_switches = []

                for _switch in _switches:
                    if _switch["fqdn"].lower().find("roce") != -1:
                        if "spine" not in _switch["swdesc"].lower():
                            _roceSwitches.append(_switch["fqdn"])
                '''
                 Spine switch patching will be performed only in 
                 case of PatchSwitchType set to "rocespine" or "spine"
                 or "all"
                '''
                if aEnableSpineSwitchPatching:
                    _roce_spine_switches = self.mReturnandConnectRoceSpineSwitches()
                    if len(_roce_spine_switches) > 0:
                        _roceSwitches.extend(_roce_spine_switches)

                get_gcontext().mSetRegEntry("ROCE_SWITCHES", _roceSwitches)

        else:

            if get_gcontext().mCheckRegEntry("ROCE_SWITCHES"):
                get_gcontext().mDelRegEntry("ROCE_SWITCHES")

        # This method should not be called in the context of KVM -- log a warning and return [] as no Switches exists
        if self.mIsKVM() and not aRoceQinQ:
            ebLogTrace('*** Access to switches not a valid code flow on KVM systems')
            return []

        # Return switch list on QnQ
        if get_gcontext().mCheckRegEntry("ROCE_SWITCHES") and aRoceQinQ:
            return get_gcontext().mGetRegEntry("ROCE_SWITCHES")

        return list(set(mCalculateSwitches(self, aMode)))

    def mGetExaKmsHostMap(self, aAddIloms=True):

        _exakms = get_gcontext().mGetExaKms()

        def mUnmaskNat(aList):
            return list(map(lambda x: _exakms.mGetEntryClass().mUnmaskNatHost(x), aList))

        _dom0s, _domUs, _cells, _switches = self.mReturnAllClusterHosts()
        _hostList = mUnmaskNat(_dom0s + _domUs + _cells + _switches)

        _hostDict = {}

        for _host in _hostList:

            _type = None
            if _host in mUnmaskNat(_dom0s):
                _hostDict[_host] = ExaKmsHostType.DOM0
            elif _host in mUnmaskNat(_domUs):
                _hostDict[_host] = ExaKmsHostType.DOMU
            elif _host in mUnmaskNat(_cells):
                _hostDict[_host] = ExaKmsHostType.CELL
            elif _host in mUnmaskNat(_switches):
                _hostDict[_host] = ExaKmsHostType.SWITCH

        # Expand switches
        if _switches:

            if not get_gcontext().mCheckRegEntry("ADDED_SWITCHES"):

                _node = exaBoxNode(get_gcontext())

                try:
                    _node.mConnect(_switches[0])

                    _cmd = """/usr/sbin/ibswitches | /bin/awk '{print "/bin/echo -n "$10".; /bin/hostname -d"}' | /bin/bash"""
                    _, _o, _ = _node.mExecuteCmd(_cmd)
                    _addedSwitches = list(map(lambda x: x.strip(), _o.readlines()))

                    get_gcontext().mSetRegEntry("ADDED_SWITCHES", _addedSwitches)

                except Exception as e:
                    ebLogWarn(e)

                finally:
                    _node.mDisconnect()

            if get_gcontext().mCheckRegEntry("ADDED_SWITCHES"):
                for _host in get_gcontext().mGetRegEntry("ADDED_SWITCHES"):
                    _hostDict[_host] = ExaKmsHostType.SWITCH

        # Expand roce switches
        if get_gcontext().mCheckRegEntry("ROCE_SWITCHES"):
            for _host in get_gcontext().mGetRegEntry("ROCE_SWITCHES"):
                _hostDict[_host] = ExaKmsHostType.SWITCH

        # Add iloms
        if aAddIloms:

            _macList = self.__machines.mGetMachineConfigList()
            for _mac in _macList:
                _macConfig = self.__machines.mGetMachineConfig(_mac)

                _iloms = _macConfig.mGetMacIlomNetworks()

                for _ilom in _iloms:
                    _ilomCfg = self.__iloms.mGetIlomConfig(_ilom)
                    _ilomNet = self.__networks.mGetNetworkConfig(_ilomCfg.mGetIlomNetworkId())
                    _ilomName = "{0}.{1}".format(_ilomNet.mGetNetHostName(), _ilomNet.mGetNetDomainName())

                    _hostDict[_ilomName] = ExaKmsHostType.ILOM

        return _hostDict

    def mReturnAllClusterHosts(self, aClusterId=None, aRetDummyDomu=True):

        _ddpair   = self.mReturnDom0DomUPair(aClusterId=aClusterId, aIsClusterLessXML=self.mIsClusterLessXML(), aRetDummyDomu=aRetDummyDomu)
        _dom0s    = []
        _domUs    = []
        _cells    = list(self.mReturnCellNodes(aIsClusterLessXML=self.mIsClusterLessXML()).keys())
        _switches = self.mReturnSwitches(True)

        for _d0, _du in _ddpair:
            _dom0s.append(_d0)
            if _du:
                _domUs.append(_du)

        return _dom0s, _domUs, _cells, _switches

    def mGenerateExadataImageMapEntry(self, aHostname):

        #Get the name of the nat
        _ctx = get_gcontext()
        _host = aHostname
        if _ctx.mCheckRegEntry('_natHN_' + _host):
            _host = _ctx.mGetRegEntry('_natHN_' + _host)

        _domUMap = {}
        for _dom0, _domU in self.mReturnDom0DomUPair():
            _domUNat = _domU
            if _ctx.mCheckRegEntry('_natHN_' + _domU):
                _domUNat = _ctx.mGetRegEntry('_natHN_' + _domU)
            _domUMap[_domUNat] = _dom0

        #Add the operating system to the dictionary
        if _host not in self.mGetExadataImagesMap().keys():

            try:

                if _host in _domUMap.keys():

                    # In case of domU, is going to review the "System.first.boot.<Version>.img"
                    if not self.__ut:

                        _dom0 = _domUMap[_host]

                        _firstBoot = ""

                        # Verify image version from payload
                        if self.__options.jsonconf:

                            # Tmp key case
                            try:
                                if "domus_image_version" in self.__options.jsonconf:
                                    for _key, _value in self.__options.jsonconf["domus_image_version"].items():
                                        if _host == _key:
                                            _firstBoot = _value
                            except Exception as e:
                                ebLogTrace(f"Error: {e}")
                                _firstBoot = ""

                            # Patching case
                            if "Params" in self.__options.jsonconf:
                                for _param in self.__options.jsonconf["Params"]:

                                    if "TargetType" in _param and \
                                       "domu" in list(map(lambda x: x.lower(), _param["TargetType"])):
                                        _firstBoot = _param["TargetVersion"]

                        # Verify the exabox.conf parameter
                        if not _firstBoot:
                            _custom_img_ver = hasDomUCustomOS(self)
                            if _custom_img_ver:
                                _firstBoot = _custom_img_ver

                        # Verify the env
                        if not _firstBoot:

                            dom0sImagesVersion = mGetDom0sImagesListSorted(self)
                            vmImageVersionNum = dom0sImagesVersion[0]

                            if not vmImageVersionNum: # empty string if Dom0 has broken

                               _msg = "One Dom0 have a broken image."
                               ebLogError(_msg)
                               raise ExacloudRuntimeError(0x0730, 0xA, _msg, aStackTrace=False)

                            _firstBoot = vmImageVersionNum

                        self.mGetExadataImagesMap()[_host] = _firstBoot

                else:

                    _version = self.mGetImageVersion(_host)
                    self.mGetExadataImagesMap()[_host] = _version

            except Exception as e:
                ebLogWarn(e)
                self.mGetExadataImagesMap()[_host] = None


    def mRemoveExaCCMacrosVerify(self, aHostType):

        if not self.mIsOciEXACC():
            ebLogInfo("mRemoveExaCCMacrosVerify is only for ExaCC env")
            return

        if self.mCheckConfigOption('exacc_remove_macros_verify', 'False'):
            ebLogInfo("Skip mRemoveExaCCMacrosVerify, exacc_remove_macros_verify property not True")
            return

        _dom0s, _domUs, _, _ = self.mReturnAllClusterHosts()
        _hosts = []

        if aHostType == "Dom0":
            _hosts = _dom0s

        elif aHostType == "DomU":
            _hosts = _domUs

        for _host in _hosts:

            with connect_to_host(_host, get_gcontext()) as _node:

                # Detect OL8
                _, _o, _ = _node.mExecuteCmd("/bin/cat /etc/oracle-release | grep 'release 8'")
                _hostOL8 = _node.mGetCmdExitStatus() == 0

                if _hostOL8:
                    ebLogInfo(f"Removing /etc/rpm/macros.verify in {_host}")

                    if not _node.mFileExists("/etc/rpm/macros.verify"):
                        ebLogInfo(f"File not found in {_host}")
                    else:
                        _node.mExecuteCmdLog("/bin/rm -f /etc/rpm/macros.verify")

                        if _node.mGetCmdExitStatus() == 0:
                            ebLogInfo(f"File removed in {_host}")
                        else:
                            ebLogInfo(f"Could not remove file in {_host}")


                else:
                    ebLogInfo(f"Skip removal of /etc/rpm/macros.verify in {_host}")


    def mHandlerPatchClusterInfo(self):
        aOptions = self.mGetArgsOptions()
        return self.mPatchClusterInfo(aOptions)

    def mPatchClusterInfo(self, aOptions=None, aData=None):

        _cluname = self.__clusters.mGetCluster().mGetCluName()
        if aData:
            _jconf = aData
        else:
            _jconf = aOptions.jsonconf

        if 'TIMEZONE' in list(_jconf.keys()):
            _key = 'TIMEZONE'
            _value = _jconf['TIMEZONE']
        else:
            raise ExacloudRuntimeError(0x0746, 0xA, 'Invalid Input', aStackTrace=False)

        _injector = V1OedaXMLRebuilder()
        _injector.SavePropertiesFromTemplate(self.__patchconfig)

        _dom0s, _domUs, _cells, _ = self.mReturnAllClusterHosts()
        _host_list = _dom0s + _domUs + _cells

        for _host in _host_list:
            _cmd = 'oeda/oedacli -c {0} -e ALTER MACHINE {1} = \"{2}\" WHERE HOSTNAME = \"{3}\"'.format(self.__patchconfig, _key, _value, _host)
            self.mExecuteLocal(_cmd, aCurrDir=self.mGetBasePath())

        _injector.ProcessOedaCliXML(self.__patchconfig)

        _reqobj = self.mGetRequestObj()
        if _reqobj:
            _reqobj.mSetXml(self.__patchconfig)
            _db = ebGetDefaultDB()
            _db.mUpdateRequest(_reqobj)

        ebLogInfo(f'Updated {_key}: {_value} in the cluster xml: {self.__patchconfig}')

    def mPatchSSHDConfig(self):

        _pair = self.mReturnDom0DomUPair()
        for _dom0, _domU in _pair:

            _domU_mac = self.__machines.mGetMachineConfig(_domU)
            _domU_net_list = _domU_mac.mGetMacNetworks()
            _config_changed = False

            with connect_to_host(_domU, get_gcontext()) as _node:
                for _net in _domU_net_list:
                    _priv = self.__networks.mGetNetworkConfig(_net)
                    # If IPv4 and IPv6 client networks are present, we can have 2 networks
                    # same network type i.e. client. And the below condition will be executed
                    # for both IPv4 and IPv6 networks
                    if _priv.mGetNetType() == 'client':
                        _client_ip = _priv.mGetNetIpAddr()
                        ebLogInfo('*** Checking SSHD configuration on : %s (%s)' % (_domU, _client_ip))
                        _cmdstr = 'grep '+_client_ip +' /etc/ssh/sshd_config'
                        _i, _o, _e = _node.mExecuteCmd(_cmdstr)
                        _out = _o.readlines()
                        _found = False
                        if _out and len(_out):
                            for _l in _out:
                                if _client_ip in _l:
                                    _found = True
                                    ebLogInfo('*** SSHD Config already setup for Client network')
                        if not _found:
                            ebLogInfo('*** Patching SSHD Config to add Client network')
                            # if ipv6 address is present in client network, we need AddressFamily to be any
                            if ":" in _client_ip:
                                _cmdstr = "sed -i 's/AddressFamily inet/AddressFamily any/g' /etc/ssh/sshd_config"
                                _node.mExecuteCmdLog(_cmdstr)
                            _cmdstr = 'echo "ListenAddress %s" >> /etc/ssh/sshd_config' % (_client_ip)
                            _node.mExecuteCmdLog(_cmdstr)
                            _config_changed = True
                        if self.isATP() and self.mCheckClusterNetworkType() and self.__exabm:
                            # ADBD doesn't want to configure IPv6 for backup network
                            _backup_ip = self.__ATP.mGetBackupIP(_domU, self.mGetMachines(), self.mGetNetworks())
                            if _backup_ip and ':' not in _backup_ip:
                                ebLogInfo('*** ATP Patching SSHD Config to add Backup network')
                                _cmdstr = 'echo "ListenAddress %s" >> /etc/ssh/sshd_config' % (_backup_ip)
                                _node.mExecuteCmdLog(_cmdstr)
                            AtpCreateAtpIni(_node, self.__ATP, self.__options.jsonconf["customer_network"], _domU).mExecute()
                            _config_changed = True
                if _config_changed:
                    ebLogInfo('*** Restarting SSHD server with new configuration')
                    _cmdstr = 'service sshd restart'
                    _node.mExecuteCmdLog(_cmdstr)


    def mPatchCellsSSHDConfig(self):

        if ebCluCmdCheckOptions(self.__cmd, ['dont_patch_cells_sshd_config']):
            return

        def _patch_sshd(_cell_name):
            #patch sshd_config of cell nodes to add 'MaxStartups 100'
            #take backup of original sshd_config as sshd_config.backup_by_exacloud
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_cell_name)

            #check if MaxStartups has already set as 100
            _cmdstr = "/usr/sbin/sshd -T | grep  -i maxstartups | cut -d ' ' -f 2 |  cut -d ':' -f 1"
            _i, _o, _e = _node.mExecuteCmd(_cmdstr)
            _out = _o.readlines()
            if _out and _out[0] and '100' == _out[0].strip():
                _node.mDisconnect()
                return

            ebLogInfo('*** Patching SSHD Config on %s to update MaxStartups to 100' % (_cell_name))
            _cmdstr = "sed -i'.backup_by_exacloud' '/MaxStartups/d; ${p;s/.*/MaxStartups 100/}' /etc/ssh/sshd_config"
            _node.mExecuteCmdLog(_cmdstr)

            ebLogInfo('*** Restarting SSHD server with new configuration')
            _cmdstr = 'service sshd restart'
            _node.mExecuteCmdLog(_cmdstr)

            _node.mDisconnect()

        _plist = ProcessManager()

        for _cell_name in self.mReturnCellNodes():
            _p = ProcessStructure(_patch_sshd, [_cell_name,])
            _p.mSetMaxExecutionTime(30*60) #30 minutes timeout
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()


    def mCheckCellNodes(self):

        for _cell in self.mReturnCellNodes():
            ebLogInfo('*** Cell: %s' % (_cell))

    """
    Secure SSH on Cells

    !!! Warning more than likely you will not be able to access the cells after running this command
        unless you use the management network of the ED rack. !!!
    """
    def mSecureCellsSSH(self, aCellList=None):
        #
        # Secure Dom0 SSH should not be enforced in non production env. and/or shared env (until multi-vm suppoprt is added)
        #
        # commenting out check for shared env
        if self.mEnvTarget() is False:
            ebLogInfo('*** SecureCells SSH is skipped for non production environment')
            return
        #
        # Retrieve also ECRA/localhost IP
        #
        _localip = socket.gethostbyname(socket.gethostname())
        _localmask = '255.255.255.255'
        #
        # Retrieve also ECRA/localhost IP (obsolete because of NAT env.) - kept as reference
        #
        _host = socket.gethostname()
        _cmd = '/bin/nslookup ' + _host
        _, _, _o, _ = self.mExecuteLocal(_cmd)
        if _o is not None:
            try:
                _ip2 = _o.strip().split('\n')[-1].split(' ')[-1]
                if re.match('\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', _ip2) is None:
                    _ip2 = None
            except:
                _ip2 = None
        if _ip2 is not None:
            _localip = _ip2

        if aCellList:
            _cell_list = aCellList
        else:
            _cell_list = self.mReturnCellNodes()
        #
        # Build string for hostctrl white-list (hostctrl_whitelist) based on configuration
        #
        _subnetset = ebSubnetSet()
        _subnetips = list()
        _current_options = self.mGetArgsOptions()
        _cidr_subnet_ips = list()
        _cidr_network_identifiers = list()
        if _current_options is not None and _current_options.jsonconf is not None and "ecra" in _current_options.jsonconf.keys():
            _ecra_options_dict = _current_options.jsonconf.get('ecra')
            if _ecra_options_dict is not None:
                _current_ecra_subnet_cidrs = _ecra_options_dict.get('whitelist_cidr',[])
                if len(_current_ecra_subnet_cidrs) > 0:
                    ebLogInfo(f"Updating host access control information with CIDR block: {_current_ecra_subnet_cidrs}")
                    for _current_ecra_subnet_cidr in _current_ecra_subnet_cidrs:
                        _info_split = _current_ecra_subnet_cidr.split("/")
                        if len(_info_split) < 2 or _info_split[1].isnumeric() == False:
                            _error_msg = f"Invalid subnet CIDR information: {_current_ecra_subnet_cidr}"
                            ebLogError(_error_msg)
                            raise ExacloudRuntimeError(0x0119, 0xA, _error_msg, aStackTrace=False)

                        _cidr_subnet_ips.append(_info_split[0])
                        _cidr_network_identifiers.append(_info_split[1])

        if len(_cidr_subnet_ips) == 0:
            _subnetset.mAppendList(self.mCheckConfigOption('hostctrl_whitelist'))
            _subnetips = _subnetset.mGetSubnetList()
        #
        # Add extra whitelist IPs
        #
        def _add_extra_ips(_ruleList, _user, _extra_ips_list):
            for _extra_ips in _extra_ips_list:
                for _extra_ip in _extra_ips:
                    _addr, _mask = _extra_ip.split('/')
                    _ruleList.append([_user, _addr, _mask])
        #
        # Restrict SSH access to mgmt network (also add ECRA/localhost)
        #
        def _secure_cell(_cell_name):

            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_cell_name)

            _cmdstr = 'ip addr show eth0 | grep -w inet'
            _i, _o, _e = _node.mExecuteCmd(_cmdstr)
            if _node.mGetCmdExitStatus():
                _node.mDisconnect()
                raise Exception('mExecuteCmd Failed', _node.mGetHostname(), _cmdstr)
            _ipaddr =  _o.readline().strip().split(' ')[1]
            _cmdstr = 'ipcalc -nm ' + _ipaddr
            _i, _o, _e = _node.mExecuteCmd(_cmdstr)
            if _node.mGetCmdExitStatus():
                _node.mDisconnect()
                raise Exception('mExecuteCmd Failed', _node.mGetHostname(), _cmdstr)

            _mask = _o.readline().split('=')[1].strip()
            _addr = _o.readline().split('=')[1].strip()

            #Get list of IPs for whitelist ECRA
            _ecraips = self.mOpenSSHFromECRA(_node)

            #Export the configuration file
            _timestamp = str(time.time()).replace(".", "")
            _exportFile = "/tmp/cellos_access_export{0}.txt".format(_timestamp)
            _importFile = "/tmp/cellos_access_import{0}.txt".format(_timestamp)

            _cmdstr = "/opt/oracle.cellos/host_access_control access-export --file='{0}'".format(_exportFile)
            ebLogInfo("Running on {0} the cmd {1}".format(_node.mGetHostname(), _cmdstr))
            _node.mExecuteCmdLog(_cmdstr)

            #Verify all the rules to match on Root
            _newRules = {}
            _ruleList = []

            if len(_cidr_subnet_ips) > 0 and len(_cidr_network_identifiers) > 0:
                _ruleList.append(["root", _addr, _mask])
                for _cidr_idx, _cidr_subnet_ip in enumerate(_cidr_subnet_ips):
                    _ruleList.append(["root", _cidr_subnet_ip, _cidr_network_identifiers[_cidr_idx]])
            else:
                _ruleList.append(["root", _localip, _localmask])
                _ruleList.append(["root", _addr, _mask])
                _add_extra_ips(_ruleList, "root", [_subnetips, _ecraips])

            #Verify all the rules to match on secscan
            if len(_cidr_subnet_ips) > 0 and len(_cidr_network_identifiers) > 0:
                _ruleList.append(["secscan", _addr, _mask])
                for _cidr_idx, _cidr_subnet_ip in enumerate(_cidr_subnet_ips):
                    _ruleList.append(["secscan", _cidr_subnet_ip, _cidr_network_identifiers[_cidr_idx]])
            else:
                _ruleList.append(["secscan", _localip, _localmask])
                _ruleList.append(["secscan", _addr, _mask])
                _add_extra_ips(_ruleList, "secscan", [_subnetips, _ecraips])

            # Bug 25404373 (b): Add cellmonitor to the list of user allowed to access cell
            _cellmonitor_ip = self.mCheckConfigOption('cellmonitor_subnet')
            if _cellmonitor_ip is not None:
                _localip2   = _cellmonitor_ip.split('/')[0]
                _localmask2 = _cellmonitor_ip.split('/')[1]

                _ruleList.append(['cellmonitor', _localip2, _localmask2])

            if len(_cidr_subnet_ips) > 0 and len(_cidr_network_identifiers) > 0:
                _ruleList.append(['cellmonitor', _addr, _mask])
                for _cidr_idx, _cidr_subnet_ip in enumerate(_cidr_subnet_ips):
                    _ruleList.append(["cellmonitor", _cidr_subnet_ip, _cidr_network_identifiers[_cidr_idx]])
            else:
                _ruleList.append(['cellmonitor', _localip, _localmask])
                _ruleList.append(['cellmonitor', _addr, _mask])
                _add_extra_ips(_ruleList, 'cellmonitor', [_subnetips])

            # Add additional users/ips to the whitelist
            _extra_users_whitelist = self.mCheckConfigOption('hostctrl_extra_users')
            if not _extra_users_whitelist:
                _extra_users_whitelist = {}

            for _user, _user_data in _extra_users_whitelist.items():
                _extra_whitelist = list(_user_data.get('ips', []))
                if 'cellips' in _extra_whitelist:
                    _extra_whitelist.remove('cellips')
                    _extra_whitelist += list(_user_data.get('cellips', []))
                if 'dom0ips' in _extra_whitelist:
                    _extra_whitelist.remove('dom0ips')
                    _extra_whitelist += list(_user_data.get('dom0ips', []))
                _extrasubnetset = ebSubnetSet()
                _extrasubnetset.mAppendList(_extra_whitelist)
                _extrasubnetips = _extrasubnetset.mGetSubnetList()

                if len(_cidr_subnet_ips) > 0 and len(_cidr_network_identifiers) > 0:
                    for _cidr_idx, _cidr_subnet_ip in enumerate(_cidr_subnet_ips):
                        _ruleList.append([_user, _cidr_subnet_ip, _cidr_network_identifiers[_cidr_idx]])
                else:
                    _ruleList.append([_user, _localip, _localmask])
                    _ruleList.append([_user, _addr, _mask])
                    _add_extra_ips(_ruleList, _user, [_subnetips, _ecraips, _extrasubnetips])

            #Search every rule on the export file
            for _rule in _ruleList:
                _user = _rule[0]
                _ip   = "{0}/{1}".format(_rule[1], _rule[2])

                if _user not in list(_newRules.keys()):
                    _cmdstr = "cat {0} | grep {1}".format(_exportFile, _user)
                    _node.mExecuteCmd(_cmdstr)

                    if _node.mGetCmdExitStatus() != 0:
                        _newRules[_user] = "+ : {0} : console tty1 ttyS0 hvc0 localhost ip6-localhost".format(_user)
                    else:
                        _i, _o, _e = _node.mExecuteCmd(_cmdstr)
                        _curRule = _o.readline().strip()
                        if _curRule.endswith('ALL'):
                            _newRules[_user] = "+ : {0} : console tty1 ttyS0 hvc0 localhost ip6-localhost".format(_user)
                        else:
                            _newRules[_user] = _curRule

                if _newRules[_user].find(_ip) == -1:
                    _newRules[_user] += " {0}".format(_ip)

            #Dump new rules
            _importFilebyHost = "/tmp/cellos_access_import_{0}.txt".format(
                    _node.mGetHostname())
            with open(_importFilebyHost, "w") as _file:
                for _user in _newRules:
                    _file.write("{}\n".format(_newRules[_user]))
                _file.write("- : ALL  : ALL \n")

            _node.mCopyFile(_importFilebyHost, _importFile)

            #Apply new rules
            _cmdstr = "/opt/oracle.cellos/host_access_control access-import --file='{0}' <<< yes".format(_importFile)
            ebLogInfo("Running on {0} the cmd {1}".format(_node.mGetHostname(), _cmdstr))
            _node.mExecuteCmdLog(_cmdstr)
            _rc = _node.mGetCmdExitStatus()

            #Clean the files
            _cmdstr2 = "rm {0} {1}".format(_importFile, _exportFile)
            _node.mExecuteCmdLog(_cmdstr2)

            _cmdstr2 = "/bin/rm {0}".format(_importFilebyHost)
            self.mExecuteLocal(_cmdstr2)

            #Check fails
            if _rc:
                _node.mDisconnect()
                raise Exception('mExecuteCmd Failed', _node.mGetHostname(), _cmdstr)

            #
            # Check final access in place
            #
            _cmdstr = """/opt/oracle.cellos/host_access_control access --status"""
            ebLogInfo("Running on {0} the cmd {1}".format(_node.mGetHostname(), _cmdstr))
            _node.mExecuteCmdLog(_cmdstr)
            if _node.mGetCmdExitStatus():
                _node.mDisconnect()
                raise Exception('mExecuteCmd Failed', _node.mGetHostname(), _cmdstr)
            _node.mDisconnect()

        _plist = ProcessManager()

        for _cell_name in sorted(_cell_list.keys()):
            _p = ProcessStructure(_secure_cell, [_cell_name,])
            _p.mSetMaxExecutionTime(30*60) # 30 minutes timeout
            _p.mSetJoinTimeout(10)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()


    def mOpenSSHFromECRA(self, aNode):
        #
        # Whitelist specific ECRA IP for Sanitycheck
        # ECRA IP is stored in ecra_whitelist.conf of each dom0/cell like below
        # {"whitelist_ips" : ["10.0.1.37"]}
        #
        ebLogInfo('*** Whitelist ECRA for Sanitycheck, Host: %s' %
                aNode.mGetHostname())
        _whitelist_ips = []
        _config_file = self.mCheckConfigOption('ecra_whitelist_conf')
        if _config_file is None:
            ebLogInfo('*** Cannot load ecra_whitelist_conf from exabox.conf')
            return _whitelist_ips
        _cmdstr = 'cat %s' % _config_file
        _, _o, _ = aNode.mExecuteCmd(_cmdstr)
        if aNode.mGetCmdExitStatus():
            ebLogInfo('*** Cannot read file %s in %s' % (_config_file,
                        aNode.mGetHostname()))
            return _whitelist_ips
        try:
            _whitelist_ips = json.loads(_o.read())['whitelist_ips']
        except Exception as e:
            ebLogInfo('*** Cannot load json %s in %s: %s' % (_config_file,
                        aNode.mGetHostname(), str(e)))
            return _whitelist_ips
        _subnetset = ebSubnetSet()
        _subnetset.mAppendList(_whitelist_ips)
        _ecraips = _subnetset.mGetSubnetList()
        return _ecraips

    def mPublishNewNodeKey(self, aOptions):

        if not self.__ociexacc and not self.mIsKVM():
            ebLogInfo('*** mPublishNewNodeKey()  -- not supported on non KVM platform')
            return
        ebLogInfo('*** mPublishNewNodeKey()')
        _jconf = aOptions.jsonconf
        _added_computes = None
        _added_cells = None
        if not 'reshaped_node_subset' in list(_jconf.keys()):
            ebLogWarn('*** reshaped_node_subset field not found in elastic payload')
            return
        if 'added_computes' in _jconf['reshaped_node_subset']:
            _added_computes = _jconf['reshaped_node_subset']['added_computes']
        if 'added_cells' in _jconf['reshaped_node_subset']:
            _added_cells = _jconf['reshaped_node_subset']['added_cells']
        #
        # Check if we have any added_computes & cells
        #
        if _added_computes is None and _added_cells is None:
            ebLogWarn('*** added_computes & added_cells no present in elastic payload')
        #
        # Host to be added to WorkDir
        #
        _addedHostList = {}
        _exakms = get_gcontext().mGetExaKms()
        _user = "root"

        #
        # Add new computes keys -- this need to be done for each both Dom0 & DomU
        #
        if _added_computes is not None:
            for _compute in _added_computes:
                if not 'network_info' in _compute:
                    ebLogError('*** network_info not present in added_computes part of elastic payload')
                    return
                #
                # Fetch Dom0 network FQDN hostanme
                #
                _network_info = _compute['network_info']
                _admin = None
                for _network in _network_info['computenetworks']:
                    if 'admin' in _network:
                        _admin = _network['admin'][0]
                        break
                if _admin is None:
                    ebLogError('** dom0 admin hostname not found of elastic payload')
                    return
                _fqdn_admin = _admin['fqdn']
                self.mAppendToHostList(_fqdn_admin)
                ebLogInfo('*** mPublishNewNodeKey -- found new admin node : {0}'.format(_fqdn_admin))
                #
                # Fetch DomU network FQDN hostname
                #
                _virtual_compute = _compute['virtual_compute_info']
                _fqdn_client = None
                _network_info = _virtual_compute['network_info']
                _client = None
                for _network in _network_info['virtualcomputenetworks']:
                    if 'client' in _network:
                        _client = _network['client'][0]
                        break
                if _client is None:
                    ebLogError('** client network not found in elastic payload')
                    return
                if 'hostname' in _admin and 'domainname' in _admin:
                    _fqdn_client = _admin['hostname']+'.'+_admin['domainname']
                else:
                    _fqdn_client = _client['nathostname']+'.'+_client['natdomain']
                _nathostname_client = _fqdn_client
                self.mAppendToHostList(_fqdn_client)
                ebLogInfo('*** mPublishNewNodeKey -- found new NAT compute node : {0}'.format(_fqdn_client))

                # Pregenerate DomU Key
                _cparam = {"FQDN": _nathostname_client, "user": _user}
                _entry = _exakms.mGetExaKmsEntry(_cparam)

                if not _entry:

                    _entry = _exakms.mBuildExaKmsEntry(
                        _nathostname_client,
                        _user,
                        _exakms.mGetEntryClass().mGeneratePrivateKey(),
                        ExaKmsHostType.DOMU
                    )

                    _exakms.mInsertExaKmsEntry(_entry)

                # Add export entries to new nodes
                _addedHostList[_fqdn_admin] = ExaKmsHostType.DOM0
                _addedHostList[_nathostname_client] = ExaKmsHostType.DOMU

        #
        # Add new cell keys
        #
        if _added_cells is not None:
            for _cell in _added_cells:
                if not 'network_info' in _cell:
                    ebLogError('*** network_info not present in added_cells part of elastic payload')
                    return
                #
                # Fetch Cell network FQDN hostname
                #
                _network_info = _cell['network_info']
                _admin = None
                for _network in _network_info['cellnetworks']:
                    if 'admin' in _network:
                        _admin = _network['admin'][0]
                        break
                if _admin is None:
                    ebLogError('** cell admin hostname not found of elastic payload')
                    return
                _fqdn_admin = _admin['fqdn']
                self.mAppendToHostList(_fqdn_admin)
                ebLogInfo('*** mPublishNewNodeKey -- found new admin node : {0}'.format(_fqdn_admin))

                _addedHostList[_fqdn_admin] = ExaKmsHostType.CELL

        # Insert elastic keys into workdir
        if _addedHostList:

            _oedaReqPath = f"{self.__oeda_path}/WorkDir"
            _exakms.mSaveEntriesToFolder(
                _oedaReqPath,
                _addedHostList,
                self.mExaKmsEntryExtraValidation
            )

    def mSecureDom0SSH(self, aDom0List=None):
        #
        # Secure Dom0 SSH should not be enforced in non production env. and/or shared env (until multi-vm suppoprt is added)
        #
        if self.mEnvTarget() is False:
            ebLogInfo('*** SecureDom0 SSH is skipped for non production environment')
            return
        
        # Retrieve also ECRA/localhost IP
        #
        _localip = socket.gethostbyname(socket.gethostname())
        _localmask = '255.255.255.255'
        #
        # Check DNS local hostname IP (req. for NAT)
        #
        _host = socket.gethostname()
        _cmd = '/bin/nslookup ' + _host
        _, _, _o, _ = self.mExecuteLocal(_cmd)
        if _o is not None:
            try:
                _ip2 = _o.strip().split('\n')[-1].split(' ')[-1]
                if re.match('\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', _ip2) is None:
                    _ip2 = None
            except:
                _ip2 = None
        if _ip2 is not None:
            _localip = _ip2

        if aDom0List:
            _dpairs = aDom0List
        else:
            _dpairs = self.mReturnDom0DomUPair()

        #
        # Build string for hostctrl white-list (hostctrl_whitelist) based on configuration
        #
        _host_wlf = ''
        _subnetset = ebSubnetSet()
        _current_options = self.mGetArgsOptions()
        _cidr_subnet_ips = list()
        _cidr_network_identifiers = list()
        if _current_options is not None and _current_options.jsonconf is not None and "ecra" in _current_options.jsonconf.keys():
            _ecra_options_dict = _current_options.jsonconf.get('ecra')
            if _ecra_options_dict is not None:
                _current_ecra_subnet_cidrs = _ecra_options_dict.get('whitelist_cidr',[])
                if len(_current_ecra_subnet_cidrs) > 0:
                    ebLogInfo(f"Updating host access control information with CIDR block: {_current_ecra_subnet_cidrs}")
                    for _current_ecra_subnet_cidr in _current_ecra_subnet_cidrs:
                        _info_split = _current_ecra_subnet_cidr.split("/")
                        if len(_info_split) < 2 or _info_split[1].isnumeric() == False:
                            _error_msg = f"Invalid subnet CIDR information: {_current_ecra_subnet_cidr}"
                            ebLogError(_error_msg)
                            raise ExacloudRuntimeError(0x0119, 0xA, _error_msg, aStackTrace=False)

                        _cidr_subnet_ips.append(_info_split[0])
                        _cidr_network_identifiers.append(_info_split[1])

        if len(_cidr_subnet_ips) == 0:
            _subnetset.mAppendList(self.mCheckConfigOption('hostctrl_whitelist'))
            _subnetips = _subnetset.mGetSubnetList()
            if len(_subnetips ) != 0:
                _host_wlf = ' ' + ' '.join(_subnetset.mGetSubnetList())
        
        _networkInfo = set()
        # Fetch the network info from all the dom0s
        for _dom0, _ in _dpairs:

            _node = exaBoxNode(get_gcontext()) 
            _node.mConnect(aHost=_dom0) 
            #      
            # Retrieve Mgmt IP from Dom0
            #
            _master = self.mGetCurrentMasterInterface(aDom0=_dom0)
            _cmdstr = f"/sbin/ip addr show {_master} | /bin/grep -w inet"
            _i, _o, _e = _node.mExecuteCmd(_cmdstr)   
            if _node.mGetCmdExitStatus():  
                _node.mDisconnect()     
                raise Exception('mExecuteCmd Failed', _node.mGetHostname(), _cmdstr)  
            _ipaddr =  _o.readline().strip().split(' ')[1]    
            _cmdstr = '/bin/ipcalc -nm ' + _ipaddr  
            _i, _o, _e = _node.mExecuteCmd(_cmdstr)  
            if _node.mGetCmdExitStatus():   
                _node.mDisconnect()     
                raise Exception('mExecuteCmd Failed', _node.mGetHostname(), _cmdstr)

            #
            #  Compute network mask for /24 (ideally we should retrieve this from the XML config)
            #
            _mask = _o.readline().split('=')[1].strip()  
            _addr = _o.readline().split('=')[1].strip()

            _node.mDisconnect()

            _networkInfo.add(_addr+"/"+_mask)

        _network = list(_networkInfo)

        # Legacy mechanism: include all the information about
        # all the Dom0 networks
        _ipaddr = _network[0].split('/')[0]
        _ipmask = _network[0].split('/')[1]

        # Legacy mechanism: we already added the
        # first element above, now we
        # add the rest of them in here
        for _info in _network[1:]:
            _ipmask = _ipmask + " " + _info
        
        #
        # Restrict SSH access to mgmt network (also add ECRA/localhost)
        #
        def _secure_dom0(_dom0):
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            
            #
            # Build string for whitelist ECRA
            #
            _ecra_wlf = ''
            _ecraips = self.mOpenSSHFromECRA(_node)
            if len(_ecraips) != 0:
                _ecra_wlf = ' ' + ' '.join(_ecraips)
            #
            # Lockdown SSHD access and whitelist ECRA/DOM0-mgmt network + hostctrl_whitelist
            #
            _newRules = {}
            _ruleList = []
            _localip2 = ''
            _localmask2 = ''

            _cellmonitor_ip = self.mCheckConfigOption('cellmonitor_subnet')
            if _cellmonitor_ip:
                _localip2 = _cellmonitor_ip.split('/')[0]
                _localmask2 = _cellmonitor_ip.split('/')[1]

                _ruleList.append(["root", _localip2, _localmask2])
                _ruleList.append(["root", _localip, _localmask])
                _ruleList.append(["root", _ipaddr, _ipmask+_host_wlf+_ecra_wlf])

                _ruleList.append(["secscan", _localip2, _localmask2])
                _ruleList.append(["secscan", _localip, _localmask])
                _ruleList.append(["secscan", _ipaddr, _ipmask+_host_wlf+_ecra_wlf])
            if len(_cidr_subnet_ips) > 0 and len(_cidr_network_identifiers) > 0:

                # Add the Dom0s Network Info for root and secscan
                _ruleList.append(["root", _ipaddr, _ipmask+_host_wlf+_ecra_wlf])
                _ruleList.append(["secscan", _ipaddr, _ipmask+_host_wlf+_ecra_wlf])

                # Add the CIRD given in the payload
                for _cidr_idx, _cidr_subnet_ip in enumerate(_cidr_subnet_ips):
                    _ruleList.append(["root", _cidr_subnet_ip, _cidr_network_identifiers[_cidr_idx]])
                    _ruleList.append(["secscan", _cidr_subnet_ip, _cidr_network_identifiers[_cidr_idx]])
            else:
                _ruleList.append(["root", _localip, _localmask])
                _ruleList.append(["root", _ipaddr, _ipmask+_host_wlf+_ecra_wlf])
                _ruleList.append(["secscan", _localip, _localmask])
                _ruleList.append(["secscan", _ipaddr, _ipmask+_host_wlf+_ecra_wlf])

            ebLogTrace(f"Rulelist {_ruleList}")
            # Add additional users/ips to the whitelist
            _extra_users_whitelist = self.mCheckConfigOption('hostctrl_extra_users')
            if not _extra_users_whitelist:
                _extra_users_whitelist = {}
            _extra_host_wlf = ''

            for _user, _user_data in _extra_users_whitelist.items():
                _extra_whitelist = list(_user_data.get('ips', []))
                if 'cellips' in _extra_whitelist:
                    _extra_whitelist.remove('cellips')
                    _extra_whitelist += list(_user_data.get('cellips', []))
                if 'dom0ips' in _extra_whitelist:
                    _extra_whitelist.remove('dom0ips')
                    _extra_whitelist += list(_user_data.get('dom0ips', []))

                _extrasubnetset = ebSubnetSet()
                _extrasubnetset.mAppendList(_extra_whitelist)
                _extrasubnetips = _extrasubnetset.mGetSubnetList()
                _extra_host_wlf = _host_wlf[:]
                if _extrasubnetips:
                    _extra_host_wlf += ' ' + ' '.join(_extrasubnetips)

                if _cellmonitor_ip:
                    _ruleList.append([_user, _localip2, _localmask2])

                if len(_cidr_subnet_ips) > 0 and len(_cidr_network_identifiers) > 0:
                    for _cidr_idx, _cidr_subnet_ip in enumerate(_cidr_subnet_ips):
                        _ruleList.append([_user, _cidr_subnet_ip, _cidr_network_identifiers[_cidr_idx]])
                else:
                    _ruleList.append([_user, _localip, _localmask])
                    _ruleList.append([_user, _ipaddr, _ipmask + _extra_host_wlf + _ecra_wlf])

            ebLogTrace(f"Rulelist {_ruleList}")

            #Export the configuration file
            _timestamp = str(time.time()).replace(".", "")
            _exportFile = "/tmp/cellos_access_export{0}.txt".format(_timestamp)
            _importFile = "/tmp/cellos_access_import{0}.txt".format(_timestamp)

            _cmdstr = "/opt/oracle.cellos/host_access_control access-export --file='{0}'".format(_exportFile)
            ebLogInfo("Running on {0} the cmd {1}".format(_node.mGetHostname(), _cmdstr))
            _node.mExecuteCmdLog(_cmdstr)
            _rc = _node.mGetCmdExitStatus()
            #Check fails
            if _rc:
                _node.mDisconnect()
                raise Exception('mExecuteCmd Failed', _node.mGetHostname(), _cmdstr)

            #Search every rule on the export file
            for _rule in _ruleList:
                _user = _rule[0]
                _ip   = "{0}/{1}".format(_rule[1], _rule[2])

                if _user not in _newRules.keys():
                    _cmdstr = "cat {0} | grep {1}".format(_exportFile, _user)
                    _node.mExecuteCmd(_cmdstr)

                    if _node.mGetCmdExitStatus() != 0:
                        _newRules[_user] = "+ : {0} : console tty1 ttyS0 hvc0 localhost ip6-localhost".format(_user)
                    else:
                        _i, _o, _e = _node.mExecuteCmd(_cmdstr)
                        _curRule = _o.readline().strip()
                        if _curRule.endswith('ALL'):
                            _newRules[_user] = "+ : {0} : console tty1 ttyS0 hvc0 localhost ip6-localhost".format(_user)
                        else:
                            _newRules[_user] = _curRule

                if _newRules[_user].find(_ip) == -1:
                    _newRules[_user] += " {0}".format(_ip)

            #Dump new rules
            _importFilebyHost = "/tmp/cellos_access_import_{0}.txt".format(
                    _node.mGetHostname())
            with open(_importFilebyHost, "w") as _file:
                for _user in _newRules:
                    _file.write("{}\n".format(_newRules[_user]))                
                _file.write("- : ALL  : ALL \n")

            _node.mCopyFile(_importFilebyHost, _importFile)

            #Apply new rules
            _cmdstr = "/opt/oracle.cellos/host_access_control access-import --file='{0}' <<< yes".format(_importFile)
            ebLogInfo("Running on {0} the cmd {1}".format(_node.mGetHostname(), _cmdstr))
            _node.mExecuteCmdLog(_cmdstr)
            _rc = _node.mGetCmdExitStatus()

            #Clean the files
            _cmdstr2 = "rm {0} {1}".format(_importFile, _exportFile)
            _node.mExecuteCmdLog(_cmdstr2)

            _cmdstr2 = "/bin/rm {0}".format(_importFilebyHost)
            self.mExecuteLocal(_cmdstr2)

            #Check fails
            if _rc:
                _node.mDisconnect()
                raise Exception('mExecuteCmd Failed', _node.mGetHostname(), _cmdstr)
                    
            #
            # Check final access status
            #
            _cmdstr = """/opt/oracle.cellos/host_access_control access --status"""
            _node.mExecuteCmdLog(_cmdstr)
            if _node.mGetCmdExitStatus():
                _node.mDisconnect()
                raise Exception('mExecuteCmd Failed', _node.mGetHostname(), _cmdstr)

            _node.mDisconnect()

        _plist = ProcessManager()

        for _dom0, _ in _dpairs:
            _p = ProcessStructure(_secure_dom0, [_dom0])
            _p.mSetMaxExecutionTime(30*60) # 30 minutes
            _p.mSetJoinTimeout(60)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()

    def mGetScanIps(self, aIPv6=False):
        """Get cluster's SCAN IPs.

        :returns: list of SCAN IPs; can be empty.
        """
        # get first cluster
        cnode = self.__clusters.mGetCluster()

        if cnode:
            scans = cnode.mGetCluScans()

            if scans:
                for _scan_obj in scans:
                    scan_ip_strs = self.__scans.mGetScan(_scan_obj).mGetScanIps()
                    for scan_ip in scan_ip_strs:
                        if ":" not in scan_ip and not aIPv6:
                            return tuple(map(IPv4Address, scan_ip_strs))
                        elif ":" in scan_ip and aIPv6:
                            return tuple(map(IPv6Address, scan_ip_strs))

        return () # no clusters (clusterless XML) or no Scan IPs

    def mGetEbtablesScanVip(self, aDomU: str) -> List[str]:

        #
        # Retrieve VIP IP (1 per VM)
        #
        _cname = self.__clusters.mGetClusters()
        _cnode = self.__clusters.mGetCluster(_cname[0])

        _vip_list  = _cnode.mGetCluVips()
        _vip_ips  = []
        for _vip in list(_vip_list.keys()):
            _vip_name = _vip_list[_vip].mGetCVIPMachines()[0]
            _ip = _vip_list[_vip].mGetCVIPAddr()
            _mac_config = self.__machines.mGetMachineConfig(_vip_name)
            _mac_name   = _mac_config.mGetMacHostName()
            if _mac_name == aDomU:
                _vip_ips.append(_ip)

        # retrieve SCAN IPs (in string format)
        _scan_iplist = list(map(str, self.mGetScanIps()))

        return _scan_iplist + _vip_ips

    def mGetPkeysConfig(self,aMode=False):

        _pkey_config = {}
        _storage_pkey = None
        _cluster_pkey = None
        _dpairs = self.mReturnDom0DomUPair()

        if self.mIsKVM():
            # Computing a unique pkey pair (storage and cluster) in order
            # to assgin  unique clustername at OEDA XML patching

            def _get_mac_id(aDom0):
                _dom0 = aDom0
                _mac_id = ''
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_dom0)
                _cmd = 'cat /sys/class/net/virbr0/address'
                _, _o, _e = _node.mExecuteCmd(_cmd)
                if _o and _node.mGetCmdExitStatus() == 0:
                    _out = _o.readlines()
                    _mac_id = _out[0]
                    if _mac_id:
                        _mac_id = _mac_id.split(':')[-1].strip()
                else:
                    ebLogWarn(f"*** Interface virbr0 is down in {aDom0} ***")
                _node.mDisconnect()
                return _mac_id

            _vlan_id = ''
            _mac_id = ''
            _domU = _dpairs[0][1]
            _domU_mac = self.__machines.mGetMachineConfig(_domU)
            _domU_net_list = _domU_mac.mGetMacNetworks()
            for _net_id in _domU_net_list:
                _net_conf = self.__networks.mGetNetworkConfig(_net_id)
                _vlan_id = _net_conf.mGetNetVlanId()
                if _vlan_id == 'UNDEFINED':
                    _vlan_id = ''

            _mac_id = _get_mac_id(_dpairs[0][0])
            _skm =  "0xaa"
            _skm += _mac_id
            _skm = int(_skm, 16)
            _ckm =  "0xa"

            if len(_dpairs) == 1:
                ebLogWarn('*** Small cluster configuration detected -- applying WA for SKM/CKM')
                _mac_id = _get_mac_id(_dpairs[0][0])
            else:
                _mac_id = _get_mac_id(_dpairs[1][0])

            if _vlan_id == '':
                _ret = ''.join(filter(str.isdigit, str(_dpairs[0][1]))).replace('0','')[-3:]
                _id = _ret[0]
            else:
                _id = _vlan_id[0]

            _ckm += _mac_id + str(_id)
            _ckm = int(_ckm, 16)
            ebLogInfo('*** mGetPkeysConfig / KVM SKM/CKM generation : %s/ %s' % (str(hex(_skm)),str(hex(_ckm))))
            return hex(_skm), hex(_ckm)

        for _, _domU in _dpairs:
                _domU_mac = self.__machines.mGetMachineConfig(_domU)
                if _domU_mac is None:
                    ebLogError('*** failed to retrieve machine config for domu: %s' %(_domU))
                _domU_net_list = _domU_mac.mGetMacNetworks()
                for _net in _domU_net_list:
                    _netcnf = self.__networks.mGetNetworkConfig(_net)
                    if _netcnf.mGetNetType() in [ 'private' ]:
                        _key      = _netcnf.mGetPkey()
                        _pkeyname = _netcnf.mGetPkeyName()
                        if not _pkeyname in list(_pkey_config.keys()):
                            _pkey_config[ _pkeyname ] = _key
                        else:
                            if _key != _pkey_config[ _pkeyname ]:
                                ebLogError('*** Invalid PKEY value detected ***')
                        if _pkeyname[:2] == 'st':
                            if _storage_pkey is None:
                                _storage_pkey = _key
                            elif _storage_pkey != _key:
                                ebLogError('*** Invalid STORAGE PKEY detected ***')
                        if _pkeyname[:2] == 'cl':
                            if _cluster_pkey is None:
                                _cluster_pkey = _key
                            elif _cluster_pkey != _key:
                                ebLogError('*** Invalid CLUSTER PKEY detected ***')
        _skm = int(_storage_pkey,16) & int(0x7FFF)
        _ckm = int(_cluster_pkey,16) & int(0x7FFF)
        ebLogInfo('*** Storage PKEY: %s/0x%x' % (_storage_pkey,_skm))
        ebLogInfo('*** Cluster PKEY: %s/0x%x' % (_cluster_pkey,_ckm))

        return hex(_skm), hex(_ckm)

    def mGetAllGUID(self):
        _GUIDs = {}
        _plist = ProcessManager()
        _managedGUIDDict = _plist.mGetManager().dict()

        #Create the hosts
        _dom0s, _, _cells, _ = self.mReturnAllClusterHosts()
        _host_list = _dom0s + _cells
        if self.__ociexacc:
            _remote_cps = self.mCheckConfigOption('remote_cps_host')
            if _remote_cps:
                _host_list.append(_remote_cps)

        #Create the process to execute
        for _host in _host_list:
            _p = ProcessStructure(self.mGetHostGUIDs, [_host, False, _managedGUIDDict], _host)
            _p.mSetMaxExecutionTime(15*60) #15 minutes timeout
            _p.mSetJoinTimeout(20)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        # Passing localFlag to True
        if self.__ociexacc:
            _p = ProcessStructure(self.mGetHostGUIDs, ['localCPS', True, _managedGUIDDict], 'localCPS')
            _p.mSetMaxExecutionTime(15*60) #15 minutes timeout
            _p.mSetJoinTimeout(20)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        #Join the process
        _plist.mJoinProcess()

        if _plist.mGetStatus() != 'killed':
            _GUIDs = dict(_managedGUIDDict)

        return _GUIDs


    # Allow appending to existing GUID
    # IT IS A DICTIONNARY:
    # { 'hostname':[GUID1 of host,GUID2 of host], 'hostname2':[GUIDs] ]
    def mGetHostGUIDs(self, aHost, aLocal=False, _guids={}):
        _host_guids = []
        if aLocal:
            _node = exaBoxNode(get_gcontext(),aLocal=True)
            _node.mConnect()
        else:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=aHost)
        # KVM TODO: Pushing GUID host of a host as a host attribute instead of computing directly in dom0/kvm host.
        _cmd = "ibstat mlx4_0 | grep 'Port GUID'; ibstat mlx5_0 | grep 'Port GUID'"
        _, _o, _e = _node.mExecuteCmd(_cmd)
        _output = _o.readlines()
        if _output:
            for _out in _output:
                if ':'  in _out:
                    _guid = _out.split(':')[1].strip()
                    if self.__debug:
                        ebLogDebug('Fetched host ({}) IB GUID: ({})'.format(aHost,_guid))
                    _host_guids.append(_guid)
            if _host_guids:
                _guids[aHost] = _host_guids #(cell1,[GUID1,GUID2])
        _node.mDisconnect()
        return _guids

    def mCheckSwitchesRegistration(self):

        if self.mIsKVM():
            ebLogWarn('*** Validate SM is not supported on RoCE')
            return

        ebLogInfo('*** Validate SM state and switches registration')
        _switches=[]
        _ipswitches=[]
        _list = self.__switches.mGetSwitchesNetworkId(True)
        _master = None
        # Build switches / ips list from the XML
        for _h in _list:
            _neto = self.__networks.mGetNetworkConfig(_h)
            _switches.append(_neto.mGetNetHostName()+'.'+_neto.mGetNetDomainName())
            _ipswitches.append(_neto.mGetNetIpAddr())
        # Use ibswitches to find all switches in the fabric
        _switchesl = []
        for _idx in range(len(_switches)): # xxx/MR: move to zip eventually

            _switch = _switches[_idx]

            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_switch)
            # Run ibswitches on first ibswtich from XML
            _cmdstr = 'ibswitches'
            _i, _o, _e = _node.mExecuteCmd(_cmdstr)
            if _o:
                for _line in _o.readlines():
                    try:
                        _ipsw = _line.split('"')[1].split(' ')[-1]
                        _switchesl.append(_ipsw)
                    except:
                        pass
            _node.mDisconnect()
            if len(_switchesl):
                break
        # Iterate through all switches and check if the ibswithes from the XML are part of smnodes list else add them
        for _switch in _switchesl:
            ebLogInfo('*** Validating switch: %s' % (_switch))
            _cmdstr = 'smnodes list'
            _listsm = []

            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_switch)
            # Retrieve all known switches from smnodes list
            _i, _o, _e = _node.mExecuteCmd(_cmdstr)
            if _o:
                for _line in _o.readlines():
                    _swi = _line.rstrip()
                    _listsm.append(_swi)
            # Check if SMnodes list on the current _switch is empty or not
            if len(_listsm) and self.__debug:
                ebLogDebug('*** SMnodes List:')
                for _entry in _listsm:
                    ebLogDebug('    %s' % (_entry.rstrip()))
            if not len(_listsm):
                ebLogWarn('*** SMnodes list is EMTPY on : %s' % (_switch))
            # Update smnode list with missing ip id any
            for _isw in _ipswitches:
                if not _isw in _listsm:
                    ebLogWarn('*** Switch %s: %s/%s not found in smnodes list' % (_switch,_switches[_ipswitches.index(_isw)],_isw))
                    _cmdstr = 'smnodes add %s' % (_isw)
                    _i, _o, _e = _node.mExecuteCmd(_cmdstr)
                    ebLogInfo('*** Adding ip to smnode list : %s' % (_cmdstr))
            _node.mDisconnect()
        ebLogInfo('*** IB Switches registration/validation done.')

    # This function returns a tuple
    # (MasterSwitch, StoragePartitionName, PKEY, isCPSSet (True|False))
    # aMode=False : no modifications on switch status
    def mCheckPkeysConfig(self, aAllGUIDs, aMode=False):

        # This method should not be called in the context of KVM -- log a warning and return None as no Pkeys exists
        if self.mIsKVM():
            ebLogWarn('*** Check PKEYs configuration is an invalid call for KVM based target')
            return None

        _skm, _ckm     = self.mGetPkeysConfig()
        _switches      = self.mReturnSwitches(True)
        _cells         = list(self.mReturnCellNodes().keys())
        #Keep first GUID of eatch cell in a flat tuple (GUID1CELL1)
        _cells_guids   = [aAllGUIDs[i][0] for i in _cells]
        #Eventual GUIDs for CPS
        _cps_guids     = list(aAllGUIDs.get('localCPS',[]))
        _remote_cps    = self.mCheckConfigOption('remote_cps_host')
        if _remote_cps:
            _cps_guids += aAllGUIDs.get(_remote_cps,[])

        _cps_set       = False
        _storage_pname = None
        _clusterName   = self.__clusters.mGetCluster().mGetCluName()

        ebLogInfo('*** Cluster Name used for PKEY: %s' % (_clusterName))
        #
        # Check on the MASTER SM current partition and ensure cluster partition does not exist
        #
        _master = None
        _gm_switch = None
        for _switch in _switches:

            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_switch)

            _cmdstr = 'getmaster'
            _i, _o, _e = _node.mExecuteCmd(_cmdstr)
            _out = _o.readlines()
            for _line in _out:
                # IB SW 2.2.6 format change, exclude additional lines
                if 'sm lid' not in _line:
                   continue
                if _gm_switch is None:
                    _gm_switch = _line.split(' ')[-2]
                    if _gm_switch not in _switches:
                        ebLogInfo('*** IB Master Switch reported by getmaster not in XML switch : %s' % (_gm_switch))
                        if "." not in _gm_switch:
                            # Append domain name with _gm_switch
                            _domain_name = _switch[_switch.index("."):] # Will be of the form ".us.oracle.com"
                            _gm_switch = _gm_switch + _domain_name
                        _switches.append(_gm_switch)
                elif _gm_switch != _line.split(' ')[-2]:
                    ebLogError('*** IB Master Switch reported by getmaster reports multiple Master %s != %s' % (_gm_switch, _line.split(' ')[-2]))

            if _out[0].find('MASTER') == -1:
                _node.mDisconnect()
                continue
            else:
                ebLogInfo('*** CHECK MASTER SM Switch %s' % (_switch))
                _master = _switch

            _cmdstr = 'smpartition list active no-page'
            _i, _o, _e = _node.mExecuteCmd(_cmdstr)
            _out = _o.readlines()

            _pnamesContainingCellGUID = []
            _pnamesContainingCPSGUID = []
            _pKeyDir = {}
            _plist = []
            for _e in _out:
                if not len(_e.strip()) or _e.strip()[0] == '#':
                    continue
                _plist.append(_e)
            _plist = re.sub('[\n]','',''.join(_plist)).split(';')

            for _partition in _plist:
                _p = _partition.strip()
                if _p:
                    try:
                        #Bug 25832848: use regexp as _p can vary (see bug)
                        _pname, _pkey = re.search('([^\s=]+)\s*=\s*([\dxXa-fA-F]+).*',_p).groups()
                        _pKeyDir[_pname] = _pkey
                        #Add partition name to list if it contains CellGUID
                        for _cellGuid in _cells_guids:
                            #GUID are fixed size, no need to split
                            if _cellGuid in _p:
                                _pnamesContainingCellGUID.append(_pname)
                                break
                        # For CPS, we are not looking for conflict,
                        # but for all of THEM in partition
                        if _cps_guids:
                            _i = 0
                            for _cps_guid in _cps_guids:
                                if _cps_guid in _p:
                                    _i += 1
                            if _i == len(_cps_guids): #all CPS GUIDs found
                                _pnamesContainingCPSGUID.append(_pname)
                    except:
                        ebLogError('*** SM MASTER Partition parsing error (%s)' % (_p))
            #
            # Dump current PKEY partition if __debug enabled
            #
            if self.__debug:
                for _k in list(_pKeyDir.keys()):
                    print(_k, _pKeyDir[_k])
            #
            # Check no partition conflicts with clusterName pkey name (already exists)
            #
            for _k in list(_pKeyDir.keys()):
                if _k == 'cl_'+_clusterName and _pKeyDir[_k] != _ckm:
                    ebLogWarn('*** Cluster PKEY partition name exists with different PKEY value')
                    ebLogWarn('*** Deleting conflicting partition : %s/%s' % (_k,_pKeyDir[_k]))
                    if aMode:
                        _cmdstr = 'smpartition start ; smpartition delete -pkey %s ; smpartition commit' % (_pKeyDir[_k])
                        _node.mExecuteCmdLog(_cmdstr)
            #
            # Check Cluster / Storage PKEY
            #
            if _skm in list(_pKeyDir.values()):
                for _k in list(_pKeyDir.keys()):
                    if _skm == _pKeyDir[_k]:
                        ebLogInfo('*** Storage PKEY already defined on switch (%s)' % (_k))
                        _storage_pname = _k
                        # Do not remove valid partition, to not impact MVM cells
                        if _k in _pnamesContainingCellGUID:
                            _pnamesContainingCellGUID.remove(_k)
                        if _k in _pnamesContainingCPSGUID:
                            # CPS GUID present on good storage partition
                            _cps_set = True
            if _ckm in list(_pKeyDir.values()):
                for _k in list(_pKeyDir.keys()):
                    if _ckm == _pKeyDir[_k]:
                        ebLogInfo('*** Cluster PKEY already defined on switch (%s)' % (_k))
                        #
                        # if aMode is True then delete partition on the switch
                        #
                        if aMode:
                            _cmdstr = 'smpartition start ; smpartition delete -pkey %s ; smpartition commit' % (_ckm)
                            if self.__debug:
                                ebLogInfo('*** SM CMD: %s' % (_cmdstr))
                            ebLogInfo('*** Removing Cluster PKEY partition on switch (%s)' % (_k))
                            _node.mExecuteCmdLog(_cmdstr)

            if _pnamesContainingCellGUID:
                ebLogInfo('*** Removing Invalid Storage Partitions on switch ({})'\
                      .format(_pnamesContainingCellGUID))
                if aMode:
                    _cmdstr = 'smpartition start ; '
                    for _leftover_storage_pname in _pnamesContainingCellGUID:
                        _cmdstr += 'smpartition delete -n {} ; '\
                                   .format(_leftover_storage_pname)
                    _cmdstr += 'smpartition commit'
                    if self.__debug:
                        ebLogInfo('*** SM CMD: %s' % (_cmdstr))
                    _node.mExecuteCmdLog(_cmdstr)

            _node.mDisconnect()
            #
            # Skip other switches since only MASTER SM really counts
            #
            break

        if not _master:
            ebLogError("*** IB Master Switch not found in IB Switches List: %s" % (str(_switches)))
            return False

        return (_master, _storage_pname, _skm, _cps_set)

    def mCheckSwitchFreeSpace(self):
        #
        # Check Switches Disk Free Space to avoid undefined behavior. Bug 24284943
        #
        if self.mIsKVM():
            return 0
        if not self.mCheckConfigOption('enable_diskspacecheck','False'):
            _switches = self.mReturnSwitches(True)

            for _switch in _switches:
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_switch)

                #Check logrotate healthiness to avoid switch disk space starvation

                _cmdstr = 'logrotate -d /etc/logrotate.conf'
                _node.mExecuteCmd(_cmdstr)
                if _node.mGetCmdExitStatus() == 1:
                    if self.mIsDebug():
                        ebLogWarn('*** Warning: corrupted logrotate file, attempting to recover in switch %s' % (_switch))
                    _cmdstr = 'rm -f /var/lib/logrotate.status; logrotate --force /etc/logrotate.conf'
                    _node.mExecuteCmd(_cmdstr)
                    _cmdstr = 'logrotate -d /etc/logrotate.conf'
                    _node.mExecuteCmd(_cmdstr)
                    if _node.mGetCmdExitStatus() == 1:
                        if self.mIsDebug():
                            ebLogWarn('***** Check logrotate config and state on switch: %s' % (_switch))
                    else:
                        ebLogInfo('*** Logrotate repaired on switch %s' % (_switch))

                #Check free disk space

                _cmdstr = 'df /|tail -1|awk \'{print $4}\''
                _i, _o, _e = _node.mExecuteCmd(_cmdstr)
                _out = _o.readlines()

                if int(_out[0]) < 10000:
                    ebLogError('***** Free disk space fail switch %s' % (_switch) )
                    ebLogError('*** Free disk space: %d KB' % int(_out[0]))
                else:
                    ebLogInfo('*** Reasonable free diskspace on switch %s / %d KB' % (_switch, int(_out[0])))
                _node.mDisconnect()
            if self.mIsDebug():
                ebLogInfo('*** Switch free disk space sanity check completed')
        #
        # Disable this by Default until we have a clean way of dealing w/ concurrent access
        # TODO: MultiVM
        #
        if self.mCheckConfigOption('enable_ibpartition_cleanup','True'):
            _switches = self.mReturnSwitches(True)
            for _switch in _switches:
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_switch)

                # Clean up previous failed partition operations in switches.
                _cmdstr = 'smpartition abort'
                _node.mExecuteCmd(_cmdstr)
                ebLogInfo('*** Cleaned up IB partition status on switch %s' % (_switch))

                _node.mDisconnect()


    def mCheckCellsPkeyConfig(self, aMode=True):

        if self.mIsKVM():
            ebLogWarn('*** Check Cells Pkey Config is not support on KVM/Roce -- remove from flow')
            return

        _ifcfg_path = '/etc/sysconfig/network-scripts/ifcfg-'
        _ipconf_path = '/opt/oracle.cellos/pkey.conf'
        _replace = "sed -i 's/PKEY_ID.*/PKEY_ID=%s/g' %s%s"

        # Check cell and appends node to be rebooted to managed _reboot list
        def _singleCellCheck(_cell, _reboot):
            _node = exaBoxNode(get_gcontext(), Cluctrl = self)
            _node.mConnect(aHost=_cell)
            _cell_mac = self.__machines.mGetMachineConfig(_cell)
            _net_list = _cell_mac.mGetMacNetworks()
            _pkey = None
            _ipconf_pkeys = {}

            #This block will parse cellos XML and fill _ipconf_pkeys dict:
            #{'stib0': ('0xaa18', '9'), 'stib1': ('0xaa18', '17')}
            _i, _ipconf_out, _e = _node.mExecuteCmd('cat {0}'.format(_ipconf_path))

            if _ipconf_out and _node.mGetCmdExitStatus() == 0:
                #decorate each tag in pkey.conf XML with line number
                _line_num = 0
                _ipconf_xml = []
                for _line in _ipconf_out.readlines():
                    _line_num += 1
                    _ipconf_xml.append(re.sub(r'<(\w+)',r'<\1 _line="{0}"'.format(_line_num),_line))

                if _line_num == 0:
                    ebLogInfo("File {0} is empty".format(_ipconf_path))
                    return

                #parse XML to get for each Interface Pkey info
                _ipconf_tree = etree.fromstring("".join(_ipconf_xml))
                for _iface in _ipconf_tree.findall("Cell/Interfaces"):
                    _pkeyElem = _iface.find('Pkey')
                    if _pkeyElem is not None:
                        #Store pkey value and line number (to modify easily)
                        _ipconf_pkeys[_iface.find('Intname').text.strip()] = (_pkeyElem.text,_pkeyElem.get('_line'))
            else:
                ebLogInfo("File {0} not found".format(_ipconf_path))
                return


            if self.mIsDebug():
                ebLogDebug('Done parsing Cellos ipconf XML on cell %s : %s' % (_cell, str(_ipconf_pkeys)))

            for _net in _net_list:
                _priv = self.__networks.mGetNetworkConfig(_net)
                # Get inteface/pkey information from stib0/stib1
                if _priv.mGetNetType() == 'private':
                    _ifcfg_ok       = False
                    _ipconf_ok      = False
                    _current_pkey   = ''

                    if not _pkey:
                        _pkey = _priv.mGetPkey()
                        _pkey = "0x%.4x" % int(int(_pkey, 16)|0x8000)

                    if _pkey and int(_pkey,16) != int(int(_priv.mGetPkey(),16)|0x8000):
                        ebLogError('*** Invalid PKEY found during CheckCellsPkeyConfig in cell %s. Expecting %s found %.4x.' % \
                                  (_cell, _pkey, int(int(_priv.mGetPkey(),16)|0x8000)))

                    # Get current pkey value in the cell ifcfg file
                    _priv_iface = _priv.mGetPkeyName()
                    _i, _o, _e = _node.mExecuteCmd('grep -ai PKEY_ID %s%s' % (_ifcfg_path, _priv_iface))
                    _ifcfg_out = _o.readlines()
                    if _ifcfg_out:
                        try:
                            _current_pkey = _ifcfg_out[0].strip().split('=')[1]
                            if _current_pkey == _pkey:
                                _ifcfg_ok    = True
                                if self.mIsDebug():
                                    ebLogDebug('Network Scripts PKEY matches XML on cell %s (%s)' % (_cell, _priv_iface))
                            else:
                                ebLogWarn('Network Scripts PKEY does not matches XML on cell %s (%s)' % (_cell, _priv_iface))
                        except Exception as ex:
                            ebLogError(str(ex))

                    # Lookup IPCONF Pkey for this iface using prebuilt dict
                    _ipconf_pkey_info = _ipconf_pkeys.get(_priv_iface)
                    if _ipconf_pkey_info:
                        _ipconf_value, _ipconf_line = _ipconf_pkey_info
                        if (_ipconf_value.strip().lower() == _pkey):
                            _ipconf_ok = True
                            if self.mIsDebug():
                                ebLogDebug('Cellos ipconf pkey.conf matches XML on cell %s (%s)' % (_cell, _priv_iface))
                        else:
                            ebLogInfo('Cellos PKEY configuration does not matches XML on cell %s (%s), patching Cellos XML...' % (_cell, _priv_iface))
                            #Replace the Pkey only on the good line/interface
                            if aMode:
                                _node.mExecuteCmdLog('sed -i {0}s/{1}/{2}/i {3}'.format(_ipconf_line, _ipconf_value, _pkey, _ipconf_path))

                    if not _ifcfg_ok:
                        ebLogInfo('Changing pkey in cell %s (%s) from %s to %s' % (_cell, _priv_iface, _current_pkey, _pkey))
                        if aMode:
                            #Network script update is still needed
                            ebLogDebug(_replace % (_pkey, _ifcfg_path, _priv.mGetPkeyName()))
                            _node.mExecuteCmdLog(_replace % (_pkey, _ifcfg_path, _priv.mGetPkeyName()))
                        if _cell not in _reboot:
                            _reboot.append(_cell)

            #If Pkey has been changed, the ipconf XML has been updated.
            #Execute ipconf to ensure everything is in sync
            if aMode and _cell in _reboot:
                ebLogInfo('Executing ipconf scripts on cell %s' % _cell)
                _node.mExecuteCmdLog('cellcli -e alter cell shutdown services all ignore redundancy')
                _node.mExecuteCmdLog('ipconf -pkey-add {0} -pkey-apply -force'.format(_ipconf_path))

            _node.mDisconnect()

        # --- Start of Main Process function
        _plist = ProcessManager()
        _managedRebootList = _plist.mGetManager().list()

        # --- First obtain list of cells in parallel
        for _cellName in list(self.mReturnCellNodes().keys()):
            if self.mIsDebug():
                ebLogDebug('Starting PKEY check of cell %s' % _cellName)

            _p = ProcessStructure(_singleCellCheck, [_cellName, _managedRebootList], _cellName)
            _p.mSetMaxExecutionTime(60*60)
            _p.mSetJoinTimeout(30)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        # the _managedRebootList now contains list of nodes to reboot
        # We create a copy to use it with a new process manager
        _plist.mJoinProcess()
        _list_reboot_cells = list(_managedRebootList)

        # Then Reboot the modified cells in parallel as well
        if aMode and _list_reboot_cells:
            _plist = ProcessManager()

            for _cellToReboot in _list_reboot_cells:
                ebLogInfo("Rebooting cell %s." % _cellToReboot)

                _p = ProcessStructure(self.mRebootNode, (_cellToReboot,), _cellToReboot)
                _p.mSetMaxExecutionTime(60*60)
                _p.mSetJoinTimeout(30)
                _p.mSetLogTimeoutFx(ebLogWarn)
                _plist.mStartAppend(_p)

            _plist.mJoinProcess()
  
    def mGenBondMap(self, aNode : exaBoxNode, aOnline : bool) -> Mapping[str, str]:
        """
        Objective: Determine mapping between physical network interfaces and bonded network interfaces.
        Returns a dictionary of master-slave (bondeth<x> - eth<x>) configuration in the form:
         {'eth4': 'bondeth0', 'eth5': 'bondeth0', 'eth6': 'bondeth1', 'eth7': 'bondeth1'}
        _node : host on which to check the ifcfg-eth interfaces
        aOnline - True indicates getting live master slave configuration (from /proc)
        aOnline - False indicates getting master slave configuration from XML and
                  netowkr scripts /etc/sysconfig/network-scripts/ifcfg-eth

        """
        _node = aNode
        _map = {}
        _backup_interfaces = []
        _100gbs_supported_backup = False
        _100gbs_supported_client = False        

        if aOnline:
            _i, _o, _e = _node.mExecuteCmd("/bin/cat /sys/class/net/bondeth0/bonding/slaves")
            if not _node.mGetCmdExitStatus():
                _out = _o.readlines()
                if not _out:
                    _msg = "The interface bondeth0 exists, but it seems to have no configured slaves. "\
                           "Please check the status of bondeth0 and its slave interfaces status."
                    ebLogError(_msg)
                    raise ExacloudRuntimeError(aErrorMsg=_msg)
                _slaves = _out[0].split(" ")
                for _s in _slaves:
                    _map[_s.strip()] = "bondeth0"

            #Check client and backup network supports 100Gbs speed
            if self.mIsOciEXACC():
                _dom0 = _node.mGetHostname()
                _exadata_model = self.mGetNodeModel(_dom0)
                _physical_eth_attr = self.mCheckConfigOption('exacc_high_speed_physical_network')
                if _physical_eth_attr is not None:
                    _eth_attr = _physical_eth_attr.get(_exadata_model, {})
                    _backup_interfaces = _eth_attr.get("backup_interfaces", ["eth5", "eth6"])
                    _client_interfaces = _eth_attr.get("client_interfaces", ["eth1", "eth2"])
                    ebLogInfo(f"Interface list: backup-{_backup_interfaces}, client-{_client_interfaces} from exabox.conf")
                else:
                    _backup_interfaces = ["eth5", "eth6"]
                    _client_interfaces = ["eth1", "eth2"]
                _100gbs_supported_backup = self.Is100GbsSpeedSupported(_dom0, 'backup')
                _100gbs_supported_client = self.Is100GbsSpeedSupported(_dom0, 'client')

            if _100gbs_supported_backup:
                for _s in _backup_interfaces:
                    _map[_s.strip()] = "bondeth1"
            if _100gbs_supported_client:
                for _s in _client_interfaces:
                    _map[_s.strip()] = "bondeth0"
            else:
                _i, _o, _e = _node.mExecuteCmd("/bin/cat /sys/class/net/bondeth1/bonding/slaves")
                _out = _o.readlines()
                # If bondeth1 does not exist, then the map generated as part of this function, 
                # should not contain bondeth1
                if not _node.mGetCmdExitStatus():
                    if not _out:
                        _msg = "The interface bondeth1 exists, but it seems"\
                               " to be attached to non existent slaves. Remove the stale interfaces"\
                               " and check the configuration."
                        ebLogError(_msg)
                        raise ExacloudRuntimeError(aErrorMsg=_msg)
                    else:
                        _slaves = _out[0].split(" ")
                        for _s in _slaves:
                            _map[_s.strip()] = "bondeth1"
        else:
            _domU = self.mReturnDom0DomUPair()[0][1]
            _domU_mac = self.__machines.mGetMachineConfig(_domU)                                                                      
            _domU_net_list = _domU_mac.mGetMacNetworks()                                                                              
                                                                                                                                      
            for _netc in _domU_net_list:                                                                                                                                                                                                                                
                _net = self.__networks.mGetNetworkConfig(_netc)                                                                       
                if _net.mGetNetType() == 'client' or _net.mGetNetType() == 'backup':
                    _slaves = _net.mGetNetSlave()
                    _master = _net.mGetNetMaster()
                    if _slaves != 'UNDEFINED' and _master != 'UNDEFINED':
                        _slave1 = _slaves.split(' ')[0]
                        _slave2 = _slaves.split(' ')[1]
                        _map[_slave1] = _master
                        _map[_slave2] = _master

            # Initialize _map because current usecase does not require 
            # validation of in-memory config with that in xml
            _map = {}
            _cmd_str = "/bin/grep MASTER /etc/sysconfig/network-scripts/ifcfg-eth?"
            _i, _o, _e = _node.mExecuteCmd(_cmd_str)
            _out = _o.readlines()
            """ 
            Expected Output:
            [root@scas22adm05 ~]# grep MASTER /etc/sysconfig/network-scripts/ifcfg-eth?
            /etc/sysconfig/network-scripts/ifcfg-eth2:MASTER=bondeth1
            /etc/sysconfig/network-scripts/ifcfg-eth3:MASTER=bondeth1
            /etc/sysconfig/network-scripts/ifcfg-eth4:MASTER=bondeth0
            /etc/sysconfig/network-scripts/ifcfg-eth5:MASTER=bondeth0
            """
            for _e in _out:
                try:
                    _eth = _e.strip().split(':')[0][-4:]
                    _bond = _e.strip().split('=')[1]
                    _map[_eth] = _bond
                except:
                    pass

            # returns dictionary of the form 
            # {'eth4': 'bondeth0', 'eth5': 'bondeth0', 'eth6': 'bondeth1', 'eth7': 'bondeth1'}
        ebLogTrace(f'Mapping of physical network interface and bonding interface : {_map}')
        return _map

    def mGetNetMasterInterface(self, aDom0=None):
        _dom0 = aDom0
        _master = ""

        if not _dom0:
            _dpairs = self.mReturnDom0DomUPair()
            _dom0, _ = _dpairs[0]

        _dom0_mac = self.__machines.mGetMachineConfig(_dom0)
        _net_list = _dom0_mac.mGetMacNetworks()

        for _net in _net_list:
            _priv = self.__networks.mGetNetworkConfig(_net)
            if _priv.mGetNetType() == 'admin':
                _master = _priv.mGetNetMaster()

        return _master

    def mCheckDom0NetworkType(self, aSingleDom0 = None):

            # called when network_interface_type is 'auto', check type on eth6
            def _check_extra_interface_type(_node, _exadata_model, _exadata_model_gt_X7):
                # on X5/X6 extra card is eth6,7. On X7/X8 it is eth5/6
                if _exadata_model_gt_X7:
                    _extraifidx = 5
                else:
                    _extraifidx = 6

                if self.__debug:
                    ebLogDebug('Entering Network autodetection code. Exadata model: {0}, greater than X7 : {1}'.format(_exadata_model, _exadata_model_gt_X7))
                _cmdstr = 'ethtool eth{} | grep Port'.format(_extraifidx)
                _i, _o, _e = _node.mExecuteCmd(_cmdstr)
                _out = _o.readlines()
                if not _out or not len(_out):
                    return # no extra card => default conf
                # Check if link up on idx and idx+1
                if  (_bump_interface(_node, _extraifidx) >= 0) and \
                    (_bump_interface(_node, _extraifidx+1) >= 0):
                    if 'FIBRE' in _out[0] and not _exadata_model_gt_X7:
                        ebLogInfo('*** Fiber Backup Network card detected')
                        self.__fiber_backup = True
                    elif 'Twisted Pair' in _out[0]:
                        #X7/X8 AutoDetect only supports Extra Fortpond card
                        if _exadata_model_gt_X7:
                            ebLogInfo('*** {0} Fortpond card detected'.format(_exadata_model))
                            self.__fortpond_net = True
                        else:
                            ebLogInfo('*** Copper Client Network card detected')
                            self.__copper_client = True

            def _check_interface(_node, _port):
                _cmdstr = 'ethtool eth%d | grep "Link detected"' % (_port)
                _i, _o, _e = _node.mExecuteCmd(_cmdstr)
                _out = _o.readlines()
                if not _out or not len(_out):
                    ebLogError('*** Missing Ethernet port: %d not expected' % (_port))
                    return False
                if _out[0].find('yes') != -1:
                    return True
                else:
                    return False

            def _bump_interface(_node, _port):
                #
                # Check Link status if online/up then skip bump
                #
                _cmdstr = 'ethtool eth%d | grep "Link detected"' % (_port)
                _,_o,_e = _node.mExecuteCmd(_cmdstr)
                _out = _o.readlines()
                if _out and (_out[0].find('yes') != -1):
                    return 0
                #
                # Force bump if offline/down
                #
                _cmdstr = 'ifdown eth%d ; sleep 5 ; ifup eth%d ; sleep 15 ; ifconfig eth%d up ; sleep 5 ;' % (_port, _port, _port)
                _i, _o, _e = _node.mExecuteCmd(_cmdstr)
                _cmdstr = 'ethtool eth%d | grep "Link detected"' % _port
                _i, _o, _e = _node.mExecuteCmd(_cmdstr)
                _out = _o.readlines()
                if _out and _out[0].find('yes') != -1:
                    return 1
                else:
                    return -1

            def _clean_missing_bond(_node):
                '''Remove MASTER and SLAVE from ethx if the corresponding config
                file of bond interface is missing'''
                if aSingleDom0 is not None:#No need to clean up bond since if this option is set, it is not a provisioning flow.
                    return

                for _ethx, _bondx in list(self.mGenBondMap(_node, False).items()):
                    _cmd_str = "if [ ! -f /etc/sysconfig/network-scripts/ifcfg-{1} ]; then "
                    _cmd_str += "sed -i '/MASTER.*$/d' ifcfg-{0}; "
                    _cmd_str += "sed -i '/SLAVE.*$/d' ifcfg-{0}; fi"
                    _cmd_str = _cmd_str.format(_ethx, _bondx)
                    _i, _o, _e = _node.mExecuteCmd(_cmd_str)
                    _out = _o.readlines()
                    if self.__verbose:
                        ebLogInfo(_out)

            def _dr_net_check(aSingleDom0, aFiber=False, aNewAPINetwork=None):
                if not aSingleDom0:
                    ebLogWarn("*** DOM0 is not known. Skipping DR network check. ***")
                    return False
                _exadata_model = self.mGetExadataDom0Model(aSingleDom0)
                _clu_utils = ebCluUtils(self)
                _missing_link_dr = []
                with connect_to_host(aSingleDom0, get_gcontext()) as _node:
                    if aNewAPINetwork:
                        if aNewAPINetwork.name not in ["OCIEXACC_FULL_FIBER", "OCIEXACC_FULL_COPPER"]:
                            _err_str = f'*** The interface configuration should be either full fiber or full copper for DR network. It is currently {aNewAPINetwork.name}. ***'
                            _action = "Adjust the client and backup network configuration to be either full fiber or full copper for DR network support."
                            ebLogError(_err_str)
                            _allowed_flow = _clu_utils.mIsAllowedFlowDownInterfaces(_node, _missing_link_dr, aSingleDom0, _exadata_model, aCause=_err_str, aAction=_action, aDRNet=True)
                            if not _allowed_flow:
                                ebLogError(_err_str)
                                raise ExacloudRuntimeError(0x0751, 0xA, _err_str, aStackTrace=False)
                            else:
                                return False
                        # For X10 and above, use DR net interfaces as eth1 and eth2
                        # Else, for X9, use eth5 and eth6 interfaces for DR network.
                        if mCompareModel(_exadata_model, 'X10') >= 0:
                            # Check if 100gb supported on eth1, eth2
                            _100gbs_supported_client = self.Is100GbsSpeedSupported(aSingleDom0, 'client')
                            # if 100gb supported , move dr to third slot
                            if _100gbs_supported_client:
                                _possible_interfaces = [9, 10]
                            else:
                                _possible_interfaces = [1, 2]
                        else:
                            _possible_interfaces = [5, 6]
                    else:
                        if aFiber:
                            _possible_interfaces = [5, 6]
                        else:
                            _possible_interfaces = [7, 8]
                    _node = exaBoxNode(get_gcontext())
                    _node.mConnect(aHost=aSingleDom0)
                    for _port in _possible_interfaces:
                        _irc = _bump_interface(_node, _port)
                        if _irc >= 0:
                            if _irc:
                                ebLogWarn('*** Dom0 (DR Net): %s interface: eth%s UP (after bump)' % (aSingleDom0,_port))
                            else:
                                ebLogInfo('*** Dom0 (DR Net): %s interface eth%s UP' % (aSingleDom0,_port))
                        else:
                            _missing_link_dr.append('eth%d' % (_port))
                            ebLogWarn('*** Dom0 (DR Net): %s interface: eth%s not UP (after bump)' % (aSingleDom0,_port))
                    if _missing_link_dr:
                        _err_str = f'*** One or more interfaces were not UP (DR Net): {_missing_link_dr}. Check for possible hardware issue. ***'
                        _allowed_flow = _clu_utils.mIsAllowedFlowDownInterfaces(_node, _missing_link_dr, aSingleDom0, _exadata_model, aCause=_err_str, aDRNet=True)
                        if not _allowed_flow:
                            ebLogError(_err_str)
                            raise ExacloudRuntimeError(0x0751, 0xA, _err_str, aStackTrace=False)
                        else:
                            return False
                _good_interfaces_str = 'eth%s,eth%s' % (_possible_interfaces[0], _possible_interfaces[1])
                _dr_net_interfaces = f"vmbondeth2:{_good_interfaces_str}:bondeth2"
                # Set OEDA Network properties
                self.mSetNetworkDiscovered(aDRNetInterfaces=_dr_net_interfaces)
                return True

            _exadata_model = self.mGetExadataDom0Model(aSingleDom0)
            _exadata_model_gt_X7 = False
            _compare_exadata = self.mCompareExadataModel(_exadata_model, 'X7')
            if _compare_exadata >= 0:
                _exadata_model_gt_X7 = True

            if self.__oeda_path:
                ebLogInfo(f"OEDA PATH: {self.__oeda_path}")
                _oeda_properties_path = os.path.join(self.__oeda_path, 'properties', 'es.properties')
                if os.path.exists(_oeda_properties_path):
                    _, _, _o, _ = self.mExecuteLocal(f"/bin/grep PAAS {_oeda_properties_path}")
                    _output = _o.strip()
                    ebLogTrace(f"OEDA PAAS PROPERTIES:{_output}")

            if self.mIsNoOeda():
                ebLogInfo("*** NO-OEDA flag is set to True. Skip updating network configuration in es.properties")
                return True

            # Skip checks for (X7|X8|X9|X10)/up and OCI
            if self.__exabm and _exadata_model_gt_X7:
                ebLogInfo('*** OCI {0} environment detected skipping Dom0 Net checks'.format(_exadata_model))
                return True

            # on X7/X8/X9/X10 ignore fiber_backup/copper_client parameters
            if _exadata_model_gt_X7 is True:
                if self.__debug:
                    ebLogDebug('Exadata Model {0} Detected, set network configuration to default'.format(_exadata_model))
                self.__fiber_backup  = False
                self.__copper_client = False

            # configuration mismatch as 6/7 are either fiber/copper
            if self.__fiber_backup and self.__copper_client:
                _err_str = '*** Exacloud configuration is invalid, Fiber backup link type is incompatible with Copper client link type ***'
                ebLogError(_err_str)
                raise ExacloudRuntimeError(0x0751, 0xA, _err_str, aStackTrace=False)

            # X9 and above is specific as it have different naming depending of PCI Slot
            # and a more complex matrix
            _new_api_network = None
            _exacc_base_x8 = False
            _exacc_base_x9 = False
            _exacc_base_x10 = False
            _exacc_base_x11 = False
            _rc = True
            _dpairs = None
            _missing_link = []
            _clu_utils = ebCluUtils(self)
            if aSingleDom0 is None:
                _dpairs = self.mReturnDom0DomUPair()
            else:
                _dpairs = [[aSingleDom0, None]]
            for _dom0, _ in _dpairs:
                _exadata_model = self.mGetExadataDom0Model(_dom0)
                _exadata_model_gt_X7 = False
                _compare_exadata = self.mCompareExadataModel(_exadata_model, 'X7')
                if _compare_exadata >= 0:
                    _exadata_model_gt_X7 = True

                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_dom0)
                _missing_link = []
                #
                # Older version of OEDA may require explicit bond clean up (to be removed eventually)
                #
                _cmb_flag = self.mCheckConfigOption('clean_missing_bond', 'False')
                if not _cmb_flag:
                    _clean_missing_bond(_node)

                # This NEW API is only supported for X9 and above,
                # We must extend it for other flavors in the future
                if mCompareModel(_exadata_model, 'X9') >= 0:
                    _new_api_network = ebDiscoverOEDANetwork(_node, _exadata_model, self).mGetNetwork()
                    if _new_api_network is None:
                        _rc = False
                    continue # NEW ITERATION TO CHECK INTERFACES ON ALL Dom0
                #
                # Check state of interface and try to bring them up if down
                #
                _cluster_network_type = self.mCheckClusterNetworkType()

                # auto detect fiber / copper /fortpond extra interface
                if self.__network_type == 'auto':
                   _check_extra_interface_type(_node, _exadata_model, _exadata_model_gt_X7)

                if self.__fiber_backup:
                    ebLogInfo('*** Fiber Backup Network Mode enabled')
                    _network_list = [0,1,4,5,6,7]
                elif self.__copper_client:
                    ebLogInfo('*** Copper Client Network Mode enabled')
                    _network_list = [0,1,2,3,6,7]
                elif _cluster_network_type is True:
                    _network_list = list(range(0,6))
                    if _exadata_model_gt_X7 is True:
                        if self.__fiber_backup:
                            ebLogInfo('*** {0} Network configuration enforced (DFLT FBR)'.format(_exadata_model))
                            _network_list = [0,1,2,3,4]
                        elif self.__copper_client:
                            ebLogInfo('*** {0} Network configuration enforced (DFLT CPR)'.format(_exadata_model))
                            _network_list = [0,1,2,5,6]
                        elif self.__fortpond_net:
                            ebLogInfo('*** {0}Network configuration enforced (DFLT FORTPOND)'.format(_exadata_model))
                            _network_list = [0, 1, 2, 5, 6]
                        else:
                            ebLogInfo('*** {0} Network configuration enforced (DFLT NTP)'.format(_exadata_model))
                            _network_list = [0, 1, 2, 3, 4]

                else: # Only Client network in XML
                    if mCompareModel(_exadata_model, 'X8') >= 0 and self.__ociexacc:
                        _network_list = [0,1,2]
                        _exacc_base_x8 = True
                        _exacc_base_x9 = True
                        _exacc_base_x10 = True
                        _exacc_base_x11 = True
                    else:
                        _network_list = [0,4,5]
                if self.__exabm is True:
                    ebLogInfo('*** BM Network Mode enabled')
                    if self.mEnvTarget() is False:
                        _network_list = list(range(0,7))  # DEV/QA non BM environment
                    else:
                        _network_list = [0,1,4,5,6,7]
                for _port in _network_list:
                    if _check_interface(_node,_port) is True:
                        continue
                    else:
                        _irc = _bump_interface(_node,_port)
                        if  _irc >= 0:
                            if _irc:
                                ebLogWarn('*** Dom0: %s interface: eth%s UP (after bump)' % (_dom0,_port))
                            else:
                                ebLogInfo('*** Dom0: %s interface eth%s UP' % (_dom0,_port))
                            continue
                        else:
                            _missing_link.append('eth%d' % (_port))
                            _rc = False
                            ebLogWarn('*** Dom0: %s interface: eth%s not UP (after bump)' % (_dom0,_port))
                #
                # Enforce no discovery code
                #
                if not self.mCheckConfigOption('allow_net_discovery', 'True'):
                    ebLogInfo('*** Enable SKIP_DOMU_NET_CHECK : {}'.format(_dom0) )
                    _cmd = "sed 's/export EXADATA_SKIP_DOMU_NETWORK_CHECK.*//' -i /root/.bashrc"
                    _cmd += "; echo 'export EXADATA_SKIP_DOMU_NETWORK_CHECK=yes' >> /root/.bashrc"
                    _node.mExecuteCmd(_cmd)

                _node.mDisconnect()

            if not _rc:
                if self.__exabm:
                    _rc = True      # In BM mode it is OK to have partial cabling since bonding is not yet supported
                elif self.__exacm or self.__ociexacc:
                    if _exadata_model == 'X8' and sorted(_missing_link) == ['eth3','eth4']:
                        #On EXACC X8 BaseSystem could be partially cabled with only eth0/eth1/eth2 UP, allow that case
                        _exacc_base_x8 = True
                        _rc = True
                    elif _exadata_model == 'X9' and sorted(_missing_link) == ['eth9','eth10']:
                        #On EXACC X9 BaseSystem could be partially cabled with only eth0/eth1/eth2 UP, allow that case
                        _exacc_base_x9 = True
                        _rc = True
                    elif _exadata_model == 'X10' and sorted(_missing_link) == ['eth9','eth10']:
                        #On EXACC X10 BaseSystem could be partially cabled with only eth0/eth1/eth2 UP, allow that case
                        _exacc_base_x10 = True
                        _rc = True
                    elif _exadata_model == 'X11' and sorted(_missing_link) == ['eth9','eth10']:
                        #On EXACC X11 BaseSystem could be partially cabled with only eth0/eth1/eth2 UP, allow that case
                        _exacc_base_x11 = True
                        _rc = True

                ebLogWarn('*** Network Configuration in Dom0 (%s) NOT complete / interfaces not cabled/present (%s)' % (_dom0, str(_missing_link)))
            else:
                ebLogInfo('*** Network Configuration in Dom0 (%s) is complete - Discovery Disabled' % (_dom0))
            #
            # Change oeda property (not global one - change link to real file)
            #
            if not _rc:
                #
                # If discovery is disable by configuration (e.g. new default) then throw a FATAL/CRITICAL ERROR
                #
                if self.mCheckConfigOption('allow_net_discovery','True'):
                    _oeda_prop_path = self.__oeda_path+'/properties'
                    if not os.path.exists(_oeda_prop_path):
                        raise ExacloudRuntimeError(0x0730, 0xA, "OEDA property path doesn't exist. Aborting.", aStackTrace=False)
                    _cmd_str = "/bin/sed 's/^DISABLEPAASDISCOVERY=true/DISABLEPAASDISCOVERY=false/' -i "+_oeda_prop_path+'/es.properties'
                    ebLogInfo('*** Updating OEDA properties : DISCOVERY ENABLED')
                    self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                else:
                    _allowed_flow = _clu_utils.mIsAllowedFlowDownInterfaces(_node, _missing_link, aSingleDom0, _exadata_model)
                    if not _allowed_flow:
                        raise Exception('mCheckDom0NetworkType Failed - Network Configuration Error in DOM0', _node.mGetHostname(),
                                        'Network Check For Ethernet Interfaces Availability')
            else:
                # X9 BLOCK/NEW API, We should use this API for all other shapes later on
                if _new_api_network:
                    ebLogInfo(f"***Updating OEDA properties: NETWORK AUTODETECTED AS: {_new_api_network.name}")
                    self.mSetNetworkDiscovered(str(_new_api_network.value.mGetAdminNet()),
                                               str(_new_api_network.value.mGetClientNet()),
                                               str(_new_api_network.value.mGetBackupNet()))
                    if self.mIsDRNetPresent() is True:
                        _rc = _dr_net_check(aSingleDom0, aNewAPINetwork=_new_api_network)

                    _oeda_properties_path = os.path.join(self.__oeda_path,'properties','es.properties')
                    if not os.path.exists(_oeda_properties_path):
                        raise ExacloudRuntimeError(0x0730, 0xA, "OEDA property path doesn't exist. Aborting.", aStackTrace=False)
                    _cmd_str = f"/bin/sed 's/^DISABLEPAASDISCOVERY=false/DISABLEPAASDISCOVERY=true/' -i {_oeda_properties_path}"
                    self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                    _cmd_str = f"/bin/sed 's/^\(.*\)_ADMIN_IFACE.*/\\1_ADMIN_IFACE={str(_new_api_network.value.mGetAdminNet())}/' -i {_oeda_properties_path}"
                    self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                    _cmd_str = f"/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE={str(_new_api_network.value.mGetClientNet())}/' -i {_oeda_properties_path}"
                    self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                    _cmd_str = f"/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE={str(_new_api_network.value.mGetBackupNet())}/' -i {_oeda_properties_path}"
                    self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                # X8 AND BELOW ====================
                elif _cluster_network_type is True:
                    # Set OEDA Network properties
                    self.mSetNetworkDiscovered(aAdminNet='vmeth1::eth1')
                    _oeda_prop_path = self.__oeda_path+'/properties'
                    if not os.path.exists(_oeda_prop_path):
                        raise ExacloudRuntimeError(0x0730, 0xA, "OEDA property path doesn't exist. Aborting.", aStackTrace=False)
                    _cmd_str = "/bin/sed 's/^DISABLEPAASDISCOVERY=false/DISABLEPAASDISCOVERY=true/' -i "+_oeda_prop_path+'/es.properties'
                    self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                    _cmd_str = "/bin/sed 's/^\(.*\)_ADMIN_IFACE.*/\\1_ADMIN_IFACE=vmeth1::eth1/' -i " + _oeda_prop_path + '/es.properties'
                    self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                    if _exacc_base_x8 or _exacc_base_x9 or _exacc_base_x10 or _exacc_base_x11:
                        # Set OEDA Network properties
                        self.mSetNetworkDiscovered(aAdminNet='vmeth0::eth0',
                                                   aClientNet='vmbondeth0:eth1,eth2:bondeth0',
                                                   aBackupNet='vmbondeth0:eth1,eth2:bondeth1')

                        _cmd_str = "/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE=vmbondeth0:eth1,eth2:bondeth0/' -i "+_oeda_prop_path+'/es.properties'
                        self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                        _cmd_str = "/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth0:eth1,eth2:bondeth1/' -i " + _oeda_prop_path + '/es.properties'
                        self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                        _cmd_str = "/bin/sed 's/^\(.*\)_ADMIN_IFACE.*/\\1_ADMIN_IFACE=vmeth0::eth0/' -i " + _oeda_prop_path + '/es.properties'
                        self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                        ebLogInfo('*** Updating OEDA properties : DISCOVERY DISABLED (X8/X9/X10/X11 CC BASE-SYSTEM 1/2 CLIENT+BACKUP)')
                        if self.__ociexacc:
                            ebLogInfo('*** Updating BMC properties for OCIEXACC X8/X9/X10/X11 BASE-SYSTEM to 1/2 CLIENT+BACKUP')
                        # X8/X9/X10 ROCE/KVM - UPDATE
                        if self.__ociexacc and self.mIsKVM():
                            # Set OEDA Network properties
                            self.mSetNetworkDiscovered(aBackupNet='vmbondeth0:eth1,eth2:bondeth1')

                            _cmd_str += "; sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth0:eth1,eth2:bondeth1/' -i " + _oeda_prop_path + '/es.properties'
                            ebLogInfo('*** X8/X9/X10/X11 ROCE/KVM OCIEXACC (BASE-SYSTEM) properties updated to 1/2 CLIENT+BACKUP (admin:0)')
                    elif self.__exabm is True:
                        # Set OEDA Network properties
                        self.mSetNetworkDiscovered(aClientNet='vmbondeth0:eth4,eth5:bondeth0',
                                                   aBackupNet='vmbondeth1:eth6,eth7:bondeth1')

                        _cmd_str = "/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE=vmbondeth0:eth4,eth5:bondeth0/' -i "+_oeda_prop_path+'/es.properties'
                        self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                        _cmd_str = "/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth1:eth6,eth7:bondeth1/' -i " + _oeda_prop_path + '/es.properties'
                        self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                        ebLogInfo('*** Updating OEDA properties : DISCOVERY DISABLED (BM 6-7)')
                    elif self.__fiber_backup:
                        if  _exadata_model_gt_X7 is True:
                            # X7/X8 switch from CLIENT and BACKUP w/ resect w/ previous Gen.
                            # Set OEDA Network properties
                            self.mSetNetworkDiscovered(aAdminNet='vmeth0::eth0',
                                                       aClientNet='vmbondeth1:eth3,eth4:bondeth1',
                                                       aBackupNet='vmbondeth0:eth1,eth2:bondeth0')

                            _cmd_str = "/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE=vmbondeth1:eth3,eth4:bondeth1/' -i "+_oeda_prop_path+'/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            _cmd_str = "/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth0:eth1,eth2:bondeth0/' -i " + _oeda_prop_path + '/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            _cmd_str = "/bin/sed 's/^\(.*\)_ADMIN_IFACE.*/\\1_ADMIN_IFACE=vmeth0::eth0/' -i " + _oeda_prop_path + '/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            ebLogInfo('*** Updating OEDA properties : DISCOVERY DISABLED ( {0} DEFAULT ECS CONFIG 1/2 and 3/4 FIBER)'.format(_exadata_model))
                            if self.__ociexacc:
                                # Set OEDA Network properties
                                self.mSetNetworkDiscovered(aClientNet='vmbondeth0:eth1,eth2:bondeth0',
                                                           aBackupNet='vmbondeth1:eth3,eth4:bondeth1')
                                if self.mIsDRNetPresent() is True:
                                    _rc = _dr_net_check(aSingleDom0, aFiber=True)
                                _cmd_str = "/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE=vmbondeth0:eth1,eth2:bondeth0/' -i " + _oeda_prop_path + '/es.properties'
                                self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                                _cmd_str = "/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth1:eth3,eth4:bondeth1/' -i " + _oeda_prop_path + '/es.properties'
                                self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                                ebLogInfo('*** Updating BMC properties for OCIEXACC FIBER to 1/2 and 3/4')
                            # X8 ROCE/KVM - UPDATE
                            if self.__ociexacc and self.mIsKVM():
                                # Set OEDA Network properties
                                self.mSetNetworkDiscovered(aClientNet='vmbondeth0:eth1,eth2:bondeth0',
                                                           aBackupNet='vmbondeth1:eth3,eth4:bondeth1')
                                if self.mIsDRNetPresent() is True:
                                    _rc = _dr_net_check(aSingleDom0, aFiber=True)
                                _cmd_str = "/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE=vmbondeth0:eth1,eth2:bondeth0/' -i " + _oeda_prop_path + '/es.properties'
                                self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                                _cmd_str = "/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth1:eth3,eth4:bondeth1/' -i " + _oeda_prop_path + '/es.properties'
                                self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                                ebLogInfo('*** X8 ROCE/KVM OCIEXACC (FIBER) properties updated to 1/2 and 3/4 (admin:0)')
                        else:
                            # Set OEDA Network properties
                            self.mSetNetworkDiscovered(aAdminNet='vmeth1::eth1',
                                                       aClientNet='vmbondeth0:eth4,eth5:bondeth0',
                                                       aBackupNet='vmbondeth1:eth6,eth7:bondeth1')

                            _cmd_str = "/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE=vmbondeth0:eth4,eth5:bondeth0/' -i "+_oeda_prop_path+'/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            _cmd_str = "/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth1:eth6,eth7:bondeth1/' -i " + _oeda_prop_path + '/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            _cmd_str = "/bin/sed 's/^\(.*\)_ADMIN_IFACE.*/\\1_ADMIN_IFACE=vmeth1::eth1/' -i " + _oeda_prop_path + '/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            ebLogInfo('*** Updating OEDA properties : DISCOVERY DISABLED (BACKUP: FIBER 6-7)')
                            if self.__ociexacc:
                                # Set OEDA Network properties
                                self.mSetNetworkDiscovered(aAdminNet='vmeth0::eth0')

                                _cmd_str = "/bin/sed 's/^\(.*\)_ADMIN_IFACE.*/\\1_ADMIN_IFACE=vmeth0::eth0/' -i " + _oeda_prop_path + '/es.properties'
                                self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                                ebLogInfo('*** Updating BMC properties for OCIEXACC <=X6 FIBER to 4/5 and 6/7')
                            # X8 ROCE/KVM - UPDATE
                            if self.__ociexacc and self.mIsKVM():
                                ebLogInfo('*** X8 ROCE/KVM OCIEXACC (FIBER <=X6 ??? MR - NA ???) properties updated to 4/5 and 6/7 (admin:0)')
                    elif self.__copper_client:
                        if  _exadata_model_gt_X7 is True:
                            # X7/X8 switch from CLIENT and BACKUP w/ resect w/ previous Gen.
                            # Set OEDA Network properties
                            self.mSetNetworkDiscovered(aAdminNet='vmeth0::eth0',
                                                       aClientNet='vmbondeth0:eth1,eth2:bondeth0',
                                                       aBackupNet='vmbondeth1:eth5,eth6:bondeth1')
                            if self.mIsDRNetPresent() is True:
                                _rc = _dr_net_check(aSingleDom0, aFiber=False)
                            _cmd_str = "/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE=vmbondeth0:eth1,eth2:bondeth0/' -i "+_oeda_prop_path+'/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            _cmd_str = "/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth1:eth5,eth6:bondeth1/' -i " + _oeda_prop_path + '/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            _cmd_str = "/bin/sed 's/^\(.*\)_ADMIN_IFACE.*/\\1_ADMIN_IFACE=vmeth0::eth0/' -i -i " + _oeda_prop_path + '/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            ebLogInfo('*** Updating OEDA properties : DISCOVERY DISABLED ({0} DEFAULT ECS CONFIG 1/2 and 5/6 COPER)'.format(_exadata_model))
                            if self.__ociexacc:
                                ebLogInfo('*** Updating BMC properties for OCIEXACC COPPER to 1/2 and 5/6')
                            # X8 ROCE/KVM - UPDATE
                            if self.__ociexacc and self.mIsKVM():
                                ebLogInfo('*** X8 ROCE/KVM OCIEXACC (COPPER) properties updated to 1/2 and 5/6 (admin:0)')
                        else:
                            # Set OEDA Network properties
                            self.mSetNetworkDiscovered(aAdminNet='vmeth0::eth0',
                                                       aClientNet='vmbondeth0:eth6,eth7:bondeth0',
                                                       aBackupNet='vmbondeth1:eth2,eth3:bondeth1')

                            _cmd_str = "/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE=vmbondeth0:eth6,eth7:bondeth0/' -i "+_oeda_prop_path+'/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            _cmd_str = "/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth1:eth2,eth3:bondeth1/' -i " + _oeda_prop_path + '/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            _cmd_str = "/bin/sed 's/^\(.*\)_ADMIN_IFACE.*/\\1_ADMIN_IFACE=vmeth0::eth0/' -i " + _oeda_prop_path + '/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            ebLogInfo('*** Updating OEDA properties : DISCOVERY DISABLED (CLIENT: COPPER 6-7)')
                            if self.__ociexacc:
                                ebLogInfo('*** Updating BMC properties for OCIEXACC <=X6 CLIENT COPPER 6-7')
                            if self.__ociexacc and self.mIsKVM():
                                ebLogInfo('*** X8 ROCE/KVM OCIEXACC (COPPER <=X6 ??? MR - NA ???) properties updated to 6/7 and 2/3 (admin:0)')
                    else:
                        if  _exadata_model_gt_X7 is True:
                            if self.__fortpond_net:
                                # Set OEDA Network properties
                                self.mSetNetworkDiscovered(aAdminNet='vmeth0::eth0',
                                                           aClientNet='vmbondeth0:eth1,eth2:bondeth0',
                                                           aBackupNet='vmbondeth1:eth5,eth6:bondeth1')
                                if self.mIsDRNetPresent() is True:
                                    _rc = _dr_net_check(aSingleDom0, aFiber=False)
                                _cmd_str = "/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE=vmbondeth0:eth1,eth2:bondeth0/' -i " + _oeda_prop_path + '/es.properties'
                                self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                                _cmd_str = "/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth1:eth5,eth6:bondeth1/' -i " + _oeda_prop_path + '/es.properties'
                                self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                                _cmd_str = "/bin/sed 's/^\(.*\)_ADMIN_IFACE.*/\\1_ADMIN_IFACE=vmeth0::eth0/' -i " + _oeda_prop_path + '/es.properties'
                                self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                                ebLogInfo('*** Updating OEDA properties : DISCOVERY DISABLED ({0} FORTPOND ECS CONFIG CL: 1/2 and BK: 5/6)'.format(_exadata_model))
                                if self.__ociexacc:
                                    ebLogInfo('*** Updating BMC properties for OCIEXACC FORTPOND to 1/2 and 5/6')
                                if self.__ociexacc and self.mIsKVM():
                                    ebLogInfo('*** X8 ROCE/KVM OCIEXACC (FORTPOND) properties updated to 1/2 and 5/6 (admin:0)')

                            else:
                                # Set OEDA Network properties
                                self.mSetNetworkDiscovered(aClientNet='vmbondeth0:eth1,eth2:bondeth0',
                                                           aBackupNet='vmbondeth1:eth3,eth4:bondeth1')
                                if self.mIsDRNetPresent() is True:
                                    _rc = _dr_net_check(aSingleDom0, aFiber=True)
                                _cmd_str = "/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE=vmbondeth0:eth1,eth2:bondeth0/' -i "+_oeda_prop_path+'/es.properties'
                                self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                                _cmd_str = "/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth1:eth3,eth4:bondeth1/' -i " + _oeda_prop_path + '/es.properties'
                                self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                                ebLogInfo('*** Updating OEDA properties : DISCOVERY DISABLED ({0} DEFAULT ECS CONFIG 1/2 and 3/4)'.format(_exadata_model))
                                if self.__ociexacc:
                                    # Set OEDA Network properties
                                    self.mSetNetworkDiscovered(aAdminNet='vmeth0::eth0')

                                    _cmd_str = "/bin/sed 's/^\(.*\)_ADMIN_IFACE.*/\\1_ADMIN_IFACE=vmeth0::eth0/' -i " + _oeda_prop_path + '/es.properties'
                                    self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                                    # X8 ROCE/KVM - UPDATE
                                    if self.mIsKVM():
                                        ebLogInfo('*** X8 ROCE/KVM OCIEXACC properties updated to 1/2 and 3/4 (admin:0)')
                                    ebLogInfo('*** Updating BMC properties for OCIEXACC to 1/2 and 3/4')
                        else:
                            # Set OEDA Network properties
                            self.mSetNetworkDiscovered(aClientNet='vmbondeth0:eth4,eth5:bondeth0',
                                                       aBackupNet='vmbondeth1:eth2,eth3:bondeth1')

                            _cmd_str = "/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE=vmbondeth0:eth4,eth5:bondeth0/' -i "+_oeda_prop_path+'/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            _cmd_str = "/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth1:eth2,eth3:bondeth1/' -i " + _oeda_prop_path + '/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            ebLogInfo('*** Updating OEDA properties : DISCOVERY DISABLED (NON BM 2-3)')
                            # VGE, tentative support for X6, UNTESTED
                            if self.__ociexacc:
                                # Set OEDA Network properties
                                self.mSetNetworkDiscovered(aAdminNet='vmeth0::eth0')

                                _cmd_str = "/bin/sed 's/^\(.*\)_ADMIN_IFACE.*/\\1_ADMIN_IFACE=vmeth0::eth0/' -i " + _oeda_prop_path + '/es.properties'
                                self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                                ebLogInfo('*** Updating BMC properties for OCIEXACC <=X6 to 1/2 and 5/6')
                else: #_cluster_network_type == FALSE : NO BACKUP network in XML (ONLY X8 Base system for OCIEXACC Support)
                    _oeda_prop_path = self.__oeda_path+'/properties'
                    if not os.path.exists(_oeda_prop_path):
                        raise ExacloudRuntimeError(0x0730, 0xA, "OEDA property path doesn't exist. Aborting.", aStackTrace=False)
                    _cmd_str = "/bin/sed 's/^DISABLEPAASDISCOVERY=false/DISABLEPAASDISCOVERY=true/' -i "+_oeda_prop_path+'/es.properties'
                    self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)

                    # Set OEDA Network properties
                    self.mSetNetworkDiscovered(aAdminNet='vmeth0::eth1')

                    _cmd_str = "/bin/sed 's/^\(.*\)_ADMIN_IFACE.*/\\1_ADMIN_IFACE=vmeth0::eth1/' -i " + _oeda_prop_path + '/es.properties'
                    self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                    if _exacc_base_x8 or _exacc_base_x9 or _exacc_base_x10 or _exacc_base_x11:
                        # Set OEDA Network properties
                        self.mSetNetworkDiscovered(aAdminNet='vmeth0::eth0',
                                                   aClientNet='vmbondeth0:eth1,eth2:bondeth0',
                                                   aBackupNet='vmbondeth0:eth1,eth2:bondeth1')

                        _cmd_str = "/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE=vmbondeth0:eth1,eth2:bondeth0/' -i "+_oeda_prop_path+'/es.properties'
                        self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                        _cmd_str = "/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth0:eth1,eth2:bondeth1/' -i " + _oeda_prop_path + '/es.properties'
                        self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                        _cmd_str = "/bin/sed 's/^\(.*\)_ADMIN_IFACE.*/\\1_ADMIN_IFACE=vmeth0::eth0/' -i " + _oeda_prop_path + '/es.properties'
                        self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                        ebLogInfo('*** Updating OEDA properties : DISCOVERY DISABLED (X8/X9/X10/X11 and above CC BASE-SYSTEM 1/2 CLIENT NON REG NET)')
                        if self.__ociexacc:
                            ebLogInfo('*** Updating BMC properties for OCIEXACC X8/X9/X10/X11 and above BASE-SYSTEM to 1/2 CLIENT NON REG NET')
                        if self.__ociexacc and self.mIsKVM():
                            # Set OEDA Network properties
                            self.mSetNetworkDiscovered(aBackupNet='vmbondeth0:eth1,eth2:bondeth1')

                            _cmd_str += "; sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth0:eth1,eth2:bondeth1/' -i " + _oeda_prop_path + '/es.properties'
                            ebLogInfo('*** X8/X9/X10/X11 ROCE/KVM OCIEXACC (X8/X9/X10/X11 BASE-SYSTEM) properties updated to 1/2 and 1/2 (admin:0)')

                    elif self.__exabm is True:
                        # Set OEDA Network properties
                        self.mSetNetworkDiscovered(aAdminNet='vmeth1::eth1',
                                                   aClientNet='vmbondeth0:eth4,eth5:bondeth0',
                                                   aBackupNet='vmbondeth1:eth6,eth7:bondeth1')

                        _cmd_str = "/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE=vmbondeth0:eth4,eth5:bondeth0/' -i "+_oeda_prop_path+'/es.properties'
                        self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                        _cmd_str = "/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth1:eth6,eth7:bondeth1/' -i " + _oeda_prop_path + '/es.properties'
                        self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                        _cmd_str = "/bin/sed 's/^\(.*\)_ADMIN_IFACE.*/\\1_ADMIN_IFACE=vmeth1::eth1/' -i " + _oeda_prop_path + '/es.properties'
                        self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                        ebLogInfo('*** Updating OEDA properties : DISCOVERY DISABLED (BM NON REG NET 6-7 - NOT SUPPORTED -)')
                    elif self.__fiber_backup:
                        if  _exadata_model_gt_X7 is True:
                            # Set OEDA Network properties
                            self.mSetNetworkDiscovered(aClientNet='vmbondeth1:eth3,eth4:bondeth1',
                                                       aBackupNet='vmbondeth0:eth1,eth2:bondeth0')

                            # X7/X8 switch from CLIENT and BACKUP w/ resect w/ previous Gen.
                            _cmd_str = "/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE=vmbondeth1:eth3,eth4:bondeth1/' -i "+_oeda_prop_path+'/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            _cmd_str = "/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth0:eth1,eth2:bondeth0/' -i " + _oeda_prop_path + '/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            ebLogInfo('*** Updating OEDA properties : DISCOVERY DISABLED ({0} DEFAULT ECS CONFIG 1/2 and 3/4 FIBER)'.format(_exadata_model))
                        else:
                            # Set OEDA Network properties
                            self.mSetNetworkDiscovered(aClientNet='vmbondeth0:eth4,eth5:bondeth0',
                                                       aBackupNet='vmbondeth1:eth6,eth7:bondeth1')

                            _cmd_str = "/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE=vmbondeth0:eth4,eth5:bondeth0/' -i "+_oeda_prop_path+'/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            _cmd_str = "/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth1:eth6,eth7:bondeth1/' -i " + _oeda_prop_path + '/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            ebLogInfo('*** Updating OEDA properties : DISCOVERY DISABLED (FIBER BACKUP NON REG NET 6-7)')
                    elif self.__copper_client:
                        if  _exadata_model_gt_X7 is True:
                            # X7/X8 switch from CLIENT and BACKUP w/ resect w/ previous Gen.
                            # Set OEDA Network properties
                            self.mSetNetworkDiscovered(aClientNet='vmbondeth0:eth1,eth2:bondeth0',
                                                       aBackupNet='vmbondeth1:eth5,eth6:bondeth1')
                            if self.mIsDRNetPresent() is True:
                                _rc = _dr_net_check(aSingleDom0, aFiber=False)
                            _cmd_str = "/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE=vmbondeth0:eth1,eth2:bondeth0/' -i "+_oeda_prop_path+'/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            _cmd_str = "/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth1:eth5,eth6:bondeth1/' -i " + _oeda_prop_path + '/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            ebLogInfo('*** Updating OEDA properties : DISCOVERY DISABLED ({0} DEFAULT ECS CONFIG 1/2 and 5/6 COPER)'.format(_exadata_model))
                        else:
                            # Set OEDA Network properties
                            self.mSetNetworkDiscovered(aClientNet='vmbondeth0:eth6,eth7:bondeth0',
                                                       aBackupNet='vmbondeth1:eth2,eth3:bondeth1')

                            _cmd_str = "/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE=vmbondeth0:eth6,eth7:bondeth0/' -i "+_oeda_prop_path+'/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            _cmd_str = "/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth1:eth2,eth3:bondeth1/' -i " + _oeda_prop_path + '/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            ebLogInfo('*** Updating OEDA properties : DISCOVERY DISABLED (COPPER CLIENT NON REG NET 6-7)')
                    else:
                        if  _exadata_model_gt_X7 is True:
                            if self.__fortpond_net:
                                # Set OEDA Network properties
                                self.mSetNetworkDiscovered(aClientNet='vmbondeth0:eth1,eth2:bondeth0',
                                                           aBackupNet='vmbondeth1:eth5,eth6:bondeth1')
                                if self.mIsDRNetPresent() is True:
                                    _rc = _dr_net_check(aSingleDom0, aFiber=False)
                                _cmd_str = "/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE=vmbondeth0:eth1,eth2:bondeth0/' -i " + _oeda_prop_path + '/es.properties'
                                self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                                _cmd_str = "/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth1:eth5,eth6:bondeth1/' -i " + _oeda_prop_path + '/es.properties'
                                self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                                ebLogInfo('*** Updating OEDA properties : DISCOVERY DISABLED ({0} FORTPOND ECS CONFIG CL: 1/2 and BK: 5/6)'.format(_exadata_model))
                            else:
                                # X7/X8 switch from CLIENT and BACKUP w/ resect w/ previous Gen.
                                # Set OEDA Network properties
                                self.mSetNetworkDiscovered(aClientNet='vmbondeth1:eth3,eth4:bondeth1',
                                                           aBackupNet='vmbondeth0:eth1,eth2:bondeth0')

                                _cmd_str = "/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE=vmbondeth1:eth3,eth4:bondeth1/' -i "+_oeda_prop_path+'/es.properties'
                                self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                                _cmd_str = "/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth0:eth1,eth2:bondeth0/' -i " + _oeda_prop_path + '/es.properties'
                                self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                                ebLogInfo('*** Updating OEDA properties : DISCOVERY DISABLED ({0} DEFAULT ECS CONFIG 1/2 and 3/4)'.format(_exadata_model))
                        else:
                            # Set OEDA Network properties
                            self.mSetNetworkDiscovered(aClientNet='vmbondeth0:eth4,eth5:bondeth0',
                                                       aBackupNet='vmbondeth1:eth2,eth3:bondeth1')

                            _cmd_str = "/bin/sed 's/^\(.*\)_CLIENT_IFACE.*/\\1_CLIENT_IFACE=vmbondeth0:eth4,eth5:bondeth0/' -i "+_oeda_prop_path+'/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            _cmd_str = "/bin/sed 's/^\(.*\)_BACKUP_IFACE.*/\\1_BACKUP_IFACE=vmbondeth1:eth2,eth3:bondeth1/' -i " + _oeda_prop_path + '/es.properties'
                            self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL)
                            ebLogInfo('*** Updating OEDA properties : DISCOVERY DISABLED (NON BM NON REG NET 2-3)')

            #
            # NOTE: Only check the first Dom0 for now and assume similar configuration in other clusters.
            #
            return _rc

    """
    Function to query the network information.
    Name: mGetNetworkSetupInformation
    param1: aNetworkType, type: string, accepted values: admin, client, backup, all(admin, client and backup network information)
    param2: aDom0, type: string, value: dom0 name for which network information is required.
    return: _compiled_network_information, type: dictionary, value: as below.
    {
        "admin" : {
                    "bond_master": "",
                    "bond_slaves" : "",
                    "bridge: ""
        },
        "client": ...,
        "backup": ...
    }
    """
    def mGetNetworkSetupInformation(self, aNetworkType: str, aDom0: str) -> dict:
        _compiled_network_information = dict()
        _all_network_types = ["all", "admin", "client", "backup", "dr"]
        if aNetworkType not in _all_network_types:
            ebLogError(f"Invalid network type: {aNetworkType}")
            return _compiled_network_information

        # Reset previously discovered network information before trying to discover it
        self.__network_discovered = {}
        self.mCheckDom0NetworkType(aSingleDom0 = aDom0)
        _discovered_network_info = self.mGetNetworkDiscovered()
        """
        Current layout of _discovered_network_info
        {
            'admin_net': {
                'bridge': _bridge,
                'bond_master': _bond_master,
                'bond_slaves': _bond_slaves.replace(',', ' ')
            },
            'client_net': {...},
            'backup_net': {...}
        }
        """
        if aNetworkType != "all":
            if f"{aNetworkType}_net" in _discovered_network_info.keys():
                _network_info = copy.deepcopy(_discovered_network_info[f"{aNetworkType}_net"])
                _compiled_network_information[aNetworkType] = _network_info
        else:
            for _this_network_idx in range(1, len(_all_network_types)):
                _this_network = _all_network_types[_this_network_idx]
                if f"{_this_network}_net" in _discovered_network_info.keys():
                    _network_info = copy.deepcopy(_discovered_network_info[f"{_this_network}_net"])
                    _compiled_network_information[_this_network] = _network_info

        if self.mGetCmd() in ["checkcluster"]:
            # Add error information to the dictionary returned to healthcheck if there is an error
            # mGetNetDetectError will return {} - empty dict if there is no error.
            if self.mGetNetDetectError():
                _error_dict = {"ERROR": self.mGetNetDetectError()}
                _compiled_network_information.update(_error_dict)
                self.__netDetectError = {}

        return _compiled_network_information
    
    def mCheckDomUImageFor23ai(self, vmImageVersionNum):
        """Check domU image for compatibility for 23ai

        Arguments:
            vmImageVersionNum -- domU image version

        Raises:
            ExacloudRuntimeError: Error instance
        """
        if self.mGetGiMultiImageSupport():
            _gi_version = self.mGetVersionGiMultiImages()
        else:
            _gi_version = self.mGetVersionGi()
        ebLogTrace(f'23ai precheck: domU image version {vmImageVersionNum}, GI version {_gi_version}')
        if vmImageVersionNum is not None:
            domU_image_version_major = int(vmImageVersionNum.split('.')[0])
            if _gi_version.startswith('23') and domU_image_version_major < 23:
                _error_str = f'*** ERROR - DomU Image Version {vmImageVersionNum} not compatible for 23ai'
                ebLogError(_error_str)
                raise ExacloudRuntimeError(0x0826, 0xA, _error_str,aStackTrace=True)
        ebLogTrace(f'23ai precheck:Passed for domU image version {vmImageVersionNum}, GI version {_gi_version}')

    def mCheckSystemImage(self, aCustomVersion:str=None):
        """
        aCustomVersion has format 22.1.0.0.0.YYMMDD
        """

        vmImageVersionNum = ""
        _all_dom0s_same_version = False
        _all_dom0s_are_rtg_compat = False
        _enforceRtg = False
        
        dom0sImagesVersion = mGetDom0sImagesListSorted(self)
        if aCustomVersion:
            vmImageVersionNum = aCustomVersion
        else:            
            vmImageVersionNum = dom0sImagesVersion[0]
            if not vmImageVersionNum: # empty string if Dom0 has broken img
                _msg = "One Dom0 have a broken image."
                ebLogError(_msg)
                raise ExacloudRuntimeError(0x0730, 0xA, _msg, aStackTrace=False)
        
        # Only one version across dom0s
        if len(list(set(dom0sImagesVersion))) == 1:
            ebLogInfo(f"All dom0s have same version: {vmImageVersionNum}")
            _all_dom0s_same_version = True
                
        # Check image version for 23ai precheck
        self.mCheckDomUImageFor23ai(vmImageVersionNum)

        # RTG logic does not apply on XEN 
        if self.mIsKVM():
            # If aCustomVersion is NOT RTG, then it has precedence over the infra
            if (aCustomVersion is None) \
            or (aCustomVersion and mIsRtgImg(aCustomVersion)):
                # Dom0's may have slight diff images, such as 24.1 or 24.2
                # As long as ALL are Greater Than 24.0, these will be RTG compatible
                if all( mIsRtgImg(img) for img in dom0sImagesVersion ):
                    ebLogInfo(f"All dom0s are rtg compat")
                    _all_dom0s_are_rtg_compat = True
                
                # At this point it is known if the infra supports RTG, but it is
                # also required to verify if the RTG file exists somewhere.
                # If RTG IMG is not present, then FALLBACK is to use legacy img.
                isRtgImgPresentInDom0s, isRtgImgPresentInLocal = \
                    mIsRtgImgPresent(self,vmImageVersionNum)
                if not isRtgImgPresentInDom0s and not isRtgImgPresentInLocal:                     
                    _all_dom0s_are_rtg_compat = False
                
                # Making sure that if all the dom0s are rtg compatible then, rtg image is picked up.
                if _all_dom0s_are_rtg_compat:
                    _enforceRtg = True
        else:
            ebLogInfo("XEN detected, RTG logic does NOT apply. Use IMG.")

        # Copy minimal image to images folder
        _imgName = formatVMImageBaseName(vmImageVersionNum, False, _all_dom0s_are_rtg_compat)
        _imgNameXML = formatVMImageBaseName(vmImageVersionNum, False, False)
        _Dom0sWithImg = mSearchImgInDom0s(self,_imgName)
        
        # It may be the case that all Dom0s ARE RTG compatible
        # but Dom0's DO NOT have RTG, so we need to try if legacy img is present
        # if len(_Dom0sWithImg) == 0 and _all_dom0s_are_rtg_compat:
        #     ebLogInfo("RTG was attempted but does NOT exist.")
        #     _Dom0sWithImg = mSearchImgInDom0s(self,_imgName)

        if _all_dom0s_same_version:
            
            # If all Dom0s can support RTG 
            # and the rtg.img file is NOT present in all Dom0s
            # and the rtg image is present in one Dom0 at least
            # then we need to ensure this particular rtg is copied
            if _all_dom0s_are_rtg_compat and len(_Dom0sWithImg) != len(dom0sImagesVersion):
                _enforceRtg = True
            
            mCopySystemImgLocalToDOM0(self, vmImageVersionNum, _Dom0sWithImg, _enforceRtg)


        # Set vmImageVersionNum in properties/es.properties
        self.mSetImageVersionProperty(vmImageVersionNum)

        if not aCustomVersion:
            # In this case, since we listed Dom0s and ImgVersions before
            # we are SURE that at least one Dom0 contains the _imgName 
            ebLogInfo(f"More than one version detect, syncronizing image: {_imgName}")

            # Ensure compressed file is in images folder
            if _Dom0sWithImg:
                mGetImageFromDom0ToLocal(self, vmImageVersionNum, _Dom0sWithImg[0])
        else:
            # Search custom version in images and if missing get from bucket
            if not _Dom0sWithImg:
                # If Dom0s DO NOT have the image, exacloud need to try to 
                # fetch from local.
                mCopySystemImgLocalToDOM0(self, vmImageVersionNum, None, _enforceRtg)

                # Deprecating option here, as OPS is not placing images in OSS.                
                # 
                # if not self.mIsOciEXACC():
                #     raise ExacloudRuntimeError(0x0730, 0xA, 
                #         "No suitable System first boot Image found. Aborting", 
                #         aStackTrace=False)
                #     # mGetImageFromOSSToLocal(self,vmImageVersionNum, self.mIsKVM())
            else:
                # Dom0 folder (EXAVMIMAGES) always has ONLY ONE name per Img
                # regardless of the hypervisors (XEN or KVM)
                mGetImageFromDom0ToLocal(self,vmImageVersionNum,_Dom0sWithImg[0])

        # At this point, local (images/) contains the *.BZ2 file that is needed
        # for non-ExaCC environments. For ExaCC we will try to get the image
        # from exabox.conf 'ociexacc_exadata_patch_download_loc'. This logic
        # is available inside of mVerifyImagesMultiProc

        # Trigger Process
        _rc_status = mVerifyImagesMultiProc(self,vmImageVersionNum, _enforceRtg)

        # validate the return codes
        _rc_all = 0
        _dpairs = self.mReturnDom0DomUPair()

        # Patch XML (mSetMacVMImgName & mSetMacVMImgVersion)
        for _dom0, _domU in _dpairs:
            _domU_mac = self.__machines.mGetMachineConfig(_domU)
            _domUImageName = _domU_mac.mGetMacVMImgName()
            _domU_mac.mSetMacVMImgName(_imgName) # Add rtg to image in XML after latest oeda agreement (38225207)
            _domU_mac.mSetMacVMImgVersion(vmImageVersionNum)
            if _rc_status[_dom0]:
                _rc_all = ebError(_rc_status[_dom0])
                ebLogError("*** Could not copy System Image to Dom0 '{0}' " \
                           "return status = '{1}' ***".format(_dom0, _rc_status[_dom0]))
                ebLogTrace("Exacloud DID NOT found a valid System image " \
                    f"file for {vmImageVersionNum} in Local or Dom0s, " \
                    "Please verify that: " \
                    "1) valid BZ2 images are present in local " \
                    "2) RTG.IMG or IMG are present in Dom0s.")

            elif self.__verbose:
                ebLogInfo("*** Successfully copied System Image to Dom0 {0} ***".format(_dom0))

        if _rc_all:
            raise ExacloudRuntimeError(0x0730, 0xA, "No suitable System first boot Image found. Aborting", aStackTrace=False)

        self.mSaveXMLClusterConfiguration()
        ebLogInfo('ebCluCtrl: Saved patched Cluster Config: ' + self.__patchconfig)

        #
        # !!! WARNING !!! Past this _point_ the XML Cluster Configuration has been saved 
        # anything below should _NOT_ change the XML Cluster Configuration file.
        #
        self.__remoteconfig = self.__oeda_path+'/exacloud.conf/'+os.path.basename(self.__patchconfig)
        self.mExecuteCmd('/bin/mkdir -p '+self.__oeda_path+'/exacloud.conf')
        self.mCopyFile(self.__patchconfig, self.__remoteconfig)

        ebLogInfo (f'clucontrol mCheckSystemImg _imgName: {_imgName}')
        return _imgName
    
    def mSetImageVersionProperty(self, aImgVersion):
        _oeda_path  = self.mGetOedaPath()        
        _prop_file = os.path.join(_oeda_path, 'properties/es.properties')
        _imgVersionNoDate = '.'.join(aImgVersion.split('.')[:5])

        if self.mIsKVM():
            if mIsRtgImg(aImgVersion):
                aImgVersion = f"{aImgVersion}.rtg"        

        ebLogInfo('*** Updating OEDA properties for image version : {}'.format(aImgVersion))

        # Example :  20.1.2.0.0,System.first.boot.20.1.2.0.0.200905.img,20.1.2.0.0,\
        # System image version of source domU : 20.1.2.0.0.200905
        # 20.1.2.0.0 should point to the System image of source domU
        self.mExecuteLocal("/bin/sed -i 's/^{0},System.first.boot.*/{0},System.first.boot.{1}.img,{0},\\\/g' {2}".format(_imgVersionNoDate, aImgVersion, _prop_file))
        self.mExecuteLocal("/bin/sed -r 's/(\s*p.*,){0}.*:\\\/\\1{1}:\\\/g' -i {2}".format(_imgVersionNoDate, aImgVersion, _prop_file))
        self.mExecuteLocal("/bin/sed -r 's/(\s*p.*,){0}.*[0-9]$/\\1{1}/g' -i {2}".format(_imgVersionNoDate, aImgVersion, _prop_file))

    #
    # Get Image Version for a host 'aHost'
    #
    def mGetImageVersion(self, aHost, aUseCache=True) -> str:
        """
        Returns a str with format 21.1.2.3.4.YYMMDD
        """

        # Small cache to optimize image version lookup
        _hostNat = aHost
        _ctx = get_gcontext()
        if _ctx.mCheckRegEntry('_natHN_' + _hostNat):
            _hostNat = _ctx.mGetRegEntry('_natHN_' + _hostNat)

        if aUseCache:
            if _hostNat in self.mGetExadataImagesMap().keys():
                return self.mGetExadataImagesMap()[_hostNat]

        if not self.__ut and not self.mPingHost(aHost):
            ebLogError(f"Node {aHost} is not reachable, cannot obtain image version.")
            self.mGetExadataImagesMap()[aHost] = None
            return None

        # In case to not be defined in the map, use the command to get the version
        _host = aHost
        _node = exaBoxNode(get_gcontext())
        _node.mConnect(aHost=_host)
        _imgver = ""

        _cmdstr = '/usr/local/bin/imageinfo -version'
        _i, _o, _e = _node.mExecuteCmd(_cmdstr)
        _out = _o.readlines()
        if _out:
            _imgver = _out[0].strip()

        if not _out or not _imgver:
            ebLogError('*** Unable to extract image version from imageinfo for the host %s' % _host)

        _node.mDisconnect()
        ebLogVerbose('mGetImageVersion _imgver={}'.format(_imgver))

        self.mGetExadataImagesMap()[_hostNat] = _imgver

        return _imgver

    def mGetSwitchFirmwareVersion(self, aHost):

        _host = aHost
        _node = exaBoxNode(get_gcontext())
        _node.mConnect(aHost=_host)
        _frmwrver = ""

        _cmdstr = '/usr/local/bin/version | /usr/bin/head -1'
        _i, _o, _e = _node.mExecuteCmd(_cmdstr)
        _out = _o.readlines()
        if _out:
            _frmwrver = _out[0].strip().split()[-1]

        if not _out or not _frmwrver:
            ebLogError('*** Unable to extract firmware version for the switch %s' % _host)

        _node.mDisconnect()
        ebLogInfo('mGetSwitchFirmwareVersion _frmwrver={}'.format(_frmwrver))
        return _frmwrver

    def mGetLastRebootTime(self, aHost):

        _host = aHost
        _node = exaBoxNode(get_gcontext())
        _node.mConnect(aHost=_host)
        _lastreboot = ""

        _cmdstr = '/usr/bin/who -b'
        _i, _o, _e = _node.mExecuteCmd(_cmdstr)
        _out = _o.readlines()
        if _out:
            _lastreboot = _out[0].strip().split('system boot  ')[-1]

        if not _out or not _lastreboot:
            ebLogError('*** Unable to find last reboot time for the host %s' % _host)

        _node.mDisconnect()
        ebLogInfo('mGetLastRebootTime _lastreboot={}'.format(_lastreboot))
        return _lastreboot

    #
    # Add PreReq check for missing disks
    #
    def mCheckDiskResources(self):
        def mCheckDiskResourcesOnCell(aCell):
            _cell = aCell
            try:
                # Check all cell services up or not. before running cellcli
                _retry_wait_time = 30
                _retry_count = 5
                _cellservice_up = False
                while _retry_count != 0:
                    _failedCelllist =[]
                    self.mSingleCellCheckCellServiceUp(_cell, _failedCelllist)
                    if len(_failedCelllist) > 0:
                        ebLogInfo("Cell services not running on %s. retrying in %i second" % (_cell, _retry_wait_time) )
                        time.sleep(_retry_wait_time)
                    else:
                        _cellservice_up = True
                        ebLogInfo("*** Cell Services running on cell %s " % _cell)
                        break
                    _retry_count -= 1
                    if _retry_count == 0 and _cellservice_up == False:
                        ebLogError("*** Cell services is not running on cell %s " % _cell)
                        raise ExacloudRuntimeError(0x0743, 0xA, 'Cell services is not running on cell', Cluctrl=self)

                # Check Physical disk status
                _node = exaBoxNode(get_gcontext(), Cluctrl = self)
                _node.mConnect(aHost=_cell)
                ebLogInfo('*** Verifying physical disk status on cell: (%s)' % (_cell))
                _cmdstr = "cellcli -e list physicaldisk attributes name where diskType like \\'.*Disk\\' and status!=\\'normal\\' detail;"
                _i,_o,_e = _node.mExecuteCmdCellcli(_cmdstr)
                _output = _o.readlines()
                if _output:
                    ebLogInfo('*** Following physicaldisk not in normal state present on cell: (%s)' % (_cell))
                    _disk_failures = []
                    for _line in _output:
                        _disk_split = _line.split()
                        if _disk_split and len(_disk_split) == 2:
                            _disk_name = _disk_split[1]
                            ebLogInfo(f"*** Disk name : {_disk_name} ")
                            if self.mCheckConfigOption('ignore_m2_sys_error'):
                                if _disk_name.lower() not in ['m2_sys_0','m2_sys_1']:
                                    _disk_failures.append(_disk_name)
                            else:
                                _disk_failures.append(_disk_name)
                            
                    _node.mDisconnect()
                    if _disk_failures:
                        raise ExacloudRuntimeError(0x0743, 0xA, 'Physical Disk not in normal state', Cluctrl = self)
                else:
                    #
                    # Just Keeping this in the log to have a quick trc/entries of each cells
                    #
                    ebLogInfo('*** PhysicalDisk status on Cell: (%s) in normal state' % (_cell))
                    _node.mDisconnect()
            
            except Exception as exep:
                ebLogError(f"*** Exception Message Detail on Cell {_cell} {exep}")
                raise Exception(f"Physical Disk not in normal state: {exep} on host {_cell}")


        _plist = ProcessManager()

        for _cell in self.mReturnCellNodes():
            _p = ProcessStructure(mCheckDiskResourcesOnCell, [_cell])
            _p.mSetMaxExecutionTime(30*60) #30 minutes timeout
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()

    # Check physical interfaces and remove config files of non-existing interfaces from network-scripts
    def mCheckEthInterfaces(self,aNode):
        ebLogInfo("*** Checking physical network interfaces and interfaces configuration files ***")

        _node = exaBoxNode(get_gcontext())
        _node.mConnect(aHost=aNode)

        _cmd = "/sbin/ip link show"
        ebLogInfo("*** Getting list of physical interfaces from node:{0} ***".format(aNode))

        _fin, _out, _err = _node.mExecuteCmd(_cmd)
        _rc = _node.mGetCmdExitStatus()

        if _rc != 0:
            _node.mDisconnect()
            raise ExacloudRuntimeError(aErrorMsg="Error while trying to get physical network interfaces from node:{0}".format(aNode))

        # Filter using eth2 to obtain only the bridges corresponding to the admin net
        _admin_cmd = "/sbin/brctl show | /bin/awk '/eth2/{ print $1 }' | /bin/cut -c 3-"
        ebLogInfo("*** Getting list of admin physical interfaces from node:{0} ***".format(aNode))

        _admin_fin, _admin_out, _admin_err = _node.mExecuteCmd(_admin_cmd)
        _admin_rc = _node.mGetCmdExitStatus()

        if _admin_rc != 0:
            _node.mDisconnect()
            raise ExacloudRuntimeError(
                aErrorMsg="Error while trying to get admin physical network interfaces from node:{0}".format(aNode))

        _interfaces_result = _out.readlines()
        _admin_interfaces_result = _admin_out.readlines()
        _interfaces = self.mFindEthInterfaces(_interfaces_result, _admin_interfaces_result)
        ebLogInfo(f"*** Interfaces List:{_interfaces}")

        _cmd = "/bin/ls /etc/sysconfig/network-scripts/ifcfg-eth*"
        ebLogInfo("*** Getting list of physical interfaces config files from /etc/sysconfig/network-scripts ***")

        _fin, _out, _err = _node.mExecuteCmd(_cmd)
        _rc = _node.mGetCmdExitStatus()

        if _rc != 0:
            _node.mDisconnect()
            raise ExacloudRuntimeError(aErrorMsg="Error while trying to get physical network interfaces config files")

        _network_files_result = _out.readlines()
        _network_files = [nf[nf.find('eth'):].strip() for nf in _network_files_result]
        ebLogInfo(f"*** Network Files Interfaces List:{_network_files}")

        _network_files_eth_removed_list = []
        for _network_file in _network_files:
            if _network_file not in _interfaces:
                ebLogInfo("*** Removing config file:/etc/sysconfig/networ-scripts/ifcfg-{0} of non existing physical interface:{0} ***".format(_network_file.strip()))
                _cmd = "rm -f /etc/sysconfig/network-scripts/ifcfg-{0}".format(_network_file.strip())
                _network_files_eth_removed_list.append(_cmd)
                _node.mExecuteCmd(_cmd)
                _rc = _node.mGetCmdExitStatus()

                if _rc != 0:
                    _node.mDisconnect()
                    raise ExacloudRuntimeError(aErrorMsg="Error while trying to remove physical network interfaces config file")

        _node.mDisconnect()

        return _network_files_eth_removed_list

    def mFindEthInterfaces(self, aInterfacesList, aInterfacesAdminList):
        # Pattern _eth_pattern to match these formats from the command output: /sbin/ip link show
        # N[NN]: ethN[NN]:  or  N[NN]: ethN[NN].N[NN]@ethN[NN]:
        # Examples:
        # 2: eth1:              139: eth0.100@eth0:
        # 3: eth2:              141: eth0.101@eth0:
        # 4: eth0:              143: eth0.102@eth0:
        # 9: eth204:            168: eth0.103@eth0:
        _eth_pattern = re.compile(r"[0-9]+:[\s+](eth[0-9]+(\.[0-9]+@eth[0-9]+)?):")
        _matches_list = set()
        for l in aInterfacesList:
            eth_interface = _eth_pattern.match(l)
            if eth_interface:
                _grp_eth_if = eth_interface.groups()[0]
                if '@' in _grp_eth_if:
                    _matches_list.add(_grp_eth_if.split("@")[0])
                else:
                    _matches_list.add(_grp_eth_if)

        # Pattern _admin_pattern to validate eth interfaces
        # ethN[NN]
        # Examples:
        # eth201
        # eth204
        _admin_pattern = re.compile(r"eth[0-9]+")
        _matches_list.update({x.strip() for x in aInterfacesAdminList if _admin_pattern.match(x)})

        return list(_matches_list)

    #
    # Execute reboot on Cells that need Fips and SELinux to be enabled
    #
    def mCheckCellCompliance(self):

        _reboot_set = set()
        _str = ""
        _cell_list = self.mReturnCellNodes()
        for _cell in _cell_list:
            if self.mIsOciEXACC() and self.mIsFedramp():
                _rc, _str = self.mMakeFipsCompliant(self.__options, aHost=_cell)
            elif self.mGetSELinuxMode("cell") and self.mIsHostOL8(_cell) and not self.mIsOciEXACC():
                _rc, _str = self.mMakeFipsCompliant(self.__options, aHost=_cell)
            else:
                _rc, _str = self.mMakeFipsCompliant(self.__options, aHost=_cell, aSshConfOnly=True)
            if _str == "reboot_host":
                ebLogInfo('*** Need to reboot host ' + _cell + ' for activating fips')
                _reboot_set.add(_cell)

        if self.__cmd not in ["vmgi_reshape", "elastic_cell_update"] and self.__options.jsonconf:
            _sestatus = self.mGetSELinuxMode("cell")
            if _sestatus:
                for _cell in _cell_list:
                    _node = exaBoxNode(get_gcontext())
                    _node.mConnect(aHost=_cell)       
                    if self.mSetSeLinux(_node, _sestatus, "cell"):
                        _reboot_set.add(_cell)
                    _node.mDisconnect()
            else:
                ebLogWarn("*** se_linux key not present")

        # Reboot the cells on single VM or log message to advertise reboot needed for SE to be activated
        # This wont be executed if _sestatus is None since _reboot_set would be empty
        if _reboot_set:
            self.mRebootNodesIfNoVMExists(_reboot_set, "cell")

    # These checks are especially needed on multi-VM racks
    #
    def mCheckDom0Resources(self, aOptions):

            _sestatus = None
            if self.__options.jsonconf:
                _sestatus = self.mGetSELinuxMode("dom0")



            # Detect vNUMA
            _jconf = aOptions.jsonconf
            _vnumaMode = None
            if _jconf and 'vm' in _jconf and "cloud_vnuma" in _jconf['vm']:
                _vnumaMode = _jconf['vm']['cloud_vnuma']

            def _mConfigFipsSelinux(aDom0, aRebootlist):
                _dom0 = aDom0
                _reboot_list = aRebootlist
                _node = exaBoxNode(get_gcontext())
                with connect_to_host(_dom0, get_gcontext()) as _node:
                # Put ADB-S Marker in Dom0s if applicable
                    self.mCreateVnumaMarker(aOptions, _dom0)
                    _rc, _str = self.mMakeFipsCompliant(aOptions, aHost=_dom0)
                    if _str == "reboot_host":
                        _reboot_list.append(_dom0)
                        ebLogInfo('need to reboot host')

                #SE LINUX check
                    if self.__cmd not in ["vmgi_reshape", "elastic_cell_update"] and _sestatus is not None:
                        if self.mSetSeLinux(_node, _sestatus, "dom0"):
                            _reboot_list.append(_dom0)
                    else:
                        ebLogWarn("*** se_linux key not present")

                    if not self.mIsKVM():

                    # Force pinning in Dom0
                        if _vnumaMode == "disabled":

                            _remotePath = "/tmp/cpu_vnuma_to_pinning{0}.sh".format(str(self.__uuid))
                            ebLogInfo("Executing force pinning using {0} script".format(_remotePath))
                            _node.mCopyFile("scripts/cpu_vnuma_to_pinning.sh", _remotePath)
                            _node.mExecuteCmdLog("/bin/bash {0}".format(_remotePath))
                            _node.mExecuteCmd("/bin/rm {0}".format(_remotePath))

                        # Calculate the Dom0s that are not rebooted yet and add them to _reboot_set
                            _node.mExecuteCmd("/usr/sbin/xm info | /bin/grep 'dom0_vcpus_pin '")
                            if _node.mGetCmdExitStatus() != 0:
                                _reboot_list.append(_dom0)

                    # Force vNUMA in Dom0
                        elif _vnumaMode in ["enabled_without_dom0_overlap", "enabled_with_dom0_overlap"]:

                            _remotePath = "/tmp/cpu_pinning_to_vnuma{0}.sh".format(str(self.__uuid))
                            ebLogInfo("Executing force vnuma using {0} script".format(_remotePath))
                            _node.mCopyFile("scripts/cpu_pinning_to_vnuma.sh", _remotePath)

                            _cmd = "/bin/bash {0}".format(_remotePath)
                            if _vnumaMode == "enabled_with_dom0_overlap":
                                _cmd = "{0} -overlap".format(_cmd)

                            _node.mExecuteCmdLog(_cmd)
                            _node.mExecuteCmd("/bin/rm {0}".format(_remotePath))

                        # Calculate the Dom0s that are not rebooted yet and add them to _reboot_set
                            if _vnumaMode == "enabled_with_dom0_overlap":
                                _node.mExecuteCmd("/usr/sbin/xm info | /bin/grep dom0_vcpus_pin")
                                if _node.mGetCmdExitStatus() == 0:
                                    _reboot_list.append(_dom0)
                            elif _vnumaMode == "enabled_without_dom0_overlap":
                                _node.mExecuteCmd("/usr/sbin/xm info | /bin/grep dom0_vcpus_pin=numa")
                                if _node.mGetCmdExitStatus() != 0:
                                    _reboot_list.append(_dom0)

                #Disconnect node
            _plist = ProcessManager()
            _reboot_list = _plist.mGetManager().list()
            for _dom0, _ in self.mReturnDom0DomUPair():
                _p = ProcessStructure(_mConfigFipsSelinux, [_dom0, _reboot_list])
                _p.mSetMaxExecutionTime(60*60)
                _p.mSetJoinTimeout(5)
                _p.mSetLogTimeoutFx(ebLogWarn)
                _plist.mStartAppend(_p)
            _plist.mJoinProcess()

            _reboot_set = set(_reboot_list)
            # Reboot the dom0s if single VM or log banner to advertise reboot is needed to apply SELinux changes
            if _reboot_set:
                self.mRebootNodesIfNoVMExists(_reboot_set, "dom0")

            # Maximum of 24 clusters allowed unless config says otherwise
            if self.__shared_env:
                if self.mCheckConfigOption('vm_clusters_limit') is not None:
                    _vm_clusters_limit = self.mCheckConfigOption('vm_clusters_limit')
                else:
                    _vm_clusters_limit = 24

                _numVMs = self.mCheckNumVM()
                if (_numVMs == -1) or (_numVMs >= int(_vm_clusters_limit)):
                    ebLogError('*** Cluster limit of %s reached. New cluster can\'t be created' %(_vm_clusters_limit))
                    raise ExacloudRuntimeError(0x0733, 0xA, "Cluster limit of %s reached. New cluster can\'t be created", aStackTrace=False)
                elif self.__debug:
                    ebLogInfo('*** Number of existing clusters within limit of %s. Proceeding with create cluster' %(_vm_clusters_limit))

                if self.mEnvTarget() is False and _numVMs > 8:
                    ebLogInfo('*** Modifiying OEDA property for cluster limit over 8 VMs.')
                    _cmd = ['{0}/oedacli'.format(self.__oeda_path), '-e', 'alter', 'property', 'name=MAXDOMULIMIT', 'value=16']
                    _strCmd = ""
                    for _arg in _cmd:
                        _strCmd = _strCmd + _arg + " "
                    _rc, _, _, _ = self.mExecuteLocal(_strCmd, aStdOut=DEVNULL, aStdErr=DEVNULL)
                    if _rc != 0:
                        ebLogError('*** Could not run oedacli cmd: %s ' % (_cmd))
                        raise ExacloudRuntimeError(0x0100, 0xA, "OEDA install/env not found", aStackTrace=False)

            # Fetch u02 disk size
            _disk_u02_size = self.mGetu02Size()

            # Images take up about 40G. Buffer of 60 to ensure we don't empty storage
            if self.mCheckConfigOption('disk_images_size') is not None:
                _disk_images_size = self.mCheckConfigOption('disk_images_size')
            else:
                _disk_images_size = '100G'

            def _mCheckMemStorage(aDom0):
                _dom0 = aDom0
                _node = exaBoxNode(get_gcontext())
                with connect_to_host(_dom0, get_gcontext()) as _node:

                    vm = getHVInstance(_dom0)
                    _memfree = vm.getDom0FreeMem()
                    _memint = int(_memfree) / 1024

                    ebLogInfo('*** Memory available on %s is %dGb' %(_dom0, _memint))

                    _memXML = None
                    if (self.mIsKVM() or self.mGetUiOedaXml()) and self.__machines and self.__machines.mGetMachineConfig(_domU):
                        ebLogInfo('*** Using guestMemory section from xml.')
                        _domU_mac = self.__machines.mGetMachineConfig(_domU)
                        _memXML = _domU_mac.mGetGuestMemory()
                # Each of the below can be None
                    elif self.__vmsizes and self.__vmsizes.mGetVMSize('Large') and self.__vmsizes.mGetVMSize('Large').mGetVMSizeAttr('MemSize'):
                    # All MemSize in XML are same
                        ebLogInfo('*** Using vmSizes section from xml.')
                        _memXML = self.__vmsizes.mGetVMSize('Large').mGetVMSizeAttr('MemSize')
                    else:
                        _memXML = None
                
                    if _memXML:
                        ebLogInfo('*** Memory size requirement in XML is %s' %(_memXML))
                        if _memint < (int(_memXML[:-2])):
                            ebLogError('*** Not enough memory available on dom0 %s. Aborting' %(_dom0))
                            _domUs = vm.mRefreshDomUs()
                            ebLogInfo("***List of created VMs:")
                            for _line in _domUs:
                                ebLogInfo(_line)
                            raise ExacloudRuntimeError(0x0731, 0xA, "Not enough memory available on dom0. Aborting", aStackTrace=False)
                        else:
                            ebLogInfo('*** Enough memory available on dom0 %s' %(_dom0))
                    else:
                        ebLogInfo('*** vmSizes/guestMemory sections are not present in the xml. Skipping memory size check.')

                    if not self.mCheckConfigOption('skip_storage_checks','True'):
                    # Make sure enough storage on /EXAVMIMAGES...
                        _cmdstr = 'df -h -B G |grep EXAVMIMAGES'
                        _i, _o, _e = _node.mExecuteCmd(_cmdstr)
                        _rc = _node.mGetCmdExitStatus()
                        if _rc != 0:
                            ebLogError(f'*** EXAVMIMAGES not loaded on dom0:{_dom0},{_cmdstr} failed to execute. Aborting')
                            raise ExacloudRuntimeError(0x0813, 0xA, "EXAVMIMAGES not loaded on dom0, cannot determine storage. Aborting", aStackTrace=False)
                        _o_list = _o.readlines()
                        try:
                            _fspace = _o_list[0].strip()
                            _fspace = " ".join(_fspace.split())
                            _fspace = _fspace.split(" ")[-3].lstrip().rstrip()[:-1]
                            ebLogInfo('*** Free space required for images: %s' %(_disk_images_size))
                            ebLogInfo('*** Free space required for u02 partition: %s' %(_disk_u02_size))
                            ebLogInfo('*** Free space available on /EXAVMIMAGES partition on %s is %sG' %(_dom0, _fspace))
                        # At least twice the amount of required u02 size should be available
                        # Otherwise existing VMs misbehave
                            if int(_fspace) < (int(_disk_images_size[:-1]) + int(_disk_u02_size[:-1])):
                                ebLogError('*** Free space available on /EXAVMIMAGES partition on dom0 %s is insufficient. Aborting' %(_dom0))
                                raise ExacloudRuntimeError(0x0732, 0xA, "Free space available on /EXAVMIMAGES partition on dom0 is insufficient. Aborting", aStackTrace=False)
                        except Exception as e:
                            ebLogError(f'*** Space compution on /EXAVMIMAGES partition on dom0 {_dom0} failed with error {e}')
                            raise ExacloudRuntimeError(0x0814, 0xA, "Space compution on /EXAVMIMAGES partition on dom0 failed", aStackTrace=False)

                    if not self.__ociexacc:
                        self.mCheckEthInterfaces(_dom0)


            _plist = ProcessManager()
            _dpairs = self.mReturnDom0DomUPair()
            for _dom0, _domU in _dpairs:
                _p = ProcessStructure(_mCheckMemStorage, [_dom0])
                _p.mSetMaxExecutionTime(60*60)
                _p.mSetJoinTimeout(5)
                _p.mSetLogTimeoutFx(ebLogWarn)
                _plist.mStartAppend(_p)
            _plist.mJoinProcess()

    #
    # Method to put ADB-S marker file for ADB-S Features in ExaCS
    #
    def mCreateVnumaMarker(self, aOptions, aDom0=None):

        # Detect adb-s
        _jconf = aOptions.jsonconf
        _adbs = None
        if _jconf and 'adb_s' in _jconf :
            _adbs = _jconf.get("adb_s")


        if not _adbs or _adbs == "False":
            ebLogInfo('*** Skipping VNUMA Marker file ')
            return

        _dom0_dict = {
            'target':["dom0s"],
            'lifecycle':"request",
            'data': {"group":"adbs"},
            'filename_prefix': "patching"
        }

        _file_str = {"markerData":[_dom0_dict]}

        _str_config = json.dumps(_file_str, sort_keys=True, indent=4, separators=(',', ' : '))

        #
        # Save configuration in /opt/exacloud/tenant-type/adbs in all dom0s
        #
        with  NamedTemporaryFile(mode='w', delete=True) as _ntp:
            _ntp.file.write(_str_config)
            _ntp.flush()
            #
            # Save the adbs configuration on each dom0
            #

            _target_dir = "/opt/exacloud/tenant-type/"
            _file_name = "adbs"

            if aDom0 is not None:
                self.mPlaceVnumaMarker(aDom0, _target_dir, _file_name, _ntp.name)
            else:
                # if no Dom0 is passed, marker will be put in all current dom0s
                _dpairs = self.mReturnDom0DomUPair()
                for _dom0, _ in _dpairs:
                    self.mPlaceVnumaMarker(_dom0, _target_dir, _file_name, _ntp.name)

    # Method that places Vnuma Marker in given Dom0
    def mPlaceVnumaMarker(self, aDom0, aTargetDir, aFile_name, aLocalFileName):
        _node = exaBoxNode(get_gcontext())
        try:
            _node.mConnect(aHost=aDom0)
            _node.mExecuteCmd(f"/bin/mkdir -p  {aTargetDir}")
            _target_file = os.path.join(aTargetDir, aFile_name)
            ebLogInfo(f'*** Saving VNUMA Marker file on dom0: {aDom0} in {_target_file}')
            _node.mCopyFile(aLocalFileName, _target_file)
        except Exception as e:
            ebLogError(f"*** Failed to put VNUMA marker file in dom0: {aDom0}, ERROR: {e}")
        finally:
            _node.mDisconnect()

    #
    # Method to delete VNUMA marker file for ADB-S Features in ExaCS
    #
    def mDeleteVnumaMarker(self, aDom0=None):
        ebLogInfo("*** Deleting Vnuma Markers in Dom0's ***")
        _target_file = "/opt/exacloud/tenant-type/adbs"

        if aDom0 is not None:
            _node = exaBoxNode(get_gcontext())
            try:
                _node.mConnect(aHost=aDom0)
                _node.mExecuteCmd("/bin/rm -f  {0}".format(_target_file))
                ebLogInfo(f'*** Deleting VNUMA Marker file on dom0: {aDom0} in {_target_file}')
            except Exception as e:
                ebLogError(f"*** Failed to delete VNUMA marker file in dom0: {aDom0}, ERROR: {e}")
            finally:
                _node.mDisconnect()
        else:
                # If dom0 was not passed, delete on all current nodes
            _dpairs = self.mReturnDom0DomUPair()
            for _dom0, _ in _dpairs:
                _node = exaBoxNode(get_gcontext())
                try:
                    _node.mConnect(aHost=_dom0)
                    _node.mExecuteCmd("/bin/rm -f  {0}".format(_target_file))
                    ebLogInfo('*** Deleting VNUMA Marker file on dom0: {0} in {1}'.format(_dom0, _target_file))
                except Exception as e:
                    ebLogError(f"*** Failed to delete VNUMA marker file in dom0: {_dom0}, ERROR: {e}")
                finally:
                    _node.mDisconnect()

    # BUG 31540575
    # If ADBS env:
    # Method to create cloud_user_ADBS exacli user on all the storage servers
    #
    def mCreateAdbsUser(self, aOptions, aCellList=None, aPasswd=None):
        _options = aOptions
        # Detect adb-s
        _jconf = aOptions.jsonconf
        _adbs = None
        if _jconf and 'adb_s' in _jconf :
            _adbs = _jconf.get("adb_s")

        if not _adbs or _adbs == "False":
            ebLogInfo('*** Skipping Creation of ADBS exacli user')
            return
        else:
            ebLogInfo('*** Creating ADBS exacli user : cloud_uesr_ADBS')

        # If the password is not passed as argument, pick it up from payload
        if aPasswd:
            _passwd = aPasswd

        # As part of starterDB removal effort, ECRA should send this password
        # under aOptions.jsonconf.vm.adminPassword
        elif _options.jsonconf:
            _passwd = _options.jsonconf.get(
                    "vm", {}).get("adminPassword", "")

            # If password not present in adminPassword, try to fetch it from legacy
            # sshkey section in the payload
            if not _passwd:
                ebLogTrace("adminPassword valued not provided in payload, "
                    "fetching from legacy dbParams section")
                if self.IsZdlraProv():
                    _passwd = self.mGetZDLRA().mGetWalletViewEntry("passwd")
                else:
                    _passwd = _options.jsonconf.get(
                            "dbParams", {}).get("passwd", "")
                    _passwd = b64encode(_passwd.encode("utf-8")).decode("utf-8")

            # Verify password is not empty and is base64 encoded
            if not _passwd or not check_string_base64(_passwd):
                _err = ("cloud_user / exacli password provided on payload "
                    f"by ECRA is invalid/not base64 encoded: '{_passwd}'")
                ebLogError(_err)
                raise ExacloudRuntimeError(0x0823, 0xA, _err)

        else:
            _err = "No password provided to be used for cloud_user/exacli"
            ebLogError(_err)
            raise ExacloudRuntimeError(0x0823, 0xA, _err)

        # Ensure b64 password
        if not check_string_base64(_passwd):
            _passwd = b64encode(_passwd.encode("utf-8")).decode("utf-8")

        if _options.jsonconf is None:
            _options.jsonconf = {}
        _options.jsonconf["user"] = "cloud_user_ADBS"
        _options.jsonconf["role"] = "cloud_role"

        # This password will come in base64 encoding on the payload,
        # we just need to convert it to bytes object
        _options.jsonconf["password"] = _passwd.encode()

        _usrconfig = ebCluResManager(self, _options)

        # Execute the cmds on the list of cells provided
        if aCellList:
            _usrconfig.mSetCells(aCellList)

        # Create user cloud_user_ADBS
        _options.user_operation = "create_user"
        _rc = _usrconfig.mUserConfig(_options)

        # Grant cloud_role role to the user
        _options.user_operation = "grant_role"
        _rc = _usrconfig.mUserConfig(_options)
        
        # Grant additional privilege to cloud_role
        # Provide diskmap access for x8m and above
        if mCompareModel(self.mGetExadataDom0Model(), 'X8') >= 0:
            _options.jsonconf["object"] = "diskmap"
            _options.user_operation = "grant_privilege"
            _rc = _usrconfig.mUserConfig(_options)

        # Store Cloud user passwd in the DomUs
        _domUs = list(map(operator.itemgetter(1),self.mReturnDom0DomUPair()))
        ebExaCCSecrets(_domUs).mPushExacliPasswdToDomUs(b64decode(_passwd.encode()).decode())

    def mDeleteAdbsUser(self, aOptions, aCellList=None):
        _options = aOptions
        # Detect adb-s
        _jconf = aOptions.jsonconf
        _adbs = None
        if _jconf and 'adb_s' in _jconf :
            _adbs = _jconf.get("adb_s")

        if not _adbs or _adbs == "False":
            ebLogInfo('*** Skipping Deletion of ADBS exacli user')
            return
        else:
            if self.__shared_env and not self.mIsLastCluster(aOptions):
                ebLogWarn("*** Another cluster's VMs exists. Not deleting ADBS user")
                return
            ebLogInfo('*** Deleting ADBS exacli user : cloud_uesr_ADBS')

        if _options.jsonconf is None:
            _options.jsonconf = {}
        _options.jsonconf["user"] = "cloud_user_ADBS"

        _usrconfig = ebCluResManager(self, _options)

        # Execute the cmds on the list of cells provided
        if aCellList:
            _usrconfig.mSetCells(aCellList)

        # Delete user cloud_user_ADBS
        _options.user_operation = "delete_user"
        _rc = _usrconfig.mUserConfig(_options)


    # BUG 26942102
    # To allow overlap between exadata network and customer networks
    def mConfigureArp(self,enable=True):
        _dpairs = self.mReturnDom0DomUPair()

        _arp_ignore = 1
        _arp_announce = 2
        _arp_message = "Enabling"

        if not enable:
            _arp_ignore = 0
            _arp_announce = 0
            _arp_message = "Disabling"

        for _dom0,_ in _dpairs:
            ebLogInfo("*** {0} ARP configuring in host:{1}".format(_arp_message,_dom0))

            with connect_to_host(_dom0, get_gcontext()) as _node:
                self.mSetSysCtlConfigValue(_node, "net.ipv4.conf.all.arp_ignore", _arp_ignore)
                self.mSetSysCtlConfigValue(_node, "net.ipv4.conf.all.arp_announce", _arp_announce)

    # BUG 29671545
    def mRemoveXleaveFromNtpConf(self):

        _dpairs = self.mReturnDom0DomUPair()
        for _dom0, _ in _dpairs:
            if self.mGetImageVersion(_dom0).split('.')[0] == '19':
                ebLogInfo('*** Performing removal of xleave option and restarting the ntpd service.')
                try:
                    _node = exaBoxNode(get_gcontext())
                    _node.mConnect(aHost=_dom0)
                    if self.mIsKVM():
                        _cmd = "sed 's/xleave//g' -i /etc/chrony.conf"
                        _node.mExecuteCmdLog(_cmd)
                        _cmd = "systemctl restart chronyd"
                        _node.mExecuteCmdLog(_cmd)
                    else:
                        _cmd = "sed 's/xleave//g' -i /etc/ntp.conf"
                        _node.mExecuteCmdLog(_cmd)
                        _cmd = "service ntpd restart"
                        _node.mExecuteCmdLog(_cmd)
                    _node.mDisconnect()
                except:
                    ebLogWarn('*** Unable to connect to host: {}'.format(_dom0))
                    continue

    #
    # Get total and available storage in GB's in /EXAVMIMAGES partition
    #
    def mHandlerGetEXAVMStorage(self, aOptions = None):

        if not aOptions: 
            aOptions = self.mGetArgsOptions()

        _total_storage = {}
        _available_storage = {}

        try:
            _cmd = "df -BG -P /EXAVMIMAGES | tail -1"
            if self.__debug:
               ebLogInfo("%s: mHandlerGetEXAVMStorage: Enter." % (datetime.datetime.now()))

            def _mExecute(aDom0, aList):

                _dom0 = aDom0
                _list = aList
                _json = {}

                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_dom0)
                _i, _o, _e = _node.mExecuteCmd(_cmd)
                _values = _o.readlines()[0].split()
                _json[_dom0]=_values

                # Store the available and total storage in /EXAVMIMAGES
                # partition in a json in a List.
                _list.append(_json.copy())

                _node.mDisconnect()
                if self.__debug:
                    ebLogInfo('%s : _mExecute completed' % aDom0)
            
            _plist = ProcessManager()
            _list = _plist.mGetManager().list()

            # Iterate through the list of dom0's
            for _dom0, _ in self.mReturnDom0DomUPair():
                if not self.mPingHost(_dom0):
                    _msg = f"Dom0 {_dom0} is not reachable!! Please run \"who -b\" and compare with the timestamp of this error."
                    ebLogError(_msg)
                    raise ExacloudRuntimeError(0x0761, 0xA, _msg)
            for _dom0, _ in self.mReturnDom0DomUPair():
                _p = ProcessStructure(_mExecute, (_dom0, _list))
                _plist.mStartAppend(_p)

                if self.__debug:
                   ebLogInfo('%s : _mExecute started for %s ' %(_p.name, _dom0))

            _plist.mJoinProcess()

            for _json in _list:                
                _key = list(_json.keys())[0]
                # Segregate the total and available storage value from
                # individual entries in the list.
                _total_storage[_key] = _json[_key][1]
                _available_storage[_key] = _json[_key][3]

        except Exception as e:
            ebLogError("*** Failed to compute storage in /EXAVMIMAGES partition in dom0s")
            raise ExacloudRuntimeError(0x0761, 0xA, "Failed to compute storage in /EXAVMIMAGES partition in dom0s") from e

        if self.__debug:
           ebLogInfo("%s: mHandlerGetEXAVMStorage: Finished compute storage in /EXAVMIMAGES partition in dom0s." % (datetime.datetime.now()))

        if self.__cmd == 'dom0_storage_info':
            nsParams = {}

            if aOptions is not None and aOptions.jsonconf is not None and 'node_subset' in list(aOptions.jsonconf.keys()):
                nsParams = aOptions.jsonconf['node_subset'].copy()
                _participant_node_count = nsParams['num_participating_computes']

                if _participant_node_count != len(nsParams['participating_computes']):
                    ebLogError("*** NodeSubset Json Payload: num_participating_computes mismatch with participating_computes count")
                    raise ExacloudRuntimeError(0x0765, 0xA, "NodeSubset Json Payload: num_participating_computes"\
                    "mismatch with participating_computes count")

                for _dom0 in  nsParams['participating_computes']:
                    if _dom0['compute_node_hostname'] in list(_total_storage.keys()):
                        _dom0['total_storage_gb']        = _total_storage[_dom0['compute_node_hostname']]
                        _dom0['available_storage_gb']    = _available_storage[_dom0['compute_node_hostname']]
                    else:
                        ebLogError('Storage info not found for dom0 %s, xml not updated with node subset info' % (_dom0['compute_node_hostname']))
                        raise ExacloudRuntimeError(0x0761, 0xA, "Failed to compute storage in /EXAVMIMAGES partition in dom0s")
            else:
                raise ExacloudRuntimeError(0x0765, 0xA, "Missing nodesubset json payload ")

            _reqobj = self.mGetRequestObj()
            if _reqobj is not None:
                _reqobj.mSetData(json.dumps(nsParams, sort_keys=True))
                _db = ebGetDefaultDB()
                if self.__debug:
                   ebLogInfo("%s: mHandlerGetEXAVMStorage: Make DB UpdateRequest." % (datetime.datetime.now()))
                _db.mUpdateRequest(_reqobj)
            else:
                print(json.dumps(nsParams, sort_keys=True))

            if self.__debug:
               ebLogInfo("%s: mHandlerGetEXAVMStorage: Done with request." % (datetime.datetime.now()))

        else:
            if self.__debug:
               ebLogInfo("%s: mHandlerGetEXAVMStorage: Done with request, use min of Storage values." % (datetime.datetime.now()))
            return min(_total_storage.values()), min(_available_storage.values())



    def mCheckDisablePinningComputeImage(self):
        # Connect to first dom0 and verify if vnuma is active
        _novnuma = True

         # For now, disable vnuma support for KVM
        if self.mIsKVM():
            ebLogDebug('*** Disable vnuma support for KVM temporarily')
            return
        else:
            for _dom0, _ in self.mReturnDom0DomUPair():
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_dom0)

                _cmdstr = 'grep vnuma /etc/xen/xend-config.sxp'
                _i, _o, _e = _node.mExecuteCmd(_cmdstr)
                if _o:
                    _out = _o.readlines()
                    if not _out:
                        #no vnuma line, go to next dom0
                        _node.mDisconnect()
                        continue
                    if '0' not in _out[0]:
                        #Dom0 with a vnuma line, but no 0
                        _novnuma = False
                        _node.mDisconnect()
                        break

                _node.mDisconnect()
        if _novnuma:
            ebLogInfo('*** All Computes node have vnuma disabled, keep pinning configuration')
        else:
            ebLogInfo('*** Compute node with vnuma enabled detected, force disable pinning')
            self.__disable_vcpus_pinning = True


    def mCheck19cCapableComputeImage(self):

        _minversion = '182000'
        _dom0s = list(map(operator.itemgetter(0),self.mReturnDom0DomUPair()))
        _rc = self.mCheckMinSystemImage(_dom0s,_minversion)
        if _rc:
            ebLogInfo('*** All Computes have a System Image above version {}: (19c and above supported)'.format(_minversion))
            self.__over18cSupported = True
        else:
            ebLogInfo('*** All Computes have a System Image below version {}: (19c and above not supported)'.format(_minversion))
            self.__over18cSupported = False
        return _rc


    def mCheck23cCapableComputeImage(self):

        _minversion = '221940'
        _dom0s = list(map(operator.itemgetter(0),self.mReturnDom0DomUPair()))
        _rc = self.mCheckMinSystemImage(_dom0s,_minversion)
        if _rc:
            ebLogInfo('*** All Computes have a System Image above version {}: (23c and above supported)'.format(_minversion))
            self.__over23cSupported = True
        else:
            ebLogInfo('*** All Computes have a System Image below version {}: (23c and above not supported)'.format(_minversion))
            self.__over23cSupported = False
        return _rc

    def mCheckRouterAndDnsAfterVMInstall(self):
        _domUs = [_domu for _, _domu in self.mReturnDom0DomUPair()]
        _rc = 0
        for _host in _domUs:
            with connect_to_host(_host, get_gcontext()) as _node:
                try:
                    _flush_op = node_exec_cmd(_node, "/usr/sbin/ip route show default| /bin/grep bondeth0")
                    if _flush_op.exit_code:
                        _rc = _flush_op.exit_code
                        ebLogError("*** Ip route validation failed for node %s"%(_host))
                    _cmd = "/usr/bin/nslookup %s"%(_host)
                    _flush_op = node_exec_cmd(_node, _cmd)
                    if _flush_op.exit_code:
                        _rc = _flush_op.exit_code
                        ebLogError("*** DNS validation failed for node %s"%(_host))
                except Exception as e:
                    ebLogError(f"*** Exception Message Detail on host {_host} {e}")
        return _rc

    def mCheckCellsSystemImage(self):

        if not self.mCheckConfigOption('force_check_sysimage','True'):
            ebLogInfo('*** QUORUM CELL IMAGE VERSION CHECK IS BEING SKIPPED ON EXABM')
            return True

        _minversion = '121212'
        _rc = self.mCheckMinSystemImage(list(self.mReturnCellNodes().keys()),_minversion)
        if _rc:
            ebLogInfo('*** All Cells have a System Image above version {}: (quorum supported)'.format(_minversion))
        else:
            ebLogInfo('*** Cells have a System Image below version {}: (quorum not suppored)'.format(_minversion))
        return _rc


    def mCheckMinSystemImage(self, aHostList, aMinVersion):

        _rc = True
        for _host in aHostList:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_host)

            _cmdstr = 'imageinfo -version'
            _i, _o, _e = _node.mExecuteCmd(_cmdstr)

            if _node.mGetCmdExitStatus() != 0:
                _node.mDisconnect()
                _msg = "Could not verify system image version {0} in host:{1}\n{2}".format(aMinVersion,_host,"\n".join(_e.readlines()))
                ebLogError("*** {0} ***".format(_msg))
                raise ExacloudRuntimeError(0x0760, 0xA, _msg)

            _imgrev = _o.readlines()[0].strip()
            _srev   = ''.join(_imgrev.split('.')[:5])

            if _srev > aMinVersion:
                ebLogInfo('*** System Image: %s/%s found in Host: %s' % (_imgrev, _srev, _host))
            else:
                ebLogInfo('*** System Image: %s/%s found in Host: %s' % (_imgrev, _srev, _host))
                _rc = False

            _node.mDisconnect()

        return _rc

    def mResetDom0NetworkMappingPartiallyCabled(self):

        _dpairs = self.mReturnDom0DomUPair()
        for _dom0, _ in _dpairs:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            #
            # Check Network Configuration - if not matching expected mapping then reset network configuration
            #
            _cmd_str = "grep MASTER /etc/sysconfig/network-scripts/ifcfg-eth?"
            _i, _o, _e = _node.mExecuteCmd(_cmd_str)
            _out = _o.readlines()
            _mappind_d = {}
            for _e in _out:
                try:
                    _eth = _e.strip().split(':')[0][-4:]
                    _bond = _e.strip().split('=')[1]
                    _mappind_d[_eth] = _bond
                except:
                    pass
            #
            # Check if any bonding interfaces were found
            #
            if not len(_out):
                _node.mDisconnect()
                continue
            #
            # Check bonding interfaces
            #
            _count = 0
            if 'eth4' in list(_mappind_d.keys()) and _mappind_d['eth4'] == 'bondeth0':
                _count += 1
            if 'eth5' in list(_mappind_d.keys()) and _mappind_d['eth5'] == 'bondeth0':
                _count += 1
            if _count != 2:
                ebLogWarn('*** PCB *** Invalid Network Mapping detected ***')
                for _k in list(_mappind_d.keys()):
                    ebLogWarn('*** PCB *** %s -> %s' % (_k, _mappind_d[_k]))
            else:
                ebLogInfo('*** Valid Network Mapping detected ***')
                _node.mDisconnect()
                continue
            #
            # Enforce Network Mapping expected in Dom0
            #
            _cmd_str = "sed 's/^MASTER=bondeth1/MASTER=bondeth0/' -i /etc/sysconfig/network-scripts/ifcfg-eth4"
            _node.mExecuteCmd(_cmd_str)
            _cmd_str = "sed 's/^MASTER=bondeth1/MASTER=bondeth0/' -i /etc/sysconfig/network-scripts/ifcfg-eth5"
            _node.mExecuteCmd(_cmd_str)
            _cmd_str = "rm /etc/exadata/ovm/bridge.conf.d/bridge.vmbondeth*"
            _node.mExecuteCmd(_cmd_str)

            self.mRebootNode(_dom0)

            _node.mDisconnect()

    def mInstallSuricataRPM(self,_hostlist, _hosttype):
        def _mInstallSuricataRPM(_host, _host_type):
            try:
                _suricata_rpm_tar_path=os.path.join(get_gcontext().mGetBasePath(),"../suricata/suricata_installer.tgz")
                ebLogInfo(f"Suricata Installer Tar path: {_suricata_rpm_tar_path}")

                if not os.path.exists(_suricata_rpm_tar_path):
                    _err = f"Suricata RPM Installer File Not Found at {_suricata_rpm_tar_path}"
                    self.mUpdateErrorObject(gProvError['ERROR_RPM_NOT_FOUND'],_err)
                    ebLogError('*** ' + _err)
                    raise ExacloudRuntimeError(0x0130,0x0A,_err)

                with connect_to_host(_host, get_gcontext(), "root") as _node:
                    ebLogTrace(f"connected node detail : {_host}")
                    _remote_rpm_tar_path = "/tmp/rpm"
                    _rm_cmd = node_cmd_abs_path_check(_node, "rm")

                    #delete the installation directory if already existing
                    if _node.mFileExists(_remote_rpm_tar_path):
                        ebLogInfo("RPM Directory exists! Deleting the Directory..")
                        _cmd = f"{_rm_cmd} -rf {_remote_rpm_tar_path}" 
                        _node.mExecuteCmdLog(_cmd)
                        #unsuccessful removal of existing Directory
                        if _node.mGetCmdExitStatus() != 0:
                            raise Exception("Unsuccessful removal of existing Directory")
                    #create directory for installation files
                    _node.mMakeDir(_remote_rpm_tar_path)

                    if _node.mFileExists(_remote_rpm_tar_path):
                        ebLogInfo(f"Directory created successfully at {_remote_rpm_tar_path}")
                    else:
                        raise Exception("Unsuccessful Directory creation at remote path")

                    _remote_rpm_tar_location = os.path.join(_remote_rpm_tar_path, os.path.basename(_suricata_rpm_tar_path))
                    #copy suricata tar file to remote location
                    _node.mCopyFile(_suricata_rpm_tar_path, _remote_rpm_tar_path)

                    if _node.mFileExists(_remote_rpm_tar_location):
                        ebLogInfo(f"Installer tar copied successfully at {_remote_rpm_tar_location}")
                    else:
                        raise Exception("Unsuccessful copy of installer tar file at remote path")

                    #untar 
                    _tar_cmd = node_cmd_abs_path_check(_node, "tar")
                    _cmd = f"{_tar_cmd} -xzf {_remote_rpm_tar_location} -C {_remote_rpm_tar_path}/." 
                    _node.mExecuteCmdLog(_cmd)
                    #unsuccessful untar of the rpm folder
                    if _node.mGetCmdExitStatus() != 0:
                        raise Exception("Unsuccessful untar of the RPM at remote path")

                    #find if install.py is present
                    _installer_file_name=f"{_remote_rpm_tar_path}/install.py"
                    if _node.mFileExists(_installer_file_name):
                        ebLogInfo(f"Installer file found at {_installer_file_name}")
                    else:
                        raise Exception(f"Installer file not present at {_installer_file_name}")

                    #run install.py
                    _python_cmd=node_cmd_abs_path_check(_node, "python3")
                    if not _python_cmd:
                        _python_cmd = node_cmd_abs_path_check(_node, "python")
                    _install_type = _host_type
                    _install_action = "Install"
                    _cmd = f"{_python_cmd} {_installer_file_name} --type {_install_type} --action {_install_action}"
                    _node.mExecuteCmdLog(_cmd)
                    #unsuccessful installation
                    if _node.mGetCmdExitStatus() != 0:
                        _remote_log_file_name = f"{_remote_rpm_tar_path}/suricata-installer.log"
                        _suricata_log_name = "suricata-installer.log"
                        _suricata_log_timestamp = str(time.time()).replace(".", "")
                        _suricata_log_location = _remote_rpm_tar_path
                        _node_name = str(_host).split(".")[0]
                        _remote_log_file = f"{_suricata_log_timestamp}_{_node_name}_{_suricata_log_name}"
                        _remote_log_file_path = f"{_suricata_log_location}/{_remote_log_file}"
                        _mv_cmd=node_cmd_abs_path_check(_node, "mv")
                        _cmd = f"{_mv_cmd} {_remote_log_file_name} {_remote_log_file_path}"
                        _node.mExecuteCmdLog(_cmd)
                        _local_log_file_path=os.path.join(gLogMgrDirectory, _remote_log_file)
                        if _node.mFileExists(_remote_log_file_path):
                            ebLogInfo(f"suricata-installer.log present at remote location: {_remote_log_file_path} \n Initiating copy to local.")
                            _node.mCopy2Local(_remote_log_file_path,_local_log_file_path)
                            ebLogInfo(f"local log file path: {os.path.abspath(_local_log_file_path)}")
                        else:
                            raise Exception(f"Suricata Installer log not found at remote location: {_remote_log_file_path}")
                        raise Exception("Unsuccessful RPM installation.")

                    #delete tar file once installation is done
                    _cmd = f"{_rm_cmd} -rf {_remote_rpm_tar_path}"
                    _node.mExecuteCmdLog(_cmd)
                    #unsuccessful delete
                    if _node.mGetCmdExitStatus() != 0:
                        ebLogError("Could not delete the RPM tar directory after installation!")
   

            except Exception as exep:
                ebLogError(f"*** Exception Message Detail on host {_host} {exep}")
                raise Exception(f"Suricata Installation Failed with Exception: {exep} on host {_host}")

        _host_list = _hostlist
        _host_type = _hosttype
        ebLogInfo(f"Proceeding with Suricata rpm installation on the {_host_type}s : {_hostlist}")
        _plist = ProcessManager()
        for _host in _host_list:
            _p = ProcessStructure(_mInstallSuricataRPM, [_host,_host_type])
            _p.mSetMaxExecutionTime(30*60) #30 minutes timeout
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()

    def mDisableDom0IBRdsModule(self, aMode=True):
        _default_conf  = '/etc/ofed/openib.conf'
        _failback_conf = '/etc/rdma/rdma.conf'
        _rds_grep_cmd  = 'grep -n RDS_LOAD {0}'
        _reboot_list   = []

        _dpairs = self.mReturnDom0DomUPair()
        for _dom0, _ in _dpairs:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            #
            # Check IB RDS_LOAD
            #
            _active_conf = _default_conf
            _rds_line = 0
            _i, _o, _e = _node.mExecuteCmd(_rds_grep_cmd.format(_active_conf))
            _out = _o.readlines()
            if not len(_out):
                #switch to failback conf if no output
                ebLogDebug('openib.conf check failed, trying rdma.conf')
                _active_conf = _failback_conf
                _i, _o, _e = _node.mExecuteCmd(_rds_grep_cmd.format(_active_conf))
                _out = _o.readlines()
            for _l in _out:
                _lnum,_str = _l.split(':')
                if 'yes' in _l.lower():
                    _rds_line = _lnum
                    ebLogInfo('Infiniband RDS module is enabled on dom0 ({0}) by {1}'.format(_dom0,_active_conf))
                    break
            #
            # If RDS enabled, disable it
            #
            if _rds_line:
                ebLogWarn('Disabling RDS module, dom0 ({0}) will reboot'.format(_dom0))
                _cmd_str = "sed '{0}s/yes/no/gI' -i {1}".format(_rds_line, _active_conf)
                if self.__debug:
                    ebLogDebug(_cmd_str)
                if aMode:
                    _node.mExecuteCmd(_cmd_str)
                    _reboot_list.append(_dom0)
            else:
                ebLogInfo('RDS module is already disabled on {0}'.format(_dom0))

            _node.mDisconnect()


        if _reboot_list:

            #Get the process list
            _plist = ProcessManager()
            for _dom0 in _reboot_list:
                ebLogInfo("Rebooting dom0 ({0}).".format(_dom0))
                _p = ProcessStructure(self.mRebootNode, [_dom0], _dom0)
                _p.mSetMaxExecutionTime(30*60) # 30 minutes timeout
                _p.mSetJoinTimeout(10)
                _p.mSetLogTimeoutFx(ebLogWarn)
                _plist.mStartAppend(_p)
            _plist.mJoinProcess()

    def mDom0PostVMCreateNetConfig(self,aMode=True):
        """
        DEPRECATED: Use instead clunetworkvalidations.mUpdateDom0EthernetSpeeds()
            and clunetworkvalidations.mCheckDom0EthernetSpeed()

        aMode: True - Executed in PreVM Setup
        aMode: False - Executed in Post VM Install Setup
        """
        _exadata_model = self.mGetExadataDom0Model()
        _exadata_model_gt_X7 = False
        _compare_exadata = self.mCompareExadataModel(_exadata_model, 'X7')
        if _compare_exadata >= 0:
            _exadata_model_gt_X7 = True
        if _exadata_model_gt_X7 and self.__exabm:
            #TODO: FOR BUG 29236323 GENERIC FLAG?
            if self.mCheckConfigOption('fix_x7_dom0_bond','True'):
                ebLogWarn('*** {0} OCI environment detected and {0} dom0 bond fix enabled'.format(_exadata_model))
                self.mDom0X7BondFix(aMode)

        if _exadata_model in [ 'X5', 'X6']:
            ebLogInfo(f"skipping speed check for ethernet interfaces on {_exadata_model} env.")
            return

        # Pre VM creation, check speed of ethernet interfaces
        if aMode:
            for _dom0, _ in self.mReturnDom0DomUPair():
                _node = exaBoxNode(get_gcontext())
                try:
                    _node.mConnect(_dom0)
                    _map = self.mGenBondMap(_node, True)

                    for _ethx, _bondx in _map.items():

                        self.__ethConfig.mValidateInterface(_node, _ethx)

                        _exadata_model = self.mGetExadataDom0Model(_dom0)
                        if self.mCompareExadataModel(_exadata_model, 'X9') >= 0:
                            if self.mCompareExadataModel(_exadata_model, 'X10') >= 0:
                                _base_speed = 100000
                            else:
                                _base_speed = 50000

                            _current_speed = int(node_read_text_file(_node, f"/sys/class/net/{_ethx}/speed").strip())
                            ebLogInfo(f"{_dom0}: {_ethx} link speed is {_current_speed}")
 
                            if _base_speed != _current_speed:
                                ebLogInfo(f"{_dom0}: Updating link speed to {_base_speed} in interface {_ethx} ")
                                _node.mExecuteCmdLog(f"/usr/sbin/ethtool -s {_ethx} speed {_base_speed} autoneg on")

                                _current_speed = int(node_read_text_file(_node, f"/sys/class/net/{_ethx}/speed").strip())
                                ebLogInfo(f"{_dom0}: {_ethx} link speed is {_current_speed}")

                                _count = 0
                                while _base_speed != _current_speed and _count < 5:
                                    _count = _count + 1
                                    time.sleep(30)

                                    _current_speed = int(node_read_text_file(_node, f"/sys/class/net/{_ethx}/speed").strip())
                                    ebLogInfo(f"{_dom0}: updated link speed to {_current_speed} in interface {_ethx} ")

                                if _base_speed != _current_speed:
                                    # configure custom speeds when the default speed fails
                                    self.__ethConfig.mUpdateCustomEthernetSpeed(_node, _dom0, _ethx, _current_speed, _exadata_model)
                        else:
                            _cmd = f"/bin/cat /etc/sysconfig/network-scripts/ifcfg-{_ethx} | /bin/grep ETHTOOL_OPTS |  /bin/cut -d '=' -f 2"
                            _, _o, _ = _node.mExecuteCmd(_cmd)
                            _out = _o.readlines()
                            _base_speed = 25000
                            try:
                                if _out:
                                    _base_speed = int(_out[0].strip().split()[1])
                                    ebLogInfo(f"{_dom0}: speed in ifcfg-{_ethx} is configured to {_base_speed}")
                            except:
                                _base_speed = 25000
                                ebLogInfo(f"{_dom0}: unable to fetch link speed from ifcfg-{_ethx} defaulting the link speed to {_base_speed}")

                            _current_speed = int(node_read_text_file(_node, f"/sys/class/net/{_ethx}/speed").strip())
                            ebLogInfo(f"{_dom0}: {_ethx} link speed is {_current_speed}")

                            if _base_speed != _current_speed:
                                _node.mExecuteCmd(f"/usr/sbin/ethtool -s {_ethx} speed {_base_speed} autoneg off")

                                _count = 0
                                while _base_speed != _current_speed and _count < 5:
                                    _count = _count + 1
                                    time.sleep(30)

                                    _current_speed = int(node_read_text_file(_node, f"/sys/class/net/{_ethx}/speed").strip())
                                    ebLogInfo(f"{_dom0}: {_ethx} updated link speed is {_current_speed}")

                                if _base_speed != _current_speed:
                                    _node.mExecuteCmd(f"ifdown {_ethx}; ifup {_ethx}")
                                    time.sleep(30)

                                    _current_speed = int(node_read_text_file(_node, f"/sys/class/net/{_ethx}/speed").strip())
                                    ebLogInfo(f"{_dom0}: {_ethx} updated link speed is {_current_speed}")

                                    if _base_speed != _current_speed:
                                        # configure custom speeds if ETHTOOL_OPTS is not configured in /etc/sysconfig/network-scripts/ifcfg-{_ethx}
                                        self.__ethConfig.mUpdateCustomEthernetSpeed(_node, _dom0, _ethx, _current_speed, _exadata_model)

                        _ethx_link_detect = node_read_text_file(_node, f"/sys/class/net/{_ethx}/carrier")
                        _issue_soft_warning = self.mIssueSoftWarningOnLinkfailure(_dom0, _ethx)
                        if int(_ethx_link_detect) == 1:
                            ebLogInfo(f"{_dom0}: {_ethx} link detected: yes")
                        else:
                            _err = f"Ethernet interfaces {_ethx} link detected: no"
                            if _issue_soft_warning:
                                ebLogWarn(f"{_dom0}: {_err}")
                            else:
                                ebLogError(f"{_dom0}: {_err}")
                                raise ExacloudRuntimeError(0x0126, 0x0A, _err)
                finally:
                    _node.mDisconnect()


    def mIssueSoftWarningOnLinkfailure(
        self,
        aDom0: str,
        aEthLink: str) -> bool:
        """Case 1: In the event of bonding being supported in aDom0,
        the function will return False indicating to raise a ExacloudRuntimeError when any of the links are down.

        Case 2: In the event that bonding is not supported in aDom0, decision will be made
        whether to issue a soft warning or a ExcaloudRuntimeError depending on the dom0 number and the corresponding link number.

        For example: if the dom0 nodes ends with a even number and the eth link ends with a even number, then function will return False as this
        link will be the primary link for this node and it should be up. Similarly for odd numbered dom0s and odd numbered eth links.

        In the case that the dom0 ends with an odd number and the passed eth link is even(or vice versa), the function will return True indicating to raise a warning
        instead of an ExacloudRuntimeError.

        If function is unable to determine the odd/even number of either the dom0/eth link, False will be returned so that as a pre check any issue can
        be addressed.
        """

        def _extract_last_number(_local_string: str) -> int:
            _local_string = _local_string.split(".")[0]
            _last_character = _local_string[-1]
            if _last_character.isdigit():
                return int(_last_character)
            else:
                return 0

        if not clubonding.is_bonding_supported_dom0(
            self, self.__options.jsonconf, aDom0):
            if _extract_last_number(aDom0) % 2 == 0 and _extract_last_number(aEthLink) % 2 == 0:
                ebLogInfo(f"{aDom0} does not support bonding, "
                    f"skipping check of {aEthLink}")
                return False
            elif _extract_last_number(aDom0) % 2 != 0 and _extract_last_number(aEthLink) % 2 != 0:
                ebLogInfo(f"{aDom0} does not support bonding, "
                    f"skipping check of {aEthLink}")
                return False

            # Dom0 number and Interface parity different
            else:
                return True

        # If bonding is supported always return False
        return False



    def mDom0X7BondFix(self,aMode=True):

        _dpairs = self.mReturnDom0DomUPair()
        _idx = 0
        for _dom0, _ in _dpairs:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            if aMode:
                _cmd_str = "ifup eth1 ; ifup eth2"
            else:
                if _idx % 2:
                    _cmd_str = "ifdown eth1 ; ifup eth2"
                else:
                    _cmd_str = "ifup eth1 ; ifdown eth2"
            _idx = _idx + 1
            _i, _o, _e = _node.mExecuteCmd(_cmd_str)
            time.sleep(10)
            _port = 1
            _cmdstr = 'ethtool eth%d | grep "Link detected"' % (_port)
            _,_o,_e = _node.mExecuteCmd(_cmdstr)
            _out = _o.readlines()
            if _out:
               ebLogVerbose('*** Dom0 (%s) eth%d - %s' % (_dom0,_port,_out[0].strip()))
            _port = 2
            _cmdstr = 'ethtool eth%d | grep "Link detected"' % (_port)
            _,_o,_e = _node.mExecuteCmd(_cmdstr)
            _out = _o.readlines()
            if _out:
               ebLogVerbose('*** Dom0 (%s): eth%d - %s' % (_dom0,_port,_out[0].strip()))
            _node.mDisconnect()

    def mHandlerResetDom0NetworkMapping(self):
        ebLogInfo('*** Reset Network Mapping')
        return self.mResetDom0NetworkMapping()

    def mResetDom0NetworkMapping(self):    
        _exadata_model = self.mGetExadataDom0Model()
        #TODO: FOR BUG 29236323 GENERIC FLAG?
        _exadata_model_gt_X7 = False
        _compare_exadata = self.mCompareExadataModel(_exadata_model, 'X7')
        if _compare_exadata >= 0:
            _exadata_model_gt_X7 = True

        if _exadata_model_gt_X7 and self.__exabm:
            if self.mCheckConfigOption('fix_x7_dom0_bond','True'):
                ebLogWarn('*** {0} OCI environment detected and {0} dom0 bond fix enabled'.format(_exadata_model))
                self.mDom0X7BondFix(aMode=False)
            else:
                ebLogWarn('*** {0} OCI Dom0 reset network mapping not supported'.format(_exadata_model))
            return

        ebLogTrace("mResetDom0NetworkMapping: Reset dom0 network mapping.")
        _cluster_network_type = self.mCheckClusterNetworkType()
        if _cluster_network_type is False:
            return self.mResetDom0NetworkMappingPartiallyCabled()

        _dpairs = self.mReturnDom0DomUPair()
        for _dom0, _ in _dpairs:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            #
            # Check Network Configuration - if not matching expected mapping then reset network configuration
            #
            _cmd_str = "grep MASTER /etc/sysconfig/network-scripts/ifcfg-eth?"
            _i, _o, _e = _node.mExecuteCmd(_cmd_str)
            _out = _o.readlines()
            _mappind_d = {}
            for _e in _out:
                try:
                    _eth = _e.strip().split(':')[0][-4:]
                    _bond = _e.strip().split('=')[1]
                    _mappind_d[_eth] = _bond
                except:
                    pass
            #
            # Check if any bonding interfaces were found
            #
            if not len(_out):
                continue
            #
            # Check bonding interfaces
            #
            _count = 0
            if (self.__exabm is True) or self.__fiber_backup:
                if 'eth6' in list(_mappind_d.keys()) and _mappind_d['eth6'] == 'bondeth1':
                    _count += 1
                if 'eth7' in list(_mappind_d.keys()) and _mappind_d['eth7'] == 'bondeth1':
                    _count += 1
            else:
                if 'eth2' in list(_mappind_d.keys()) and _mappind_d['eth2'] == 'bondeth1':
                    _count += 1
                if 'eth3' in list(_mappind_d.keys()) and _mappind_d['eth3'] == 'bondeth1':
                    _count += 1

            if self.__copper_client:
                if 'eth6' in list(_mappind_d.keys()) and _mappind_d['eth6'] == 'bondeth0':
                    _count += 1
                if 'eth7' in list(_mappind_d.keys()) and _mappind_d['eth7'] == 'bondeth0':
                    _count += 1
            else:
                if 'eth4' in list(_mappind_d.keys()) and _mappind_d['eth4'] == 'bondeth0':
                    _count += 1
                if 'eth5' in list(_mappind_d.keys()) and _mappind_d['eth5'] == 'bondeth0':
                    _count += 1

            if _count != 4:
                ebLogWarn('*** REG MAP *** Invalid Network Mapping detected ***')
                for _k in list(_mappind_d.keys()):
                    ebLogWarn('*** REG MAP *** %s -> %s' % (_k, _mappind_d[_k]))
            else:
                ebLogInfo('*** Valid Network Mapping detected ***')
                _node.mDisconnect()
                continue
            #
            # Enforce Network Mapping expected in Dom0
            #
            if self.__exabm or self.__fiber_backup:
                _cmd_str = "sed 's/^MASTER=bondeth0/MASTER=bondeth1/' -i /etc/sysconfig/network-scripts/ifcfg-eth6"
                _node.mExecuteCmd(_cmd_str)
                _cmd_str = "sed 's/^MASTER=bondeth0/MASTER=bondeth1/' -i /etc/sysconfig/network-scripts/ifcfg-eth7"
                _node.mExecuteCmd(_cmd_str)
            else:
                _cmd_str = "sed 's/^MASTER=bondeth0/MASTER=bondeth1/' -i /etc/sysconfig/network-scripts/ifcfg-eth2"
                _node.mExecuteCmd(_cmd_str)
                _cmd_str = "sed 's/^MASTER=bondeth0/MASTER=bondeth1/' -i /etc/sysconfig/network-scripts/ifcfg-eth3"
                _node.mExecuteCmd(_cmd_str)

            if self.__copper_client:
                _cmd_str = "sed 's/^MASTER=bondeth1/MASTER=bondeth0/' -i /etc/sysconfig/network-scripts/ifcfg-eth6"
                _node.mExecuteCmd(_cmd_str)
                _cmd_str = "sed 's/^MASTER=bondeth1/MASTER=bondeth0/' -i /etc/sysconfig/network-scripts/ifcfg-eth7"
                _node.mExecuteCmd(_cmd_str)
            else:
                _cmd_str = "sed 's/^MASTER=bondeth1/MASTER=bondeth0/' -i /etc/sysconfig/network-scripts/ifcfg-eth4"
                _node.mExecuteCmd(_cmd_str)
                _cmd_str = "sed 's/^MASTER=bondeth1/MASTER=bondeth0/' -i /etc/sysconfig/network-scripts/ifcfg-eth5"
                _node.mExecuteCmd(_cmd_str)

            _cmd_str = "rm /etc/exadata/ovm/bridge.conf.d/bridge.vmbondeth*"
            _node.mExecuteCmd(_cmd_str)

            self.mRebootNode(_dom0)

            _node.mDisconnect()

    def mHandlerResetDom0ToFreshImage(self):

        ebLogVerbose("mResetDom0ToFreshImage: Reset dom0 to fresh image.")
        _dpairs = self.mReturnDom0DomUPair()
        for _dom0, _ in _dpairs:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            _cmd_str = "/opt/exadata_ovm/exadata.img.domu_maker remove-domain -all -reset-reboot-dom0"
            _node.mExecuteCmdLog(_cmd_str)
            _node.mDisconnect()
            self.mRebootNode(_dom0,aWait=True, aForce=True)
            #
            # Additional Cleanup on Dom0 required until done by domu_maker
            #
            _eth_cfg_template = """#### DO NOT REMOVE THESE LINES ####
#### GENERATED BY EXACLOUD ####
DEVICE=eth%d
BOOTPROTO=none
ONBOOT=no
HOTPLUG=no
IPV6INIT=no"""
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            _cmd_str = "rm -f /etc/sysconfig/network-scripts/ifcfg-*bondeth* /etc/sysconfig/network-scripts/ifcfg-*eth1* /etc/exadata/ovm/bridge.conf.d/bridge.*"
            _node.mExecuteCmd(_cmd_str)
            for _ethid in range(1,6):
                _config = _eth_cfg_template % (_ethid)
                _cmd_str = 'echo "%s" > /etc/sysconfig/network-scripts/ifcfg-eth%d' % (_config,_ethid)
                _node.mExecuteCmd(_cmd_str)
            _node.mDisconnect()
            self.mRebootNode(_dom0)

    def mHandlerGetClusterDetailsExacc(self):
        ebLogInfo("mHandlerGetClusterDetailsExacc: get cluster details.")
        _dpairs = self.mReturnDom0DomUPair()

        _dom0 = _dpairs[0][0]
        _domU = _dpairs[0][1]
        _hv = getHVInstance(_dom0)
        _domains = _hv.mGetDomains()
        _memory_in_gb = 0
        _currvmem = 0
        _cpu = 0
        _currcpu = 0
        cluster_detail = {}
        domu_info = []

        for _dom0, _ in self.mReturnDom0DomUPair():
            ebLogInfo("getting details from dom0 %s" %_dom0)
            _hv = getHVInstance(_dom0)
            _domains = _hv.mGetDomains()
            # get current VM memory from Dom0
            for _domU in _domains:
                _currvmem = _hv.mGetVMMemory(_domU, 'CUR_MEM')
                _currcpu = _hv.mGetVMCpu(_domU, 'CUR_CPU')
                _vmstatus = _hv.mGetVmStatus(_domU)

                if not _currvmem:
                    _currvmem = _hv.mGetVMMemoryFromConfig(_domU)
                if _currvmem:
                    _memory_in_gb = int(_currvmem) // 1024
                else:
                    _memory_in_gb = 0

                if not _currcpu:
                    _currcpu= _hv.mGetVMCPUFromConfig(_domU)
                if _currcpu:
                    _cpu = int(_currcpu)
                else:
                    _cpu = 0
                if not _vmstatus:
                    _vmstatus = ""
                domu_info.append({"domu": _domU, "cpu" : _cpu, "memory_in_gb": _memory_in_gb, "dom0": _dom0, "status": _vmstatus})

        cluster_detail["domu_info"] = domu_info
        ebLogInfo("cluster detail json obtained : %s: " %(json.dumps(cluster_detail, indent=4)))
        _reqobj = self.mGetRequestObj()
        if _reqobj is not None:
            _reqobj.mSetData(json.dumps(cluster_detail, sort_keys=True))
            _db = ebGetDefaultDB()
            if self.__debug:
                ebLogInfo("%s: mHandlerGetClusterDetailsExacc: Make DB UpdateRequest." % (datetime.datetime.now()))
            _db.mUpdateRequest(_reqobj)
        else:
            print(json.dumps(cluster_detail, sort_keys=True))

    def mHandlerGetClusterDetailsforConsistency(self):
        _pchecks = ebCluPreChecks(self)
        _pchecks.mGetCoreAndMemInfo()
        return 0
        
    def mHandlerSetupNATIptablesOnDom0(self):
        ebLogInfo("mHandlerSetupNATIptablesOnDom0: Reset dom0 nat iptables .")

        _nftDom0s = self.mGetHostsByTypeAndOLVersion(ExaKmsHostType.DOM0, ["OL8"])
        _iptDom0s = self.mGetHostsByTypeAndOLVersion(ExaKmsHostType.DOM0, ["OL7", "OL6"])

        if _nftDom0s:
            self.mSetupNatNfTablesOnDom0v2(aDom0s=_nftDom0s)

        if _iptDom0s:
            self.mSetupNATIptablesOnDom0v2(aDom0s=_iptDom0s)

        return 0


    def mSetupNatNfTablesOnDom0v2(self, aMode=False, aDom0s=None):

        def mConfigureRulesDom0(aDom0):

            _dom0 = aDom0

            if self.mCheckConfigOption('iptables_nat_rules', 'False'):
                return

            if not self.mHasNatAndCustomerNet() or (self.__ociexacc and self.isATP()):
                ebLogWarn('*** NAT NFtables setup only apply to EXABM or OCICC (non-ATP) environment')
                return

            ebLogTrace(f"mSetupNATfttablesOnDom0v2: Setup NAT NFT tables on Dom0 {_dom0}")

            if self.mCheckConfigOption('vm_clusters_limit') is not None:
                _vm_clusters_limit = int(self.mCheckConfigOption('vm_clusters_limit'))
            else:
                _vm_clusters_limit = 24

            # ExaDB-D flow
            if self.__exabm or not self.__shared_env:
                _range = _vm_clusters_limit
                _vmeth_range = [100] + [x for x in range(200, 200 + _range)]
            else:
                _range = 16
                # Optional configuration Parameter, takes int from 1 to XX
                _maxVMOption = self.mCheckConfigOption('iptables_nat_rules_range')
                _exadata_model = self.mGetExadataDom0Model()
                if not _maxVMOption and (self.mCompareExadataModel(_exadata_model, 'X9') >= 0 or self.mCompareExadataModel(_exadata_model, 'X10') >= 0):
                    _range = 24
                    ebLogInfo(f"*** for Model {_exadata_model}, the default nattables_nat_rules_ranges value is {_range}")
                if _maxVMOption:
                    if _maxVMOption.isdigit() and int(_maxVMOption) > 1:
                        _range = int(_maxVMOption)
                    else:
                        ebLogWarn(
                            "nftables_nat_rules_ranges takes an integer >= 1, fallback to default value: {}".format(_range))
                # IB is limited to 8 VM, precreate vmeth100 to vmeth107 rules.
                # In the future, to be done dynamically at VM creation AND using the vmethXXX bridge name in configuration
                _vmeth_range = list(range(100, 100 + _range)) + list(range(200, 200 + _range))

            _ip_rules_list = []
            _ip_drop_rules_list = []
            _ip_nat_rules_list = []
            _ip_default_rules = []
            nftObj = NfTables()

            # Open JSON file with NFT table rules info
            fname = 'properties/nftrules.json'
            try:
                f = open(fname, 'r')
                iprules = json.load(f)
            except Exception as e:
                _msg = f"Failed to load nft tables rules from file {fname}: {e}"
                ebLogError(_msg)
                raise ExacloudRuntimeError(0x0750, 0xA, _msg) from e
            finally:
                f.close()

            _master = self.mGetCurrentMasterInterface(aDom0=_dom0)
            _replaceAdminInterface = {"master": _master}

            # This list will delete conflicting NAT rules with same PORT
            _ip_nat_ports_list = set()
            for i in _vmeth_range:

                _replaceValues = {"vmeth_num": i}

                _ip_rules_list += nftObj.mReplaceValuesJsonRules(iprules, 'ip_rules_list', _replaceValues)

                # Adding rules to enable communication with agent BUG:30833376
                if self.isATP() and not self.__ociexacc:
                    _ip_rules_list += nftObj.mReplaceValuesJsonRules(iprules, 'ip_rules_list_atp_not_ociexacc', _replaceValues)

                # Adding rule to enable communication with dcs agent incase of Exacc BUG:34139276
                if self.__ociexacc:
                    _ip_rules_list += nftObj.mReplaceValuesJsonRules(iprules, 'ip_rules_list_ociexacc', _replaceValues)

                # Adding rules to allow communication between ecra vm and agent
                if self.mCheckConfigOption("iptables_ecra_vm_agent", "True"):
                    _ip_rules_list += nftObj.mReplaceValuesJsonRules(iprules, 'ip_rules_list_ecra_vm_agent', _replaceValues)

                # OCIEXACC NAT Fileserver, only accept forward one a single ip and port
                _nat_info = self.mGetOciExaCCServicesSetup()
                # Dictionnary may have multiple natted services (fileserver/fwdproxy/etc)
                if _nat_info:
                    for _nat_service,_nat_info in _nat_info.items():
                        ebLogInfo(f'Oci-ExaCC: Adding Nat rules for service {_nat_service} : {_nat_info}')

                        _agentValues = {"vmeth_num": i, "ip": _nat_info['ip'], "port": _nat_info['port']}
                        _ip_rules_list += nftObj.mReplaceValuesJsonRules(iprules, 'ip_rules_list_forward', _agentValues)
                        _ip_nat_rules_list += nftObj.mReplaceValuesJsonRules(iprules, 'ip_rules_list_nat_info', _agentValues)

                        _ip_nat_ports_list.add(_nat_info['port'])

            # Adding LOG rule for FORWARD packets
            if self.mCheckConfigOption("log_dropped_packets", "True"):
                _ip_rules_list += nftObj.mReplaceValuesJsonRules(iprules, 'log_forward_dropped_packets')

            _ip_rules_list += nftObj.mReplaceValuesJsonRules(iprules, 'ip_rules_list_output', _replaceAdminInterface)

            if self.__exaccoci_services:
                _ip_nat_rules_list += nftObj.mReplaceValuesJsonRules(iprules, 'ip_nat_rules_exaccoci_services', _replaceAdminInterface)

            #
            # Set Default initial rules on -P and INPUT (moved outside loop)
            #
            _ip_default_rules += nftObj.mReplaceValuesJsonRules(iprules, 'ip_default_rules', _replaceAdminInterface)

            # Adding LOG rule for INPUT packets
            if self.mCheckConfigOption("log_dropped_packets", "True"):
                _ip_default_rules += nftObj.mReplaceValuesJsonRules(iprules, 'log_input_dropped_packets')

            #Final DROP
            _ip_default_rules_final_drop = nftObj.mReplaceValuesJsonRules(iprules, 'ip_default_rules_final_drop')
            _ip_drop_rules_list += _ip_default_rules_final_drop
            _ip_default_rules += _ip_default_rules_final_drop

            # IPv6 Rules
            _ip6_default_rules = nftObj.mReplaceValuesJsonRules(iprules, 'ip6_default_rules')

            # Test hook
            if aMode:
                return _ip_default_rules, _ip_rules_list, _ip_nat_rules_list, _ip_nat_ports_list


            _node = exaBoxNode(get_gcontext())

            try:
                _node.mConnect(aHost=_dom0)

                if not self.__ociexacc:
                    # 35812394: Copy fix-up script even for OL8 envs to avoid breaking dependent components.
                    # Copy dom0_iptables_setup.sh script to dom0s
                    ebLogInfo(f"*** Copying dom0_iptables_setup.sh to Dom0: {_dom0} ***")
                    # Create directory /opt/exacloud/network/ if needed
                    _node.mExecuteCmd('/bin/mkdir -p /opt/exacloud/network/')
                    _node.mCopyFile('scripts/network/dom0_iptables_setup.sh', '/opt/exacloud/network/dom0_iptables_setup.sh')
                    ebLogInfo(f"*** File copied to {_dom0} ***")

                # Bug 38447783 - We need to make sure the "drop" rules are
                # strictly added at the end of the chain, so we will just
                # remove duplicates first.
                nftObj.mDeleteNFTRules(_node, _ip_drop_rules_list)

                # Get rules current state
                _nft_cmd = "/usr/sbin/nft"
                _cmd_str = f"{_nft_cmd} -as list table ip filter"
                _, _o, _e = _node.mExecuteCmd(_cmd_str)
                _out = _o.readlines()
                _curr_filter_rules_json = nftObj.convertConfigToJson(_out)

                #38204376 - Create Missing NFTables
                known_tables = {
                    ("ip", "filter"),
                    ("ip6", "filter"),
                    ("bridge", "filter"),
                    ("ip", "nat"),
                    ("ip", "mangle"),
                    ("ip6", "nat"),
                    ("ip6", "mangle")
                }
                
                for _family, _table in known_tables:
                    nftObj.mEnsureNFTableExist(_node, _family, _table)

                # Apply default rules
                for _rule in _ip_default_rules:
                    _exist_rule = nftObj.mRuleExists(_rule, _curr_filter_rules_json)
                    if not _exist_rule:
                        _cmd = nftObj.convertJsonConfigToCmd(_rule)
                        node_exec_cmd_check(_node, f"{_nft_cmd} {_cmd}",
                            log_error=True, log_stdout_on_error=True)
                    else:
                        ebLogTrace(f"Rule already exists: {_rule}")

                # Apply non default rules
                for _rule in _ip_rules_list:
                    _exist_rule = nftObj.mRuleExists(_rule, _curr_filter_rules_json)
                    if not _exist_rule:
                        _cmd = nftObj.convertJsonConfigToCmd(_rule)
                        node_exec_cmd_check(_node, f"{_nft_cmd} {_cmd}",
                            log_error=True, log_stdout_on_error=True)
                    else:
                        ebLogTrace(f"Rule already exists: {_rule}")

                # Apply NAT rules
                if _ip_nat_rules_list:
                    _cmd_str = f"{_nft_cmd} -sa list table ip nat"
                    _, _o, _e = _node.mExecuteCmd(_cmd_str)
                    _out = _o.readlines()
                    _currNatJsonCfg = nftObj.convertConfigToJson(_out)
                    _postcleanup_nat_rules = []

                    # Remove Conflicting rules with same port (even with same IP, as VM is anyway creating)
                    # space after {_nat_port} is to avoid superset match (port 10 matching 100)
                    _dport_list = [f'dport {_nat_port} ' for _nat_port in _ip_nat_ports_list]

                    for _curr_nat_cfg in _currNatJsonCfg:
                        _curr_nat_rule = _curr_nat_cfg['details']
                        if any(map(lambda x: x in _curr_nat_rule, _dport_list)):

                            _cmd = nftObj.convertJsonConfigToCmd(_curr_nat_cfg, operation="DELETE")
                            node_exec_cmd_check(_node, f"{_nft_cmd} {_cmd}",
                                log_error=True, log_stdout_on_error=True)

                    # Apply nat rules after cleanup
                    for _rule in _ip_nat_rules_list:
                        _exist_rule = nftObj.mRuleExists(_rule, _curr_filter_rules_json)
                        if not _exist_rule:
                            _cmd = nftObj.convertJsonConfigToCmd(_rule)
                            node_exec_cmd_check(_node, f"{_nft_cmd} {_cmd}",
                                log_error=True, log_stdout_on_error=True)
                    else:
                        ebLogTrace(f"Rule already exists: {_rule}")

                # Apply IPtables V6 Rules
                for _rule in _ip6_default_rules:
                    _exist_rule = nftObj.mRuleExists(_rule, _curr_filter_rules_json)
                    if not _exist_rule:
                        _cmd = nftObj.convertJsonConfigToCmd(_rule)
                        node_exec_cmd_check(_node, f"{_nft_cmd} {_cmd}",
                            log_error=True, log_stdout_on_error=True)
                    else:
                        ebLogTrace(f"Rule already exists: {_rule}")

                # Remove duplicated Rules
                nftObj.mDeleteDuplicateNFTRules(_node)

                #
                # Commit/Save rules for NFT table v4/v6
                #
                ebLogInfo('*** Saving nftables rules...')
                _node.mExecuteCmdLog("/bin/cp /etc/nftables/exadata.nft /etc/nftables/exadata.`date +%y.%j.%H.%m.%s`")
                _node.mExecuteCmdLog("%s list ruleset >/etc/nftables/exadata.nft" % _nft_cmd)
                self.mStartDom0Service('nftables', _node, _dom0)

                #
                # For Debug only dump NFT tables being set/save
                #
                if self.mIsDebug():
                    _node.mExecuteCmdLog(f"{_nft_cmd} -s list ruleset")
                    _node.mExecuteCmdLog("/bin/cat /etc/nftables/exadata.nft")

            finally:
                _node.mDisconnect()

        # Parallel Execution
        _plist = ProcessManager()

        for _dom0, _domU in self.mReturnDom0DomUPair():
            if aDom0s and _dom0 in aDom0s:

                _p = ProcessStructure(mConfigureRulesDom0, [_dom0])
                _p.mSetMaxExecutionTime(-1) # No timeout
                _p.mSetJoinTimeout(5)
                _p.mSetLogTimeoutFx(ebLogWarn)
                _plist.mStartAppend(_p)

        _plist.mJoinProcess()


    def mSetupNATIptablesOnDom0v2(self, aMode=False, aDom0s=None):

        if self.mCheckConfigOption('iptables_nat_rules', 'False'):
            return

        if not self.mHasNatAndCustomerNet() or (self.__ociexacc and self.isATP()):
            if self.mIsDebug():
                ebLogWarn('*** NAT Iptables setup only apply to EXABM or OCICC (non-ATP) environment')
            return

        ebLogVerbose("mSetupNATIptablesOnDom0v2: Setup NAT IP tables on Dom0")

        # Higgs SVM also use only vmeth100
        if self.__exabm or not self.__shared_env:
            _vmeth_range = [100] + [x for x in range(200, 216)]
        else:
            _range = 16
            # Optional configuration Parameter, takes int from 1 to XX
            _maxVMOption = self.mCheckConfigOption('iptables_nat_rules_range')
            _exadata_model = self.mGetExadataDom0Model()
            if not _maxVMOption and (self.mCompareExadataModel(_exadata_model, 'X9') >= 0 or self.mCompareExadataModel(_exadata_model, 'X10') >= 0):
                _range = 24
                ebLogInfo(f"*** for Model {_exadata_model}, the default iptables_nat_rules_ranges value is {_range}")
            if _maxVMOption:
                if _maxVMOption.isdigit() and int(_maxVMOption) > 1:
                    _range = int(_maxVMOption)
                else:
                    ebLogWarn(
                        "iptables_nat_rules_ranges takes an integer >= 1, fallback to default value: {}".format(_range))
            # IB is limited to 8 VM, precreate vmeth100 to vmeth107 rules.
            # In the future, to be done dynamically at VM creation AND using the vmethXXX bridge name in configuration
            _vmeth_range = list(range(100, 100 + _range)) + list(range(200, 200 + _range))

        _ip_rules_list = []
        _ip_nat_rules_list = []
        _ip_default_rules = []

        def read_json(f_name):
            fname = f_name
            try:
                f = open(fname, 'r')
                json_content = json.load(f)
            except Exception as e:
                _msg = f"Failed to load iptables rules from file {fname}: {e}"
                ebLogError(_msg)
                raise ExacloudRuntimeError(0x0750, 0xA, _msg) from e
            finally:
                f.close()

            return json_content

        iprules = read_json('properties/iprules.json')

        _replaceAdminInterface = self.mGetCurrentMasterInterface(aEth0Present=True)

        # This list will delete conflicting NAT rules with same PORT
        _ip_nat_ports_list = set()
        for i in _vmeth_range:
            _ip_rules_list += [ip_rules_list.format(vmeth_num=i) for ip_rules_list in iprules['ip_rules_list']]

            # Adding rules to enable communication with agent BUG:30833376
            if not self.__ociexacc:
                _ip_rules_list += [ip_rules_list.format(vmeth_num=i) for ip_rules_list in iprules['ip_rules_list_atp_not_ociexacc']]

            # Adding rule to enable communication with dcs agent incase of Exacc BUG:34139276
            if self.__ociexacc:
                _ip_rules_list += [ip_rules_list.format(vmeth_num=i) for ip_rules_list in
                                   iprules['ip_rules_list_ociexacc']]

            # Adding rules to allow communication between ecra vm and agent
            if self.mCheckConfigOption("iptables_ecra_vm_agent", "True"):
                _ip_rules_list += [ip_rules_list.format(vmeth_num=i) for ip_rules_list in
                                   iprules['ip_rules_list_ecra_vm_agent']]

            # OCIEXACC NAT Fileserver, only accept forward one a single ip and port
            _nat_info = self.mGetOciExaCCServicesSetup()
            # Dictionnary may have multiple natted services (fileserver/fwdproxy/etc)
            if _nat_info:
                _ocps_path = self.mCheckConfigOption("ocps_jsonpath")
                if _ocps_path:
                    # Adding forwardProxy and patchServer ips/ports found in ocps_jsonpath property to _nat_info
                    _ocps_dict = {
                        "fp": {
                            "ip": "",
                            "port": "3080"
                        },
                        "ps": {
                            "ip": "",
                            "port": "2080"
                        }
                    }
                    _ocps_json = read_json(_ocps_path)

                    if "forwardProxy_ip" in _ocps_json:
                        _ocps_dict["fp"]["ip"] = _ocps_json["forwardProxy_ip"]
                    if "forwardProxy_port" in _ocps_json:
                        _ocps_dict["fp"]["port"] = _ocps_json["forwardProxy_port"]
                    if "patchServer_ip" in _ocps_json:
                        _ocps_dict["ps"]["ip"] = _ocps_json["patchServer_ip"]
                    if "patchServer_port" in _ocps_json:
                        _ocps_dict["ps"]["port"] = _ocps_json["patchServer_port"]

                    # Merging dictionaries
                    _nat_info.update(_ocps_dict)

                for _nat_service,_nat_info in _nat_info.items():
                    ebLogInfo(f'Oci-ExaCC: Adding Nat rules for service {_nat_service} : {_nat_info}')
                    _ip_rules_list += [ip_rules_list.format(vmeth_num=i, ip=_nat_info['ip'], port=_nat_info['port']) for ip_rules_list in iprules['ip_rules_list_forward']]
                    _ip_nat_rules_list += [ip_nat_rules.format(vmeth_num=i, ip=_nat_info['ip'], port=_nat_info['port']) for ip_nat_rules in iprules['ip_rules_list_nat_info']]
                    _ip_nat_ports_list.add(_nat_info['port'])

        # Adding LOG rule for FORWARD packets
        if self.mCheckConfigOption("log_dropped_packets", "True"):
            _ip_rules_list += iprules['log_forward_dropped_packets']

        _ip_rules_list += [ip_rules_list.format(master=_replaceAdminInterface) for ip_rules_list in iprules['ip_rules_list_output']]

        if self.__exaccoci_services:
            _ip_nat_rules_list += [ip_rules_list.format(master=_replaceAdminInterface) for ip_rules_list in iprules['ip_nat_rules_exaccoci_services']]

        #
        # Set Default initial rules on -P and INPUT (moved outside loop)
        #
        _ip_default_rules += [ip_rules_list.format(master=_replaceAdminInterface) for ip_rules_list in iprules['ip_default_rules']]

        # Adding LOG rule for INPUT packets
        if self.mCheckConfigOption("log_dropped_packets", "True"):
            _ip_default_rules += iprules['log_input_dropped_packets']

        #Final DROP
        _ip_default_rules += iprules['ip_default_rules_final_drop']

        # Test hook
        if aMode:
            return _ip_default_rules, _ip_rules_list, _ip_nat_rules_list, _ip_nat_ports_list

        for _dom0, _domU in self.mReturnDom0DomUPair():
            if aDom0s and _dom0 not in aDom0s:
                continue

            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)

            #
            # IPTables binary
            #
            _iptables_lock = "/var/lock/iptables"
            _flock_bin = "/usr/bin/flock"
            _iptables_wait = "--wait"
            # Test if iptables binary supports wait timeout
            _, _o, _ = node_exec_cmd_check(
                _node, f"{_flock_bin} {_iptables_lock} /sbin/iptables --help")
            if "wait" in _o and "seconds" in _o:
                _iptables_wait = "--wait 10"
            _iptables_bin = (f"{_flock_bin} {_iptables_lock} "
                             f"/sbin/iptables {_iptables_wait}")

            if not self.__ociexacc:
                # Copy dom0_iptables_setup.sh script to dom0s
                ebLogInfo(f"*** Copying dom0_iptables_setup.sh to Dom0: {_dom0} ***")
                # Create directory /opt/exacloud/network/ if needed
                _node.mExecuteCmd('/bin/mkdir -p /opt/exacloud/network/')
                _node.mCopyFile('scripts/network/dom0_iptables_setup.sh', '/opt/exacloud/network/dom0_iptables_setup.sh')
                ebLogInfo(f"*** File copied to {_dom0} ***")

            #
            # IP rules cleanup
            #
            _cmd_str = f"{_iptables_bin} -S"
            _, _o, _e = _node.mExecuteCmd(_cmd_str)
            _out = _o.readlines()

            # Order rules to drop starting by REJECT rules to avoid connection lost
            _ordered_rules_to_drop = []
            for _c in _out:
                if 'REJECT' not in _c:
                    _ordered_rules_to_drop.append(_c)
                else:
                    _ordered_rules_to_drop.insert(0, _c)

            _def_count = 0
            _chains_to_remove = []
            for _rule in _ordered_rules_to_drop:
                if _rule:
                    _curr_rule = _rule.strip()
                    _rcount = _ip_default_rules.count(_curr_rule)
                    if not _rcount:
                        if not any(f"{_chain}-" in _curr_rule for _chain in self.mCheckConfigOption('keep_iptables_chains')) and \
                            not (_curr_rule in self.mCheckConfigOption('iptables_skip_rules')):
                            _ip_set_rule_del = None
                            if _curr_rule.startswith("-A"):
                                _ip_set_rule_del = f"{_iptables_bin} {_curr_rule.replace('-A', '-D', 1)}"
                                node_exec_cmd_check(_node, _ip_set_rule_del,
                                    log_error=True, log_stdout_on_error=True)
                            elif _curr_rule.startswith("-N"):
                                _chains_to_remove.append(_curr_rule)
                            if self.mIsDebug():
                                ebLogDebug('*** IP_R: %s:%s' % (_dom0, _ip_set_rule_del))
                    else:
                        _rcount = _out.count(_rule)
                        if _rcount > 1:
                            ebLogWarn('*** DEFAULT IPrule defined multiple times:%s' % (_curr_rule))
                        _def_count += _rcount
            for _chain in _chains_to_remove:
                _ip_set_chain_del = f"{_iptables_bin} {_chain.replace('-N', '-X', 1)}"
                _node.mExecuteCmd(_ip_set_chain_del)

            if _def_count != len(_ip_default_rules):
                ebLogWarn('*** DEFAULT IPtables rules have not been set yet on dom0: %s' % (_dom0))

                if self.mGetSharedEnv():
                    ebLogInfo(f"*** Removing INPUT CHAIN {_iptables_bin} -F INPUT ***")
                    _node.mExecuteCmd(f"{_iptables_bin} -F INPUT")
                else:
                    ebLogInfo(f"*** Removing all iptables CHAINS {_iptables_bin} -F ; {_iptables_bin} -X")
                    _node.mExecuteCmd(f"{_iptables_bin} -F ; {_iptables_bin} -X")

                #
                # Apply the DEFAULT rules (if not already there)
                #
                for _rule in _ip_default_rules:
                    _ip_set_rule_add = f"{_iptables_bin} {_rule}"
                    node_exec_cmd_check(_node, _ip_set_rule_add,
                        log_error=True, log_stdout_on_error=True)
                    if self.mIsDebug():
                        ebLogDebug('*** IP_D: %s:%s' % (_dom0, _ip_set_rule_add))

            #
            # Dump rules (DEBUG mode only)
            #
            if self.mIsDebug():
                _cmd_str = f"{_iptables_bin} -S"
                _node.mExecuteCmdLog(_cmd_str)
            #
            # Set the non DEFAULT rules
            #
            for _rule in _ip_rules_list:
                _ip_set_rule_add = f"{_iptables_bin} {_rule}"
                node_exec_cmd_check(_node, _ip_set_rule_add,
                    log_error=True, log_stdout_on_error=True)
                if self.mIsDebug():
                    ebLogDebug('*** IP_R: %s:%s' % (_dom0, _ip_set_rule_add))

            if _ip_nat_rules_list:
                _cmd_str = f"{_iptables_bin} -S -t nat"
                _, _o, _e = _node.mExecuteCmd(_cmd_str)
                _out = _o.readlines()
                _curr_nat_rules = [x.strip() for x in _out]
                _postcleanup_nat_rules = []

                # Remove Conflicting rules with same port (even with same IP, as VM is anyway creating)
                # space after {_nat_port} is to avoid superset match (port 10 matching 100)
                _dport_list = [f'--dport {_nat_port} ' for _nat_port in _ip_nat_ports_list]

                for _curr_nat_rule in _curr_nat_rules:
                    if any(map(lambda x: x in _curr_nat_rule, _dport_list)):
                        _ip_set_rule_del = f"{_iptables_bin} -t nat {_curr_nat_rule.replace('-A', '-D', 1)}"
                        node_exec_cmd_check(_node, _ip_set_rule_del,
                            log_error=True, log_stdout_on_error=True)
                        if self.mIsDebug():
                            ebLogDebug('*** NAT_R: %s:%s' % (_dom0, _ip_set_rule_del))
                    else:
                        _postcleanup_nat_rules.append(_curr_nat_rule)

                for _nat_rule in _ip_nat_rules_list:
                    if _nat_rule not in  _postcleanup_nat_rules:
                        _node.mExecuteCmd(f"{_iptables_bin} -t nat {_nat_rule}")
                        if self.mIsDebug():
                            ebLogDebug('*** NAT_R: %s:%s' % (_dom0, _nat_rule))

            #
            # IPTables V6 RULES
            #
            for _ip6_default_rule in iprules['ip6_default_rules']:
                _node.mExecuteCmd(f'/sbin/ip6tables {_ip6_default_rule}')
            #
            # Checks ALL rules have been applied correctly or bail out
            #
            _cmd_str = f"{_iptables_bin} -S"
            _, _o, _e = _node.mExecuteCmd(_cmd_str)
            _out = _o.readlines()
            _iptables_curr = []
            for _ipr in _out:
                _iptables_curr.append(_ipr.strip())

            _ip_rules_list_final = _ip_rules_list + _ip_default_rules

            for _rule in _ip_rules_list_final:
                if _rule:
                    _rcount = _iptables_curr.count(_rule)
                    if not _rcount:
                        ExacloudRuntimeError(0x0750, 0xA, 'Failed to apply iptables rules during PREVM CS step',
                                             aStackTrace=False)
            #
            # Commit/Save rules for iptables v4/v6
            #
            ebLogInfo('*** Saving iptables rules...')
            _node.mExecuteCmdLog("/bin/cp /etc/sysconfig/iptables /etc/sysconfig/iptables.`date +%y.%j.%H.%m.%s`")
            _node.mExecuteCmdLog("/sbin/service iptables save")
            _node.mExecuteCmdLog("/sbin/service ip6tables save")

            #
            # Clean dynamic rules from /etc/sysconfig/iptables file
            #
            _iptables_file = "/etc/sysconfig/iptables"
            _dynamic_prefixes = ["LIBVIRT", "FI-", "FO-", "HI-", "libvirt"]
            ebLogInfo(f'*** Cleaning file {_iptables_file}...')
            for _prefix in _dynamic_prefixes:
                _node.mExecuteCmdLog(f"/bin/sed -i '/{_prefix}/d' {_iptables_file}")

            #
            # For Debug only dump iptables being set/save
            #
            if self.mIsDebug():
                _node.mExecuteCmdLog(f"{_iptables_bin} -S")
                _node.mExecuteCmdLog("/bin/cat /etc/sysconfig/iptables")

            _node.mDisconnect()

    def mCleanupEbtablesOnDom0(self):

        if self.mGetEbtableSetup() == False:
            return

        ebLogVerbose("mCleanupEbtablesOnDom0: Cleanup eb tables on dom0")
        _dpairs = self.mReturnDom0DomUPair()
        for _dom0, _ in _dpairs:

            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            #
            # Get list of current running VM id (domainid)
            #
            _domainid_list = []
            _cmdstr = 'xm list'
            _,_o,_ = _node.mExecuteCmd(_cmdstr)
            if _o is not None:
                _out = _o.readlines()
                for _line in _out:
                    _domain_info = _line.split()
                    if len(_domain_info) >= 2 and _domain_info[1] not in ['ID', '0']:
                        _domainid = _domain_info[1]
                        _domainid_list.append(_domainid)
            #
            # Get All ebtables chains
            #
            _P_LIST = ['INPUT', 'FORWARD', 'OUTPUT']
            _vifid_list = []
            for _policy in _P_LIST:
                _cmdstr = 'ebtables -L ' + _policy
                _,_o,_ = _node.mExecuteCmd(_cmdstr)
                if _o is not None:
                    _out = _o.readlines()
                    for _line in _out:
                        if _line[:2] == '-i':
                            _vifid = _line.split(' ')[1]
                            if _vifid not in _vifid_list:
                                _vifid_list.append(_vifid)
            #
            # Remove ebtables belonging to vmid not running
            #
            _cp_vifid_list = _vifid_list[:]
            for _vmid in _domainid_list:
                _pvifid = 'vif' + _vmid
                for _vifid in _cp_vifid_list:
                    if _pvifid == _vifid[:len(_pvifid)]:
                        try:
                            _vifid_list.remove(_vifid)
                        except ValueError:
                            ebLogWarn('*** {0} was already revomed from vif list'.format(_vifid))

            if _vifid_list != []:
                ebLogInfo('*** Found orphan ebtables rules for vif_id : %s' % (str(_vifid_list)))
            else:
                ebLogInfo('*** Ebtables state in dom0: %s is clean (no orphan chains found)' % (_dom0))

            _cmdstr_base = "flock /var/lock/ebtables /sbin/ebtables"
            for _vifid in _vifid_list:
                _ebchain_out=_vifid+'_OUT'
                _ebchain_in=_vifid+'_IN'

                _cmdstr = _cmdstr_base + ' -D FORWARD -o "%s" -j "%s"' % (_vifid, _ebchain_out)
                _node.mExecuteCmdLog(_cmdstr)
                _cmdstr = _cmdstr_base + ' -D OUTPUT  -o "%s" -j "%s"' % (_vifid, _ebchain_out)
                _node.mExecuteCmdLog(_cmdstr)
                _cmdstr = _cmdstr_base + ' -F "%s"' % (_ebchain_out)
                _node.mExecuteCmdLog(_cmdstr)
                _cmdstr = _cmdstr_base + ' -X "%s"' % (_ebchain_out)
                _node.mExecuteCmdLog(_cmdstr)
                _cmdstr = _cmdstr_base + ' -D FORWARD -i "%s" -j "%s"' % (_vifid, _ebchain_in)
                _node.mExecuteCmdLog(_cmdstr)
                _cmdstr = _cmdstr_base + ' -D INPUT   -i "%s" -j "%s"' % (_vifid, _ebchain_in)
                _node.mExecuteCmdLog(_cmdstr)
                _cmdstr = _cmdstr_base + ' -F "%s"' % (_ebchain_in)
                _node.mExecuteCmdLog(_cmdstr)
                _cmdstr = _cmdstr_base + ' -X "%s"' % (_ebchain_in)
                _node.mExecuteCmdLog(_cmdstr)

            _node.mDisconnect()


    def mHandlerUnSetupEbtablesOnDom0(self):
        return self.mSetupEbtablesOnDom0(False)

    def mHandlerSetupEbtablesOnDom0(self):
        return self.mSetupEbtablesOnDom0(True)

    def mSetupEbtablesOnDom0(self,aMode=True):

        if self.mIsKVM():
            ebLogWarn('*** mSetupEbtablesOnDom0 is currently disabled for KVM -- FIXME !!!')
            return

        ebLogInfo("mSetupEbtablesOnDom0: aMode = %s" % aMode)

        if self.mGetEbtableSetup() == False:
            _ebtables_cleanup = False
            # Check if ebTables cleanup is needed
            for _dom0, _ in self.mReturnDom0DomUPair():
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_dom0)
                _ebt_wl_file   = '/opt/exacloud/network/vif-whitelist'+'.'+self.__key
                # if the WL file exists on at least one dom0:
                #    clean rules (this function in False mode)
                if _node.mFileExists(_ebt_wl_file):
                    ebLogInfo('Ebtables disabled but there is leftover ebtables rules on dom0: ({0}), cleaning up...'.format(_dom0))
                    _ebtables_cleanup = True
                    aMode = False
                    break

            if not _ebtables_cleanup:
                return

        if aMode:
            #
            # Setup ebtables automatic setup for VM
            #
            _dpairs = self.mReturnDom0DomUPair()

            _domU_ip_list = ''
            for _, _domU in _dpairs:

                _domU_mac = self.__machines.mGetMachineConfig(_domU)
                _domU_net_list = _domU_mac.mGetMacNetworks()
                _domU_dns_list = _domU_mac.mGetDnsServers()
                _domU_ntp_list = _domU_mac.mGetNtpServers()

                _domU_ip_local = ''
                for _net in _domU_net_list:
                    _netcnf = self.__networks.mGetNetworkConfig(_net)
                    if self.__exabm:
                        _nat_hn = _netcnf.mGetNetNatHostName()
                        _nat_dn = _netcnf.mGetNetNatDomainName()
                        _nat_ip = _netcnf.mGetNetNatAddr()
                    else:
                        _nat_hn = None
                        _nat_dn = None
                        _nat_ip = None
                    if _netcnf.mGetNetType() in [ 'admin', 'client']:
                        _ip     = _netcnf.mGetNetIpAddr()
                        _gw     = _netcnf.mGetNetGateWay()
                        if _gw is not None and _gw.lower() != 'undefined':
                            #
                            if not _gw in _domU_ip_list.split(' '):
                                _domU_ip_list = _domU_ip_list + _gw + ' '
                            if not _gw in _domU_ip_local.split(' '):
                                _domU_ip_local = _domU_ip_local + _gw + ' '
                        _domU_ip_local = _domU_ip_local + _ip + ' '
                        _domU_ip_list = _domU_ip_list + _ip + ' '
                        if self.__exabm and _netcnf.mGetNetType() == 'client' and _nat_ip is not None:
                            _domU_ip_local = _domU_ip_local + _nat_ip + ' '
                            _domU_ip_list = _domU_ip_list + _nat_ip + ' '
                            _nat_lrg = '169.254.200.0/3'
                            ebLogInfo('*** EXABMC EBT_NAT_LOCAL_IP: %s %s (%s)' % (_nat_hn, _nat_ip, _nat_lrg))
                            _nat_ip_list = '169.254.200.1 169.254.200.2 169.254.200.3 '
                            _domU_ip_local = _domU_ip_local + _nat_ip_list
                            _domU_ip_list  = _domU_ip_list  + _nat_ip_list

                #
                # Add NTP and DNS IPs
                #
                for _ip in _domU_ntp_list:
                    _domU_ip_local = _domU_ip_local + _ip + ' '
                    _domU_ip_list = _domU_ip_list + _ip + ' '

                #
                # Fetch Scan and VIP IPs
                #
                _scan_vip_ips = self.mGetEbtablesScanVip(_domU)
                for _ip in _scan_vip_ips:
                    _domU_ip_local = _domU_ip_local + _ip + ' '
                    if not _ip in _domU_ip_list.split(' '):
                        _domU_ip_list = _domU_ip_list + _ip + ' '

                ebLogInfo('*** ebtables 2.0 whitelist for DomU: %s - %s' % (_domU, _domU_ip_local))

            _dom0_dict = {}
            _dom0_ip_list = ''
            for _dom0, _domU in _dpairs:
                _dom0_dict[ _dom0 ] = socket.gethostbyname(_dom0)
            for _dom0 in list(_dom0_dict.keys()):
                _dom0_ip_list = _dom0_ip_list + _dom0_dict[_dom0] + ' '
                ebLogInfo('*** ebtables 2.0 whitelist for Dom0: %s - %s' % (_dom0,_dom0_dict[_dom0]))

            if not self.mCheckConfigOption('ebtables_skip_dom0', 'True'):
                _domX_list = _dom0_ip_list + _domU_ip_list
            else:
                _domX_list = _domU_ip_list

            for _dom0, _domu in _dpairs:

                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_dom0)
                _cmdstr = 'mkdir -p /opt/exacloud/network ; touch /opt/exacloud/network/vif-lockdown'

                _ebt_wl_file   = '/opt/exacloud/network/vif-whitelist'+'.'+self.__key
                if self.__exabm:
                    _ebt_wl_file   = '/opt/exacloud/network/vif-whitelist'+'.'+_domu
                    ebLogInfo('*** EBT_WL_File: %s' % (_ebt_wl_file))
                _ebt_wl_script = '/etc/xen/scripts/vif-bridge'
                _ebt_wl_script_cmn = '/etc/xen/scripts/vif-common.sh'
                _ebt_wl_script_orig = _ebt_wl_script+'.ORIG'
                _ebt_wl_script_cmn_orig = _ebt_wl_script_cmn+'.ORIG'
                _ebt_wl_script_ebt  = _ebt_wl_script+'.EBT'
                _ebt_wl_script_cmn_ebt  = _ebt_wl_script_cmn+'.EBT'
                #
                # Build WhiteList host-string
                #
                _ebt_wlc = self.mCheckConfigOption('ebtables_default_wl')
                _ebt_cst = self.mCheckConfigOption('ebtables_custom_wl')
                if _ebt_cst is not None:
                    if _ebt_wlc is not None:
                        _ebt_wlc += _ebt_cst
                    else:
                        _ebt_wlc = _ebt_cst

                _subnetset = ebSubnetSet()
                _subnetset.mAppendList(_ebt_wlc)
                _subnetset.mAppendList(_domX_list.split(' '))

                # Add localhost to the list
                _host = socket.gethostname()
                _ip = socket.gethostbyname(_host)
                # Retrieve _also_ localhost via dns (required in case of NAT)
                _cmd = '/bin/nslookup ' + _host
                _, _, _o, _ = self.mExecuteLocal(_cmd)
                if _o is not None:
                    try:
                        _ip2 = _o.strip().split('\n')[-1].split(' ')[-1]
                        if re.match('\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', _ip2) is None:
                            _ip2 = None
                    except:
                        _ip2 = None

                _subnetset.mAddSubnet(_ip)
                _subnetset.mAddSubnet(_ip2)
                _ebt_wlf = ' '.join(_subnetset.mGetAllIPs())

                ebLogInfo('*** ebtables whitelist: %s on (%s)' % (_ebt_wlf, _dom0))

                _cmdstr = _cmdstr + " ; (/bin/echo '%s' > " +_ebt_wl_file+ ')'
                #
                # Check if the EBT vif-bridge xen script has been already installed
                #
                if not _node.mFileExists(_ebt_wl_script_ebt):
                    _node.mCopyFile('scripts/network/vif-bridge', _ebt_wl_script_ebt)
                    _node.mCopyFile('scripts/network/vif-common.sh', _ebt_wl_script_cmn_ebt)
                    _cmdstr = _cmdstr + ' ; cp -f '+_ebt_wl_script+' '+_ebt_wl_script_orig + ' ; chmod 755 ' + _ebt_wl_script_ebt
                    _cmdstr = _cmdstr + ' ; cp -f '+_ebt_wl_script_cmn+' '+_ebt_wl_script_cmn_orig + ' ; chmod 755 ' + _ebt_wl_script_cmn_ebt
                else:
                    # BUG: 22114518 - overwrite existing vif-bridge script with newer version
                    _node.mCopyFile('scripts/network/vif-bridge', _ebt_wl_script_ebt)
                    _node.mCopyFile('scripts/network/vif-common.sh', _ebt_wl_script_cmn_ebt)
                    ebLogInfo('*** ebtables vif-bridge already installed (update done to latest release)')
                _cmdstr = _cmdstr + ' ; ln -sf '+_ebt_wl_script_ebt+' '+ _ebt_wl_script
                _cmdstr = _cmdstr + ' ; chmod 755 ' + _ebt_wl_script_cmn_ebt + ' ; ln -sf '+_ebt_wl_script_cmn_ebt+' '+ _ebt_wl_script_cmn
                _cmdstr = _cmdstr % (_ebt_wlf) + ' ; touch /opt/exacloud/network/vif-whitelist'

                _node.mExecuteCmdLog(_cmdstr)
                if _node.mGetCmdExitStatus():
                    _node.mDisconnect()
                    raise Exception('mExecuteCmd Failed', _node.mGetHostname(), _cmdstr)
                _node.mDisconnect()
        else:
            #
            # Remove ebtables for a give cluster
            #
            _dpairs = self.mReturnDom0DomUPair()
            for _dom0, _domU in _dpairs:
                # TODO: Use vif-lockdown
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_dom0)
                ebLogInfo('*** Removing ebtables whitelist for VM: %s' % (_domU))
                #
                # Remove content of whitelist and flush all ebtables rules (no need to reboot the VM)
                #
                if self.mCheckConfigOption('ebtables_wipe', 'True'):
                    _cmdstr = """rm -f /opt/exacloud/network/vif-whitelist* ; touch /opt/exacloud/network/vif-whitelist ;
                    ebtables -F ; ebtables -X"""
                    _node.mExecuteCmdLog(_cmdstr)
                    if _node.mGetCmdExitStatus():
                        _node.mDisconnect()
                        raise Exception('mExecuteCmd Failed', _node.mGetHostname(), _cmdstr)
                else:
                    _ebt_wl_file   = '/opt/exacloud/network/vif-whitelist'+'.'+self.__key
                    if self.__exabm:
                        _ebt_wl_file   = '/opt/exacloud/network/vif-whitelist'+'.'+_domU
                        ebLogInfo('*** EBT_WL_File: %s' % (_ebt_wl_file))
                    _cmdstr = 'rm -f '+_ebt_wl_file
                    _node.mExecuteCmdLog(_cmdstr)

                    _cmdstr = """xm network-list %s | grep vif | awk 'BEGIN { FS = "/" } ; { print $7$8"."$9 }'""" % (_domU)
                    _i,_o,_ = _node.mExecuteCmd(_cmdstr)
                    _vif_list = []
                    if not _node.mGetCmdExitStatus():
                        _out = _o.readlines()
                        for _l in _out:
                            _vif_list.append(_l.strip())
                    else:
                        _node.mDisconnect()
                        raise Exception('mExecuteCmd Failed', _node.mGetHostname(), _cmdstr)

                    _vif_list_s = []
                    for _vif in _vif_list:

                        _cmdstr = """ebtables -L | grep %s | grep Bridge | grep IN""" % (_vif)
                        _,_o,_ = _node.mExecuteCmd(_cmdstr)
                        if not _node.mGetCmdExitStatus():
                            _out = _o.readlines()
                            for _l in _out:
                                if not len(_l):
                                    ebLogInfo('*** Discard VIF: %s' % (_vif))
                                else:
                                    _vif_list_s.append(_vif)
                        else:
                            ebLogInfo('*** Discard VIF: %s' % (_vif))

                    if len(_vif_list_s):

                        for _vif in _vif_list_s:
                            _cmdstr = """
                            vif="%s"
                            ebchain_out="${vif}_OUT"
                            ebchain_in="${vif}_IN"
                            ebtlockfile="/var/lock/ebtables"
                            ebtables="flock $ebtlockfile /sbin/ebtables"

                                    $ebtables -D FORWARD -o "$vif" -j "$ebchain_out" || true
                                    $ebtables -D OUTPUT -o "$vif" -j "$ebchain_out" || true
                                    $ebtables -F "$ebchain_out" || true
                                    $ebtables -X "$ebchain_out" || true

                                    $ebtables -D FORWARD -i "$vif" -j "$ebchain_in" || true
                                    $ebtables -D INPUT -i "$vif" -j "$ebchain_in" || true
                                    $ebtables -F "$ebchain_in" || true
                                    $ebtables -X "$ebchain_in" || true
                            """ % (_vif)
                            _node.mExecuteCmd(_cmdstr)
                            ebLogInfo('*** ebtables rules for VIF: %s have been flushed' % (_vif))

                _node.mDisconnect()

    def mEnableLogRotationOnDom0(self):

        for _dom0, _domU in self.mReturnDom0DomUPair():
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            _r_file = '/etc/logrotate.d/exacloud'
            _cmd = 'test -f ' + _r_file
            _node.mExecuteCmd(_cmd)
            _rc = _node.mGetCmdExitStatus()
            if _rc:
                ebLogVerbose("Logrotate file for exacloud does not exist. Creating it.")
                _cmd = "echo '/opt/exacloud/clusters/operations/*.txt {\n\tmissingok\n\tnotifempty\n\tcompress\n\tsize 250k\n\tcreate 0644 root root\n}' > /etc/logrotate.d/exacloud"
                _node.mExecuteCmd(_cmd)
            else:
                ebLogVerbose("Logrotate file for exacloud exists")
            _node.mDisconnect()


    def mConfigV6Client(self):

        ebLogVerbose("mConfigV6Client: Configure v6 client.")
        for _, _domU in self.mReturnDom0DomUPair():
            ebLogInfo('*** DomU NAME : %s' % (_domU))
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domU)
            _domU_mac = self.__machines.mGetMachineConfig(_domU)
            _domU_net_list = _domU_mac.mGetMacNetworks()
            for _net in _domU_net_list:
                _netcnf = self.__networks.mGetNetworkConfig(_net)
                if _netcnf.mGetNetType() in [ 'client_v6' ]:
                    _ip6 =  _netcnf.mGetNetIpAddr()
                    _ipv6addr = _ip6 + '/' + _netcnf.mGetNetMask()
                    ebLogInfo(" ipv6addr = %s \n"% _ipv6addr)

                    _cmd = 'echo 0 > /proc/sys/net/ipv6/conf/all/disable_ipv6 ;'
                    _cmd += 'echo 0 > /proc/sys/net/ipv6/conf/default/disable_ipv6 ;'
                    _cmd += '/sbin/ip -6 addr add ' + _ipv6addr + ' dev bondeth0 ;'
                    ebLogInfo(_cmd)
                    _node.mExecuteCmdLog(_cmd)
                    _ifcfg_path = '/etc/sysconfig/network-scripts/ifcfg-bondeth0'
                    _cmd = "sed 's/^IPV6INIT=no/IPV6INIT=yes/' -i " + _ifcfg_path
                    ebLogInfo(_cmd)
                    _node.mExecuteCmdLog(_cmd)
                    _cmd = " grep -q IPV6ADDR " + _ifcfg_path + "  && sed 's/^IPV6ADDR=[0-9,A-F,a-f,:]*/IPV6ADDR=" + _ip6 + "/' -i " + _ifcfg_path + " || echo \"IPV6ADDR="+ _ip6 + " \">> " +  _ifcfg_path

                    ebLogInfo(_cmd)
                    _node.mExecuteCmdLog(_cmd)

                    self.mSetSysCtlConfigValue(_node, "net.ipv6.conf.default.disable_ipv6", "0")
                    self.mSetSysCtlConfigValue(_node, "net.ipv6.conf.all.disable_ipv6", "0")

            _node.mDisconnect()

    def mGetGridHome(self,aDomU):

            if self.IsZdlraProv():
                return self.__ZDLRA.mGetGridHome(aDomU)

            if self.mIsAdbs():
                return self.__ADBSobj.mGetGridHome(aDomU)

            _node = aDomU
            _disconnect = False
            if isinstance(_node, str):
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=aDomU)
                _disconnect = True

            _ls_cmd = node_cmd_abs_path_check(_node, "ls")
            _cmd = f"{_ls_cmd} -la /var/opt/oracle/creg/grid | head -2 | grep ...x..x..."
            #Some customers might have set a custom locale. For eg: JAPANESE-SJIS                                                                                                                                  
            #Hence, pass aDecodeUtf8=True to mExecuteCmd to decode such output! 
            _i, _o, _e = _node.mExecuteCmd(_cmd, aDecodeUtf8=True)                                                                                                                                                 
            _out = _o.readlines()
            ebLogInfo(f'*** grid file permissions: {_out}')
            if not _out or len(_out) == 0:
                ebLogWarn("x permission not found on /var/opt/oracle/creg/grid.")
                ebLogWarn("Granting x permission..")
                _chmod_cmd = node_cmd_abs_path_check(_node, "chmod")
                _cmd = f"{_chmod_cmd} ug+x /var/opt/oracle/creg/grid"
                _node.mExecuteCmdLog(_cmd)
                if _node.mGetCmdExitStatus() != 0:
                    _msg = "Could not succesfully run the command {}".format(_cmd)
                    ebLogWarn("*** {0} ***".format(_msg))
            #
            # Fetch GRID_HOME Entry
            #
            _error_str = '*** GRID.INI entry not found for grid'
            _cmd = "cat /var/opt/oracle/creg/grid/grid.ini | grep \"^sid\" | cut -d '=' -f 2"
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            _out = _o.readlines()
            if not _out or len(_out) == 0:
                if _disconnect:
                    _node.mDisconnect()
                ebLogWarn(_error_str)
                raise ExacloudRuntimeError(0x0792, 0x0A, _error_str)
            _out = _out[0].strip()
            _sid = _out
            _cmd = "cat /var/opt/oracle/creg/grid/grid.ini | grep \"^oracle_home\" | cut -d '=' -f 2"
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            _out = _o.readlines()
            if not _out or len(_out) == 0:
                if _disconnect:
                    _node.mDisconnect()
                ebLogWarn(_error_str)
                raise ExacloudRuntimeError(0x0792, 0x0A, _error_str)

            _out = _out[0].strip()
            _path= _out

            if _disconnect:
                _node.mDisconnect()
            return _path, _sid

    
    def mGetOracleHome(self, aDomU, aDbName):
        """
            Check if <DB Name>.ini path exists, if not then use the oratab entry for the DB name
            Return Oracle home path for the given DB name
        """
        _node = exaBoxNode(get_gcontext())
        _node.mSetUser('root')
        _node.mConnect(aHost=aDomU)
        _path = ""
        _cmd = "cat /var/opt/oracle/creg/{}.ini | grep \"^oracle_home\" | cut -d '=' -f 2".format(aDbName)
        _i, _o, _e = _node.mExecuteCmd(_cmd)
        _out = _o.readlines()
        if not _out or len(_out) == 0:
            ebLogInfo('*** {}.INI entry not found. Looking for entry in oratab'.format(aDbName))
            _cmd = "cat /etc/oratab | grep %s | awk -F : '{print $2}' "%(aDbName)
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            _out = _o.readlines()
            if not _out or len(_out) == 0:
                _node.mDisconnect()
                return _path
                
        _node.mDisconnect()
        _path = _out[0].strip()
        ebLogInfo(f"*** oracle_home:{_path} for DB:{aDbName}")
        return _path

    def mConfigScanV6IPsPostDBInstall(self):
        ebLogVerbose("mConfigScanV6IPsPostDBInstall: Configure SCAN V6 IPs post database install.")

        if self.mCheckConfigOption('all_scanip_enabled','False'):
            ebLogInfo('*** All SCAN IPs is disabled. Only 1 SCAN IP is available')
            return
        #
        # Fetch SCAN NAME
        #
        _cluScans = self.__clusters.mGetCluster().mGetCluScans()
        _clusters_keys = self.__clusters.mGetClusters()
        ebLogInfo('*** Clusters_keys %s '% _clusters_keys)
        _k = self.__scans.mGetScans()
        _scanName = None
        for _e in _k:
            _o = self.__scans.mGetScan(_e)
            if  '_v6' in _o.mGetScanName():
               _scanName = _o.mGetScanName()[:-3]
               ebLogInfo('Scan Name %s '% _scanName)
               ebLogInfo('Cluster ID %s'% _o.mGetCluId())
               break

        if _scanName is not None:
            ebLogInfo('*** SCAN_NAME: %s' % (_scanName))
        else:
            ebLogWarn('*** SCAN_NAME not found !')
            return

        _set_scan_complete = False
        for _, _domU in self.mReturnDom0DomUPair():

            # Scan setup is done only on one node.
            if _set_scan_complete:
                break
            _cluVips = self.__clusters.mGetCluster().mGetCluV6Vips()
            _vipIp = ''
            for _vip in list(_cluVips.keys()):
                ebLogInfo('*** DomU NAME : %s' % (_domU))
                _vipID = _cluVips[_vip].mGetCVipId()
                _vipID = _cluVips[_vip].mGetCVipId()[:-7]
                _vipID = _vipID[3:]
                if   _vipID in _domU :
                    _vipIp = _cluVips[_vip].mGetCVIPAddr()
                    break
            if _vipIp == '':
               ebLogError("Error : vip not available")
               return

            ebLogInfo('*** DomU NAME : %s' % (_domU))

            _path, _sid = self.mGetGridHome(_domU)
            #
            # Check SCAN/SCAN LISTENER
            #
            _node = exaBoxNode(get_gcontext())
            _node.mSetUser('grid')
            _node.mConnect(aHost=_domU)
            _cmd_pfx = 'ORACLE_HOME=%s;export ORACLE_HOME;ORACLE_SID=%s; export ORACLE_SID;PATH=$PATH:$ORACLE_HOME/bin;export PATH;' % (_path,_sid)
            _cmd = _cmd_pfx + '$ORACLE_HOME/bin/srvctl status scan'
            _node.mExecuteCmdLog(_cmd)
            _cmd = _cmd_pfx + '$ORACLE_HOME/bin/srvctl config scan'
            _node.mExecuteCmdLog(_cmd)
            _cmd = _cmd_pfx + '$ORACLE_HOME/bin/srvctl status scan_listener'
            _node.mExecuteCmdLog(_cmd)
            #
            # Change Listener configuration
            #
            _nodeRoot = exaBoxNode(get_gcontext())
            _nodeRoot.mConnect(aHost=_domU)

            # Stop Listener and change configuration
            _cmd = _cmd_pfx + '$ORACLE_HOME/bin/srvctl stop scan_listener; $ORACLE_HOME/bin/srvctl stop scan;'
            _nodeRoot.mExecuteCmdLog(_cmd)

            _domU_mac = self.__machines.mGetMachineConfig(_domU)
            _domU_net_list = _domU_mac.mGetMacNetworks()
            _config_flag = True
            for _net in _domU_net_list:
                _netcnf = self.__networks.mGetNetworkConfig(_net)
                if  _netcnf.mGetNetType() == 'client_v6' and _config_flag == True:
                    _pattern = re.compile(r"::")
                    _v6_network = _netcnf.mGetNetIpAddr()
                    _netw = _pattern.split(_netcnf.mGetNetIpAddr())

                    _v6_network = _netw[0] + "::/" + _netcnf.mGetNetMask()
                    _v6_address = _vipIp  + "/" + _netcnf.mGetNetMask()
                    _netnum = '1'
                    _node_name = _domU.split('.')[0]
                    _domU_scan_name = _scanName

                    ebLogInfo('node_name :  ' + _node_name + ' netw ' + _netw[0])
                    ebLogInfo('v6_address : ' +  _v6_address)
                    ebLogInfo('v6_network : ' + _v6_network)

                    _cmd = _cmd_pfx + '$ORACLE_HOME/bin/srvctl  modify network -subnet ' +  _v6_network
                    ebLogInfo(_cmd)
                    _nodeRoot.mExecuteCmdLog(_cmd)

                    _cmd = _cmd_pfx + '$ORACLE_HOME/bin/srvctl  modify vip -node ' +  _node_name  + ' -netnum ' + _netnum + ' -address ' +  _v6_address
                    ebLogInfo(_cmd)
                    _nodeRoot.mExecuteCmdLog(_cmd)

                    _cmd = _cmd_pfx + '$ORACLE_HOME/bin/oifcfg  setif -global ' +  'bondeth0' + '/' + _v6_network[:-3] + ':public'
                    ebLogInfo(_cmd)
                    _nodeRoot.mExecuteCmdLog(_cmd)

                    _cmd = _cmd_pfx + '$ORACLE_HOME/bin/srvctl  modify  scan -scanname ' +  _domU_scan_name
                    ebLogInfo(_cmd)
                    _nodeRoot.mExecuteCmdLog(_cmd)

                    _cmd = _cmd_pfx + '$ORACLE_HOME/bin/srvctl  modify  network -iptype both '
                    ebLogInfo(_cmd)
                    _nodeRoot.mExecuteCmdLog(_cmd)

                    _cmd = _cmd_pfx + ' $ORACLE_HOME/bin/srvctl start scan; $ORACLE_HOME/bin/srvctl start scan_listener'
                    ebLogInfo(_cmd)
                    _nodeRoot.mExecuteCmdLog(_cmd)

                    _cmd = _cmd_pfx + '$ORACLE_HOME/bin/srvctl status scan'
                    ebLogInfo(_cmd)
                    _node.mExecuteCmdLog(_cmd)

                    _cmd = _cmd_pfx + '$ORACLE_HOME/bin/srvctl config scan'
                    ebLogInfo(_cmd)
                    _node.mExecuteCmdLog(_cmd)

                    _cmd = _cmd_pfx + '$ORACLE_HOME/bin/srvctl status scan_listener'
                    ebLogInfo(_cmd)
                    _node.mExecuteCmdLog(_cmd)
                    _config_flag = False
            _set_scan_complete = True
            _node.mDisconnect()


    def mHandlerDisableEbtablesOnDom0(self):
        return self.mEnableEbtablesOnDom0(False)

    def mHandlerEnableEbtablesOnDom0(self):
        return self.mEnableEbtablesOnDom0(True)

    def mEnableEbtablesOnDom0(self,aMode=True):

        if self.mGetEbtableSetup() == False:
            return

        ebLogVerbose("mEnableEbtablesOnDom0: aMode = %s" % aMode)

        #
        # Enable ebtables on the VMs
        #
        _dpairs = self.mReturnDom0DomUPair()

        for _dom0, _domU in _dpairs:

            # Obtain the Domain ID
            _domainid = None
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            _cmdstr = 'xm list | grep ' + _domU
            _i, _o, _e = _node.mExecuteCmd(_cmdstr)
            if _o is not None:
                _out = _o.readlines()
                for _line in _out:
                    _domainid = ' '.join(_line.split()).split(' ')[1]

            if _domainid is None:
                ebLogInfo("*** Could not read domain ID of VM %s" %(_dom0))
                _node.mDisconnect()
                return

            # Obtain the list of vif IDs and network paths for this domain
            _viflist = {}
            _cmdstr = 'xm network-list ' + _domainid
            _i, _o, _e = _node.mExecuteCmd(_cmdstr)
            if _o is not None:
                _out = _o.readlines()

                if len(_out) < 2:
                    ebLogInfo("Incorrect output from xm network-list command on %s" %(_dom0))
                    _node.mDisconnect()
                    return

                _out.pop(0)

                for _line in _out:
                    _viftemp = ' '.join(_line.split()).split(' ')[0]
                    _bepathtemp = ' '.join(_line.split()).split(' ')[8]
                    _viflist[_viftemp] = _bepathtemp

            for _vifid, _bepath in list(_viflist.items()):

                _op = "enabled"
                _cmdstr = 'export vif=vif' + _domainid + '.' + _vifid + '; ' + 'export XENBUS_PATH=' + _bepath + '; '
                if aMode:
                    # Now invoke the vif-bridge script
                    _cmdstr = _cmdstr + '/etc/xen/scripts/vif-bridge.EBT online'
                else:
                    _cmdstr = _cmdstr + '/etc/xen/scripts/vif-bridge.EBT disable'
                    _op = "disabled"

                _node.mExecuteCmd(_cmdstr)
                ebLogInfo('*** ebtables rules for VIF: %s have been %s' % (_vifid, _op))

            _node.mDisconnect()
        return

    def mPatchDom0BM(self):

        if not self.__exabm:
            return

        _cell_list = self.mReturnCellNodes()
        for _cell in _cell_list:
            ebLogInfo('*** Patching Dom0 BM setting - WLS SecRules : %s' % (_cell))
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_cell)
            #
            # Fetch Cell ImageInfo version
            #
            _cmd = "imageinfo | grep 'Active image version'"
            _, _o, _ = _node.mExecuteCmd(_cmd)
            _out = _o.readlines()
            if not _out or len(_out) == 0:
                _node.mDisconnect()
                raise ExacloudRuntimeError(0x0722,0xA,'Cell ImageInfo version not available',aStackTrace=False)
            else:
                _version = _out[0].split(':')[1].strip()
                if _version != '12.2.1.1.1.170419':
                    ebLogWarn('*** Cell ImageInfo version (%s) skipping cellwall update' % (_version))
                    _node.mDisconnect()
                    continue
            #
            # Update/Pach cellwalld (See bug 26109810) - Note: This should eventually get fixed in Exadata Cell Image
            #
            _node.mCopyFile('scripts/network/etc_init.d_cellwall', '/etc/init.d/cellwall')
            _node.mCopyFile('scripts/network/etc_sysconfig_cellwall','/etc/sysconfig/cellwall')
            _cmdstr = 'chmod 740 /etc/init.d/cellwall ; chmod 740 /etc/sysconfig/cellwall ; service cellwall restart'
            _node.mExecuteCmdLog(_cmdstr)
            _node.mDisconnect()

    def mHandlerDeleteEbtablesRuleOnDom0(self):

        aOptions = self.mGetArgsOptions()
        return self.mAddEbtablesRuleOnDom0(aOptions, False)


    def mHandlerAddEbtablesRuleOnDom0(self):

        aOptions = self.mGetArgsOptions()
        return self.mAddEbtablesRuleOnDom0(aOptions, True)

    def mAddEbtablesRuleOnDom0(self,aOptions,aMode=True):
        _ipaddrlist = []

        if self.mGetEbtableSetup() == False:
            return

        ebLogVerbose("mAddEbtablesRuleOnDom0: aMode = %s" % aMode)

        #
        # IP address is a mandatory parameter
        #
        if aOptions.ip is None:
            ebLogInfo("*** IP address not specified. Ebtables rule cannot be added/removed ***")
            return

        _ipaddrlist = aOptions.ip.rstrip().lstrip().split(',')

        # Input params better be valid
        for _ipaddr in _ipaddrlist:
            try:
                socket.inet_aton(_ipaddr)
            except socket.error:
                ebLogInfo("*** One or more IP addresses specified not valid. Ebtables rule cannot be added/removed ***")
                return

        #
        # Add a new ebtables rule on the VMs
        #
        _dpairs = self.mReturnDom0DomUPair()

        for _dom0,_domU in _dpairs:
            #Code was moved to a function as some exits are returns (like VM stopped)
            #And code like whitelist update was only ran on first dom0
            self.mAddEbtablesRuleOnSingleDom0(_dom0,_domU,_ipaddrlist,aMode)


    def mAddEbtablesRuleOnSingleDom0(self,aDom0,aDomU,aIpAddrList,aMode=True):

        def _runEbtcmd(_cmdstr,_node):
            ebLogInfo("*** Command to add ebtables rules %s ***" %(_cmdstr))
            _i, _o, _e = _node.mExecuteCmd(_cmdstr)
            if _node.mGetCmdExitStatus():
                ebLogError("*** Command to add ebtables rules failed.  out: %s error: %s cmd: %s***" %("\n".join( _o.readlines()), "\n".join( _e.readlines()), _cmdstr))
                _node.mDisconnect()
                raise ExacloudRuntimeError(0x0751,0xA,"Failed to add ebtable rules")

        _node = exaBoxNode(get_gcontext())
        _node.mConnect(aHost=aDom0)

        # Check for presence of vif-whitelist file
        _ebt_wl_file   = '/opt/exacloud/network/vif-whitelist'+'.'+self.__key
        _cmdstr = 'ls -l ' + _ebt_wl_file
        _i, _o, _e = _node.mExecuteCmd(_cmdstr)
        if _o is not None:
            _out = _o.readlines()
            if len(_out):
                ebLogInfo('*** %s: Ebtables whitelist file exists. Updating same ***' %(aDomU))
            else:
                _node.mDisconnect()
                ebLogError('*** %s: Ebtables whitelist file does not exist for given cluster ***' %(aDomU))
                return

        _ipstring = ''
        for _ipaddr in aIpAddrList:
            _ipstring = _ipstring + " " + _ipaddr
        if aMode: # Add the IP address to the whitelist
            _cmdstr = 'echo ' + _ipstring.lstrip().rstrip() + ' >> /opt/exacloud/network/vif-whitelist'+'.'+self.__key
            _i, _o, _e = _node.mExecuteCmd(_cmdstr)
            if _node.mGetCmdExitStatus():
                _node.mDisconnect()
                raise Exception('mExecuteCmd Failed', _node.mGetHostname(), _cmdstr)
        else: # remove IP address from whitelist
            _iplist = []
            _cmdstr = 'cat /opt/exacloud/network/vif-whitelist'+'.'+self.__key
            _i, _o, _e = _node.mExecuteCmd(_cmdstr)
            if _o is not None:
                _out = _o.readlines()
                for _line in _out:
                    _iplisttemp = _line.lstrip().rstrip().split(" ")
                    _iplist = _iplist + _iplisttemp
                _iplist = set(_iplist)
            else:
                _node.mDisconnect()
                raise Exception('mExecuteCmd Failed', _node.mGetHostname(), _cmdstr)

            # Remove ipaddr from list
            _foundip = False
            for _ipaddr in aIpAddrList:
                if _ipaddr in _iplist:
                    _iplist.remove(_ipaddr)
                    _foundip = True
            if not _foundip:
                _node.mDisconnect()
                ebLogError("*** %s: None of the IPs were found in whitelist ***" %(aDomU))
                return

            _ipchain = ""
            for _ip in _iplist:
                _ipchain = _ipchain + " " + _ip

            ebLogInfo("new ip chain is %s" %(_ipchain.lstrip()))

            _cmdstr = 'echo ' + _ipchain.lstrip() + ' > /opt/exacloud/network/vif-whitelist'+'.'+self.__key
            _i, _o, _e = _node.mExecuteCmd(_cmdstr)
            if _node.mGetCmdExitStatus():
                _node.mDisconnect()
                raise Exception('mExecuteCmd Failed', _node.mGetHostname(), _cmdstr)

        # Obtain the Domain ID
        _domainid = None
        _cmdstr = 'xm list | grep ' + aDomU
        _i, _o, _e = _node.mExecuteCmd(_cmdstr)
        if _o is not None:
            _out = _o.readlines()
            for _line in _out:
                _domainid = ' '.join(_line.split()).split(' ')[1]

        # If VM is not running, nothing more to do
        if _domainid is None:
            ebLogInfo("*** %s is not running. Ebtables whitelist updated ***" %(aDomU))
            _node.mDisconnect()
            return

        # Obtain the list of vif IDs and network paths for this domain
        _viflist = []
        _cmdstr = 'xm network-list ' + _domainid
        _i, _o, _e = _node.mExecuteCmd(_cmdstr)
        if _o is not None:
            _out = _o.readlines()

            if len(_out) < 2:
                ebLogInfo("Incorrect output from xm network-list command on %s" %(aDom0))
                _node.mDisconnect()
                return

            _out.pop(0)

            for _line in _out:
                _viftemp = ' '.join(_line.split()).split(' ')[0]
                _viflist.append(_viftemp)

        for _vif in _viflist:

            # Both chains must exist. Else we return
            _chainlist = ['IN', 'OUT']
            for _cl in _chainlist:
                _chainname = 'vif' + _domainid + "." + _vif + "_" + _cl

                _cmdstr = 'ebtables -L ' + _chainname
                _i, _o, _e = _node.mExecuteCmd(_cmdstr)
                if _node.mGetCmdExitStatus():
                    ebLogError("*** Expected ebtables chain %s. Not found ***" %(_chainname))
                    _node.mDisconnect()
                    return

                # Modify the chain
                _cmdstr = ""
                _iptag = "src="
                if _cl == "IN":
                    _iptag = "dst="
                for _ipaddr in aIpAddrList:
                    if aMode:
                        #sleep for 1 sec after every addition, refer bug #27022629
                        _cmdstr = "ebtables -A " + _chainname + " -p ip4 --ip-" + _iptag + _ipaddr + " -j ACCEPT; sleep 1;\n"
                        _runEbtcmd(_cmdstr,_node);
                        _cmdstr = "date; ebtables -A " + _chainname + " -p arp -j ACCEPT; sleep 1; \n"
                        _runEbtcmd(_cmdstr,_node);
                    else:
                        _cmdstr = " ebtables -D " + _chainname + " -p ip4 --ip-" + _iptag + _ipaddr + " -j ACCEPT; sleep 1; \n"
                        _runEbtcmd(_cmdstr,_node);
                if aMode:
                    ebLogInfo("*** %s: Successfully added given IPs to ebtables chain %s" %(aDomU, _chainname))
                else:
                    ebLogInfo("*** %s: Successfully removed given IPs from ebtables chain %s" %(aDomU, _chainname))

        _node.mDisconnect()


    def mCheckIfVMExists(self, aDomU = None, aDom0 = None):

        if aDomU is None or aDom0 is None:
            return False

        vmInstance = getHVInstance(aDom0)
        domUs = vmInstance.mRefreshDomUs()
        for domU in domUs:
            if str(domU) == aDomU:
                return True

        return False

    """
    ::mRunVMDiagnosticOnOEDAFailure Connect
    """
    def mRunVMDiagnosticOnOEDAFailure(self):
        ebLogTrace("***mRunVMDiagnosticOnOEDAFailure: Testing connectivity and DNS lookup for all domUs.")
        _dbpair       = self.mReturnDom0DomUPair()
        #_domUs        = map(operator.itemgetter(1),_dbpair)

        _currentDomU = None
        try:
            for _dom0, _domu in self.mReturnDom0DomUPair():
                if not self.mCheckIfVMExists(_domu, _dom0):
                    ebLogTrace("*** domU {0} does not exist or isn't running. Skipping VM diagnostics. ***".format(_domu))
                    continue
                _currentDomU = _domu
                ebLogTrace("*** Trying to connect as root user: %s"%(_domu))
                _node = exaBoxNode(get_gcontext())
                _node.mSetUser("root")
                # The mIsConnectable have no retry and improve delete service
                # time by 30min
                if _node.mIsConnectable(aHost=_domu):
                    _node.mConnect(aHost=_domu)
                    _cmd = "nslookup -type=any oracle.com"
                    _node.mExecuteCmdLog(_cmd)
                    _remote_rc = _node.mGetCmdExitStatus()
                    if _remote_rc == 0:
                        ebLogTrace("DNS lookup on domU %s is successful."%(_domu))
                    else:
                        ebLogTrace("DNS lookup on domU %s failed."%(_domu))
                    self.mCheckConnectivityBetweenDomU(_domu)
                    _node.mDisconnect()
                else:
                    ebLogTrace("DomU {} is not connectable by SSH, skip DNS tests.".format(_domu))
        except Exception as e:
            ebLogTrace("Connection test to domU %s failed. Exception reason: %s"%(_currentDomU, str(e)))

    """
    ::mCheckConnectivityBetweenDomU
    """
    def mCheckConnectivityBetweenDomU(self, _fromDomU):
        if _fromDomU is None:
            return

        _dbpair = self.mReturnDom0DomUPair()
        _domUs = map(operator.itemgetter(1),_dbpair)

        #Check if Oracle user exists.
        _nodeRoot = exaBoxNode(get_gcontext())
        _nodeRoot.mSetUser("root")
        _nodeRoot.mConnect(aHost=_fromDomU)
        _i, _o, _e = _nodeRoot.mExecuteCmd("cut -d: -f1 /etc/passwd")
        isOracleUserPresent = False
        _output = _o.read()
        for _user in _output.splitlines():
            if _user == "oracle":
                isOracleUserPresent = True
                break
        _nodeRoot.mDisconnect()

        if not isOracleUserPresent:
            ebLogTrace("Oracle user doesnt exist in VM %s. Skip inter-domU connectivity check."%(_fromDomU))
            return
        else:
            ebLogTrace("Oracle user exists in VM %s"%(_fromDomU))

        ebLogTrace("***Connecting to domU %s as oracle user."%(_fromDomU))
        _node = exaBoxNode(get_gcontext())
        _node.mSetUser("oracle")
        _node.mConnect(aHost=_fromDomU)
        for _todomU in _domUs:
            if _todomU == _fromDomU:
                continue
            ebLogTrace("***Connecting from %s to %s"%(_fromDomU,_todomU))
            def _ssh_exec(command):
                _i, _o, _e = _node.mExecuteCmd(command)
                return _node.mGetCmdExitStatus(), _o.read(), _e.read()

            _cmd = "timeout 3 sh -c 'ssh -o StrictHostKeyChecking=no " \
                   "-o BatchMode=yes -o UserKnownHostsFile=/dev/null " \
                   "%s date'" % _todomU
            _rc, _output, _err = _ssh_exec(_cmd)
            if _rc == 124:
                ebLogTrace("SSH timeout occured while connecting from %s to %s."%(_fromDomU, _todomU))
            elif _rc != 0:
                # merge multiline warning
                _errlines = []
                for _line in _err.splitlines():
                    if _line.startswith('@@@@@@@'):
                        continue
                    if _line.startswith('Warning: Permanently added '):
                        continue
                    _errlines.append(_line)
                _err = ' '.join(_err.splitlines())  # merge multiline warning
                ebLogTrace("SSH connectivity from %s to %s failed. Reason: %s"%(_fromDomU, _todomU, ' '.join(_errlines)))
            else:
                ebLogTrace("SSH connectivity from %s to %s is successful."%(_fromDomU,_todomU))
        _node.mDisconnect()

    """
    ::mPostVMCreate Patching
    """
    def mPostVMCreatePatching(self,aOptions):
        #
        # Setup SSH Key on all DomU
        # - copy tools_key_public to .ssh/authorized keys
        #
        ebLogVerbose("mPostVMCreatePatching: Patching post VM Create for all domU's.")

        _isATP        = self.isATP()
        _ExaCCATPVIPs = []
        _ExaCCATPFiltering = None
        _dbpair       = self.mReturnDom0DomUPair()
        _domUs        = list(map(operator.itemgetter(1),_dbpair))
        _ssh_comment  = ''
        _jconf        = aOptions.jsonconf

        # NAT Vips for /etc/hosts
        if _isATP and self.__ociexacc:
            _ExaCCATPFiltering = ebExaCCAtpFiltering(_jconf, self.__debug)
            # mReturnDom0DomUNatpair DO NOT return NAT FQDN.
            # _natHN Registry mapping actually does
            _natDomUs = []
            _ctx = get_gcontext()
            for _host in _domUs:
                if _ctx.mCheckRegEntry('_natHN_' + _host):
                    _nat_host = _ctx.mGetRegEntry('_natHN_' + _host)
                    _natDomUs.append((_host,_nat_host))
            if _natDomUs:
                _ExaCCATPVIPs = ebExaCCAtpEtcHostsNATVIPs.sGenerateEtcHostsData(_natDomUs)
                if _natDomUs:
                    ebLogInfo("{} NAT Vips are successfully resolved by CPS"\
                                .format(len(_ExaCCATPVIPs)))
                if not _ExaCCATPVIPs:
                    ebLogError(f"*** NAT Vips not resolvable, check /etc/hosts.exacc_infra entries")
                    raise ExacloudRuntimeError(0x0816, 0x0A, "NAT Vips not resolvable")

        # Verify if sshcomment is present under the 'vm' field in the payload
        if _jconf and 'vm' in _jconf.keys():
            _ssh_comment = _jconf.get("vm", {}).get("sshcomment", "")

        # Else, check if present in legacy 'sshcomment' value
        elif _jconf and 'sshcomment' in _jconf.keys():
            _ssh_comment = _jconf['sshcomment']

        _ahost_cmd = 'echo "%s %s" >> /root/.ssh/authorized_keys'
        for _domu in _domUs:
            ebLogInfo('*** Tools Keys VM Patching for: '+_domu)
            _node = exaBoxNode(get_gcontext())
            _node.mResetHostKey(aHost=_domu)
            _node.mConnect(aHost=_domu)

            _sestatus = self.mGetSELinuxMode("domU")
            if self.__cmd not in ["vmgi_reshape", "elastic_cell_update"] and _sestatus is not None:
                self.mSetSeLinux(_node, _sestatus, "domU")
            else:
                ebLogWarn("*** se_linux key not present")

            _node.mExecuteCmd("mkdir -p /root/.ssh")
            if self.__tools_key_public:
                _node.mExecuteCmdLog(_ahost_cmd % (self.__tools_key_public, _ssh_comment))
            _node.mExecuteCmd("chmod 600 /root/.ssh/authorized_keys")

            if _isATP and self.mCheckClusterNetworkType() and self.__exabm:
                _node.mExecuteCmdLog("mkdir -p /var/opt/oracle/misc")
                _node.mCopyFile('scripts/network/db-route', '/var/opt/oracle/misc/db-route')
                _node.mExecuteCmdLog("chmod a+rx /var/opt/oracle/misc/db-route")
                AtpAddRoutes2DomU(_node, self.__ATP, _domu, _dbpair, self.mGetMachines(),self.mGetNetworks()).mExecute()

            #Bug 27195860, /etc/resolv.conf for all DomU's should have "options single-request", on exacc/exacm
            if self.__exacm:
                #Avoids adding options single-request if already present
                _node.mExecuteCmdLog('grep -q "options single-request" /etc/resolv.conf || echo "options single-request" >> /etc/resolv.conf')

            # Add Pregenerated entries to all DomUs
            if _ExaCCATPVIPs:
                ebExaCCAtpEtcHostsNATVIPs.sWriteEtcHostsLines(_node, _ExaCCATPVIPs)
            if _ExaCCATPFiltering:
                _ExaCCATPFiltering.mProcessDomU(_node)

            _node.mDisconnect()

        if _ExaCCATPFiltering:
            self.mApplyExaCCATPDom0Filtering(_ExaCCATPFiltering)


    def mSetupArpCheckFlag(self, aDom0DomUPair=None, aUser=None):

        ebLogInfo("Enter mSetupArpCheckFlag")

        if aDom0DomUPair:
            _ddpair = aDom0DomUPair
        else:
            _ddpair = self.mReturnDom0DomUPair()

        for _, _domU in _ddpair:

            _node = exaBoxNode(get_gcontext())

            try:
                if aUser:
                    _node.mSetUser(aUser)
                _node.mConnect(aHost=_domU)

                for _iface in ["ifcfg-bondeth0", "ifcfg-bondeth1"]:
                    _file = f"/etc/sysconfig/network-scripts/{_iface}"

                    ebLogInfo(f"Configuring interface {_file} in domU {_domU} with ARPCHECK=no")

                    if _node.mFileExists(_file):
                        node_update_key_val_file(_node, _file, {'ARPCHECK':'no'})

            finally:
                _node.mDisconnect()

    """
    ::mPostVMArpingCheck
    """
    def mPostVMArpingCheck(self, aOptions):
        """
        Arping check of a Network from DomU
        :param: aOptions: Object with vm information in jsonconf

        :return: True if proccess is complete
        :raises: TypeError, ExacloudRuntimeError
        """
        ebLogVerbose("mPostVMArpingCheck: Arping check for all domU's.")

        if not aOptions.jsonconf:
            raise ExacloudRuntimeError(0x0765, 0xA, "Missing json payload")

        _jconf = aOptions.jsonconf

        if not 'customer_network' in _jconf or not 'nodes' in _jconf['customer_network']:
            raise ExacloudRuntimeError(0x0765, 0xA, f"Missing information in json payload: {_jconf}")

        _nodes_list = _jconf['customer_network']['nodes']

        _dbpair = self.mReturnDom0DomUPair()
        _domUs = list(map(operator.itemgetter(1), _dbpair))

        for _domu in _domUs:
            ebLogInfo('*** Arping check in DomU: ' + _domu)

            try:
                _node = exaBoxNode(get_gcontext())
                _node.mResetHostKey(aHost=_domu)
                _node.mConnect(aHost=_domu)

                _network_validation = NetworkValidations(_node)

                for _nl in _nodes_list:
                    _client_gateway = _nl['client']["gateway"]
                    _backup_gateway = _nl['backup']["gateway"]

                    # Arping check for client net
                    _network_validation.mArpingCheck('bondeth0', _client_gateway, None)

                    # Verify if namespace RPM installed for backup net
                    _nameSpace = None
                    _cmd = '/sbin/ip addr show bondeth1'
                    _node.mExecuteCmd(_cmd)
                    if _node.mGetCmdExitStatus() != 0:
                        _nameSpace = 'mgmt'

                    # Arping check for backup net
                    _network_validation.mArpingCheck('bondeth1', _backup_gateway, _nameSpace)
            finally:
                _node.mDisconnect()

        return True

    def mApplyExaCCATPDom0Filtering(self, aExaCCATPFiltering):
        # Apply Filtering rules using data collected on each domU
        # Post VM patching
        ebLogInfo('ATP-EXACC OCI, setting up dom0 ebtables for Admin Net')

        self.mAcquireRemoteLock()

        for _dom0,_domU in self.mReturnDom0DomUPair():
            with connect_to_host(_dom0, get_gcontext()) as _node:
                aExaCCATPFiltering.mProcessDom0(_node,_domU, self.mIsKVM())
                # Optimized Service enablement to reuse connected _node
                self.mEnableDom0Service('ebtables',_node,_dom0)

        self.mReleaseRemoteLock()

    """
    ::mPostVMCellPatching
    """
    def mPostVMCellPatching(self,aOptions, aCellList=None):
        """
        Step 1. Drop the FlashCache on that Cell
        # cellcli -e drop flashcache

        Step 2. Check the status of ASM if the grid disks go OFFLINE. The following command should return 'Yes' for the grid disks being listed:
        # cellcli -e list griddisk attributes name,asmmodestatus,asmdeactivationoutcome

        Step 3. Inactivate the griddisk on the cell
        # cellcli -e alter griddisk all inactive

        Step 4. Shut down cellsrv service
        # cellcli -e alter cell shutdown services cellsrv

        Step 5. Set the cell flashcache mode to writeback
        # cellcli -e "alter cell flashCacheMode=writeback"

        Step 6. Restart the cellsrv service
        # cellcli -e alter cell startup services cellsrv

        Step 7. Reactivate the griddisks on the cell
        # cellcli -e alter griddisk all active

        Step 8. Verify all grid disks have been successfully put online using the following command:
        # cellcli -e list griddisk destatus

        Step 9. Recreate the flash cache
        # cellcli -e create flashcache all
        """
        _cmds_list = [ 'cellcli -e drop flashcache',  'cellcli -e list griddisk attributes name,asmmodestatus,asmdeactivationoutcome',
                       'cellcli -e alter griddisk all inactive', 'cellcli -e alter cell shutdown services cellsrv',
                       'cellcli -e "alter cell flashCacheMode=writeback"', 'cellcli -e alter cell startup services cellsrv',
                       'cellcli -e alter griddisk all active', 'cellcli -e create flashcache all'
        ]

        if aCellList:
            _cell_list = aCellList
        else:
            _cell_list = sorted(self.mReturnCellNodes().keys())

        #
        # Check if flashCacheMode is already set to writeback
        #
        def check_flashcachemode(_cell):
            ebLogVerbose("check_flashcachemode: Check flash cache mode for the cell %s" % _cell)
            #
            # Check if Flash Cache is enabled
            #
            _cmd = 'cellcli -e list flashcache attributes name,size'
            _node=exaBoxNode(get_gcontext(), Cluctrl = self)
            _node.mConnect(aHost=_cell)
            _i, _o, _e = _node.mExecuteCmdCellcli(_cmd)
            _output = _o.readlines()
            if _output is None or not len(_output):
                ebLogWarn('*** FlashCache is not enabled on the Cell: %s' % (_cell))
                if self.mCheckConfigOption('force_flashcache_pcs','True'):
                    _node.mDisconnect()
                    return False
            else:
                ebLogInfo('*** Cell %s FlashCache enabled' % (_cell))
            #
            # Check WriteBack Cache enabled
            #
            _cmd = 'cellcli -e list cell detail | grep flashCacheMode'
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            _output = _o.readlines()
            _status_dict = {}
            if _output:
                for _status in _output:
                    k,v = _status.split(':')
                    k = k.strip()
                    v = v.strip()
                    _status_dict[k] = v
            _node.mDisconnect()
            # flashCacheMode
            if 'flashCacheMode' in list(_status_dict.keys()) and _status_dict['flashCacheMode'].upper() == 'WRITEBACK':
                ebLogInfo('*** Cell already setup with flashCacheMode to writeback')
                return True
            else:
                if self.__debug:
                    ebLogInfo('*** Cell flashCacheMode was not set. Review log for additional information')
                return False

        _plist = ProcessManager()

        def _mSetWriteThrough(aCell):
            _cell = aCell
            if check_flashcachemode(_cell): return

            ebLogInfo('*** Cell : '+str(_cell))
            _node = exaBoxNode(get_gcontext(), Cluctrl = self)
            _node.mConnect(aHost=_cell)
            for _cmd in _cmds_list:
                #ebLogInfo('*** : '+_cmd)
                _node.mExecuteCmdLog(_cmd)
            _node.mDisconnect()

        for _cell in _cell_list:
            _p = ProcessStructure(_mSetWriteThrough, [_cell])
            _p.mSetMaxExecutionTime(60*60)
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()

    def mEnableRemotePwdChange(self, aOptions, aCellList=None):
        if aCellList:
            _cell_list = aCellList
        else:
            _cell_list = sorted(self.mReturnCellNodes().keys())

        for _cell in _cell_list:
            ebLogInfo('*** Enable Remote Pwd Change on Cell : '+str(_cell))
            _exanode = exaBoxNode(get_gcontext(), Cluctrl = self)
            with node_connect_to_host(_exanode, _cell) as _node:
                _node.mExecuteCmdLog('cellcli -e alter cell remotePwdChangeAllowed=TRUE')

    def mEnforceFlashCache(self,aOptions):

        #
        # Check if flashCache is enabled
        #
        def check_flashcachemode(_cell):
            #
            # Check if Flash Cache is enabled
            #
            _cmd = 'cellcli -e list flashcache attributes name,size'
            _node=exaBoxNode(get_gcontext(), Cluctrl = self)
            _node.mConnect(aHost=_cell)
            _i, _o, _e = _node.mExecuteCmdCellcli(_cmd)
            _output = _o.readlines()
            if _output is None or not len(_output):
                ebLogWarn('*** FlashCache is not enabled on the Cell: %s' % (_cell))
                return False
            else:
                ebLogInfo('*** Cell %s FlashCache enabled' % (_cell))
                return True
        #
        # Safe Guard and option to rollback the Enforce
        #
        if self.mCheckConfigOption('dont_enforce_flashcache', 'True'):
            return

        for _cell in sorted(self.mReturnCellNodes().keys()):

            if check_flashcachemode(_cell): continue

            ebLogInfo('*** Cell : '+str(_cell))
            _node = exaBoxNode(get_gcontext(), Cluctrl = self)
            _node.mConnect(aHost=_cell)
            _node.mExecuteCmdLog('cellcli -e create flashcache all')
            _node.mDisconnect()

    def mHandlerCellsReset(self,aOptions=None):
        if not aOptions: 
            aOptions = self.mGetArgsOptions()

        _rc = 0

        ebLogVerbose("mHandlerCellsReset: Reset All the Cells.")

        for _cell in sorted(self.mReturnCellNodes().keys()):

            _node = exaBoxNode(get_gcontext(), Cluctrl = self)
            _node.mConnect(aHost=_cell)

            _cmd = 'cellcli -e list cell detail | grep Status'
            _i, _o, _e = _node.mExecuteCmdCellcli(_cmd)
            _output = _o.readlines()
            _status_dict = {}
            if _output:
                for _status in _output:
                    k,v = _status.split(':')
                    k = k.strip()
                    v = v.strip()
                    _status_dict[k] = v
                ebLogInfo('*** Cell detail: %s' % (str(_status_dict)))

            _cmd = "cellcli -e 'list cell attributes interconnect1,interconnect2'"
            _node.mExecuteCmdLog(_cmd)

            _cmd = '/opt/oracle.ExaWatcher/StopExaWatcher.sh'
            _node.mExecuteCmdLog(_cmd)

            _cmd = "cellcli -e 'drop cell force'"
            _node.mExecuteCmdLog(_cmd)

            _cmd = "cellcli -e 'alter cell restart services all'"
            _node.mExecuteCmdLog(_cmd)

            _cmd = "cellcli -e 'create cell interconnect1=stib0,interconnect2=stib1'"
            _node.mExecuteCmdLog(_cmd)

            _cmd = 'reboot'
            _node.mExecuteCmdLog(_cmd)

            _node.mDisconnect()

    def mCheckDom0Status(self,aReset=True):

        _skip_dom0_consistency_fix = self.mCheckConfigOption('skip_dom0_consistency_fix', 'True')
        if _skip_dom0_consistency_fix:
            ebLogInfo('*** Skipped Dom0 consistency checks')
            return

        _dom0s, _, _, _ = self.mReturnAllClusterHosts()
        for _dom0 in _dom0s:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            _cmdstr = '/usr/local/bin/ipconf -nocodes -conf /opt/oracle.cellos/cell.conf -check-consistency -semantic -at-runtime'
            _i,_o,_e = _node.mExecuteCmd(_cmdstr)
            _rc = _node.mGetCmdExitStatus()
            ebLogInfo('*** _RC {0} IPCONF consistency / semantic checks on : {1}'.format(_rc,_dom0))
            _output = _o.readlines()
            if _output:
                _output = ''.join(_output)
                if 'Consistency check PASSED' in _output:
                    ebLogInfo('*** Consistency/Semantic Check on Dom0: (%s) - SUCCESS' % (_dom0))
                    _node.mDisconnect()
                else:
                    ebLogInfo('*** Consistency/Semantic Check on Dom0: (%s) - FAILED' % (_dom0))
                    _node.mDisconnect()

            if _rc:
                raise ExacloudRuntimeError(0x0743, 0xA, 'Dom0 Consistency/Semantic Check failed (unexpected output).')

    def mCheckCellsStatus(self,aReset=True):
        _skip_cells_consistency_fix = self.mCheckConfigOption('skip_cells_consistency_fix', 'True')

        if _skip_cells_consistency_fix:
            ebLogInfo('*** Skipped Cell consistency fix')
            return 2

        def _mCheckConsistency(aCell):
            _cell = aCell
            _node = exaBoxNode(get_gcontext(), Cluctrl = self)
            _node.mConnect(aHost=_cell)
            _cmdstr = 'ipconf -check-consistency -semantic'
            _i,_o,_e = _node.mExecuteCmd(_cmdstr)
            _output = _o.readlines()
            if _output:
                _output = ''.join(_output)
                ebLogTrace(f"Output is:\n'{_output}'")
                if 'Consistency check PASSED' in _output:
                    ebLogInfo('*** Consistency/Semantic Check on Cell: (%s) - SUCCESS' % (_cell))
                    _node.mDisconnect()
                else:
                    ebLogError('*** Consistency/Semantic Check on Cell: (%s) - FAILED' % (_cell))
                    raise ExacloudRuntimeError(0x0743, 0xA, 'Cell Consistency/Semantic Check failed.', Cluctrl = self)
            else:
                _node.mDisconnect()
                raise ExacloudRuntimeError(0x0743, 0xA, 'Cell Consistency/Semantic Check failed (unexpected output).', Cluctrl = self)

        _plist = ProcessManager()

        _, _, _cells, _ = self.mReturnAllClusterHosts()
        for _cell in _cells:
            _p = ProcessStructure(_mCheckConsistency, [_cell])
            _p.mSetMaxExecutionTime(60*60)
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()


    def mCheckCellLimits(self):

        def _singlePAMCellCheck(_cell):
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_cell)

            _sshd_conf_file = "/etc/ssh/sshd_config"
            ebLogInfo("*** Getting UsePAM configuration from file:{}, cell node:{} ***".format(_sshd_conf_file,_cell))

            _cmd = 'grep -Po "^UsePAM[\\s]*(yes|no)" {}'.format(_sshd_conf_file)
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            _pam_setting = _o.read()

            if "yes" not in _pam_setting and "no" not in _pam_setting:
                ebLogWarn("*** UsePAM is not configured in file:{}, cell node:{} ***".format(_sshd_conf_file,_cell))
                ebLogInfo("*** Configuring 'UsePAM yes' in file:{}, cell node:{} ***".format(_sshd_conf_file,_cell))

                _cmd = 'echo  "UsePAM yes" >> {}'.format(_sshd_conf_file)
                _i, _o, _e = _node.mExecuteCmd(_cmd)
                _rc = _node.mGetCmdExitStatus()

                if _rc != 0:
                    ebLogError("*** Error while trying to add 'UsePAM yes' configuration to file:{},cell node:{} ***".format(_sshd_conf_file,_cell))
                    _node.mDisconnect()
                    raise ExacloudRuntimeError(0x0743, 0xA, 'Error while configuring UsePAM in file:{}, cell node:{}.'.format(_sshd_conf_file,_cell))
                else:
                    ebLogInfo("*** 'UsePAM yes' configuration successfully added to file:{}, cell node:{} ***".format(_sshd_conf_file,_cell))
                    ebLogInfo("*** Rebooting cell node:{} ***".format(_cell))
                    self.mRebootNode(_cell)
                    return

            elif "no" in _pam_setting:
                ebLogInfo("*** 'UsePAM no' found in file:{}, cell node:{}. Changing value to yes ***".format(_sshd_conf_file,_cell))
                _cmd = 'sed -i  --follow-symlinks "s/^UsePAM\s*no/UsePAM yes/g" {}'.format(_sshd_conf_file)

                _i, _o, _e = _node.mExecuteCmd(_cmd)
                _rc = _node.mGetCmdExitStatus()

                if _rc != 0:
                    ebLogError("*** Error while tyring to update UsePAM configuration to yes in file:{},cell node:{} ***".format(_sshd_conf_file,_cell))
                    _node.mDisconnect()
                    raise ExacloudRuntimeError(0x0743, 0xA, 'Error while configuring UsePAM in file:{}, cell node:{}.'.format(_sshd_conf_file,_cell))
                else:
                    ebLogInfo("*** UsePAM successfully changed to yes in file:{},cell node:{} ***".format(_sshd_conf_file,_cell))
                    ebLogInfo("*** Rebooting cell node:{} ***".format(_cell))
                    self.mRebootNode(_cell)
                    return

            else:
                ebLogInfo("*** UsePAM already configured in file:{}, cell node:{} ***".format(_sshd_conf_file,_cell))
                _node.mDisconnect()

        _cellList = list(self.mReturnCellNodes().keys())

        _plist = ProcessManager()

        for _cellName in _cellList:
            _p = ProcessStructure(_singlePAMCellCheck, [_cellName], _cellName)
            _p.mSetMaxExecutionTime(60*60)
            _p.mSetJoinTimeout(30)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)
            _plist.mJoinProcess()

    #Function to check a single cell
    def mSingleCellCheckCellServiceUp(self, aCell, aManagedCellFailed, aRestart=True):
        _cell = aCell
        _managedCellFailed = aManagedCellFailed
        _node = exaBoxNode(get_gcontext(), Cluctrl = self)
        _node.mConnect(aHost=_cell)

        # Verify that the cell is not in the boot
        # process (potentially caused by another
        # first cluster creation happenning on the same infra)
        mWaitForSystemBoot(_node)

        #
        # Check if Cell services are running
        #
        _cmd = 'cellcli -e list cell detail | grep Status'
        _i, _o, _e = _node.mExecuteCmdCellcli(_cmd)
        _output = _o.readlines()
        _status_dict = {}
        ebLogInfo('*** Verifying cell services status on cell: (%s)' % (_cell))
        if _output:
            for _status in _output:
                k, v = _status.split(':')
                k = k.strip()
                v = v.strip()
                _status_dict[k] = v
        else:
            ebLogWarn('*** Unable to get cellcli status on cell: (%s)' % (_cell))
            _node.mDisconnect()
            return

        _cell_failure = False
        for _service in ('cellsrvStatus', 'msStatus', 'rsStatus'):
            if _status_dict[_service] != 'running':
                _cell_failure = True
                ebLogWarn("The service '{0}' is not Running in cell '{1}'.".format(\
                    _service.replace('Status', '').upper(), _cell))

        if _cell_failure is True:
            _msg = '*** Stopped services detected on cell: (%s)' % (_cell)
            if aRestart:
                ebLogWarn('%s, Restarting all cell services ****' % (_msg))
                _node.mExecuteCmdLog('cellcli -e alter cell startup services all')
            else:
                #if no Restart, at least one service is not running properly
                ebLogError('%s after one restart attempt ****' % (_msg))
            #Action has been taken for this cell (either restart or logError)
            _managedCellFailed.append(_cell)

        _node.mDisconnect()

    def mCheckCellsServicesUp(self, aRestart=True, aCellList=None):
        """
        Checks the state of all the cells and make sure all services are running properly
        :param aRestart: if True, we try to restart services on each node
                         if any service was restarted, this function will call itself again with
                         aRestart=False in order to return the final status
        :param aCellList: if provided, will only check selected cells (the ones that had a restart)
        :return: True if all cell services are running or False if not.
        """


        # --- Start of Main Process function
        # --- Either take cell list from argument, or all cells
        ebLogVerbose("mCheckCellsServicesUp: aRestart = %s" % aRestart)

        _cellList = aCellList if aCellList else list(self.mReturnCellNodes().keys())
        _cellsFailed = []

        _plist = ProcessManager()
        _managedCellsFailed = _plist.mGetManager().list()

        for _cellName in _cellList:
            _p = ProcessStructure(self.mSingleCellCheckCellServiceUp, [_cellName, _managedCellsFailed, aRestart], _cellName)
            _p.mSetMaxExecutionTime(25*60)
            _p.mSetJoinTimeout(30)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)
        _plist.mJoinProcess()


        # shared _managedCellsFailed list will contains:
        # *aRestart == True  : Cells which need to be checked again
        # *aRestart == False : Cells still down after restart attempt:Error
        if len(_managedCellsFailed) > 0:
            _cellsFailed = list(_managedCellsFailed) # copy to non-managed list

        if _cellsFailed:
            if aRestart:
                #wait 30s and do a second pass on restarted nodes
                time.sleep(30)
                return self.mCheckCellsServicesUp(False,_cellsFailed)
            else:
                #cells are still down after restart: Error
                return False

        return True

    def mCheckCellDisks(self, aCellList=None,aCheckType="physicaldisk"):
        #
        # Checks whether all physical/cell disks have status 'normal'
        # Returns False if any disk status is not 'normal'
        # param aCellList: if provided, will only check selected cells
        # parm aCheckType: if provided, will do particular check
        #                  by default will do physicaldisk check.         
        #
        def _singleCellCheck(_cell, _managedCellFailed, _aCheckType):
            _node = exaBoxNode(get_gcontext(), Cluctrl = self)
            _node.mConnect(aHost=_cell)
            # Check if cell disks status is normal
            _cmd = 'cellcli -e list %s detail | grep -E \'(name|status)\'' % (_aCheckType)
            #Getting all lines with either disk name or disk status
            _i, _o, _e = _node.mExecuteCmdCellcli(_cmd)
            _output = _o.readlines()
            ebLogInfo('*** Verifying %s on cell: (%s)' % (_aCheckType, _cell))
            if _output:
                _diskName = None
                _diskStatus = None
                for _line in _output:
                    if "name" in _line:
                        _line = _line.lstrip().rstrip().split(" ")
                        _diskName = _line[-1]
                        _diskStatus = None
                    if "status" in _line:
                        _line = _line.lstrip().rstrip().split(" ")
                        _diskStatus = _line[-1]

                    if _diskStatus is not None and _diskName is not None:
                        if _diskStatus.lower() != "normal":
                            if self.mCheckConfigOption('ignore_m2_sys_error'):
                                if "m2_sys" not in _diskName.lower():
                                    _managedCellFailed.append((_cell, _diskName, _diskStatus))
                            else:
                                _managedCellFailed.append((_cell, _diskName, _diskStatus))
                        _diskStatus = None
                        _diskName = None
            else:
                ebLogError('*** Unable to get %s status on cell: (%s)' % (_aCheckType, _cell))
                _managedCellFailed.append((_cell, None, None))
            _node.mDisconnect()

        _cellList = aCellList if aCellList else list(self.mReturnCellNodes().keys())
        _cellsFailed = []

        _plist = ProcessManager()
        _managedCellsFailed = _plist.mGetManager().list()

        for _cellName in _cellList:
            _p = ProcessStructure(_singleCellCheck, [_cellName, _managedCellsFailed, aCheckType], _cellName)
            _p.mSetMaxExecutionTime(25*60)
            _p.mSetJoinTimeout(30)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)
        _plist.mJoinProcess()


        if len(_managedCellsFailed) > 0:
            _cellsFailed = list(_managedCellsFailed) # copy to non-managed list

        if len(_cellsFailed) > 0:
            for _cellDiskFailed in _cellsFailed:
                _cell, _disk, _status = _cellDiskFailed
                if _disk is None:
                    ebLogError('Could not fetch disk status from cell %s' % (_cell))
                else:
                    ebLogError('%s %s status on cell %s is %s / not normal.'% (aCheckType, _disk, _cell, _status))
            return False

        ebLogInfo("*** %s status check successful. All cell disks are in normal state." % aCheckType) 
        return True

    def mCheckCellConfig(self, aOptions, aStartup=False):
        """
        :param aOptions: Default Core context Options NS
        :param aStartup: Set to False by default. If True then cell, ms, rs Cell services are started if not running.
        :return: True if the cell is configured or False if it is not.
                 This is useful to determine if step Create Cell Disk needs to be run.
        """
        _rc = 0
        _cellcount = 0

        # Ref bug 36350280
        # We return asap in MVM, to avoid dropping the flash on the cells
        if self.mGetSharedEnv():
            ebLogInfo("Skipping mCheckCellConfig in MVM env")
            return False

        for _cell in sorted(self.mReturnCellNodes().keys()):

            _cellcount = _cellcount + 1

            _node = exaBoxNode(get_gcontext(), Cluctrl = self)
            _node.mConnect(aHost=_cell)
            #
            # Check if Cell services are running
            #
            _cmd = 'cellcli -e list cell detail | grep Status'
            _i, _o, _e = _node.mExecuteCmdCellcli(_cmd)
            _output = _o.readlines()
            _status_dict = {}
            if _output:
                for _status in _output:
                    k,v = _status.split(':')
                    k = k.strip()
                    v = v.strip()
                    _status_dict[k] = v
            # cellsrvStatus, msStatus, rsStatus
            if aStartup and _status_dict['cellsrvStatus'] == 'stopped':
                ebLogWarn('*** CELLSRV service on cell: %s is stopped (Restarting CELLSRV) ****' % (_cell))
                _node.mExecuteCmdLog('cellcli -e alter cell startup services all')
            #
            # Check for presence of Grid Disk to decide wether or not the Cells/Celldisks have been already configured
            #
            _cmd = 'cellcli -e list griddisk'
            _i, _o, _e = _node.mExecuteCmdCellcli(_cmd)
            _output = _o.readlines()
            if _output:
                if self.__verbose:
                    ebLogInfo('*** CheckCellConfig : Cell '+_cell+' already setup')
                _rc = _rc + 1
            else:
                if self.__verbose:
                    ebLogInfo('*** CheckCellConfig : Cell '+_cell+' not setup')
                #
                # Reset Flashcache/log (it will recreated during create cell)
                # - Check if Flashcache/log are present and drop if this is the case
                #
                _cmd_str = "cellcli -e 'list flashcache ; list flashlog'"
                _i, _o, _e = _node.mExecuteCmdCellcli(_cmd_str)
                _output = _o.readlines()
                if _output:
                    ebLogInfo('*** Flashcache/log needs to be reset for Cell: %s' % (_cell))
                    _cmd_list = [ 'alter flashcache ALL FLUSH', 'DROP FLASHCACHE ALL', 'DROP FLASHLOG ALL' ]
                    for _cmd_cell in _cmd_list:
                        _node.mExecuteCmdLog('cellcli -e '+_cmd_cell)

            _node.mDisconnect()

        if _rc and _rc != _cellcount:
            ebLogError('*** Unbalanced Cells Configuration status detected !!!')
            self.__storage.mDeleteForceGridDisks()
            for _cell in sorted(self.mReturnCellNodes().keys()):
                _node = exaBoxNode(get_gcontext(), Cluctrl = self)
                _node.mConnect(aHost=_cell)
                _cmd_list = [ 'alter flashcache ALL FLUSH', 'DROP FLASHCACHE ALL', 'DROP FLASHLOG ALL' ]
                for _cmd_cell in _cmd_list:
                    _node.mExecuteCmdLog('cellcli -e '+_cmd_cell)
                _node.mDisconnect()
            return False
        elif not _rc:
            if self.__verbose:
                ebLogInfo('*** Cells Configuration not done ('+str(_rc)+')')
            return False
        else:
            if self.__verbose:
                ebLogInfo('*** Cells Configuration already done ('+str(_rc)+')')
            return True


    def mCheckIsCellConfigured(self):
        """
        Check if the cell is configured, it will not be configured if it
        has recently re imaged.
        This method check the state of InfiniBand interfaces (stib0/1) and
        the configuration in the file '$OSSCONF/cellinit.ora'.
        """
        ebLogInfo("*** Checking if Cells are configurated")

        #Function for subProcess to check a single cell
        def _single_cell_check(_cell, _non_configured_cells):
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_cell)
            _cmd = "ip link show %s | grep -c LOWER_UP"
            if self.__debug:
                ebLogDebug("Checking cell: '%s'." % _cell)
            _is_configured = True
            for _inter in ["stib0", "stib1"]:
                _, _out, _ = _node.mExecuteCmd(_cmd % _inter)
                _output = _out.read().strip()
                if _output == '1':
                    continue

                _is_configured = False
                ebLogWarn("Interface '{0}' is not up in cell '{1}'.".format(\
                    _inter, _cell))

            _cmd = "grep -c ipaddress $OSSCONF/cellinit.ora"
            _, _out, _ = _node.mExecuteCmd(_cmd)
            _output = _out.read().strip()
            if _output != '2':
                _is_configured = False
                ebLogWarn("'cellinit.ora' does not have both ipaddress in cell '%s'" % _cell)

            _node.mDisconnect()

            if not _is_configured:
                _non_configured_cells.append(_cell)

        # --- Start of Main Process function

        _cell_list = list(self.mReturnCellNodes().keys())
        non_configured_cells = []

        _plist = ProcessManager()
        managed_non_configured_cells = _plist.mGetManager().list()

        for cell_name in _cell_list:
            _p = ProcessStructure(_single_cell_check, [cell_name, managed_non_configured_cells], cell_name)
            _p.mSetMaxExecutionTime(25*60)
            _p.mSetJoinTimeout(30)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)
        _plist.mJoinProcess()

        total_non_config_cells = len(managed_non_configured_cells)
        if total_non_config_cells > 0:
            non_configured_cells = list(managed_non_configured_cells)

        ebLogInfo("*** Cells configuration successfully checked")
        if non_configured_cells:
            return False

        return True


    def mPatchVMSSHKey(self, aOptions):

        if self.__tools_key_public:

            ebLogTrace("mPatchVMSSHKey - Public key in memory")

            # Add ssh public key to each domUs.
            _jconf = aOptions.jsonconf

            # Verify if sshcomment is present under the 'vm' field in the payload
            _ssh_comment = ''
            if _jconf and 'vm' in _jconf.keys():
                _ssh_comment = _jconf.get("vm", {}).get("sshcomment", "")

            # Check for legacy ssh_comment section in payload if not found under 'vm
            # section
            if _jconf and 'sshcomment' in _jconf.keys() and not _ssh_comment:
                _ssh_comment = _jconf['sshcomment']

            for _, _domU in self.mReturnDom0DomUPair():

                ebLogInfo(f'VM SSH_KEY Patching for: {_domU}')
                _node = exaBoxNode(get_gcontext())

                try:

                    _node.mResetHostKey(aHost=_domU)
                    _node.mConnect(aHost=_domU)

                    _node.mExecuteCmd("/usr/bin/mkdir -p /root/.ssh")
                    _node.mExecuteCmd("/usr/bin/mkdir -p /home/opc/.ssh")

                    _cmd = f'/bin/echo "{self.__tools_key_public} {_ssh_comment}" >> /root/.ssh/authorized_keys'
                    _node.mExecuteCmdLog(_cmd)

                    _cmd = f'/bin/echo "{self.__tools_key_public} {_ssh_comment}" >> /home/opc/.ssh/authorized_keys'
                    _node.mExecuteCmdLog(_cmd)

                    _node.mExecuteCmd("/usr/bin/chmod 600 /root/.ssh/authorized_keys")
                    _node.mExecuteCmd("/usr/bin/chmod 600 /home/opc/.ssh/authorized_keys")

                finally:
                    _node.mDisconnect()


    def mCopyFileToDomus(self,aFilename, aDestination,aMode='644', aDomUList=None, aUser="root"):

        if aDomUList is not None:
            _dbpair = aDomUList
        else:
            _dbpair = self.mReturnDom0DomUPair()

        for _pair in _dbpair:
            _domu = _pair[1]
            ebLogInfo('Copying file: '+aFilename+' to : '+_domu)
            _node = exaBoxNode(get_gcontext())
            _node.mSetUser(aUser)
            _node.mResetHostKey(aHost=_domu)
            _node.mConnect(aHost=_domu)
            _node.mCopyFile(aFilename,aDestination)
            _node.mExecuteCmd('chmod '+aMode+' '+aDestination)
            _node.mDisconnect()

    # hgaldame bug 25659813
    def mPostDBSSHKeyPatching(self,aOptions, aVmCmd, aUserList):
        _vmcmd = aVmCmd
        _vmid  = '_all_'
        _userlist = aUserList
        _users = 'users' in aOptions.jsonconf
        _vms   = 'vms' in aOptions.jsonconf
        _currentusers = None
        _currentvms = None
        #
        # Save JSON payload users/vms - not expected though
        #
        if _users is True:
            _currentusers = aOptions.jsonconf['users']
            ebLogWarn('*** users field not expected in JSON payload for DB create')
        if _vms is True:
            _currentvms = aOptions.jsonconf['vms']
            ebLogWarn('*** vms field not expected in JSON payload for DB create')

        for _user in _userlist:
            aOptions.jsonconf['users'] = [_user]

            if _vmcmd == 'addkey':

                aOptions.jsonconf['vms'] = ['_all_']
                self.mManageVMSSHKeys(_vmcmd, _vmid, aOptions,aMode=False)

            elif _vmcmd == 'deletekey':

                if not self.__fedramp:
                    ebLogWarn("fedRAMP requirement not defined/set.")

                _exakms = get_gcontext().mGetExaKms()

                for _, _vmid in self.mReturnDom0DomUPair():

                    aOptions.jsonconf['vms'] = [_vmid]

                    _cparam = {"FQDN": _vmid, "user": _user}
                    _entry = _exakms.mGetExaKmsEntry(_cparam)

                    if not _entry:
                        ebLogWarn(f"No key found for {_user}@{_vmid}")
                        continue

                    _sshkey = _entry.mGetPublicKey()

                    try:
                        self.mManageVMSSHKeys(_vmcmd, _vmid, aOptions, _sshkey, aMode=False)
                    except Exception as e:
                        ebLogError(e)
            else:
                ebLogError("Invalid command passed.")

        #
        # Restore JSON payload users/vms
        #
        if _users is True:
            aOptions.jsonconf['users'] = _currentusers
        else:
            aOptions.jsonconf.pop('users',None)
        if _vms is True:
            aOptions.jsonconf['vms'] = _currentvms
        else:
            aOptions.jsonconf.pop('vms',None)

    # hgaldame bug 25552652
    def mCheckGridVersion(self):
        _dbversion = self.__db_version
        _gridversion = None
        _result = False
        _gridversion = self.mGetGridVersion()
        if (_dbversion is not None and _gridversion is not None):
            ebLogInfo('dbversion: %s gridversion: %s' % (_dbversion, _gridversion))
            if _dbversion not in self.__compat[_gridversion]:
                return True
            else:
                return False
        return _result

    def mGetGridVersion(self, aDom0DomUPair=None):
        _gridversion = None
        #return None in case of healthcheck operation
        if ebCluCmdCheckOptions(self.__cmd, ['return_none_grid_version']):
            return None
        # Allow to Skip GI check - useful for very large configuration (e.g. SABRE)
        if self.mCheckConfigOption('skip_gv_check','True'):
            ebLogInfo('*** Skip Get Grid Version')
            return None
        try:
            if aDom0DomUPair:
                _ddpair = aDom0DomUPair
            else:
                _ddpair = self.mReturnDom0DomUPair()
            for _dom0,_domU in _ddpair:
                if not self.mPingHost(_domU):
                    # domUs are not expected to be up during CS.
                    if self.mIsDebug():
                        ebLogWarn('*** Cannot get grid version, unable to ping domU')
                    return None
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_domU)
                _cmd = "cat /etc/oratab | awk '/^\+ASM/ { split($0,a,\"/\"); print a[4] }'"
                _i, _o, _e = _node.mExecuteCmd(_cmd)
                _out = _o.readlines()
                if not _out or len(_out) == 0:
                    _node.mDisconnect()
                    ebLogError('*** ORATAB entry not found for grid')
                    return None
                _gridversion = _out[0].replace('.','')[:3]
                _node.mDisconnect()
                return _gridversion
        except Exception as e:
            ebLogError('*** Error while reading oratab file: %s' % (str(e)))
        return _gridversion

    def mCopyFinalVMConfig(self,aOedaLocation):

        ebLogVerbose("mCopyFinalVMConfig: aOedaLocation = %s" % aOedaLocation)

        for _dom0, _domu in self.mReturnDom0DomUPair():

            _final_conf = 'final-' + _domu + '-vm.xml'
            if self.mIsKVM():
                _final_conf= _domu + '-vm.xml'

            _remotefile = '/EXAVMIMAGES/conf/'+_final_conf
            _localfile  = aOedaLocation+'/WorkDir/'+_final_conf
            ebLogInfo('Fetching remote final-vm-xml file: '+_remotefile+' to OEDA staging WorkDir')
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)

            try:
                _node.mCopy2Local(_remotefile,_localfile)
            except Exception as e:
                ebLogError('*** Error fetching final-vm-xml file : %s' % (str(e)))

            _node.mDisconnect()

    def mCopyMemInfoFile(self, aOedaLocation):

        _remotefile='/proc/meminfo'
        for _dom0, _domu in self.mReturnDom0DomUPair():
            #From OEDA code, filename is (hostname before .)-memInfo.txt
            _short_domu = _domu.split('.')[0]
            _localfile= aOedaLocation +'/WorkDir/'+_short_domu+'-memInfo.txt'

            if not os.path.exists(_localfile):
                ebLogInfo('Copying meminfo to: '+_localfile)
                _node = exaBoxNode(get_gcontext())
                _node.mResetHostKey(aHost=_domu)
                _node.mConnect(aHost=_domu)
                try:
                    _node.mCopy2Local(_remotefile,_localfile)
                except Exception as e:
                    ebLogError('*** Error while copying memInfo file: %s' % (str(e)))

                _node.mDisconnect()

    def mCopyOCDELogFile(self):

        _amode='600'
        for _dom0, _domu in self.mReturnDom0DomUPair():
            _remotefile='/var/opt/oracle/ocde/ocde.out'
            _remotefile_to_fallback='/var/opt/oracle/ocde/ocde_'+self.__dbname+'.out'
            _oedaReqPath = self.mGetOEDARequestsPath()
            _localprfx=f'{_oedaReqPath}/log/ocde/'
            _localfile=f'{_localprfx}ocde_{_domu}.out'
            _cmd = f"/bin/mkdir -p {_localprfx}"
            self.mExecuteLocal(_cmd)                  
            _node = exaBoxNode(get_gcontext())
            _node.mResetHostKey(aHost=_domu)

            ebLogInfo('*** Copying ocde log files from domu to OEDA staging folder {0}'.format(_localprfx))
            with connect_to_host(_domu, get_gcontext(), "root") as _node:
                ebLogInfo('*** Copying remote log files to OEDA staging log from host:{0}'.format(_node.mGetHostname()))
                ebLogInfo('*** Checking if file:{0} exists in {1}'.format(_remotefile,_node.mGetHostname()))

                if not _node.mFileExists(_remotefile):
                    ebLogWarn('*** Warning ***: File: {0} not found. Falling back to file:{1}'.format(_remotefile,_remotefile_to_fallback))
                    ebLogInfo('*** Checking if file:{0} exists in {1}'.format(_remotefile_to_fallback,_node.mGetHostname()))

                if not _node.mFileExists(_remotefile_to_fallback):
                    ebLogWarn('*** : File: {0} not found, skipping it'.format(_remotefile_to_fallback))
                    _remotefile = None
                else:
                    _remotefile = _remotefile_to_fallback

                if _remotefile:
                    try:
                        ebLogInfo('*** Copying remote log file:{0} to OEDA staging log'.format(_remotefile))
                        _node.mCopy2Local(_remotefile,_localfile)
                        _node.mExecuteCmd('/usr/bin/chmod '+_amode+' '+_localfile)
                    except Exception as e:
                        ebLogError('*** Error while copying OCDE file: %s' % (str(e)))

                _additional_log_files='/var/opt/oracle/log/*/ocde/ocde_*.log'
 
                ebLogInfo('*** Fetching additional logs from: {0}'.format(_additional_log_files))
                # Fetch additional logs from ocde if present
                try:
                    _cmdstr = '/usr/bin/ls {0}'.format(_additional_log_files)
                    _i, _o, _e = _node.mExecuteCmd(_cmdstr)
                    _out = _o.readlines()
                    if _out and len(_out):
                        try:
                            for _line in _out:
                                _sline = _line.split('/')
                                _dbname  = _sline[5]
                                _logname = _sline[-1].strip()
                                _localname = _localprfx + 'db_'+_dbname+'_'+_logname
                                ebLogInfo('*** Copying remote log file: '+_line.strip()+' to OEDA staging folder.')
                                _node.mCopy2Local(_line.strip(),_localname)
                                _node.mExecuteCmd('/usr/bin/chmod '+_amode+' '+_localname)
                        except Exception as e:
                            ebLogError('*** Error while copying OCDE file: %s' % (str(e)))
                except Exception as e:
                    ebLogWarn('*** Error while copying extra OCDE logs: %s' %(str(e)))

                # Fetch cloud_properties logs from ocde if present
                _additional_log_files='/var/opt/oracle/log/dbaasapi/os/cloud_properties/*.log'
                ebLogInfo('*** Fetching dbaas cloud_properties logs from: {0}'.format(_additional_log_files))
                try:
                    _cmdstr = '/usr/bin/ls {0}'.format(_additional_log_files)
                    _i, _o, _e = _node.mExecuteCmd(_cmdstr)
                    _out = _o.readlines()
                    if _out and len(_out):
                        try:
                            for _line in _out:
                                _sline = _line.split('/')
                                _logname = _sline[-1].strip()
                                _localname = _localprfx + _logname
                                ebLogInfo('*** Copying cloud property log file: '+_line.strip()+' to OEDA staging folder.')
                                _node.mCopy2Local(_line.strip(),_localname)
                                _node.mExecuteCmd('/usr/bin/chmod '+_amode+' '+_localname)
                        except Exception as e:
                            ebLogError('*** Error while copying cloud property OCDE file: %s' % (str(e)))
                except Exception as e:
                    ebLogWarn('*** Error while copying cloud property OCDE logs: %s' %(str(e)))


    def mCopyDBLogFiles(self,aParams):
        _localpath = self.mGetBasePath()+'/clusters/'+self.__key+"/log"
        _logs='/var/opt/oracle/log'
        _extra_logs=['/u02/app/oracle/cfgtoollogs']

        _db= ' '+aParams['dbName'] if 'dbName' in aParams and aParams['dbName'] != None else ' *'
        _tmplog=_logs+'/'+time.strftime("%y%m%d_%H%M%S")+'_dblogs.tgz'

        for _dom0, _domu in self.mReturnDom0DomUPair():
            if 'vmName' in aParams and aParams['vmName']!= None:
                if _domu != aParams['vmName'] and not aParams['vmName'] in _domu:
                    continue

            _node = exaBoxNode(get_gcontext())
            _node.mResetHostKey(aHost=_domu)
            _node.mConnect(aHost=_domu)
            try:
                _extrastr = ' '.join(['-C ' + os.path.dirname(s) +' ./'+os.path.basename(s) for s in _extra_logs])
                _cmd ='cd '+_logs+ '; tar cvfz ' +_tmplog+ _db +' ' + _extrastr + ' &> /dev/null'
                ebLogInfo('Tar db logs command: '+_cmd)
                _node.mExecuteCmd("sh -c \'" + _cmd + "\'")
                self.mExecuteLocal('/bin/mkdir -p '+ _localpath+'/'+_domu)
                _node.mCopy2Local(_tmplog,_localpath+'/'+_domu+'/'+os.path.basename(_tmplog))
                _node.mExecuteCmd('rm  '+ _tmplog)
            except Exception as e:
                ebLogError('*** Error while copying DB files: %s' % (str(e)))

            _node.mDisconnect()
        return _localpath

    def mCopyDataguardLogfiles(self):
        _dbname = self.__dbname
        _remotefiles = ['/var/opt/oracle/log/' + _dbname +'/dgcc/*' , '/var/opt/oracle/log/' + _dbname +'/dgdeployer/*']
        _oedaReqPath = self.mGetOEDARequestsPath()
        _localprfx=f'{_oedaReqPath}/log/'
        _amode = '600'
        for _dom0, _domu in self.mReturnDom0DomUPair():
            _node = exaBoxNode(get_gcontext())
            _node.mResetHostKey(aHost=_domu)
            _node.mConnect(aHost=_domu)
            for _file in _remotefiles:
                _cmdstr = 'ls ' + _file
                _i, _o, _e = _node.mExecuteCmd(_cmdstr)
                _out = _o.readlines()
                if _out and len(_out):
                    try:
                        for _line in _out:
                            _sline = _line.split('/')
                            _logname = _sline[-1].strip()
                            _localname = _localprfx + 'db_'+_dbname+'_'+_logname
                            ebLogInfo('Copying log remote file: '+_line.strip()+' to OEDA staging log.')
                            _node.mCopy2Local(_line.strip(),_localname)
                            _node.mExecuteCmd('chmod '+_amode+' '+_localname)
                    except Exception as e:
                        ebLogWarn('*** Error while copying  Dataguard logs %s' %(str(e)))
            _node.mDisconnect()

    def mPatchXMLForNodeSubset(self, aOptions):
        '''patch XML based on participating nodes'''

        ebLogVerbose("mPatchXMLForNodeSubset: Patch XML for Node Subset")
        _participant_node_count = 0
        _participant_node_list = []
        _non_participant_dom0_list  = []
        _non_participant_domu_list  = []
        _non_participant_nodes_list = []

        if aOptions.jsonconf is not None and 'reshaped_node_subset' in list(aOptions.jsonconf.keys()):
            if self.__skip_dom0_validation:
                for _dom0, _domu in self.__delete_node_name:
                    _non_participant_nodes_list.append([_dom0, _domu])
            else:
                for _dom0, _domu in self.mReturnDom0DomUPair():
                    _non_participant_domu_list.append(self.__machines.mGetMacIdFromMacHostName(_domu))
                    _non_participant_dom0_list.append(self.__machines.mGetMacIdFromMacHostName(_dom0))
                    _non_participant_nodes_list.append([_dom0, _domu])
        else:
            nsParams = aOptions.jsonconf['node_subset']
            _participant_node_count = nsParams['num_participating_computes']

            if _participant_node_count != len(nsParams['participating_computes']):
                ebLogError("*** NodeSubset Json Payload: num_participating_computes mismatch with participating_computes count")
                raise ExacloudRuntimeError(0x0765, 0xA, "NodeSubset Json Payload: num_participating_computes"\
                "mismatch with participating_computes count")

            for _node in  nsParams['participating_computes']:
                _participant_node_list.append(_node['compute_node_hostname'])

            ebLogInfo('participating_computes: %s' %(str(_participant_node_list)))
            _pair = self.mReturnDom0DomUPair()
            for _dom0, _domu in _pair:
                if _dom0 in _participant_node_list:
                    continue
                _non_participant_domu_list.append(self.__machines.mGetMacIdFromMacHostName(_domu))
                _non_participant_dom0_list.append(self.__machines.mGetMacIdFromMacHostName(_dom0))
                _non_participant_nodes_list.append([_dom0, _domu])
        try:
            _uuid = self.mGetUUID()
            _oeda_path = self.mGetOedaPath()
            _patchconfig = self.mGetPatchConfig()
            _savexmlpath = _oeda_path + '/exacloud.conf'
            _nodesubsetxml = _savexmlpath + '/nodesubset_' + _uuid + '.xml'

            self.mExecuteLocal("/bin/mkdir -p {0}".format(_savexmlpath), aCurrDir=self.mGetBasePath())
            self.mExecuteLocal("/bin/cp {0} {1}".format(_patchconfig, _nodesubsetxml), aCurrDir=self.mGetBasePath())

            _oedacli_bin = os.path.join(_oeda_path, 'oedacli')
            with TemporaryDirectory() as tmp_dir:
                _oedacli_mgr = OedacliCmdMgr(_oedacli_bin, tmp_dir)
                for _dom0, _domU in _non_participant_nodes_list:
                    _oedacli_mgr.mDelNode(_domU, _dom0, aSrcXml=_nodesubsetxml, aDestXml=_nodesubsetxml, aDeploy=False)
            self.mUpdateInMemoryXmlConfig(_nodesubsetxml, aOptions)

            #update Dom0DomUPair and Dom0DomUNATPair forcefully
            _ddp_nat = self.mReturnDom0DomUNATPair(True)
            _ddp = self.mReturnDom0DomUPair(True)
            self.mSetOrigDom0sDomUs(_ddp)
            _host_list = self.mGetHostList(True)

            #update clusterid on basis of participant nodes
            self.mBuildClusterId()

            #store keys to newly created cluster
            self.mSaveOEDASSHKeys()

            """
            35634247: With new OEDA build, configuring quorum devices will be
            handled by OEDA which requires quorum flag in es.properties
            if self.__enable_quorum:
                if _participant_node_count == 1 or len(_ddp) == 1:
                    ebLogWarn('*** Disabling Quorum Disk supported for Single Node installation')
                    self.__enable_quorum = False
            """
        except Exception as e:
            ebLogError('*** Error *** : XML patching for node subset failed %s ' % (str(e)))
            raise ExacloudRuntimeError(0x0764, 0xA, 'Exacloud Operation Failed : Failed to patch XML for node subset')

    #
    #dynximages begin
    #

    def mGISupportDetection(self):
        ol7_supported = False

        # If Dom0 is 18.2 images => 19c
        if self.mCheck19cCapableComputeImage():
            ol7_supported = True
            self.__19cVMGI = True

        self.mImageSeparationInit(ol7_supported)
        self.mGenerateSymLinks()

    def mImageSeparationInit(self, aOl7Support):
        """ Method to parse inventory.json and create required data structures.
            Create following data structures :
                1. dbgi_config: dict
                2. imagefiles: list
        """
        # Load inventory.json and determine type of image support.
        self.mLoadRepoInventory()
        if not self.mGetRepoInventory():
            raise ExacloudRuntimeError(0x0111, 0xA, 'No image repository configured. Check repository_root value at exabox.conf')
        
        if self.mGetGiMultiImageSupport():
            # Separate method calls for multiple GI image structure.
            self.mLoadGIConfig()
            # Condition to skip image dictionary generation if command has no_check_sw_cell
            if ebCluCmdCheckOptions(self.__cmd, ['no_check_sw_cell']):
                ebLogInfo(f'*** Skip image dictionary generation for {self.__cmd} command')
            else:
                self.mGenerateMultiImageDictionary()
        else:
            # Default flow for single image support 
            self.mLoadDBGIConfig()
            self.mLoadCompatInfo(aOl7Support)
            # Condition to skip image dictionary generation if command has no_check_sw_cell
            if ebCluCmdCheckOptions(self.__cmd, ['no_check_sw_cell']):
                ebLogInfo(f'*** Skip image dictionary generation for {self.__cmd} command')
            else:
                self.mGenerateImageDictionary()

    def mVerifyDBBitsOnDomU(self):

        if not self.__ociexacc:
            ebLogWarn("DB Image replacement is not supported for non exacc environments.")
            return
        if not self.__repo_inventory:
            ebLogWarn("Image repository inventory not generated, skipping DB bits validation.")
            return

        _vms = [domU for _, domU in self.mReturnDom0DomUPair()]
        for _vm in _vms:
            with connect_to_host(_vm, get_gcontext()) as _db_node:
                _images_to_sha256sum = dict()
                _pattern_list = ["db*.zip", "exadbf*.tar.gz"]
                self.mGetDBImageInformationFromDomU(_db_node, _images_to_sha256sum, _pattern_list)
                _image_list = list(_images_to_sha256sum.keys())
                if len(_image_list) == 0:
                    ebLogWarn(f"Image list empty for host {_vm}")
                    continue

                _local_imagerepo_information = self.mGetLocalDBNIDInformation(_image_list)
                for _image in _image_list:
                    _local_image_info = _local_imagerepo_information.get(_image, None)
                    _remote_image_info = _images_to_sha256sum[_image]
                    self.mCheckAndReplaceDBImages(_db_node, _local_image_info, _remote_image_info)

    def mCheckAndReplaceDBImages(self, _node: exaBoxNode, _local_image_info: dict, _remote_image_info: dict) -> None:
        if _local_image_info is None or _remote_image_info is None:
            return

        _chown_cmd = node_cmd_abs_path_check(_node, "chown")
        _chmod_cmd = node_cmd_abs_path_check(_node, "chmod")
        if _local_image_info.get("sha256sum") != _remote_image_info.get("sha256sum"):
            ebLogInfo(f"Remote image file: {_remote_image_info['target_path']} with sha256sum: {_remote_image_info.get('sha256sum')}\
             does not match CPS image file: {_local_image_info['target_path']} with sha256sum: {_local_image_info.get('sha256sum')}")
            if node_replace_file(_node, _local_image_info["target_path"], _remote_image_info["target_path"]):
                _remote_abs_path = _remote_image_info["target_path"]
                _cmd = f"{_chown_cmd} oracle:oinstall {_remote_abs_path}"
                _node.mExecuteCmd(_cmd)
                if _node.mGetCmdExitStatus() != 0:
                    ebLogWarn(f"Command: {_cmd} execution failed.")
                _cmd = f"{_chmod_cmd} 700 {_remote_abs_path}"
                _node.mExecuteCmd(_cmd)
                if _node.mGetCmdExitStatus() != 0:
                    ebLogWarn(f"Command: {_cmd} execution failed.")
                ebLogInfo(f"Successfully replaced remote image file: {_remote_abs_path}")
            else:
                ebLogError(f"Failed to replace remote image file: {_remote_abs_path}")
        else:
            ebLogInfo(f"sha256sum is equal for both remote:{_remote_image_info['target_path']} and local:{_local_image_info['target_path']} image files.")

    def mGetLocalDBNIDInformation(self, _image_names: list) -> dict():

        ebLogTrace(f"Image names to find: {_image_names}")
        _local_images_information = dict()
        _current_service = "EXACS"
        if self.isATP():
            _current_service = "ATP"
        for _img_node in self.__repo_inventory["dbnid"]:
            _img_service = _img_node["service"]
            if _current_service not in _img_service:
                continue

            for _img in _img_node["files"]:
                _local_image_name = _img["path"]
                _local_image_sha256sum = _img["sha256sum"]
                _index_of_filesep = _local_image_name.rfind("/")
                if _index_of_filesep != -1:
                    _local_image_name = _local_image_name[_index_of_filesep+1:]
                if _local_image_name in _image_names:
                    _local_file_path = os.path.join(os.path.join(self.__repo_download_location, "dbnid"), _img["path"])
                    _local_images_information[_local_image_name] = {"target_path": _local_file_path, "sha256sum": _local_image_sha256sum}

        ebLogTrace(f"Local images information: {_local_images_information}")
        return _local_images_information

    def mGetDBImageInformationFromDomU(self, _node: exaBoxNode, _images_to_sha256sum: dict, _file_list: list) -> None:

        _find_cmd = node_cmd_abs_path_check(_node, "find")
        _readlink_cmd = node_cmd_abs_path_check(_node, "readlink")
        _sha256sum_cmd = node_cmd_abs_path_check(_node, "sha256sum")

        for _pattern in _file_list:

            dbaas_acfs_dbnid_dir = "/var/opt/oracle/dbaas_acfs/dbnid/"
            _cmd = f"{_find_cmd} {dbaas_acfs_dbnid_dir} -maxdepth 1 -name \"{_pattern}\" -print"
            _in, _out, _err = _node.mExecuteCmd(_cmd)
            if _node.mGetCmdExitStatus() != 0:
                ebLogWarn(f"Command: {_cmd} execution failed.")
                continue
            _output_lines = _out.readlines()
            ebLogTrace(f"Output of: {_cmd} -> {_output_lines}")

            for _matching_file in _output_lines:
                _cmd = f"{_readlink_cmd} -f {_matching_file}"
                _in, _out, _err = _node.mExecuteCmd(_cmd)
                if _node.mGetCmdExitStatus() != 0:
                    ebLogWarn(f"Command: {_cmd} execution failed.")
                    continue
                _target_file_path = _out.readlines()[0]
                _target_file_path = _target_file_path.rstrip('\n')
                _file_name = _target_file_path.split("/")[-1]
                _cmd = f"{_sha256sum_cmd} {_target_file_path}"
                _in, _out, _err = _node.mExecuteCmd(_cmd)
                if _node.mGetCmdExitStatus() != 0:
                    ebLogWarn(f"Command: {_cmd} execution failed.")
                    continue
                _compiled_output = _out.readlines()[0]
                _file_sha256sum = _compiled_output.split(" ")[0]
                _file_sha256sum = _file_sha256sum.rstrip('\n')
                _images_to_sha256sum[_file_name] = {"target_path": _target_file_path, "sha256sum": _file_sha256sum}
                ebLogTrace(f"Image name: {_file_name}, Absolute path: {_target_file_path}, current sha256sum: {_file_sha256sum}")

    def mGenerateImageDictionary(self):
        #do nothing if this is fallback flow (i.e. there's no repo inventory)
        if not self.__repo_inventory:
            return

        #initialize empty list of images (each image is a dictionary)
        ebLogInfo("*** Generating image mapping...")
        self.__imagefiles = []

        #save request path to be used in the mapping (self.__repo_download_location is also used)
        _oedaReqPath = self.mGetOEDARequestsPath()
        request_workdir = f"{_oedaReqPath}/WorkDir/" #request workdir (for example: oeda/requests/0000-0000-0000-0000_3d0131ac-7b69-11e9-9dc7-fa163eb08496/WorkDir/)

        #take grid-klones only.
        for imgtype in ["grid-klones"]:

            #do nothing if current patch doesn't include said image type
            if imgtype not in self.__repo_inventory:
                continue

            #take every node (one node can have multiple images) of said image type
            for imgnode in self.__repo_inventory[imgtype]:

                #determine name of the bundle patch (might be overriden if image path includes it)
                bpname = imgnode["bpname"]

                #determine day-of-year of the image node's bundle patch
                dayofyear = imgnode['xmeta']['oeda_version'].split('.')[1]

                #determine full version number of image node (in format XX.X.X.X.XXX)
                imgversion = imgnode["version"] + '.' + dayofyear

                #determine abbreviated version number (112, 121, 122, 181 or 190)
                imgshortver = imgversion.replace(".", "")[:3]

                #create entry for each image in this node
                for img in imgnode["files"]:

                    #determine filename of each image
                    #also override bundle patch name if provided
                    bpnameOverriden = False
                    if "/" in img["path"]:
                        bpname, imgfilename = img["path"].split("/")
                        bpnameOverriden = True
                    else:
                        imgfilename = img["path"]

                    #determine cdb mode for image
                    if ("exadbf" in imgfilename) and ("non_cdb" in imgfilename):
                        imgcdb = "noncdb"
                    else:
                        imgcdb = "cdb"

                    #determine map path, dom0 path and local path for image
                    if imgtype in ["grid-klones"]:
                        imgmap = "/EXAVMIMAGES/"+imgfilename
                        imgdom0 = "/EXAVMIMAGES/"+imgfilename
                        imglocal = self.__repo_download_location+"/"+imgtype+"/"+imgfilename
                        if bpnameOverriden and imgtype == "grid-klones":
                            imglocal = self.__repo_download_location+"/"+imgtype+"/{}/".format(bpname)+imgfilename

                    #assemble all gathered metadata into dictionary
                    curr_img = {
                        "filename"     : imgfilename,
                        "shortversion" : imgshortver,
                        "longversion"  : imgversion,
                        "map"          : imgmap,
                        "dom0"         : imgdom0,
                        "local"        : imglocal,
                        "service"      : imgnode["service"],
                        "cdb"          : imgcdb,
                        "sha256sum"    : img["sha256sum"]
                    }

                    # add global cache path
                    _global_cache = self.mCheckConfigOption("global_cache_dom0_folder")

                    if _global_cache:
                        _dom0_remote_path = os.path.basename(imgfilename)
                        curr_img["dom0"] = os.path.join(_global_cache, _dom0_remote_path)

                    #insert new entry into file list
                    self.__imagefiles.append(curr_img)
                #for
            #for
        #for
        if self.__debug:
            ebLogInfo("*** Image mapping successfully generated.")
            ebLogInfo("Image mapping structure: \n"+json.dumps(self.__imagefiles, indent=4))
    
    def mGenerateMultiImageDictionary(self):
        """
            Create a list of dictionaries that contain image information.
            Used for uploading images to dom0.
        """
        #do nothing if this is fallback flow (i.e. there's no repo inventory)
        if not self.__repo_inventory:
            return
        
        #initialize empty list of images (each image is a dictionary)
        ebLogInfo("*** Generating image mapping for multiple Images...")
        self.__imagefiles = []
        
        # iterate over inventory.json
        for imgnode in self.__repo_inventory['grid-klones']:
            imgversion = imgnode["version"]
            #determine abbreviated version number (112, 121, 122, 181 or 190)
            imgshortver = imgversion.replace(".", "")[:3]
            
            #create entry for each image in this node
            for img in imgnode["files"]:
                imgfilename = img["path"]
                #determine cdb mode for image
                if ("exadbf" in imgfilename) and ("non_cdb" in imgfilename):
                    imgcdb = "noncdb"
                else:
                    imgcdb = "cdb"
                
                #determine map path, dom0 path and local path for image
                imgfilename_only = os.path.basename(imgfilename)
                _dom0_path = os.path.join("/EXAVMIMAGES", imgfilename_only)
                imgmap = _dom0_path
                imgdom0 = _dom0_path
                imglocal = os.path.join(self.__repo_download_location, imgfilename)
                
                #assemble all gathered metadata into dictionary
                curr_img = {
                    "filename"     : imgfilename,
                    "longversion"  : imgversion,
                    "shortversion" : imgshortver,
                    "map"          : imgmap,
                    "dom0"         : imgdom0,
                    "local"        : imglocal,
                    "service"      : imgnode["service"],
                    "cdb"          : imgcdb,
                    "sha256sum"    : img["sha256sum"]
                }
                
                # add global cache path
                _global_cache = self.mCheckConfigOption("global_cache_dom0_folder")
                if _global_cache:
                    _dom0_remote_path = os.path.basename(imgfilename)
                    curr_img["dom0"] = os.path.join(_global_cache, _dom0_remote_path)
                
                #insert new entry into file list
                self.__imagefiles.append(curr_img)
            ebLogTrace("Image mapping structure: \n"+json.dumps(self.__imagefiles, indent=4))
    
    def mReadGenericJsonFile(self, filepath):
        __json = None
        try:
            __file = open(filepath)
            __json = json.loads(__file.read())
            __file.close()
        except Exception as e:
            ebLogWarn('*** Can not access/read ' + filepath + ' file')
            ebLogDebug('*** Caught exception: ' + str(e))
            return {}
        return __json

    # Read inventory.json from active bundle, assumming it is specified and available.
    def mLoadRepoInventory(self):
        _repository_root = self.mCheckConfigOption('repository_root')
        if not _repository_root:
            ebLogWarn('*** repository_root key not found in exabox.conf. Inventory not loaded.')
            return
        ebLogInfo(f'*** Images repository root found in exabox.conf:{_repository_root}. Parsing activeVersion.json')
        if not self.mIsOciEXACC():
            if os.path.exists(os.path.join(_repository_root,"EXACS")):
                self.mSetGiMultiImageSupport(True)
                ebLogInfo(f'*** Multiple GI Versions supported in repo : {_repository_root}')
            else:
                ebLogInfo(f'*** Single GI Version supported in repo : {_repository_root}')
        # active version parsing
        __active_version_path = os.path.join(_repository_root, 'activeVersion.json')
        if os.path.isfile(__active_version_path):
            ebLogInfo(f'*** activeVersion.json found at: {__active_version_path}')
            __active_version = self.mReadGenericJsonFile(__active_version_path)
            # dbgi is the default download location.
            _repolist = ["dbgi"]

            # Bug34752683:
            # For ADBD Service, we always want to look first in adbd section.
            # if more services or locations exist in the future, please add them
            # before this append.
            # adbd is first processed because repolist is reversed (see below).
            # dbgi repo location, which is the default, will be the last
            if self.isATP():
                _repolist.append("adbd")

            for _repo in reversed(_repolist):
                if 'active' in __active_version \
                        and _repo in __active_version['active'] \
                        and 'download_location' in __active_version['active'][_repo]:
                    ebLogInfo('*** DB/GI download location found in activeVersion.json: ' +
                                __active_version['active'][_repo]['download_location'] +
                                '. Parsing inventory.json')
                    self.__repo_download_location = __active_version['active'][_repo]['download_location']
                    _inventory_json_path = os.path.join(self.mGetRepoDownloadLocation(), 'inventory.json')
                    if not os.path.exists(_inventory_json_path):
                        ebLogWarn('*** inventory.json file not found at ' + _inventory_json_path +
                                    '. Inventory not loaded yet.')
                        continue
                    if os.path.exists(os.path.join(self.mGetRepoDownloadLocation(),"EXACS")):
                        self.mSetGiMultiImageSupport(True)
                        ebLogInfo(f'*** Multiple GI Versions supported in repo at: {self.mGetRepoDownloadLocation()}')

                    self.__repo_inventory = self.mReadGenericJsonFile(_inventory_json_path)
                    # end loop if a valid inventory.json was found in download_location
                    if not self.__repo_inventory:
                        ebLogWarn('*** Inventory not loaded yet.')
                    else:
                        break
                else:
                    ebLogWarn('*** Download location not found in ' + __active_version_path +
                                '. Inventory not loaded yet.')
        else:
            ebLogWarn('*** activeVersion.json file not found at ' + __active_version_path +
                        '. Inventory not loaded yet.')

        # If for some reason, we have not loaded the repo_inventory yet, try going directly to inventory.json
        if not self.__repo_inventory:
            _inventory_json_path = os.path.join(_repository_root, 'inventory.json')
            if os.path.exists(_inventory_json_path):
                ebLogInfo('*** Parsing inventory.json file found at: ' + _inventory_json_path)
                # No indirection needed. Download location is just
                self.__repo_download_location = _repository_root
                self.__repo_inventory = \
                    self.mReadGenericJsonFile(os.path.join(self.mGetRepoDownloadLocation(), 'inventory.json'))
            else:
                ebLogWarn('*** inventory.json file not found at ' + _inventory_json_path +
                            '. Inventory not loaded.')
        # Check repository type 
        #TODO: Improve this logic to verify from inventory.json
        if self.mGetRepoDownloadLocation() and os.path.exists(os.path.join(self.mGetRepoDownloadLocation(),"EXACS")):
            self.mSetGiMultiImageSupport(True)
            ebLogInfo(f'*** Multiple GI Versions supported in repo : {self.mGetRepoDownloadLocation()}')
        else:
            ebLogInfo(f'*** Single GI Version supported in repo : {self.mGetRepoDownloadLocation()}')
            

    def mLoadDBGIConfig(self):
        for __img in ['grid', 'db']:
            if __img + '-klones' in self.mGetRepoInventory():
                self.__dbgi_config[__img] = {}
                for __klone in self.mGetRepoInventory()[__img + '-klones']:

                    # Check if image is compatible with current service. Ignore it otherwise.
                    if ('service' in __klone.keys()) and (self.mGetServiceType() not in __klone['service']):
                        continue
                    #if

                    # Key will be inferred from version field, narrowed down to three digit string,
                    # so for example 19.0.0.0 will turn into 190 and 11.2.0.4 into 112
                    # Assumes version contains dots and after getting them out, str will be at least 3 digits long
                    __key = __klone['version'].replace('.', '')[0:3]
                    self.__dbgi_config[__img][__key] = (
                        __klone['version'],
                        __klone['xmeta']['oeda_version'],
                        __klone['xmeta']['oeda_version'].rsplit('.', 1)[1], #This assumes there is at least 1 dot.
                        __klone['xmeta']['oeda_date'] if 'oeda_date' in __klone['xmeta'] else __klone['cdate']
                    )
        if  self.__debug:
            ebLogInfo('*** DBGI config dictionary: ' + json.dumps(self.__dbgi_config, indent=4))

    def mLoadGIConfig(self):
        """
            Create dictionary mapping image version with related metadata.
            Dictionary used for verifying image versions from payload/domU.
            To set path in domu eg. /u01/app/19/0.0.0/grid
        """
        if 'grid-klones'  not in self.mGetRepoInventory():
            ebLogInfo(f'Grid klones not present in inventory.json !')
            return
        self.__dbgi_config['grid'] = {}
        for klone in self.mGetRepoInventory()['grid-klones']:
            # Check if image is compatible with current service. Ignore it otherwise.
            if ('service' in klone.keys()) and (self.mGetServiceType() not in klone['service']):
                continue
            
            version = klone['version']
            version_excluding_date = ".".join(klone['version'].split(".")[:-1])
            version_parts = klone['version'].split(".")
            major_version_only = f"{version_parts[0]}.0.0.0"
            oeda_date =  klone['version'].split(".")[-1]
            self.__dbgi_config['grid'][version] = (
                klone['version'],
                version_excluding_date,
                major_version_only,
                oeda_date
            )
        if  self.__debug:
            ebLogInfo('*** DBGI config dictionary: ' + json.dumps(self.__dbgi_config, indent=4))

    def mLoadCompatInfo(self, aOL7Supported):
        if 'grid-klones' in self.mGetRepoInventory():
            # Start overriding the structure
            self.__compat = {}

            for __klone in self.mGetRepoInventory()['grid-klones']:

                # Check if image is compatible with current service. Ignore it otherwise.
                if ('service' in __klone.keys()) and (self.mGetServiceType() not in __klone['service']):
                    continue
                #if

                # If not running an OL7 environment and this entry requires OL7, skip it entirely
                if not aOL7Supported and 'ol7_required' in __klone['xmeta'] and __klone['xmeta']['ol7_required']:
                    ebLogInfo('*** OL7 is required yet not supported. Skipping grid-klone ' +
                              __klone['version'] + ' from compat processing.')
                    continue

                # Key will be inferred from version field, narrowed down to three digit string,
                # so for example 19.0.0.0 will turn into 190 and 11.2.0.4 into 112
                # Assumes version contains dots and after getting them out, str will be at least 3 digits long
                __key = __klone['version'].replace('.', '')[0:3]
                self.__compat[__key] = tuple(__klone['xmeta']['supported_db'])

        else:
            ebLogInfo('*** No grid-klones in repo inventory, keeping the compat defaults')

        if self.__debug:
            ebLogInfo('*** Compat dictionary: ' + json.dumps(self.__compat, indent=4))

    def mGenerateSymLinks(self):
        """
        Method used to Generate the Symlink files for OEDA under
        oeda/requests/<req>/Workdir/

        :returns int:
            0 - success
            1 - Do nothing if the step doesn't use OEDA
            2 - Do nothing if cluster is exacompute/exascale
        """
        # If no OEDA request, no need to generate symlinks
        if self.mIsNoOeda():
            return 1

        # If we're in exacompute/exadb-xs, skip this symlink generation
        if self.mIsExaScale():
            ebLogInfo(f"Skipping OEDA Grid-klone symlink generation")
            return 2

        # OEDA request work dir must exist, so populate the symlinks there
        for __key in ['grid-klones']:
            if __key in self.mGetRepoInventory():
                for __klone in self.mGetRepoInventory()[__key]:

                    # Check if image is compatible with current service. Ignore it otherwise.
                    if ('service' in __klone.keys()) and (self.mGetServiceType() not in __klone['service']):
                        continue
                    #if

                    for __file in __klone['files']:
                        # Delete potential conflicting links before creating new symlinks
                        # This is safe because we only consume the links created here (so old files are irrelevant anyway)
                        try:
                            os.remove(os.path.join(self.__oeda_path, 'WorkDir', __file['path'].split("/")[-1]))
                        except OSError:
                            pass
                        if self.mGetGiMultiImageSupport():
                            _src_path = os.path.join(self.mGetRepoDownloadLocation(), __file['path'])
                        else:
                            _src_path = os.path.join(self.mGetRepoDownloadLocation(), __key, __file['path'])
                        _tgt_path = os.path.join(self.__oeda_path, 'WorkDir', __file['path'].split("/")[-1])
                        os.symlink(_src_path,_tgt_path)
                        _path = self.__oeda_path[self.__oeda_path.rfind('exacloud'):]
                        ebLogTrace('*** symlink %s to OEDA staging %s' % (__file['path'], os.path.join(_path, 'WorkDir')))
        return 0

    def mGetGridConfig(self):
        if 'grid' in self.__dbgi_config:
            return self.__dbgi_config['grid']
        else:
            return {}

    def mGetDBConfig(self):
        if 'db' in self.__dbgi_config:
            return self.__dbgi_config['db']
        else:
            return {}

    def mGetRepoDownloadLocation(self):
        return self.__repo_download_location

    def mGetRepoInventory(self):
        return self.__repo_inventory

    def mSetRepoInventory(self, aValue):
        self.__repo_inventory = aValue

    def mGetGiMultiImageSupport(self):
        return self.__GiMultiImageSupport
    
    def mSetGiMultiImageSupport(self, aValue):
        self.__GiMultiImageSupport = aValue

    def mGetOracleBaseDirectories(self, aDomU=None, aDBName=None):
        _gi_home = None
        _gi_version = None
        _gi_base = None

        # check first domU is connectable
        if aDomU:
            _domU = aDomU
        else:
            _domU = self.mReturnDom0DomUPair()[0][1]

        _node = exaBoxNode(get_gcontext())
        _node.mSetUser('root')
        if _node.mIsConnectable(aHost=_domU):
            _node.mConnect(aHost=_domU)

            #GET ORACLE HOME

            # For ADBS elastic operation there is a different way to get the grid home
            if self.mIsAdbs() and self.__cmd in ["vmgi_reshape", "elastic_cell_update"]:
                _gi_home = getGridHome(_domU)

                if _gi_home:
                    ebLogInfo(f"*** GI Home {_gi_home} found from ocssd process in {_domU}")
                else:
                    if aDBName:
                        ebLogInfo(f"Missing GI Home for DB: {aDBName}")
                        return None, None, None

                    _cmd = ("/bin/cat /var/opt/oracle/creg/grid/grid.ini "
                            "| /bin/grep \"^oracle_home\" "
                            "| /bin/cut -d '=' -f 2")
                    _gi_home = node_exec_cmd(_node, _cmd).stdout.strip()

                    if _gi_home:
                        ebLogInfo(f"*** GI Home {_gi_home} found from grid.ini in {_domU}")
                    else:
                        ebLogWarn(f"*** No GI Home found in {_domU} !!!")
                        return None, None, None

            else:

                if aDBName:
                    _, _o, _ = _node.mExecuteCmd(f"/bin/cat /etc/oratab | /bin/grep '^{aDBName}.*' | /bin/cut -f 2 -d ':' ")
                else:
                    _, _o, _ = _node.mExecuteCmd("/bin/cat /etc/oratab | /bin/grep '^+ASM.*' | /bin/cut -f 2 -d ':' ")

                _out = _o.readlines()
                if _out:
                    _gi_home = _out[0].strip()

                if _gi_home:
                    ebLogInfo(f"*** GI Home {_gi_home} found in /etc/oratab in {_domU}")
                else:

                    if aDBName:
                        _path, _, _ = self.mGetOracleBaseDirectories(_domU)
                        _ret, _o, _ = node_exec_cmd(_node, f"{_path}/bin/srvctl config database -db {aDBName}")

                        if _ret == 0:
                            _gi_home_line, *_ = ( _line for _line in _o.splitlines() if 'home' in _line)
                            _gi_home = _gi_home_line.split(':')[1].strip()

                    else:
                        _dbaascli = '/bin/dbaascli'
                        if _node.mFileExists(_dbaascli):
                            _ret, _o, _ = node_exec_cmd(_node, f"{_dbaascli} grid getDetails")
                            if _ret == 0:
                                _gi_home_line, *_ = ( _line for _line in _o.splitlines() if 'homePath' in _line)
                                _gi_home = _gi_home_line.split('"')[3]

                    if _gi_home:
                        ebLogInfo(f"*** GI Home {_gi_home} found from dbaascli in {_domU}")
                    else:

                        if aDBName:
                            ebLogInfo(f"Missing GI Home for DB: {aDBName}")
                            return None, None, None

                        _cmd = ("/bin/cat /var/opt/oracle/creg/grid/grid.ini "
                                "| /bin/grep \"^oracle_home\" "
                                "| /bin/cut -d '=' -f 2")
                        _gi_home = node_exec_cmd(_node, _cmd).stdout.strip()

                        if _gi_home:
                            ebLogInfo(f"*** GI Home {_gi_home} found from grid.ini in {_domU}")
                        else:
                            ebLogWarn(f"*** No GI Home found in {_domU} !!!")
                            return None, None, None


            #GET ORACLE BASE VERSION
            # multipleGI: Expecting full version here in format 19.19.0.0.230418
            # [root@scaqab10adm07vm08 ~]# export ORACLE_HOME=/u01/app/19.19.0.0.230418/grid; $ORACLE_HOME/bin/oraversion -baseVersion 
                # 19.0.0.0.0
            # [root@scaqab10adm07vm08 ~]# export ORACLE_HOME=/u01/app/19.19.0.0.230418/grid; $ORACLE_HOME/bin/oraversion -eeVersion 
                # 19.19.0.0.0          
            # TODO: Check which command to use here to get exact version
            if self.mGetGiMultiImageSupport():
                _cmd = f"export ORACLE_HOME={_gi_home}; $ORACLE_HOME/bin/oraversion -compositeVersion"
                _, _o, _ = _node.mExecuteCmd(_cmd)
                _out = _o.readlines()
                if _out:
                    _gi_version = _out[0].strip()
            else:
                _cmd = f"export ORACLE_HOME={_gi_home}; $ORACLE_HOME/bin/oraversion -baseVersion"
                _, _o, _ = _node.mExecuteCmd(_cmd)
                _out = _o.readlines()
                if _out:
                    _out = _out[0].strip()
                    _gi_version = _out.replace(".", "")[:3]

            #GET ORACLE BASE
            _cmd = f"export ORACLE_HOME={_gi_home}; $ORACLE_HOME/bin/orabase"
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            _out = _o.readlines()
            if _out:
                _gi_base = _out[0].strip()

            _node.mDisconnect()
            ebLogInfo(f"Using GI HOME:{_gi_home} GI VERSION:{_gi_version} ORA BASE:{_gi_base} from DomU {_domU}")
        return _gi_home, _gi_version, _gi_base

    def mGetVersionGi(self):

        _gi_version = None
        _default = None
        _latest = None

        # Fetch latest and default from inventory
        for _klone in self.mGetRepoInventory()['grid-klones']:

            if _klone['xmeta']['default']:

                _newDefault = _klone['version'].replace('.', '')[0:3]

                if _default:
                    _msg = "Two GI defaults: {0}, {1} on inventory.json".format(_default, _newDefault)
                    ebLogError(_msg)
                    raise ExacloudRuntimeError(0x0119, 0xA, _msg, aStackTrace=True)

                _default = _newDefault

            if ('service' in _klone.keys()) and (self.mGetServiceType() in _klone['service']) and _klone['xmeta']['latest']:

                _newLatest = _klone['version'].replace('.', '')[0:3]

                if _latest:
                    _msg = "Two GI latest: {0}, {1} on inventory.json".format(_latest, _newLatest)
                    ebLogError(_msg)
                    raise ExacloudRuntimeError(0x0119, 0xA, _msg, aStackTrace=True)

                _latest = _newLatest

        # Get GI from Payload
        if self.__options and \
           self.__options.jsonconf and \
           "grid_version" in self.__options.jsonconf:

            _gi_payload = self.__options.jsonconf['grid_version']
            if _gi_payload:
                # if payload is in format of 19.19.0.0.230418 then return 19 only
                _gi_payload = _gi_payload.split('.')[0]

            # Find the GI from the payload
            for _gi_compat in self.__compat:
                if _gi_compat.startswith(_gi_payload):
                    ebLogInfo("Mapping: {0} to {1}".format(_gi_payload, _gi_compat))
                    _gi_version = _gi_compat
                    break

            if not _gi_version:
                _msg = "Invalid GI from payload: {0}, supported: {1}".format(_gi_payload, list(self.__compat.keys()))
                ebLogError(_msg)
                raise ExacloudRuntimeError(0x0119, 0xA, _msg, aStackTrace=True)

            ebLogInfo("Using GI from payload: {0}".format(_gi_version))

        # Get GI from command line
        elif self.__enable_gilatest:

            if not _latest:
                _msg = "GI latest not found on inventory.json"
                ebLogError(_msg)
                raise ExacloudRuntimeError(0x0119, 0xA, _msg, aStackTrace=True)

            _gi_version = _latest
            ebLogInfo("Using GI Latest: {0}".format(_gi_version))

        elif self.__options and \
             self.__options.grid_version and \
             self.__options.grid_version in self.__compat:

            _gi_version = self.__options.grid_version
            ebLogInfo("Using GI from command line: {0}".format(_gi_version))

        else:

            # check first domU is connectable
            _domU = self.mReturnDom0DomUPair()[0][1]

            _node = exaBoxNode(get_gcontext())
            if _node.mIsConnectable(aHost=_domU):

                _, _giversion, _ = self.mGetOracleBaseDirectories(aDomU=_domU)

                _grid_config = self.mGetGridConfig()
                if _giversion in _grid_config.keys():
                    _gi_version = _giversion
                else:
                   _major_version = _giversion[:2]

                   for _key in sorted(_grid_config.keys(), reverse=True):
                       if _key.startswith(_major_version):
                            _gi_version = _key
                            break
                ebLogInfo("Using GI from DomU {0}".format(_gi_version))

            if not _gi_version:

                if ebCluCmdCheckOptions(self.__cmd, ['require_gi']):
                    raise ExacloudRuntimeError(0x0119, 0xA, "No GI Version provided by Payload, CMD nor Latest", aStackTrace=True)

                if not _default:
                    _msg = "GI default not found on inventory.json"
                    ebLogError(_msg)
                    raise ExacloudRuntimeError(0x0119, 0xA, _msg, aStackTrace=True)

                _gi_version = _default
                ebLogInfo("Using GI Default: {0}".format(_gi_version))

        return _gi_version

    def mGetLatestGIFromMajorVersion(self, aMajorVersion="19"):
        _src_major_version = aMajorVersion
        _repo_full_version = None
        ebLogInfo(f"Searching for latest GI version for major version: {_src_major_version}")
        for _klone in self.mGetRepoInventory()['grid-klones']:
            if ('service' in _klone.keys()) and \
               (self.mGetServiceType() in _klone['service']) and \
               _klone['xmeta']['latest'] and \
               _klone['version'].split('.')[0] == _src_major_version:
               _repo_full_version = _klone['version']
               break

        ebLogInfo(f"Mapped GI version {_repo_full_version} from payload major version {_src_major_version}")
        return _repo_full_version
    
    def mGetVersionGiMultiImages(self):
        _gi_version = None
        _default = None
        _latest = None
        _default_version = '19'
        # Fetch latest and default from inventory
        for _klone in self.mGetRepoInventory()['grid-klones']:

            if _klone['xmeta']['default'] and _klone['version'].split('.')[0] == _default_version:
                _newDefault = _klone['version']
                if _default:
                    _msg = "Two GI defaults: {0}, {1} on inventory.json".format(_default, _newDefault)
                    ebLogError(_msg)
                    raise ExacloudRuntimeError(0x0119, 0xA, _msg, aStackTrace=True)
                _default = _newDefault

            if ('service' in _klone.keys()) and (self.mGetServiceType() in _klone['service']) and _klone['xmeta']['latest'] and _klone['version'].split('.')[0] == _default_version:
                _newLatest = _klone['version']
                if _latest:
                    _msg = "Two GI latest: {0}, {1} on inventory.json".format(_latest, _newLatest)
                    ebLogError(_msg)
                    raise ExacloudRuntimeError(0x0119, 0xA, _msg, aStackTrace=True)
                _latest = _newLatest
        
        # Get GI from Payload
        if self.__options and self.__options.jsonconf and "grid_version" in self.__options.jsonconf:
            _gi_payload = self.__options.jsonconf['grid_version']

            # Check GI from the payload is present in inventory
            for _version in self.__dbgi_config['grid']:
                if _version == _gi_payload:
                    ebLogInfo(f"GI version {_version} from payload present in repo")
                    _gi_version = _gi_payload
                    break

            if not _gi_version:
                ebLogWarn(f"Payload GI version {_gi_payload} did not match with existing repository. Fetching latest version with major version information.")
                _gi_version = self.mGetLatestGIFromMajorVersion(_gi_payload.split('.')[0])
                

            if not _gi_version:
                _msg = "Invalid GI from payload: {0}, supported: {1}".format(_gi_payload, list(self.__dbgi_config["grid"].keys()))
                ebLogError(_msg)
                raise ExacloudRuntimeError(0x0119, 0xA, _msg, aStackTrace=True)

            ebLogInfo("Using GI from payload: {0}".format(_gi_version))
        
        # Get GI from command line
        elif self.__enable_gilatest:

            if not _latest:
                _msg = "GI latest not found on inventory.json"
                ebLogError(_msg)
                raise ExacloudRuntimeError(0x0119, 0xA, _msg, aStackTrace=True)

            _gi_version = _latest
            ebLogInfo("Using GI Latest: {0}".format(_gi_version))

        elif self.__options and \
             self.__options.grid_version and \
             self.__options.grid_version in self.__dbgi_config:

            _gi_version = self.__options.grid_version
            ebLogInfo("Using GI from command line: {0}".format(_gi_version))

        else:

            # check first domU is connectable
            _domU = self.mReturnDom0DomUPair()[0][1]

            _node = exaBoxNode(get_gcontext())
            if _node.mIsConnectable(aHost=_domU):
                # Expecting full version here in format 19.19.0.0.230418
                _, _giversion, _ = self.mGetOracleBaseDirectories(aDomU=_domU)

                _grid_config = self.mGetGridConfig()
                if _giversion in _grid_config.keys():
                    _gi_version = _giversion
                else:
                   _major_version = _giversion[:2]

                   for _key in sorted(_grid_config.keys(), reverse=True):
                       if _key.startswith(_major_version):
                            _gi_version = _key
                            break
                ebLogInfo("Using GI from DomU {0}".format(_gi_version))

            if not _gi_version:

                if ebCluCmdCheckOptions(self.__cmd, ['require_gi']):
                    raise ExacloudRuntimeError(0x0119, 0xA, "No GI Version provided by Payload, CMD nor Latest", aStackTrace=True)

                if not _default:
                    _msg = "GI default not found on inventory.json"
                    ebLogError(_msg)
                    raise ExacloudRuntimeError(0x0119, 0xA, _msg, aStackTrace=True)

                _gi_version = _default
                ebLogInfo("Using GI Default: {0}".format(_gi_version))

        return _gi_version

    #
    #dynximages end
    #

    def mSetupNamespace(self):

        _dpairs = self.mReturnDom0DomUPair()
        for _, _domU in _dpairs:
            _node = exaBoxNode(get_gcontext())
            AtpSetupNamespace(_node, self.__ATP, self.__options.jsonconf["customer_network"], _domU).mExecute()

    def mGetNamespace(self):
        _namespace = None
        _atp_config = self.mCheckConfigOption('atp')

        if self.isATP() and 'enable_namespace' in _atp_config and _atp_config['enable_namespace'] == 'True':
            _namespace = "mgmt"

        return _namespace


    def mIsClusterLessXML(self):
        if self.isBaseDB() or self.isExacomputeVM():
            return False
        if not self.__clusters.mGetCluster():
            ebLogInfo('*** No cluster found in xml.. considering this as a special rack xml. ***')
            return True

        return False

    def mRemoveUnreachableNodes(self, aOptions):
        """
        This method is applicable for clusterless xmls only.
        Since we we dont have a cluster, we are limiting the cleanup only to machineconfig.
        We will not attempt to remove vip, dbhome config, db config.. etc)
        """

        if not self.mIsClusterLessXML():
            return

        _machineList = []
        _jconf = self.__options.jsonconf

        _dom0s, _, _cells, _ = self.mReturnAllClusterHosts()
        _cluhosts = _dom0s + _cells
        for _host in _cluhosts:

            # Skip removal in case of machine found in the ECRA Payload and only for patching
            if _jconf and _host in str(_jconf) and \
               ebCluCmdCheckOptions(self.mGetCmd(), ['patch']):
                continue

            if not self.mPingHost(_host):

                _machineList.append(self.__machines.mGetMacIdFromMacHostName(_host))
                ebLogInfo('*** mRemoveUnreachableNodes: removing ' + self.__machines.mGetMacIdFromMacHostName(_host) + ' from hosts list ***')

        self.__machines.mRemoveMachinesConfig(_machineList)


    def mSkipGISupportDetection(self, aOptions):
        if self.mIsClusterLessXML() or ebCluCmdCheckOptions(self.mGetCmd(), ['skip_gi_detection']) \
             or (self.mGetCmd() == 'vm_cmd' and aOptions.vmcmd in ['memset','resizecpus','ping']) \
             or (self.mGetCmd()== 'partition' and aOptions.partitionOp =='resize') \
             or (self.mIsExaScale() and (aOptions.jsonconf is not None and aOptions.jsonconf.get("reshaped_node_subset", {}).get("removed_computes", []))):
            ebLogWarn(f"Skipping GI support detection")
            return True
        else:
            return False

    def mUpdateUserConfiguration(self):

        _remapUtil = ebMigrateUsersUtil(self)
        _usrConfig = _remapUtil.mMergeUsersGroupsConfigPayload()

        if _usrConfig:
            for _name, _configU in _usrConfig.items():

                if "gid" in _configU:

                    _xmlGroup = self.__groups.mGetGroupByName(_name)

                    if _xmlGroup:
                        _xmlGroup.mSetGroupId(str(_configU["gid"]))
                        ebLogInfo(f"Change group: {_xmlGroup.mGetGroupConfigId()} to gid: {_configU['gid']}")
                    else:
                        ebLogInfo(f"Group not found: {_name}")

                        if _name != "opc":
                            ebLogInfo(f"Creating Group: {_name}")
                            self.__groups.mCreateNewGroup(str(_configU["gid"]), _name)

                if "uid" in _configU:

                    _xmlUser = self.__users.mGetUserByName(_name)

                    if _xmlUser:
                        _xmlUser.mSetUserId(str(_configU["uid"]))
                        ebLogInfo(f"Change username: {_xmlUser.mGetUserConfigId()} to uid: {_configU['uid']}")
                    else:
                        ebLogInfo(f"Username not found: {_name}")

                        if _name != "opc":
                            ebLogInfo(f"Creating User: {_name}")
                            _xmlGroup = self.__groups.mGetGroupByName(_name)
                            self.__users.mCreateNewUser(str(_configU["uid"]), _name, _xmlGroup.mGetGroupConfigId())


    def mSetDomUImageVer(self, aImageVersion):
        """
            mSetDomUImageVer:
            Return None.

            :param aImgeVersion. The desired ExadataImageVersion in oedacli form (e.g 21.2.13.0.0.220622).

            Description: This method adds the proper oedacli command to the cmd list to be executed in the
                oedaxml patching to relax the image version validation. Only for ExaCS and MVM
        """
        ebLogTrace("Running mSetDomUImageVer")
        _cmd = ["ALTER MACHINES",
                {"IMAGEVERSION": aImageVersion},
                {"TYPE": "GUEST"}]

        ebLogTrace("Added Command: {0} {1} WHERE {2}".format(_cmd[0], _cmd[1], _cmd[2]))

        self.__extraXmlPatchingCommands.append(_cmd)

        ebLogInfo("*** Updating OEDA properties")

        _oeda_path  = self.mGetOedaPath()
        _prop_file = os.path.join(_oeda_path, 'properties/es.properties')
        _cmd = "/bin/sed -i 's/ENABLEDIFFDOM0IMAGEVERSIONS=false/"\
        "ENABLEDIFFDOM0IMAGEVERSIONS=true/g' {0}"
        _rc, _, _o, _e = self.mExecuteLocal(_cmd.format(_prop_file))
        if _rc == 0:
            ebLogInfo("*** Updated OEDA properties added ENABLEDIFFDOM0IMAGEVERSIONS=true")
        else:
            ebLogInfo(f"*** Errors ocurred while executing command \"{_cmd}\"")
            ebLogError(f"*** SYSOUT\n{_o}\n\n")
            ebLogError(f"*** SYSERR\n{_e}\n\n")


    def mPatchClusterConfig(self, aOptions):

        if self.mSkipGISupportDetection(aOptions):
            ebLogInfo('Skipping mGISupportDetection patching for %s command!'%(self.mGetCmd()))        
        else:
            self.mGISupportDetection()
        # mCellTasks include checking if ZDLRA env, checking and setting maxstartup parameter in cells 
        # and fetching exadata model
        if not self.__cellInfo:
            _cellobj = ebCluCellValidate(self, aOptions)
            _cellobj.mCellTasks(aOptions)

        # Update -priv networks
        if self.mIsOciEXACC() and ebCluCmdCheckOptions(self.mGetCmd(), ['update_cps_dns']):
            self.mUpdatePrivNetworks()
        elif self.mIsExaScale():
            # For Exascale env, update <natname>-clre/stre to <clienthostname>-clre/stre
            mPatchPrivNetworks(self)

        #
        # Change VM Configuration (use exabox.conf values) - see mPatchClusterDB for json SDI overwrite
        #
        # TODO: Support Configuration for all the size (Large, Medium, Small) not only one.

        _config = get_gcontext().mGetConfigOptions()

        if 'default_vmsize' in list(_config.keys()):
            _vmsize =  _config['default_vmsize']

        if not self.mIsKVM() and 'default_dedicated_vmsize' in list(_config.keys()) and not self.__shared_env:
            _vmsize =  _config['default_dedicated_vmsize']

        if 'default_shared_vmsize' in list(_config.keys()) and self.__shared_env:
            _vmsize =  _config['default_shared_vmsize']

        if self.mIsClusterLessXML():
            ebLogInfo('Skipping mSetVMSizeAttr patching for special command : {}'.format(aOptions.clusterctrl))
        else:
            for key in list(_vmsize.keys()):
                self.__vmsizes.mGetVMSize('Large').mSetVMSizeAttr(key, _vmsize[key])
                self.__vmsizes.mGetVMSize('Medium').mSetVMSizeAttr(key, _vmsize[key])
                self.__vmsizes.mGetVMSize('Small').mSetVMSizeAttr(key, _vmsize[key])

        # Patch configuration to add a second database home (either 11g or 12g for now)
        _coptions = get_gcontext().mGetConfigOptions()
        if 'disable_dual_dbhomes' in list(_coptions.keys()) and _coptions['disable_dual_dbhomes'] == 'True':
            ebLogWarn('*** Dual DatabaseHomes support has been disabled')
        else:
            if self.mIsClusterLessXML():
                ebLogInfo('Skipping mManageDatabaseHomes patching for command {}'.format(aOptions.clusterctrl))
            else:
                self.mManageDatabaseHomes(aOptions)

        # Patch for 12.X support w/ new Quorum disk feature
        if self.__enable_quorum and self.mCheckConfigOption('chk_cell_sysimg','True'):
            if self.mCheckCellsSystemImage() is False:
                ebLogWarn('*** Disabling Quorum Disk supported based on Cells system image requirement')
                self.__enable_quorum = False
        else:
            ebLogInfo('*** Cells System Image checks disabled')

        if self.__enable_quorum:
            # check for the # of cells before enabling quorum
            if len(self.mReturnCellNodes()) >= 5:
                ebLogWarn('*** Disabling Quorum Disk support based on number of Cells')
                self.__enable_quorum = False

            """
            35634247: With new OEDA build, configuring quorum devices will be
            handled by OEDA which requires quorum flag in es.properties
            # Disable quorum in case of one node installation.
            if len(self.mReturnDom0DomUPair()) == 1:
                ebLogWarn('*** Quorum is disabled in one node installation')
                self.__enable_quorum = False
            """

        # Patch DB configuration
        # Condition to skip patching cluster DB if command has no_check_sw_cell
        if ebCluCmdCheckOptions(self.__cmd, ['no_check_sw_cell']):
            ebLogInfo(f'*** Skip patching cluster DB for {self.__cmd} command')
        else:
            if not ebCluCmdCheckOptions(self.__cmd, ['cluster_info_tool']):
                if self.mIsClusterLessXML():
                    ebLogInfo('Skipping mPatchClusterDB patching for command {}'.format(aOptions.clusterctrl))
                else:
                    self.mPatchClusterDB(aOptions)

        # Update OEDA Properties
        ebLogInfo("Apply change of oeda properties")
        self.mUpdateOEDAProperties(aOptions, aSkipValidation=True)


        # Skip XML Patching
        if not self.mIsOciEXACC():

            if self.mIsSkipXmlPatching():

                _jconf = aOptions.jsonconf
                if _jconf and 'dbaas_api' in list(_jconf.keys()):
                    self.__dbaas_api_payload = _jconf['dbaas_api']

                self.mSaveXMLClusterConfiguration()
                self.mSetConfigPath(self.__patchconfig)

                if self.mIsExaScale():
                    _exascale = ebCluExaScale(self)
                    _exascale.mCreateOedaProperties()
                    _exascale.mCreateDummyCellsKeys()

                self.mParseXMLConfig(aOptions)

                if self.__cmd in ['deleteservice']:
                    # Patch VMs in Delete service for reconfigured cluster
                    _util = self.__factoryPreprovReconfig.mCreatePreprovUtil()
                    _util.mUpdateVmNameReconfigDeleteService(aOptions)
                    self.mSaveXMLClusterConfiguration()
                    #self.mSetConfigPath(self.__patchconfig)

                    _patchconfig = self.mGetPatchConfig()
                    self.mUpdateInMemoryXmlConfig(_patchconfig, aOptions)

                ebLogInfo('ebCluCtrl: Saved patched Cluster Config: ' + self.__patchconfig)

                return

        # Remove nodes in shutdown state
        self.mRemoveUnreachableNodes(aOptions)

        # Patch NTP servers
        self.mUpdateNTPServers()

        # Patch charactersert
        self.mUpdateCharacterSet(aOptions)

        # Patch VMs on Delete service
        _util = self.__factoryPreprovReconfig.mCreatePreprovUtil()
        _util.mUpdateVmNameReconfigDeleteService(aOptions)

        # Update users configuration
        self.mUpdateUserConfiguration()

        # Patch system image
        if self.mCheckConfigOption('use_ol6_domu') is not None and self.__enable_gilatest is False:
            ebLogInfo("*** OL6 is no loger supported, ignoring 'use_ol6_domu' config")

        # Remove machines that don't belong to current cluster from XML
        self.mRemoveUnusedVmMachines()

        #
        # Apply Basesystem changes
        #
        self.mBaseSystemConfiguration(aOptions)

        #
        # Save new XML Cluster configuration file
        #
        self.mSaveXMLClusterConfiguration()
        
        # #Apply commands OEDACLI
        self.mApplyCommandsOedacli(aOptions=aOptions)

        # Patching of ExaScale
        if self.mIsExaScale():
            _exascale = ebCluExaScale(self)
            _exascale.mCreateOedaProperties()
            if ebCluCmdCheckOptions(self.__cmd, ['exascale_patching']):
                _exascale.mApplyExaScaleXmlPatching()
            _exascale.mCreateDummyCellsKeys()

        # Patch XML with the exascale info
        _utils = self.mGetExascaleUtils()
        if aOptions.jsonconf is not None and "exascale" in list(aOptions.jsonconf.keys()) and self.__cmd in ["info", "xsconfig", "exascale_cell_update"]:
            _utils.mEnableXSService(aOptions)

            # PATCH XML WITH EDV VOLUMES
            if _utils.mIsEDVImageSupported(aOptions):
                _utils.mPatchEDVVolumes(aOptions)

        ebLogTrace("mPatchClusterConfig: Patch Cluster Configuration,")
        if aOptions.jsonconf is not None and 'node_subset' in list(aOptions.jsonconf.keys()):
            self.mPatchXMLForNodeSubset(aOptions)

        #Verify if the XML has exascale tag (XS Cluster)
        if self.mIsXS():
            _utils.mParseXMLForXS()

        if not self.mIsClusterLessXML():
            _pchecks = ebCluPreChecks(self)
            _pchecks.mValidateVersionForMVM(aOptions)

        if self.IsZdlraProv():
            self.__ZDLRA.mPatchXmlZdlra()
        # Apply OCI-EXACC ATP changes
        if self.__ociexacc and self.isATP():
            _domUs = map(operator.itemgetter(1),self.mReturnDom0DomUPair())
            _atppatch = ebExaCCAtpPatchXML(self.__patchconfig,_domUs, self.__debug)
            ebLogInfo("*** OCIEXACC with ATP Payload detected, ATP specific XML Patching ongoing")
            _atppatch.mPatchXML()

        #
        # If encryption is requested and system is KVM, patch the XML to
        # add keyapi parameter only if exabox.conf flag is enabled.
        #
        if (isEncryptionRequested(aOptions, 'domU') and
                self.mIsKVM() and get_gcontext().mGetConfigOptions().get(
                    "force_full_guest_encryption", "false").lower() == "true"):

            patchXMLForEncryption(self)

        if self.mIsOciEXACC() and self.mIsDRNetPresent():
            # Update in memory network object with dr net info
            self.mUpdateInMemoryXmlConfig(self.__patchconfig, aOptions)

        ebLogInfo('ebCluCtrl: Saved patched Cluster Config: ' + self.__patchconfig)

        # Update ociexacc cps DNS
        if self.mIsOciEXACC() and ebCluCmdCheckOptions(self.mGetCmd(), ['update_cps_dns']):
            ebDNSConfig(aOptions, self.__patchconfig).mConfigureDNS('guest') # Update DNS entries for "guest" networks


    def mRemoveUnusedVmMachines(self):
        """
        This method removes from the XML the machines that don't
        belong to the current cluster
        """

        # Get tuple of list of hosts belonging to current cluster
        _list_all_hosts = self.mReturnAllClusterHosts()
        _to_remove_machines = []

        # Iterate over XML machines and check if they are part of the cluster
        _mac_list = list(self.mGetMachines().mGetMachineConfigList())
        for _mac in _mac_list:

            _mac_config = self.mGetMachines().mGetMachineConfig(_mac)
            _mac_hostname = _mac_config.mGetMacHostName()

            # Remove machine from XML if not part of cluster
            if _mac_hostname not in itertools.chain(*_list_all_hosts):

                # Remove only vm machines
                if _mac_config.mGetMacOsType() in ["LinuxGuest", "LinuxKVMGuest"]:
                    _to_remove_machines.append(_mac)

        # Execute the remove of the machines
        for _mac in _to_remove_machines:
            ebLogWarn(f"Removing machine {_mac}")
            self.mGetMachines().mRemoveMachinesConfig(_mac, aExactId=True)

    def mDeleteClusterConfiguration(self):
        pass

    def mSaveXMLClusterConfiguration(self):
        ebLogVerbose("Save XML Cluster Configuration")
        #
        # Create Cluster directory (only if not yet created)
        #
        self.__cluster_path = 'clusters/'+self.__key
        _dir = self.__cluster_path
        try:
            os.stat(_dir)
        except:
            os.makedirs(_dir)
            # VGE: MVM-Todo, copy keys from AdminXML/keys directory

        # Lock will be applicable only when the clean_cluster_folder operation is enabled in exabox.conf
        # Performing cleanup only for exacs
        if self.mCheckConfigOption('clean_cluster_folder', 'True') and not self.mIsOciEXACC() and self.__key:
            try:
                # Adding a lock file in the clusters directory to be checked when removing the cluster directory
                _request_lock_file = os.path.join(_dir, f"cluster_lock_{self.mGenerateUUID()}")
                ebLogTrace(f"Creating cluster lock file : {_request_lock_file}.")
                open(_request_lock_file, 'a').close()
                self.mSetClusterLockFile(_request_lock_file)
            except Exception as ex:
                # Anything else to be done if the cluster lock file do not get created?
                ebLogTrace(f"The cluster lock file : {_request_lock_file} was not created.")
        #
        # Create shortcut
        #
        if self.__key:
            _scut = {}
            _found = False
            _ld = os.listdir('clusters/')
            for _e in _ld:
                if _e[:len('cluster-')] == 'cluster-':
                    _id = _e.split('-')[-1]
                    _rp = os.path.realpath('clusters/'+_e)
                    _scut[_id] = [_e,_rp]
                    self.mSetShortClusterPath(os.path.basename(_rp), os.path.join(self.mGetBasePath(), f'clusters/{_e}'))
                    if _rp.split('/')[-1] == self.__key:
                        _found = True
            if not _found:
                _scut_keys = list(_scut.keys())
                for _id in range(1,1024):
                    if str(_id) not in _scut_keys:
                        try:
                            os.symlink(_dir.split('/')[-1],'clusters/cluster-'+str(_id))
                            _realpath = os.path.join(self.mGetBasePath(), _dir)
                            self.mSetShortClusterPath(os.path.basename(_realpath), os.path.join(self.mGetBasePath(), 'clusters/cluster-'+str(_id)))
                        except FileExistsError as ex:
                            # Bug 34740150: If there is a parallel provisioning going on, the
                            # cluster id can be already allocated.
                            ebLogWarn(f"*** The cluster with id {str(_id)} already exists.")
                            continue
                        ebLogInfo('*** shortcut %s for cluster %s created.' % ('cluster-'+str(_id), _dir.split('/')[-1]))
                        break
        #
        #  Save XML Configuration (check if already present)
        #
        import hashlib
        _conf = self.__config.mGetConfigXMLData()
        _sha256  = hashlib.sha256(_conf.encode('utf8'))
        _hash = _sha256.hexdigest()

        _keys_dir = _dir + '/keys'
        try:
            os.stat(_keys_dir)
        except:
            os.mkdir(_keys_dir)

        _dir = _dir + '/config'
        try:
            os.stat(_dir)
        except:
            os.mkdir(_dir)
        #
        # Check if XML has already been saved (check hash prefix)
        #
        _path  = None
        _ld = os.listdir(_dir)
        for _e in _ld:
            if _e.split('_')[0] == _hash:
                _path  = _e
                self.__patchconfig = _dir+'/'+_path
                break
        if _path is None or self.mIsExaScale() or self.mIsXS():
            _path = _dir+'/'+_hash+'_'+str(uuid.uuid1())+'.xml'
            self.__patchconfig = _path
            self.__config.mWriteConfig(_path)
        #
        # Make sure to return absolute path
        #
        self.__patchconfig = os.getcwd() + '/' + self.__patchconfig
        self.__cluster_path = self.mGetBasePath() + '/clusters/'+self.__key
        #
        # Update XML path in request obj
        #
        _reqobj = self.mGetRequestObj()
        if _reqobj:
            _reqobj.mSetXml(self.__patchconfig)
            _db = ebGetDefaultDB()
            _db.mUpdateRequest(_reqobj)
        ebLogInfo('ebCluCtrl: Saved patched Cluster Config: ' + 
            self.mGetPatchConfig())

    #Function to copy the script for downloading latest compute updates and patchmgr from exacc to domu
    #
    def mCopyExaDataScript(self):
        _absScriptPath = 'scripts/exadata_updates/exadata_updates.sh'
        if not os.path.exists(_absScriptPath) or not self.__ociexacc:
            return None
        for _, _domU in self.mReturnDom0DomUPair():
            ebLogInfo('*** copying exadata_updates script to %s' % (_domU))
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domU)
            _absPath = '/opt/exacloud/exadata_updates/exadata_updates.sh'
            _cmd  = '/bin/mkdir -p /opt/exacloud/exadata_updates'
            _node.mExecuteCmdLog(_cmd)
            _node.mCopyFile(_absScriptPath, _absPath )
            _cmd = '/bin/chmod u+x ' + _absPath
            _node.mExecuteCmdLog(_cmd)
            _node.mDisconnect()


    def mHandlerSaveClusterDomUList(self):
        #
        # Create cluster JSON for existing cluster
        # This is for SVM->MVM migration case wherein CPS agent has dependency on this filesystem path
        #
        return self.mSaveClusterDomUList()

    #Function to Create a list of DomUs and the latest cluster config location for the given cluster and store it in a json file
    #
    def mSaveClusterDomUList(self):
        ebLogVerbose("Save JSON Cluster DomU List")
        #
        # Create Cluster directory (only if not yet created)
        #
        _dir = 'clusters/' + 'clustersjson'
        try:
            os.stat(_dir)
        except:
            os.mkdir(_dir)
        domu_list = [ _domu for _ , _domu in self.mGetOrigDom0sDomUs()]
        domUs_json_list={}
        domUs_json_list["clusterName"]=self.__header.mGetHeaderCustomerName()
        domUs_json_list["domu"]=domu_list
        _cmd = '/bin/cp ' + self.mGetPatchConfig() + ' ' + _dir + '/' + self.mGetKey() + '.xml'
        _rc, _i, _o, _e = self.mExecuteLocal(_cmd)
        if _e:
            ebLogError("Errors occured while executing %s"%(_cmd))
        domUs_json_list["configXMLpath"]= str(_dir + '/' + self.mGetKey() + '.xml')
        ebLogInfo('domU list %s' % (json.dumps(domUs_json_list)))
        #
        # Save domUs details in exacloud/cluster/clustersjson for all
        #
        _ntp = NamedTemporaryFile(delete=False)
        _ntp.file.write(six.ensure_binary(json.dumps(domUs_json_list)))
        _ntp.file.close()
        _f= '/'+ self.mGetKey() + '.json'
        self.mCopyFile(_ntp.name, _dir+_f)
        # Cleanup (remove temporary file)
        os.unlink(_ntp.name)
        assert(os.path.exists(_ntp.name)==False)
        if self.__ociexacc:
            self.mClustersSyncExaCC()

    #Function to delete json file containing the list of domUs and the config file during delete service call
    #
    def mDeleteClusterDomUList(self):
        ebLogVerbose("Delete JSON Cluster DomU List")
        #
        # Check Cluster directory (only if not yet created)

        _dir = os.path.abspath('clusters/')
        _dir = _dir + '/clustersjson'
        if not os.path.exists(_dir):
            ebLogInfo('*** clustersjson folder does not exist, cannot delete the JSON Cluster DomU List')
            return
        _cmd = "/bin/rm -rf " + _dir + "/" + self.mGetKey()
        self.mExecuteLocal(_cmd + '.json')
        ebLogInfo('*** JSON Cluster DomU List file %s Deleted'%(self.mGetKey()+'.json'))
        self.mExecuteLocal(_cmd + '.xml')
        ebLogInfo('*** JSON Cluster DomU List file %s Deleted'%(self.mGetKey()+'.xml'))
        #
        # Delete domUs details in exacloud/cluster/clustersjson for all
        #
        if self.__ociexacc:
            self.mClustersSyncExaCC()

    #Function to delete json file containing the list of domUs and the config file based on domU list provided
    #
    def mDeleteClusterFileForDomUs(self, aDomUs):
        ebLogVerbose("Delete JSON Cluster for given DomU List")
        #
        # Check Cluster directory (only if not yet created)
        _domUNat = []
        for _domu in aDomUs:
            _domu_conf_net = []
            _domu_conf = self.__machines.mGetMachineConfig(_domu)
            if _domu_conf is not None:
                _domu_conf_net = _domu_conf.mGetMacNetworks()
            for _net_id in _domu_conf_net:
                _net_conf = self.__networks.mGetNetworkConfig(_net_id)
                _nat_host= _net_conf.mGetNetNatHostName(aFallBack=False)
                if _nat_host is not None:
                    _domUNat.append(_nat_host)

        _dir = os.path.abspath('clusters/')
        _dir = _dir + '/clustersjson'
        if not os.path.exists(_dir):
            ebLogInfo('*** clustersjson folder does not exist, cannot delete the JSON Cluster DomU List')
            return
        for _file in os.listdir(_dir):
            if _file.endswith(('.xml', '.json')) and any(domU in _file for domU in _domUNat):
                _cmd = "/bin/rm -f " + _dir + "/" + _file
                self.mExecuteLocal(_cmd)
                ebLogInfo('*** JSON Cluster DomU List file %s Deleted'%(_file))
        #
        # Delete domUs details in exacloud/cluster/clustersjson for all
        #
        if self.__ociexacc:
            self.mClustersSyncExaCC()


    def mClustersSyncExaCC(self):
        """
        Synchronize clusters directory between cps hosts for oci/exacc
        """
        _config = get_gcontext().mGetConfigOptions()
        _flag_loc = "/opt/oci/exacc/cps_sw_upgrade_running"
        _is_cps_sw_upgrade_running = False
        try:
            _is_cps_sw_upgrade_running = os.path.exists(_flag_loc)
        except Exception as ex:
            ebLogWarn("Can not read file: {0}. Assuming cps sw upgrade not running, Ex: '{1}'".format(\
                _flag_loc,ex))
        ebLogInfo('*** CPS Software upgrade running: {0}'.format(_is_cps_sw_upgrade_running))
        if _is_cps_sw_upgrade_running is True:
            ebLogInfo('*** CPS Software upgrade is running, skipping synchronization')

        # Sync clusterjson directory to remote cps host could break cps sw upgrade (xpatch.py) running on remote cps host
        # sync should be executed by external component once cps sw upgrade is complete.
        if _config['remote_cps_host'] is not None and _is_cps_sw_upgrade_running is False:
            _remote_cps_host = _config['remote_cps_host']
            ebLogInfo('*** Syncing clustersjson folder to the remote node {0}'.format(_remote_cps_host))
            _cmd = '/bin/rsync -e "ssh -o StrictHostKeyChecking=no" -avzr {0}/clustersjson {1}:{0}'.format(os.path.abspath('clusters/'), _remote_cps_host)
            self.mExecuteLocal(_cmd, aStdOut=DEVNULL, aStdErr=DEVNULL)

    #Function to add error code to DB
    #
    def mUpdateErrorCode(self, aErrorCode, aErrorMsg, aErrorType, aRetryCount, aDetailError, aNodeData=''):
        if aErrorCode:
            _db = ebGetDefaultDB()
            _sqldata = (self.mGetUUID(), aErrorCode, aErrorMsg, aErrorType, aRetryCount, aDetailError, aNodeData)
            _db.mSetErrCode(_sqldata)
    
    def mUpdateErrorObject(self, aErrorObject, aDetailError, aNodeData=''):
        _db = ebGetDefaultDB()
        _sqldata = (self.mGetUUID(),aErrorObject[0],aErrorObject[1],aErrorObject[2],aErrorObject[3],aDetailError, aNodeData)
        _db.mSetErrCode(_sqldata)


    def mExecuteOnClusterDom0(self, aFunction=None,aOptions=None):

        if aFunction:
            _mac_list = self.__clusters.mGetClusterMachines()
            for _mac in _mac_list:
                _mac_config = self.__machines.mGetMachineConfig(_mac)
                _host = _mac_config.mGetMacHostName()
                _id = _mac_config.mGetMacId()

                _ml = self.__machines.mGetMachineConfigList()
                for _m in list(_ml.keys()):
                    _vml = _ml[_m].mGetMacMachines()

                    if _id in _vml:
                        _dom0 = _ml[_m].mGetMacHostName()
                        _node = exaBoxNode(get_gcontext())
                        _node.mConnect(aHost=_dom0)
                        aFunction(_node,aOptions)
                        _node.mDisconnect()

    def mExecuteOnClusterDomU(self, aFunction=None,aOptions=None):

        if aFunction:
            for _, _domu in self.mReturnDom0DomUPair():
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_domu)
                aFunction(_node,aOptions)
                _node.mDisconnect()

    def mReturnIlomHostInfo(self):

        _info_list = []
        _mac_list = self.__machines.mGetMachineConfigList()

        if not self.mIsKVM():
            _dom0s, _domUs, _cells, _switches = self.mReturnAllClusterHosts()
        else:
            _dom0s, _domUs, _cells, _ = self.mReturnAllClusterHosts()
            _switches = []

        for _mac in _mac_list:
            _mac_config = self.__machines.mGetMachineConfig(_mac)
            _mac_hostname = _mac_config.mGetMacHostName()

            _type = "unknown"
            if _mac_hostname in _dom0s:
                _type = "dom0"
            elif _mac_hostname in _cells:
                _type = "cell"
            elif _mac_hostname in _switches:
                _type = "switch"
            elif _mac_hostname in _domUs:
                _type = "domU"

            _iloms = _mac_config.mGetMacIlomNetworks()

            for _ilom in _iloms:
                _ilom_cfg = self.__iloms.mGetIlomConfig(_ilom)
                _ilom_net = self.__networks.mGetNetworkConfig(_ilom_cfg.mGetIlomNetworkId())
                _ilom_name = "{0}.{1}".format(_ilom_net.mGetNetHostName(), _ilom_net.mGetNetDomainName())

                _info = {"ilom": _ilom_name, "host": _mac_hostname, "type": _type}
                _info_list.append(_info)

        return _info_list

    def mReturnElasticAllDom0DomUPair(self):

        if self.mGetElasticOldDom0DomUPair() is None:
            return self.mReturnDom0DomUPair()

        # Create a map with unique keys
        _ddmap = {}
        for _dom0, _domU in self.mGetElasticOldDom0DomUPair():
            _ddmap[_dom0] = _domU

        for _dom0, _domU in self.mReturnElasticNewDom0DomUPair():
            _ddmap[_dom0] = _domU

        # Create the map in the format of ddkeys
        return _ddmap.items()

    def mReturnElasticAllDom0DomUPairNat(self):

        if self.mGetElasticOldDom0DomUPair() is None:
            return self.mReturnDom0DomUNATPair()

        _ddpair = []
        for _dom0, _domU in self.mReturnElasticAllDom0DomUPair():

            _ctx = get_gcontext()
            if _ctx.mCheckRegEntry('_natHN_' + _domU):
                _ddpair.append([_dom0, _ctx.mGetRegEntry('_natHN_' + _domU)])

            else:
                _ddpair.append([_dom0, _domU])

        return _ddpair

    def mReturnElasticNewDom0DomUPairNat(self, aForce=False):
        return self.mReturnDom0DomUNATPair(aForce)

    def mReturnElasticNewDom0DomUPair(self, aForce=False):
        return self.mReturnDom0DomUPair(aForce)

    def mReadBaseDBDom0DomUNATPair(self):
        _ddpair = []
        _ml = self.__machines.mGetMachineConfigList()
        for _m in list(_ml.keys()):
            _vml = _ml[_m].mGetMacMachines()
            for _vm in _vml:
                _dom0 = _ml[_m].mGetMacHostName()
                _domu_conf_net = _ml[_vm].mGetMacNetworks()
                _host = 'undefined'
                for _net_id in _domu_conf_net:
                    _net_conf = self.__networks.mGetNetworkConfig(_net_id)
                    _domu_nat = _net_conf.mGetNetNatHostName(aFallBack=False)
                    if _domu_nat is not None:
                        _host = _domu_nat
                        break
                _ddpair.append([_dom0, _host])
        _sddpair = self.mSortDom0DomUPair(_ddpair)
        return _sddpair

    def mReturnDom0DomUNATPair(self,aForce=False):

        if self.isBaseDB() or self.isExacomputeVM():
            return self.mReadBaseDBDom0DomUNATPair()

        if not aForce and self.__domus_dom0s_nat is not None:
            return self.__domus_dom0s_nat

        _ddpair = []
        _mac_list = self.__clusters.mGetClusterMachines()       # DOMU LIST
        for _mac in _mac_list:
            _mac_config = self.__machines.mGetMachineConfig(_mac)
            _id = _mac_config.mGetMacId()
            _ml = self.__machines.mGetMachineConfigList()
            for _m in list(_ml.keys()):
                _vml = _ml[_m].mGetMacMachines()
                if _id in _vml:
                    _dom0 = _ml[_m].mGetMacHostName()
                    _domu_conf_net = _mac_config.mGetMacNetworks()
                    _host = 'undefined'
                    for _net_id in _domu_conf_net:
                        _net_conf = self.__networks.mGetNetworkConfig(_net_id)
                        _domu_nat = _net_conf.mGetNetNatHostName(aFallBack=False)
                        if _domu_nat is not None:
                            _host = _domu_nat
                            break
                    _ddpair.append([_dom0, _host])

        if self.mIsXmlElasticShape():
            self.__domus_dom0s_nat = _ddpair
            return _ddpair

        _sddpair = self.mSortDom0DomUPair(_ddpair)
        self.__domus_dom0s_nat = _sddpair
        return _sddpair

    def mGetExcludedList(self):
        _jconf = None
        if self.__options:
            _jconf = self.__options.jsonconf
        if _jconf is not None:
            if 'excludeNodeList' in _jconf and _jconf['excludeNodeList']:
                return _jconf['excludeNodeList']
            else:
                return []

        return []

    def mReadBaseDBdom0domU(self):
        _ddpair = []
        _ml = self.__machines.mGetMachineConfigList()
        for _m in list(_ml.keys()):
            _vml = _ml[_m].mGetMacMachines()
            for _vm in _vml:
                _dom0 = _ml[_m].mGetMacHostName()
                _host = _ml[_vm].mGetMacHostName()
                _ddpair.append([_dom0, _host])

        self.__domus_dom0s[_ddpair[0][1]] = _ddpair

        return _ddpair

    def mReturnDom0DomUPair(self,aForce=False,aClusterId=None, aIsClusterLessXML=False, aRetDummyDomu=True, aExcludeNodeList=None):

        if self.isBaseDB() or self.isExacomputeVM():
            return self.mReadBaseDBdom0domU()

        if aIsClusterLessXML:
            _dom0List = self.mReadComputes()
            def _generateDummy(aDom0):
                _pos = aDom0.find('.')
                return [aDom0, aDom0[:_pos] + 'dummydomu' + aDom0[_pos:]]
            if aRetDummyDomu:
                _orig_node_list = [_generateDummy(_dom0) for _dom0 in _dom0List]
            else:
                _orig_node_list = []
                for _dom0 in _dom0List:
                    _orig_node_list.append([_dom0, ""])
            if aExcludeNodeList:
                _excluded_node_list = aExcludeNodeList
            else:
                _excluded_node_list = self.mGetExcludedList()

            return list(filter(lambda x: x[0] not in _excluded_node_list,_orig_node_list))


        # If aClusterId is not defined return the first cluster
        # By default contains only entry corresponding to the cluster at hand
        if not aClusterId:
            if not self.__clusters.mGetClusters():
                return []
            clusterId = self.__clusters.mGetClusters()[0]
        else:
            clusterId = aClusterId

        if not aForce and clusterId in self.__domus_dom0s:
            return self.__domus_dom0s[clusterId]

        _ddpair = []
        _mac_list = self.__clusters.mGetClusterMachines(clusterId)
        for _mac in _mac_list:
            _mac_config = self.__machines.mGetMachineConfig(_mac)
            ### _host = _mac_config.mGetMacHostName()
            _id = _mac_config.mGetMacId()
            _ml = self.__machines.mGetMachineConfigList()
            for _m in list(_ml.keys()):
                _vml = _ml[_m].mGetMacMachines()
                if _id in _vml:
                    _dom0 = _ml[_m].mGetMacHostName()
                    _host = _mac_config.mGetMacHostName()
                    _ddpair.append([_dom0, _host])

        if self.mIsXmlElasticShape():
            self.__domus_dom0s[clusterId] = _ddpair
            return _ddpair

        _sddpair = self.mSortDom0DomUPair(_ddpair)
        self.__domus_dom0s[clusterId] = _sddpair
        return _sddpair

    def mSortDom0DomUPair(self, aDdPair=None):
        _ddpair = aDdPair
        #
        # Sort list _ddpair using ascending order of _domU hostname
        #
        _sddpair = []
        _domus = []

        # Step1: Create domU list from original pair list.
        for _dom0, _domu in _ddpair:
            _domus.append(_domu)

        # Step2: Sort domUs based on length and then lexical/ascending order.
        _domus.sort(key=lambda item: (len(item), item)) 

        # Step3: Create dom0 and domU pair based on domU sorting.
        for _domu in _domus:
            for _dom0, _domu_from_ddpair in _ddpair:
                if _domu == _domu_from_ddpair:
                    _sddpair.append([_dom0, _domu])
                    break

        return _sddpair

    def mUpdateDom0DomUPair(self, aClusterId=None, aDdPair=None):
        # If aClusterId is not defined return the first cluster
        # By default contains only entry corresponding to the cluster at hand
        if not aClusterId:
            clusterId = self.__clusters.mGetClusters()[0]
        else:
            clusterId = aClusterId

        if self.__cmd == 'oedaaddkey':
            _ddpair = aDdPair
            _sddpair = self.mSortDom0DomUPair(_ddpair)
            self.__domus_dom0s[clusterId] = _sddpair

    """
    ::mSaveClusterConfiguration

    Cluster Configuration JSon format

    cluster_dict = {
        name      : "cluster_name",
        signature : "key_cluster_signature",
        config    : { TBD }
        vmnodes   : [ [ "dom0", "domu", "path2config"], ... ],
        cellnodes : [ "cellnode1", ...]
        subtype   : [ "exaopc|exacm|exabm" ]
        }
    """
    def mSaveClusterConfiguration(self):

        _cluster_dict = {}

        _cluster_dict['name']      = str(uuid.uuid1())
        _cluster_dict['config']    = {}
        _cluster_dict['vmnodes']   = []
        _cluster_dict['cellnodes'] = []
        _cluster_dict['switches']  = []
        _cluster_dict['signature'] = self.mBuildClusterId()

        # bug 29667439 : Service subtype - exaopc, exacm or exabm
        _cluster_dict['subtype'] = []
        #
        # Fill vmnodes and cellnodes fields
        #
        _dom0U_list = self.mGetOrigDom0sDomUs()
        _cell_list  = self.mReturnCellNodes()
        _switch_list= self.mReturnSwitches(True)    # Filter all non pkey switches

        for _dom0, _domu in _dom0U_list:
            _entry = [ _dom0, _domu, '/EXAVMIMAGES/GuestImages/'+_domu ]
            _cluster_dict['vmnodes'].append(_entry)

        for _cell in _cell_list:
            _cluster_dict['cellnodes'].append(_cell)

        for _switch in _switch_list:
            _cluster_dict['switches'].append(_switch)

        if self.__exabm:
            _cluster_dict['subtype'].append('exabm')
        elif self.__exacm:
            _cluster_dict['subtype'].append('exacm')
        else:
            _cluster_dict['subtype'].append('exaopc')

        _str_config = json.dumps(_cluster_dict, sort_keys=True, indent=4, separators=(',',' : '))

        if self.__debug:
            ebLogInfo('Dom0 config : ' + _str_config )
        #
        # Save configuration in /opt/exacloud/clusters/config/ in all dom0
        #
        _ntp = NamedTemporaryFile(delete=False)
        _ntp.file.write(six.ensure_binary(_str_config))
        _ntp.file.close()
        #
        # Hash file name
        #
        _config_name = hashlib.sha224(_cluster_dict['signature'].encode('utf8')).hexdigest()
        #
        # Save the cluster configuration on each dom0 of the VM RAC Cluster
        #
        _dom0_d = {}
        for _dom0, _domU in  _dom0U_list:
            _dom0_d[_dom0] = _dom0

        for _dom0 in list(_dom0_d.keys()):
            if self.__debug:
                ebLogInfo('*** Saving Cluster configuration on dom0: '+_dom0+' in '+'/opt/exacloud/clusters/config/'+_config_name+'.config')
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            _node.mExecuteCmd('mkdir -p /opt/exacloud/clusters/config')
            _node.mExecuteCmd('mkdir -p /opt/exacloud/clusters/config/'+_config_name)
            _node.mCopyFile(_ntp.name, '/opt/exacloud/clusters/config/'+_config_name+'.config')
            _node.mCopyFile(_ntp.name, '/opt/exacloud/clusters/config/vmbackup.config')
            _node.mDisconnect()
        # Cleanup (remove temporary file)
        os.unlink(_ntp.name)
        assert(os.path.exists(_ntp.name)==False)

    def mRemoveClusterConfiguration(self):

        _signature = self.mBuildClusterId()
        _config_name = hashlib.sha224(_signature.encode('utf8')).hexdigest()
        _dom0U_list = self.mReturnDom0DomUPair()

        _dom0_d = {}
        for _dom0, _domU in  _dom0U_list:
            _dom0_d[_dom0] = _dom0

        for _dom0 in list(_dom0_d.keys()):
            if self.__debug:
                ebLogInfo('*** Removing Cluster configuration on dom0: '+_dom0+' in '+'/opt/exacloud/clusters/config/'+_config_name+'.config')
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            _node.mExecuteCmd('mkdir -p /opt/exacloud/clusters/config/'+_config_name)
            _node.mExecuteCmd('rm -f /opt/exacloud/clusters/config/'+_config_name+'.config')
            _node.mExecuteCmd('rm -f /opt/exacloud/clusters/config/'+_config_name)
            _node.mDisconnect()

    def mIsLastCluster(self, aCellList: Sequence[str]) -> bool:
        """
        This function is meant to be used in both MuliVM and SingleVM environments during
        delete service.

        It returns True if there is a single cluster left in the infra, False otherwise.

        Exacloud checks if there are any DiskGroups present, to do to
        it checks if there are griddisks present in the cells.

        If this is the only cluster there should not be any DiskGroups present, or at most
        there should be only DiskGroups from this cluster

        :param aCellList: Sequence of cell hostnames to use to check DiskGroups presence
        :bool: True if this is the last cluster, False otherwise
        """

        # Declare variables to hold the griddisks lists
        _dict_all_griddisks = {}
        _dict_current_cluster_griddisks = {}

        # Acquire lock to avoid inconsistencies with parallel operation on same Infra
        with self.remote_lock():
            for _cell in aCellList:
                ebLogInfo(f"Fetching GridDisks info from: {_cell}")
                with connect_to_host(_cell, get_gcontext()) as _node:

                    # Fetch the complete list of griddisks from _cell
                    _dict_all_griddisks[_cell] = self.mGetStorage().mListCellDG(_node)

                    # Fetch the list of griddisks from current cluster from _cell
                    _dict_current_cluster_griddisks[_cell] = \
                        self.mGetStorage().mGetCurrentClusterGridDisksFromCells(_node)

        ebLogTrace(f"Dumping all griddisk: {_dict_all_griddisks}")
        ebLogTrace("Dumping griddisk belonging to the current cluster: "
            f"{_dict_current_cluster_griddisks}")

        return _dict_all_griddisks == _dict_current_cluster_griddisks

    def mGetVMStatus(self, aDom0Node: exaBoxNode, aDomU:str)-> str:
        """
        This method receives an already connected node to a dom0 and a domU
        hostname. It will check the status of the VM on the given dom0 node
        and return a string representing it

        :param aDom0Node: already connected exaBoxNode to the target dom0
        :param aDomU: a string representing the domU/VM name to check the
            status of

        :returns: a string representing the status of the VM on regards to the
            given dom0. If an error happens while checking the status, an
            appropiate message will be logged and this return an empty string
        """

        # Use 'virsh domstate' on kvm
        if self.mIsKVM():
            ebLogInfo(f"Getting vm status for '{aDomU}' in "
                f"'{aDom0Node.mGetHostname()}', KVM based")
            _bin_virt_cmd = node_cmd_abs_path_check(aDom0Node, "virsh")

        # For XEN use 'xm domstate'
        else:
            ebLogInfo(f"Getting vm status for '{aDomU}' in "
                f"'{aDom0Node.mGetHostname()}', XEN based")
            _bin_virt_cmd = node_cmd_abs_path_check(aDom0Node, "xm", sbin=True)

        _cmd_status = f"{_bin_virt_cmd} domstate {aDomU}"
        ebLogTrace(_cmd_status)
        _out_state = node_exec_cmd(aDom0Node, _cmd_status)

        # Check for errors
        if _out_state.exit_code != 0:
            ebLogError("An error happened while trying to check the status "
                f"of the VM '{aDomU}' on '{aDom0Node.mGetHostname()}'. "
                f"Output of cmd '{_cmd_status}': \n{_out_state}")
            return ""

        # Try to parse virsh/xm output
        else:
            _state = _out_state.stdout.strip()
            ebLogInfo(f"Got state: '{_state}'of the VM '{aDomU}' on "
                f"'{aDom0Node.mGetHostname()}'")
            return _state

    def mRestoreStorageVlan(self, aCellList, aVlanID=None):
        """
        This function restores storage Vlans on cells from aCellList to
        the default value from exabox.conf -> 'cell_storage_vlanid'

        No need to change XML as at the end of delete_service, ECRA will delete the XML
        used for provisioning anyway and only retain the original XML of the cluster
        (which should already have the default value)

        Change must be done on both stre0/stre1 interfaces

        Parameters:
            aCellList(list): list of cells to restore storage VlanId
        Returns:
            None
        """

        if not aCellList:
            ebLogWarn("No cells were provided to restore storage Vlan")
            return

        if aVlanID:
            _vlanId = aVlanID
        else:
            # Get vlanId to use from exabox.conf
            _vlanId = self.mCheckConfigOption("cell_storage_vlanid")

        # Declare/initilaice ebOedacli object
        _oedacli_bin = os.path.join(self.mGetOedaPath(), 'oedacli')
        _oedacli = ebOedacli(_oedacli_bin, os.path.join(self.mGetOedaPath(), "log"),
                            aLogFile="oedacli.log")
        _oedacli.mSetAutoSaveActions(True)
        _oedacli.mSetDeploy(True)

        # Declare command to use
        _hostnames = ", ".join(aCellList)
        _alter_cmd = (f"alter networks vlanid='{_vlanId}' where hostnames='{_hostnames}'"
                            " NETWORKTYPE='private'")

        # Append command to ebOedacli object
        _oedacli.mAppendCommand(_alter_cmd)

        # Run command
        _cmdout = _oedacli.mRun(self.mGetPatchConfig())
        ebLogInfo(_cmdout)

        ebLogInfo(f"Restore of Storage VlanId '{_vlanId}' was succesfull for "
                            f"cells: {_hostnames}")

    def mCleanKeysOedaFolder(self):

        if self.mIsNoOeda():
            return

        if self.mCheckConfigOption('remove_ssh_keys_workdir', 'True'):

            _exakms = get_gcontext().mGetExaKms()
            _exakms.mCleanUpKeysFolder(f"{self.__oeda_path}/WorkDir/")

    def mExecuteCmdParallel(self, aHostCmdDict, aUser="root"):
        """
            aHostCmdDict = {
                "host1": [cmd1]
            }
        """

        def mSingleExecuteCmd(aHost, aRc, aCmdList, aUser="root"):

            _localList = []
            with connect_to_host(aHost, get_gcontext(), username=aUser) as _node:

                for _cmd in aCmdList:
                    _, _o, _e = _node.mExecuteCmd(_cmd)

                    _status = {}
                    _status["cmd"] = _cmd
                    _status["stdout"] = _o.read() if _o else ""
                    _status["stderr"] = _e.read() if _e else ""
                    _status["rc"] = _node.mGetCmdExitStatus()

                    _localList.append(_status)

            aRc[aHost] = _localList

        _plist = ProcessManager()
        _rcs = _plist.mGetManager().dict()

        for _host, _cmdlist in aHostCmdDict.items():

            _p = ProcessStructure(mSingleExecuteCmd, [_host, _rcs, _cmdlist, aUser])
            _p.mSetMaxExecutionTime(30*60) # 30 minutes
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()

        _res = copy.deepcopy(dict(_rcs))
        _res = dict(sorted(_res.items(), key=lambda x: x[0]))

        return _res


    def mRemoveSshKeys(self, aUsers=None):

        def mRemoveSshKeysSingle(aVmName):

            if self.isATP() or self.mIsOciEXACC():
                ebLogInfo("Remove Ssh Keys feature is disable in ATP/OCIEXACC")
                return


            _domU = aVmName

            ebLogInfo('*** Removing SSH Keys in _domU: {0}'.format(_domU))
            _node = exaBoxNode(get_gcontext())

            try:

                if self.mPingHost(_domU) and _node.mIsConnectable(aHost=_domU):

                    _node.mConnect(aHost=_domU)

                    #Remove the keys inside the DomUs
                    if aUsers:
                        _users = aUsers
                    else:
                        _users = self.mCheckConfigOption('remove_users_ssh_authorizedkeys')
                    if _users and (isinstance(_users, list) or isinstance(_users, tuple)):
                        for _user in _users:
                            _home = _node.mSingleLineOutput("cat /etc/passwd | grep '{0}' |  awk -F: '{{print $6}}'".format(_user))
                            if _home is not None and _home != "":
                                _node.mExecuteCmd("sed --follow-symlinks -i '/OEDA_PUB/d' {0}/.ssh/authorized_keys".format(_home))
                                _node.mExecuteCmd("sed --follow-symlinks -i '/EXACLOUD KEY/d' {0}/.ssh/authorized_keys".format(_home))
                    else:
                        ebLogWarn("Invalid parameter, expected list in remove_ssh_keys_users_authorizedkeys: {0}".format(_users))

                    _node.mDisconnect()

            except Exception as e:
                ebLogWarn(e)
                _node.mDisconnect()

            if aUsers:
                _users = aUsers
            else:
                #Remove All the Keys
                _users = ["root", "opc", "oracle", "grid"]
            for _user in _users:
                ebLogInfo(f"deleting user keys : {_user}")

                #Delete Key on KMS Mode
                _exakms = get_gcontext().mGetExaKms()
                _cparam = {"FQDN": _domU, "user": _user}
                _entry = _exakms.mGetExaKmsEntry(_cparam)

                if _entry:
                    _exakms.mDeleteExaKmsEntry(_entry)


        _plist = ProcessManager()
        # Following condition: as For ADBS Exacloud has to remove domU keys from all the domUs and in other case just 
        # domU keys from the domUs returned from mReturnDom0DomUPair()
        if self.mIsAdbs() and self.__cmd in ["vmgi_reshape", "elastic_cell_update"]:
            _ddpair = self.mReturnElasticAllDom0DomUPair()
        else:
            _ddpair = self.mReturnDom0DomUPair()

        for _dom0, _domu in _ddpair:
            _p = ProcessStructure(mRemoveSshKeysSingle, [_domu])
            _p.mSetMaxExecutionTime(-1) # No timeout
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()


    def mHandlerResetVlan(self):

        ebLogInfo("Running mHandlerResetVlan")
        _cell_list = self.mReturnCellNodes()
        aOptions = self.mGetArgsOptions()
        _jconf = aOptions.jsonconf

        if _jconf:
            if 'cells' in _jconf.keys():
                _cell_list = []
                for _h in _jconf['cells']:
                    _cell_list.append(_h)

        # See Bug 35144003
        # Try to restore the VlanID in the Cell's RoCE interfaces
        # Internally OEDA will make sure there are no GridDisks present, in
        # case there are any this operation will fail
        if self.mIsKVM() and self.mIsExabm():
            self.mRestoreStorageVlan(_cell_list)


    def mHandlerRemoveDomUsKeys(self):

        ebLogInfo("Running mRemoveDomUsKeys")

        if self.isATP():
            ebLogInfo('*** SSH Keys for root users are not being removed (Operation not needed in ATP mode)')
        elif not self.mCheckConfigOption('remove_root_access', 'True'):
            ebLogInfo('*** SSH Keys for root users are not being removed (remove_root_access set to False)')
        else:
            self.mRemoveRootAccess()

        if not self.mCheckConfigOption('remove_ssh_keys_domU', 'True'):
            ebLogInfo('*** SSH Keys for root users are not being removed (remove_ssh_keys_domU set to False)')
        else:
            self.mRemoveSshKeys()


    def mRemoveRootAccess(self):

        ebLogInfo('*** Removing Root SSH Keys')
        _toExecute = {}
        _exakms = get_gcontext().mGetExaKms()

        for _, _domU in self.mReturnDom0DomUPair():

            _entries = _exakms.mSearchExaKmsEntries({"FQDN": _domU, "user": "root"}, aRefreshKey=True)
            if _entries:
                _toExecute[_domU] = ['/opt/oracle.cellos/host_access_control rootssh -l']

        try:
            if _toExecute:
                self.mExecuteCmdParallel(_toExecute)
        except Exception as e:
            ebLogError(f"*** Failed to disable root access, ERROR: {e}")

        get_gcontext().mSetRegEntry('opc_enabled', 'True')

    def mCopyFileToClusterConfiguration(self, aLocalFilename, aRemoteFilename):

        ebLogVerbose("mCopyFileToClusterConfiguration: aLocalFilename = %s, aRemoteFilename = %s" % (aLocalFilename, aRemoteFilename))
        _signature = self.mBuildClusterId()
        _config_name = hashlib.sha224(_signature.encode('utf8')).hexdigest()
        _dom0U_list = self.mReturnDom0DomUPair()

        _dom0_d = {}
        for _dom0, _domU in  _dom0U_list:
            _dom0_d[_dom0] = _dom0

        for _dom0 in list(_dom0_d.keys()):
            if self.__debug:
                ebLogInfo('*** Copying Cluster configuration on dom0: '+_dom0+' in '+'/opt/exacloud/clusters/config/'+_config_name+'.config')
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            _node.mExecuteCmd('mkdir -p /opt/exacloud/clusters/config/'+_config_name)
            _node.mCopyFile(aLocalFilename, '/opt/exacloud/clusters/config/'+_config_name+'/'+aRemoteFilename)
            _node.mDisconnect()

    # TODO: This method and mParseOEDALog needs to be refactored as on seperated class
    def mCollectExtraInfoFromErrorOEDA(self, aCmdOutput, aStep=None, aUndo=None):

        def mInspectSshKeysInWorkdir(aEbox, aCmdOutput, aStep, aUndo):

            ebLogInfo("Collection extra information from Workdir")
            _oedaReqPath = f"{aEbox.mGetOedaPath()}/WorkDir"

            _cmd = f"/bin/ls -lthr {_oedaReqPath} | /bin/grep id_rsa"
            _, _o, _ = aEbox.mExecuteLocal(_cmd)
            ebLogInfo(_o)

            _cmd = f"/bin/ls -lthr {_oedaReqPath} | /bin/grep id_ecdsa"
            _, _o, _ = aEbox.mExecuteLocal(_cmd)
            ebLogInfo(_o)


        _out, _err = aCmdOutput
        _step = aStep
        _undo = aUndo
        _err_handler = {
            'Cannot find SSH key': mInspectSshKeysInWorkdir
        }

        for _err_str, _debug_fx in _err_handler.items():

            _error_found = False

            if _err and _err_str in _err:
                _error_found = True

            if _out and _err_str in _out:
                _error_found = True

            if _error_found:
                _debug_fx(self, aCmdOutput, aStep, aUndo)

    # Todo: Retrieve Log/Zip if errors detectedebLogInfo('Patching vm.cfg to enable PKey for VM: '+_domU)
    def mParseOEDALog(self, aCmdOutput, aStep=None, aUndo=None):

        _out, _err= aCmdOutput
        _step = aStep
        _undo = aUndo
        _err_strs  = [
            '[SEVE]',
            'Errors occurred',
            'Errors occured',
            'Error running oracle.onecommand.deploy.machines.VmUtils method createVms',
            'Please run setuprootssh.sh to setup SSH key',
            'ERROR:Lock'
        ]
        _skip_strs = [
            'ORA-15039: diskgroup not dropped',
            'ORA-15063: ASM discovered an insufficient number of disks for diskgroup',
            'Successfully completed execution of step',
        ]
        _ignorable_errors = [
            "configKeys does not exists",
            "there is no configkeys",
            "Error: Command [/opt/oracle.SupportTools/exadataAIDE"
        ]
        _ignorable_error_found = False
        _rc = True
        for _l in _out:
            _ls = _l.strip()
            if any(_ignore_str in _ls for _ignore_str in _ignorable_errors):
                ebLogTrace("Ignoring OEDA error log: {}".format(_ls))
                _ignorable_error_found = True
                continue
            if _step in ["ESTP_POSTGI_NID", "ESTP_INSTALL_CLUSTER"] and _undo:
                if any(skip_str in _ls for skip_str in _skip_strs):
                    ebLogTrace("Skipping error log: {}".format(_ls))
                    _rc = True
                    break
            if any(err_str in _ls for err_str in _err_strs):
                ebLogError(_ls)
                _rc = False
                break
        if _ignorable_error_found:
            _rc = True
        ebLogDebug("mParseOEDALog _rc: %s" % str(_rc))

        if not self.mEnvTarget():
            if not self.mCheckConfigOption("ssh_diagnostic", "False"):
                if not _rc:
                    ebLogTrace("mParseOedaLog found errors, running diagnostic")
                    self.mSshDiagnostic()

        if not _rc:
            self.mCollectExtraInfoFromErrorOEDA(aCmdOutput, aStep, aUndo)

        return _rc

    def mParseOEDAErrorJson(self, aCmdOutput, aStep=None, aUndo=None):

        _oedaReqPath = f"{self.__oeda_path}/WorkDir"
        _filename = f"{self.__oeda_path}/WorkDir/OedaErrors.json"
        _rc = True
        _step = aStep
        _undo = aUndo

        _ignorable_errors = ["/opt/oracle.SupportTools/exadataAIDE -u"]
        _skip_strs = ['ORA-15039: diskgroup not dropped', 'ORA-15063: ASM discovered an insufficient number of disks for diskgroup']
        _skip_exception = False

        if not os.path.exists(_filename):
            return _rc, _skip_exception
        else:

            _rc = False
            ebLogError("OEDA Exception Occurred...")

            with open(_filename) as json_file:
                _error_json = json.load(json_file)

                for _error_list in _error_json["exceptions"]:
                    ebLogError(f"OEDA Error Code: {_error_list['errorCode']}")
                    ebLogError(f"OEDA Error Message {_error_list['errorMsg']}")
                    ebLogError(f"Cause for OEDA Error: {_error_list['cause']}")
                    ebLogInfo(f"Action to be taken: {_error_list['action']}")

                    if any(_ignore_str in _error_list['errorMsg'] for _ignore_str in _ignorable_errors):
                        ebLogTrace("Ignoring OEDA Exception...")
                        _skip_exception = True

                    if _step in ["ESTP_POSTGI_NID", "ESTP_INSTALL_CLUSTER"] and _undo:
                         if any(skip_str in _error_list['errorMsg'] for skip_str in _skip_strs):
                             ebLogTrace("Ignoring OEDA Exception...")
                             _skip_exception = True
                             break

        return _rc, _skip_exception

    def mUpdateRequestClusterName(self, aClusterName):

        _reqobj = self.mGetRequestObj()
        if _reqobj:
            if self.__debug or True:
                ebLogInfo('*** Request Object found updating clustername (%s)' % (aClusterName))
            _db = ebGetDefaultDB()
            _reqobj.mSetClusterName(aClusterName)
            _db.mUpdateRequest(_reqobj)
        else:
            if self.__debug:
                ebLogInfo('*** Request Object _not_ found no - clustername update not possible')

    # This is called by createservice stepwise execution.
    # New function is added because the step names now do not match the
    # step names used for OEDA look up table.
    # Did not want to alter the current mUpdateStatusOEDA, so added a new function
    @measure_exec_time(steal_step_substep)
    def mUpdateStatusCS(self, aStatus, aStep, aStepList, oedaStep=None, aComment=None):

        _reqobj = self.mGetRequestObj()
        if _reqobj:
            if self.__debug:
                ebLogTrace('*** Request Object found updating status')
            _db = ebGetDefaultDB()
            _pos = str(int((100.0 / len(aStepList) * (aStepList.index(aStep)+1))))
            if oedaStep is None or aComment is not None:
                _status_info = str(aStatus)+':'+_pos+':'+str(aComment)
            else:
                _status_info = str(aStatus)+':'+_pos+':'+self.mFetchOedaStep(str(oedaStep))+' - '+self.mFetchOedaString(str(oedaStep))
            ebLogInfo('*** mUpdateStatusCS : updating status_info='+_status_info)
            _reqobj.mSetStatusInfo(_status_info)
            _db.mUpdateStatusRequest(_reqobj)
        else:
            if self.__debug:
                ebLogInfo('*** Request Object _not_ found no update required')

    @measure_exec_time(steal_step_substep)
    def mUpdateStatusOEDA(self, aStatus, aStep, aStepList, aComment=None):

        _reqobj = self.mGetRequestObj()
        if _reqobj:
            if self.__debug:
                ebLogTrace('*** Request Object found updating status')
            _db = ebGetDefaultDB()
            _pos = str(int((100.0 / len(aStepList) * (aStepList.index(aStep)+1))))
            if aStep in OSTP_SKIP_LIST or aComment is not None:
                _status_info = str(aStatus)+':'+_pos+':'+str(aComment)
            else:
                _status_info = str(aStatus)+':'+_pos+':'+self.mFetchOedaStep(str(aStep))+' - '+self.mFetchOedaString(str(aStep))
            _reqobj.mSetStatusInfo(_status_info)
            _db.mUpdateStatusRequest(_reqobj)
        else:
            if self.__debug:
                ebLogInfo('*** Request Object _not_ found no update required')
    
    def mUpdateStatus(self, aStatus, aWithProgress=True):

        _reqobj = self.mGetRequestObj()
        if _reqobj:
            if self.__debug:
                ebLogInfo('*** Request Object found updating status')
            _db = ebGetDefaultDB()
            if aWithProgress:
                _reqobj.mSetStatusInfo('000:: '+aStatus)
            else:
                _reqobj.mSetStatusInfo(aStatus)
            _db.mUpdateStatusRequest(_reqobj)
        else:
            if self.__debug:
                ebLogInfo('*** Request Object _not_ found no update required')


    # BUG 28641550
    def mRemoveDNS(self):
        if self.mCheckConfigOption('disable_dns', 'False'):
            return
        ebLogInfo('*** Disable DNS during delete service for bm')
        _dpairs = self.mReturnDom0DomUPair()
        for _, _domU in _dpairs:
            if self.mPingHost(_domU):
                try:
                    _nodeU = exaBoxNode(get_gcontext())
                    _nodeU.mConnect(aHost=_domU)
                    _cmd = "sed 's/^nameserver.*/#&/' -i /etc/resolv.conf "
                    _nodeU.mExecuteCmdLog(_cmd)
                    _cmd = "sed 's/^hosts:.*/hosts: files/' -i /etc/nsswitch.conf"
                    _nodeU.mExecuteCmdLog(_cmd)
                    _cmd = "cd /etc/init.d; service nscd restart"
                    _nodeU.mExecuteCmdLog(_cmd)
                    _nodeU.mDisconnect()
                except:
                    ebLogWarn('*** Unable to connect to host: %s' % (_domU))
                    continue

    def mHandlerFaultInjection(self, aOptions=None):
        """ 
        API to handle Fault Injection API.
        mHandlerFaultInjection method has the injected the failure for following use cases:
        1. Check the Instance health for the hostname
        2. Stopping the host using the ilom
        3. Network Partition from Compute and Cell Server using shutting down str0/str1
        """
        if not aOptions:
            aOptions = self.mGetArgsOptions()
            if aOptions is None:
                ebLogWarn("Missing field in the payload")
                return 1
        try:
            ebLogInfo(f"*** {aOptions} : Fault Injection") 
            if aOptions.jsonconf is not None:
                if aOptions.jsonconf.get('hostname', None) is None:
                    _msg = "*** hostname key is not present in the jsonconf"
                    ebLogWarn(_msg)
                    raise ExacloudRuntimeError(aErrorMsg=_msg)

                if aOptions.jsonconf.get('optype', None) is None:
                    _msg = "*** optype key is not present in the jsonconf"
                    ebLogWarn(_msg)
                    raise ExacloudRuntimeError(aErrorMsg=_msg)

                if aOptions.jsonconf.get('nodetype', None) is None:
                    _msg = "*** nodetype key is not present in the jsonconf"
                    ebLogWarn(_msg)
                    raise ExacloudRuntimeError(aErrorMsg=_msg)

                _faultInjection = ebCluFaultInjection(self, aOptions)
                _faultInjection.mHandleRequest()

        except Exception as exp:
            ebLogError('*** Exception while doing faultinjection')
            _err = f"Failed to perform fault injection to target host. Exception: {exp}"
            ebLogError(_err)
            raise ExacloudRuntimeError(0x0801, 0xA, _err)

        return 0

    def mHandlerSecureCellErase(self, aOptions=None):
        """
        API to handle secure cell erase.
        mCellSecureShredding method has the validations for:
        - If ecra API has skip_serase as True, exacloud will raise ExacloudRuntimeError
        and not proceed further.
        - If it is exascale environment, exacloud will raise ExacloudRuntimeError
        and not proceed further.
        - If it is a dev or qa environment and crypto erasure is not supported on the cell,
        secure cell shredding will not be performed but no exception will be raised and API will return.
        - If cellerase_pass is not set in exabox.conf or it is set to an invalid option,
        ExacloudRuntimeError will be raised.
        - If grid disks are present on the cells, ExacloudRuntimeError will be raised.
        - If there was an issue running the secure erase command, ExacloudRuntimeError will be raised.
        """
        if not aOptions:
            aOptions = self.mGetArgsOptions()
        # Currently, infrastructure deletion does the secure cell shredding but if not
        # applicable for any of the conditions - the below API returns.
        # For this separate secure cell shredding API, we will raise an 
        # ExacloudRuntimeError to let ecra know if there was an issue observed from our end
        self.mCellSecureShredding(aOptions, aInfraDelete=True, aSecureEraseAPI=True)

    def mCellSecureShredding(self, aOptions=None, aForce=False, aInfraDelete=False, aSecureEraseAPI=False, aCellList=None, aCellData=None):
        # Do not shred if another cluster's VMs exist except if it is infra deletion
        # For infra deletion, grid disks will be checked if any are present in further checks.
        if not aInfraDelete and self.__shared_env and not self.mIsLastCluster(self.mReturnCellNodes()):
            _msg = "*** Another cluster's VMs exists. Secure Cell Shredding will not be performed"
            ebLogWarn(_msg)
            if aSecureEraseAPI:
                raise ExacloudRuntimeError(aErrorMsg=_msg)
            return
        if aOptions.skip_serase and (aOptions.skip_serase == True or str(aOptions.skip_serase).lower() == 'true'):
            _msg = '*** Secure Cell Shredding has been disabled via cmd line option sse'
            ebLogWarn(_msg)
            if aSecureEraseAPI:
                raise ExacloudRuntimeError(aErrorMsg=_msg)
            return
        if self.mIsExaScale():
            _msg = '*** Secure Cell Shredding has been disabled vi ExaScale'
            ebLogWarn(_msg)
            if aSecureEraseAPI:
                raise ExacloudRuntimeError(aErrorMsg=_msg)
            return

        # Get list of cell nodes fqdn from the payload
        _cell_nodes = []
        _clu_utils = ebCluUtils(self)
        if aSecureEraseAPI and aOptions.jsonconf is not None and 'cell_nodes' in list(aOptions.jsonconf.keys()):
            _cell_nodes_encoded = aOptions.jsonconf['cell_nodes']
            for _cell_node in _cell_nodes_encoded:
                if _clu_utils.mIsBase64(_cell_node):
                    _cell_nodes.append(base64.b64decode(_cell_node).decode('utf-8'))
                else:
                    _msg = "*** Invalid cell nodes received via payload for the Secure Cell Shredding API."
                    ebLogError(_msg)
                    raise ExacloudRuntimeError(aErrorMsg=_msg)
        # cell nodes list can be passed as part of delete cell
        if aCellList:
            _cell_nodes = aCellList
        # If no cell nodes are provided in json and not as arguments, get the list of cell nodes from the xml
        if not _cell_nodes and not aCellList:
            _cell_nodes = list(self.mReturnCellNodes().keys())

        # In dev/qa env, disable cell secire erase if crypto erase is not supported.
        if self.mCheckConfigOption('target_env') in ['dev', 'qa'] and not self.mIsCryptoEraseSupported(aCellNodes=_cell_nodes):
            _msg = '*** Secure Cell Shredding has been disabled in Dev and QA env'
            ebLogWarn(_msg)
            return

        #
        # Cell Secure Erase (7pass flashdisk only)
        #
        if self.mCheckConfigOption('cellerase_pass') is not None:
            _pass_nb = self.mCheckConfigOption('cellerase_pass').upper()
            ebLogInfo('*** EraseCell setting is: %s' % (_pass_nb))
            if _pass_nb in ['0PASS', '1PASS', '3PASS', '7PASS']:
                def cell_command(_cell_name, _cert_dict):
                    _node = exaBoxNode(get_gcontext(), Cluctrl = self)
                    _node.mConnect(aHost=_cell_name)
                    if _pass_nb != '0PASS':
                        #
                        # Check/Verify no Grid Disks are left over (e.g ACFS)
                        #
                        _griddisks = self.__storage.mListCellDG(_node)
                        if len(_griddisks):
                            _msg = '*** Cell Secure Erase can _NOT_ proceed with active Grid Disks present: %s' % (str(_griddisks))
                            ebLogWarn(_msg)
                            if aSecureEraseAPI:
                                raise ExacloudRuntimeError(aErrorMsg=_msg)
                        else:
                            #
                            # Drop FLASH FIRST
                            #
                            _cmd_cellX = 'cellcli -e alter flashcache ALL FLUSH'
                            _node.mExecuteCmdLog(_cmd_cellX)
                            ebLogInfo('*** %s : FLASHCACHE FLUSHED' % (_cell_name))
                            _cmd_cellX = 'cellcli -e DROP FLASHCACHE ALL'
                            _node.mExecuteCmdLog(_cmd_cellX)
                            ebLogInfo('*** %s : FLASHCACHE DROPED' % (_cell_name))
                            _cmd_cellX = 'cellcli -e DROP FLASHLOG ALL'
                            _node.mExecuteCmdLog(_cmd_cellX)
                            ebLogInfo('*** %s : FLASHLOG DROPED' % (_cell_name))
                            #
                            # Drop PMEM Cache and PMEM logs - ignore errors
                            #
                            ebLogInfo('*** Attempting to drop PMEMCACHE/PMEMLOG, ignore errors on cells without PMEM.')
                            _cmd_cell_pmemcache = 'cellcli -e DROP PMEMCACHE'
                            _node.mExecuteCmdLog(_cmd_cell_pmemcache)
                            ebLogInfo(f'*** {_cell_name} : PMEMCACHE DROPPED')
                            _cmd_cell_pmemlog = 'cellcli -e DROP PMEMLOG'
                            _node.mExecuteCmdLog(_cmd_cell_pmemlog)
                            ebLogInfo(f'*** {_cell_name} : PMEMLOG DROPPED')

                            _exadataImage = self.mGetImageVersion(_cell_name)
                            _skip_flashdisk_erase = self.mCheckConfigOption('cellerase_skip_flashdisk','True')
                            _skip_hdd_erase = self.mCheckConfigOption('skip_harddisk_erase')
                            _execute_legacy_secureerase = self.mCheckConfigOption('legacy_secureerase', 'True')
                            # If exadata image version is below 19.1.0, secureerase with crypto option is not supported.
                            # We will then go with legacy secure erase using cellcli command. With legacy mode, certificate
                            # generation is not supported.
                            # If exabox conf option legacy_secureerase is set to True, then also legacy secure erase
                            # will be executed.
                            if version_compare(_exadataImage, "19.1.0") < 0 or _execute_legacy_secureerase:
                                #
                                # Secure FLASHDISK erase w/ 7 PASS
                                #
                                if not _skip_flashdisk_erase or aForce:

                                    _cmd_cellY = 'cellcli -e DROP CELLDISK ALL FLASHDISK ERASE=7PASS'
                                    _node.mExecuteCmdLog(_cmd_cellY)
                                    ebLogInfo('*** %s : FLASHDISK ERASED 7PASS' % (_cell_name))
                                #
                                # If skip_harddisk_erase is set bypass celldisk harddisk erase
                                # This option is only available for testing purpose to avoid very long
                                # delay during service deletion (e.g. hours/days)
                                #
                                if _skip_hdd_erase is None or aForce:
                                    #
                                    # If cell deletion is not successful in the first attempt,
                                    # a second attempt will be performed with the FOCE option.
                                    # However even it this succeeds, we still need to erase the
                                    # celldisks once again to make sure the Crypto erase is
                                    # successful.
                                    #
                                    _cmd_cellZ = f'cellcli -e DROP CELLDISK ALL HARDDISK ERASE={_pass_nb}'
                                    _rc, _, _ = node_exec_cmd(_node, _cmd_cellZ, log_error=True, log_stdout_on_error=True)
                                    if _rc:
                                        try:
                                            node_exec_cmd(_node, f'{_cmd_cellZ} FORCE', log_error=True, log_stdout_on_error=True)
                                            _cmd_cellY = 'cellcli -e create celldisk all'
                                            node_exec_cmd(_node, _cmd_cellY, log_error=True, log_stdout_on_error=True)
                                            node_exec_cmd(_node, f'{_cmd_cellZ} FORCE', log_error=True, log_stdout_on_error=True)
                                        except Exception as exp:
                                            _err = "*** Some cell disks status is not normal and couldn't get fixed. ***"
                                            ebLogError(_err)
                                            raise ExacloudRuntimeError(0x0743, 0xA, _err, Cluctrl = self)
                            else:
                                if (not _skip_flashdisk_erase and not _skip_hdd_erase) or aForce:
                                    _secure_erase = SecureErase(self)
                                    # Get physical disk info before dropping flash and hard disks
                                    _flashdisks = []
                                    _harddisks = []
                                    _secure_erase_cellcli = self.mCheckConfigOption('secure_erase_using_cellcli', 'True')
                                    _min_exadata_ver_secureerase_using_cellcli = self.mCheckConfigOption('min_exadata_ver_secureerase_using_cellcli')
                                    if version_compare(_exadataImage, _min_exadata_ver_secureerase_using_cellcli) >= 0 or _secure_erase_cellcli:
                                        _secure_erase.mSecureEraseUseCellcli(_cell_name, _node, _cert_dict)
                                    else:
                                        _flashdisks = _secure_erase.mGetPhysicalCellDisks(_node, 'FlashDisk')
                                        _harddisks = _secure_erase.mGetPhysicalCellDisks(_node, 'HardDisk')
                                        if not _flashdisks or not _harddisks:
                                            # We need to recreate celldisks if we did not find flashdisks or harddisks
                                            # and get the _flashdisks and _harddisks again
                                            _node.mExecuteCmdLog('cellcli -e drop celldisk all')
                                            _node.mExecuteCmdLog('cellcli -e create celldisk all')
                                            _flashdisks = _secure_erase.mGetPhysicalCellDisks(_node, 'FlashDisk')
                                            _harddisks = _secure_erase.mGetPhysicalCellDisks(_node, 'HardDisk')
                                        _cmd_flashdisk_drop = 'cellcli -e DROP CELLDISK ALL FLASHDISK'
                                        _cmd_hdd_drop = f'cellcli -e DROP CELLDISK ALL HARDDISK'
                                        _node.mExecuteCmdLog(_cmd_flashdisk_drop)
                                        ebLogInfo(f"Flashdisk dropped using command: {_cmd_flashdisk_drop} on cell {_cell_name}")
                                        _node.mExecuteCmdLog(_cmd_hdd_drop)
                                        ebLogInfo(f"Harddisk dropped using command: {_cmd_hdd_drop} on cell {_cell_name}")
                                        _secure_erase.mSecureEraseCertificate(_cell_name, _node, _exadataImage, _cert_dict, _flashdisks,
                                                                            _harddisks)

                            ebLogInfo('*** %s : HARDDISK ERASED' % (_cell_name))
                            _cmd_cellY = 'cellcli -e create celldisk all'
                            _node.mExecuteCmdLog(_cmd_cellY)
                            ebLogInfo(f"Generated blank celldisks on the cell {_cell_name}")
                    _node.mDisconnect()
                #
                # Run Cell erasing in parallel to save time
                #
                _plist = ProcessManager(aTimeoutBehavior=TimeoutBehavior.IGNORE, aExitCodeBehavior=ExitCodeBehavior.IGNORE)
                # 30 minutes timeout for X7+ and 72 hours for previous models
                _ptimeout = 72*60*60
                _exadata_cell_model = self.mGetExadataCellModel()
                _cutOff_model = 'X8'
                if mCompareModel(_exadata_cell_model, _cutOff_model) >= 0 \
                    and self.mIsCryptoEraseSupported(aCellNodes=_cell_nodes):
                    _ptimeout = 30*60
                _cert_dict = _plist.mGetManager().dict()
                for _cell_name in _cell_nodes:
                    _cell_name = _cell_name.strip()
                    ebLogTrace("Timeout for secure cell shredding on {0} is set to {1} minutes.".format(_cell_name,_ptimeout/60))
                    _p = ProcessStructure(cell_command, [_cell_name, _cert_dict])
                    _p.mSetMaxExecutionTime(_ptimeout)
                    _p.mSetJoinTimeout(60)
                    _p.mSetLogTimeoutFx(ebLogWarn)
                    _plist.mStartAppend(_p)

                _plist.mJoinProcess()
                _cert_dict = dict(_cert_dict)
                if aCellData:
                    _cert_dict.update(aCellData)
                # Update the base64 encoded certificate in data column of requests table for each cell
                _clu_utils.mUpdateRequestObjectData(_cert_dict)
                return True

            else:
                _msg = '*** Invalid option for cellerase_pass in exabox.conf'
                ebLogInfo(_msg)
                if aSecureEraseAPI:
                    raise ExacloudRuntimeError(aErrorMsg=_msg)
        else:
            _msg = "*** Cells and Cell Disks secure erase has not been enabled in exabox.conf. "\
                   "Set cellerase_pass to one of ['1PASS', '3PASS', '7PASS']."
            ebLogInfo(_msg)
            if aSecureEraseAPI:
                raise ExacloudRuntimeError(aErrorMsg=_msg)

    def mIsCryptoEraseSupported(self, aCellNodes=None):
        ebLogInfo('*** Checking for crypto erase support on all cells.')

        _cell_nodes = []
        if aCellNodes:
            _cell_nodes = aCellNodes
        else:
            _cell_nodes = list(self.mReturnCellNodes().keys())
        _min_exadata_ver_secureerase_using_cellcli = self.mCheckConfigOption('min_exadata_ver_secureerase_using_cellcli')
        for _cell_name in _cell_nodes:
            _cell_name = _cell_name.strip()
            with connect_to_host(_cell_name, get_gcontext()) as _cell_node:
                _python_abs_path = node_cmd_abs_path_check(_cell_node, "python")
                _cmd = "{} /opt/oracle.cellos/lib/python/secureeraser --erase --all --hdd_erasure_method crypto --is_eligible".format(_python_abs_path)
                if version_compare(self.mGetImageVersion(_cell_name), _min_exadata_ver_secureerase_using_cellcli) >= 0:
                    ebLogInfo(f"Secureeraser command is not supported for image version >= {_min_exadata_ver_secureerase_using_cellcli}."\
                        "But, we know that crypto erase is supported with cellcli drop command with erase option as 7pass.")
                else:
                    _cell_node.mExecuteCmdLog(_cmd)
                    if _cell_node.mGetCmdExitStatus() != 0:
                        ebLogWarn("Cryptographic secure erase is not supported on cell {}".format(_cell_name))
                        return False
                ebLogTrace("Cryptographic secure erase is supported on cell {}".format(_cell_name))

        return True


    def mCellAssertNormalStatus(self, aOptions):
        """
        Make sure all of the cells disks state of all of the nodes is normal.
        Otherwise, try to fix them with mCellSecureShredding, and if that
        fails, then throw an Exception.
        """

        def allCellDisksAreNormal():
            _good_celldisks = True
            for _cell_name in self.mReturnCellNodes().keys():
                try:
                    _node = exaBoxNode(get_gcontext(), Cluctrl = self)
                    _node.mConnect(aHost=_cell_name)

                    _celldisks = self.__storage.mListCellDisksAttributes(_node, aAttributes=['STATUS'])
                    for _celldisk_name, _celldisk_attrs in _celldisks.items():
                        _celldisk_status = _celldisk_attrs['STATUS']
                        if _celldisk_status != 'normal':
                            ebLogWarn(f'*** Cell disk {_celldisk_name} state is {_celldisk_status}. At cell {_cell_name} ***')
                            _good_celldisks = False
                finally:
                    _node.mDisconnect()
            return _good_celldisks

        if not allCellDisksAreNormal():
            ebLogWarn('*** Bad Cell disks found. Trying to fix them with cells secure shredding ***')
            self.mCellSecureShredding(aOptions,aForce=True)
            if not allCellDisksAreNormal():
                _err = "*** Some cell disks status is not normal and couldn't get fixed. ***"
                ebLogError(_err)
                raise ExacloudRuntimeError(0x0743, 0xA, _err, Cluctrl = self)

    def mCheckNumVM(self, aExcludeOwn=False):
        _numVMs = 0

        for _dom0, _domU in self.mReturnDom0DomUPair():
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)

            # Some VMs may be shutdown. Hence safer to check count in GuestImages
            # if aExcludeOwn, execloud out current cluster's domU
            if aExcludeOwn:
                _cmd = "ls -l /EXAVMIMAGES/GuestImages/ |grep -Ev 'total|%s' | wc -l" % _domU
            else:
                _cmd = "ls -l /EXAVMIMAGES/GuestImages/ |grep -v total | wc -l"

            _i, _o, _e = _node.mExecuteCmd(_cmd)
            _out = _o.readlines()
            if not _out or len(_out) == 0:
                ebLogError('*** VM count unavailable on %s' %(_dom0))
                _numVMs = -1
                _node.mDisconnect()
                return _numVMs
            else:
                _imgcnt = int(_out[0].strip())
                _node.mDisconnect()

                if aExcludeOwn:
                    ebLogTrace('*** Other %s VMs are currently running on %s' %(str(_imgcnt), _dom0))
                else:
                    ebLogTrace('*** %s VMs are currently running on %s' %(str(_imgcnt), _dom0))

                #set higher vm count
                _numVMs = max(_numVMs, _imgcnt)

        return _numVMs
    # end

    # delete ssh keys on the filesystem. This will be called only in KMS mode
    def mHandlerDeleteOndiskKeys(self, aOptions=None):

        _exakms = get_gcontext().mGetExaKms()

        _clustersOedaPath = self.mCheckConfigOption('export_import_keys_folder')
        if os.path.exists(_clustersOedaPath):
            #Its ok to remove *all* keys . No need to calculate hostnames which needs switch connect to calculate additional switch hostnames !
            _exakms.mCleanUpKeysFolder(_clustersOedaPath)

    def mSyncKVDBOverNetworkSend(self, aOptions=None):
        # Check that this is an ExaCC master CPS
        if not self.__ociexacc or not self.mIsExaCCMasterCPS():
            return 
        
        # Sync exakv.db file in standby CPS
        ebLogInfo("Running mSyncKVDBOverNetworkSend, this could take a while")
        return ExaKmsEndpoint(aOptions).mSyncKVDBSend()

    def mSyncKeysOverNetworkSend(self, aOptions=None, aForceFullSync=False, aForceDiffSync=False):

        if self.__ociexacc:

            if self.mIsExaCCMasterCPS():

                if ebCluCmdCheckOptions(self.__cmd, ['fullsync_keys']) or aForceFullSync:
                    ebLogInfo("Running mSyncKeysOverNetworkSend (FullSync), this could take a while")
                    return ExaKmsEndpoint(aOptions).mSyncKeysSend(aFullSync=True)

                if ebCluCmdCheckOptions(self.__cmd, ['diffsync_keys']) or aForceDiffSync:
                    ebLogInfo("Running mSyncKeysOverNetworkSend (DiffSync), this could take a while")
                    return ExaKmsEndpoint(aOptions).mSyncKeysSend(aFullSync=False)

    def mSaveOEDASSHKeys(self, aOptions=None,aQuiet=False):
        """
        TODO: Make this remote oeda friendly
        :param aOptions:
        :return: NA
        """

        if not ebCluCmdCheckOptions(self.__cmd, ['validate_exakms_entries']):
            return

        _oedaReqPath = f"{self.__oeda_path}/WorkDir"
        _oedaReqPath = self.__oeda_path+'/WorkDir/'

        _exakms = get_gcontext().mGetExaKms()

        _exakms.mRestoreEntriesFromFolder(
            _oedaReqPath,
            self.mGetExaKmsHostMap(),
            self.mExaKmsEntryExtraValidation
        )

        ebLogTrace('*** mSaveOEDASSHKeys: Complete')

    def mRestoreOEDASSHKeys(self, aOptions=None, aForce=False):
        """
        Populate OEDA staging area (WorkDir) with the appropriate SSH Keys require for pwd less connectivity
        :param aOptions:
        :return: NA
        """

        if not aForce:

            if not ebCluCmdCheckOptions(self.__cmd, ['validate_exakms_entries']):
                #Bug 34537038 : mRestoreOEDASSHKeys function is taking more time because it 
                #is trying to store keys not part of cluster which result in OS linear search
                # until the above issue gets fixed, save oeda keys enabled only for exachk command 
                # as other health command not using oeda keys 
            
                if self.__cmd == 'checkcluster' and aOptions and aOptions.healthcheck == 'exachk':
                    ebLogInfo("Exachk command needs oeda keys,continue with Save OEDA keys");
                else:
                    return

            if not self.mIsClusterLessXML() and (not self.__oeda_path or not self.__key):
                if aOptions.debug:
                    ebLogWarn('*** OEDA Path has not been set ***')
                return -1

        _oedaReqPath = f"{self.__oeda_path}/WorkDir"

        _exakms = get_gcontext().mGetExaKms()
        _exakms.mSaveEntriesToFolder(
            _oedaReqPath,
            self.mGetExaKmsHostMap(aAddIloms=False),
            self.mExaKmsEntryExtraValidation
        )

        # Correctness
        if len(glob.glob(f"{_oedaReqPath}/id_rsa*")) == 0 or \
           len(glob.glob(f"{_oedaReqPath}/id_ecdsa*")) == 0:

            _clustersOedaPath = self.mCheckConfigOption('export_import_keys_folder')
            if os.path.exists(_clustersOedaPath):

                _exakms = get_gcontext().mGetExaKms()

                _exakms.mRestoreEntriesFromFolder(
                    _clustersOedaPath,
                    self.mGetExaKmsHostMap(),
                    self.mExaKmsEntryExtraValidation
                )

                _exakms.mSaveEntriesToFolder(
                    _oedaReqPath,
                    self.mGetExaKmsHostMap(aAddIloms=False),
                    self.mExaKmsEntryExtraValidation
                )

        #
        # Validate that we have pushed ssh_keys to WorkDir
        #
        if len(glob.glob(f"{_oedaReqPath}/id_rsa*")) == 0 or \
           len(glob.glob(f"{_oedaReqPath}/id_ecdsa*")):
            _msg = "OEDA_STAGING_ERROR: NO SSH_KEYS are present in OEDA WorkDir staged environment."
            ebLogError(80 * '*')
            ebLogError(f"*** {_msg}")
            ebLogError(f'>>> {_oedaReqPath}')
            ebLogError(80 * '*')
            if ebCluCmdCheckOptions(self.__cmd, ['raise_when_no_oeda_ssh_keys']):
                raise ExacloudRuntimeError(0x0107, 0xA, f"_msg")

        if self.mIsIPv6DualStackPresent() or self.mIsIPv6SingleStackPresent():
            # OEDA needs keys with name having short client hostname as well for IPv6
            _files = glob.glob(f'{self.mGetOedaPath()}/WorkDir/*')
            _nat_domus = {}
            for _clientdom0, _clientdomu in self.mReturnDom0DomUPair():
                _nat_domu = get_gcontext().mGetRegEntry('_natHN_' + _clientdomu)
                _nat_domus[_nat_domu.split('.')[0]] = _clientdomu.split('.')[0]
            ebLogInfo(f"mSaveEntriesToFolder _nat_domus - {_nat_domus}")
            for _file in _files:
                for _domu,_clientdomu in _nat_domus.items():
                    if _domu in _file:
                        src = _file 
                        dst = _file.replace(_domu, _clientdomu)
                        shutil.copyfile(src, dst)

    def mPushCertificatestoDom0(self):

        _dom0_cert_source_file = self.mCheckConfigOption('dom0client_certificate_path')
        _dom0_key_source_file = self.mCheckConfigOption('dom0client_key_path')
        _dom0_cert_destination_dir = '/opt/oci/exacc/dom0client/secure'
        _dom0_cert_destination_file = _dom0_cert_destination_dir + '/dom0client.crt'
        _cert_source_destination_mapping = {
            f'{_dom0_cert_source_file}' : f'/opt/oci/exacc/dom0client/secure/dom0client.crt',
            f'{_dom0_key_source_file}' : f'/opt/oci/exacc/dom0client/secure/dom0client.key'
        }

        try:
            for _source_file , _ in _cert_source_destination_mapping.items():
                if not os.path.isfile(_source_file):
                    ebLogError(f"*** source certificate/key {_source_file} does not exist")
                    raise ExacloudRuntimeError(0x0821, 0xA, 'Certificate/Key Missing!')   
            
            for _dom0, _ in self.mReturnDom0DomUPair():
                 with connect_to_host(_dom0, get_gcontext()) as _node:
                    # Existence of destination path checked, if not create it
                    if not _node.mFileExists(_dom0_cert_destination_file):
                        _mkdir_cmd = node_cmd_abs_path_check(node=_node, cmd="mkdir")
                        _node.mExecuteCmdLog(f"{_mkdir_cmd} -p {_dom0_cert_destination_dir}")

                    for _source_file , _dest_file in _cert_source_destination_mapping.items():
                        _node.mCopyFile(_source_file, _dest_file)
                        ebLogInfo(f'*** Copied {_source_file} to {_dest_file} on {_dom0}')                  
        except Exception as e:
            ebLogError(f' Error while copying certificate/key to dom0: {e}')
            raise ExacloudRuntimeError(0x0821, 0xA, e)

    def mVMImagesShredding(self,aOptions=None):

        if aOptions.skip_serase:
            ebLogWarn('*** Secure VM Shredding has been disabled via cmd line option sse')
            return

        if self.mCheckConfigOption('shredding_enabled') is not None:
            _coptions = self.mCheckConfigOption('shredding_enabled').upper()
            ebLogInfo('*** mVMImagesShredding: VM Images shredding setting is: %s' % (_coptions))
            if _coptions == 'TRUE':
                def command(_dom0,_domu):
                    _node = exaBoxNode(get_gcontext())
                    _node.mConnect(aHost=_dom0)
                    # VM are already destroyed in the Skip steps mode
                    if self.__run_all_undo_steps:
                        _vmhandle = self.__CompRegistry.mGetComponent("vm_operations")
                        _vmhandle.mSetOVMCtrl(aCtx=get_gcontext(), aNode=_node)
                        _rc = _vmhandle.mDispatchEvent("destroy", aOptions, aVMId=_domu, aCluCtrlObj=self)
                        if _rc:
                            _error_str = '*** Error 0x%x VM %s did not destroy properly' % (_rc, _domu)
                            ebLogError(_error_str)
                            raise ExacloudRuntimeError(0x0440, 0xA, _error_str, aStackTrace=False)
                    _path = '/EXAVMIMAGES/GuestImages/'+_domu+'/'
                    _pass = self.mCheckConfigOption('vmerase_pass')
                    if _pass is None:
                        _pass = 1
                    else:
                        try:
                            _pass = int(_pass[:_pass.find('pass')])
                        except:
                            _pass = 3
                    _cmd  = 'shred --verbose -n%d '+_path+'*.img'
                    _cmd  = _cmd % (_pass)
                    _cmd_to_print_time = "echo \"Start time: "+_dom0+"->"+_domu+" is `date`\""
                    _node.mExecuteCmdLog(_cmd_to_print_time)
                    _cmd_toprint_filelist = "echo \"Cumulative image size on "+_dom0+" for "+_domu+" is `du -hc "+_path+"*.img | tail -1`\""
                    _node.mExecuteCmdLog(_cmd_toprint_filelist)
                    ebLogInfo('Running cmd: '+_cmd+' (this operation can take significant amount of time)')
                    _node.mExecuteCmdLog(_cmd)#removing dev/null output cascading to show logs from shredding operation.
                    _cmd_to_print_time = "echo \"End time: "+_dom0+"->"+_domu+" is `date`\""
                    _node.mExecuteCmdLog(_cmd_to_print_time)
                    ebLogInfo('Finished shredding images on %s:%s' % (_dom0, _domu))
                    _node.mDisconnect()
                #
                # Bring down VM and shred files
                #
                _plist = ProcessManager()

                for _dom0, _domu in self.mReturnDom0DomUPair():
                    _p = ProcessStructure(command, [_dom0,_domu])
                    _p.mSetMaxExecutionTime(-1) # No timeout
                    _p.mSetJoinTimeout(5)
                    _p.mSetLogTimeoutFx(ebLogWarn)
                    _plist.mStartAppend(_p)

                _plist.mJoinProcess()
                ebLogInfo('*** Parallel Shredding is done.')
            else:
                ebLogInfo('*** Shredding option has been disabled')
        else:
            ebLogInfo('*** Shredding option not available')

    @measure_exec_time(steal_step_substep)
    def mLogStepElapsedTime(self,aStartTime,aComment=''):

        _delta = time.time() - aStartTime
        _tsec   = _delta % 60
        _tmin   = _delta % 3600 / 60
        _thur   = _delta / 3600
        ebLogInfo('*** Elapsed time for last operation/step : %02d:%02d:%02d / %s (%s)' % (int(_thur),int(_tmin),int(_tsec), time.strftime("%H:%M:%S"), aComment))
        ebLogInfo('|_%s_|' % (80*'_'))

    @ebRecordReplay.mRecordReplayWrapper
    def mAcquireRemoteLock(self, aLockName='Default', **aExtraLockInfo):
        """
        DEPRECATED: Acquire remote lock.

        Please consider using remote lock context manager instead, this way
        the 'with' keyword will handle lock release even in the case of an
        exception.

        DEPRECATED lock example:

        1 try:
        2     self.mAcquireRemoteLock()
        3     # Protected operations on dom0
        4     # ...
        5 finally:
        6     self.mReleaseRemoteLock()

        RECOMMENDED lock example:

        1 with self.remote_lock():
        2     # Protected operations on dom0
        3     # ...

        """
        self.remote_lock.acquire(aLockName, **aExtraLockInfo)

    @ebRecordReplay.mRecordReplayWrapper
    def mReleaseRemoteLock(self, aLockName='Default', aUUID=None):
        """
        DEPRECATED: Release remote lock.

        Please consider using remote lock context manager instead, this way
        the 'with' keyword will handle lock release even in the case of an
        exception.

        DEPRECATED lock example:

        1 try:
        2     self.mAcquireRemoteLock()
        3     # Protected operations on dom0
        4     # ...
        5 finally:
        6     self.mReleaseRemoteLock()

        RECOMMENDED lock example:

        1 with self.remote_lock():
        2     # Protected operations on dom0
        3     # ...

        """

        self.remote_lock.release(aLockName, aUUID)

    def mResetRemoteLocks(self, aLockName=None):
        """ Reset (remove) all locks on all dom0's """
        self.remote_lock.remove_all_locks()

    def mHandlerResetRemoteLocks(self,aLockName=None):
        return self.mResetRemoteLocks()

    def mCheckClusterNetworkType(self):

        _full = False
        for _, _domU in self.mReturnDom0DomUPair():
            _domU_mac = self.__machines.mGetMachineConfig(_domU)
            _domU_net_list = _domU_mac.mGetMacNetworks()
            for _net in _domU_net_list:
                _priv = self.__networks.mGetNetworkConfig(_net)
                _type = _priv.mGetNetType()
                if _type == 'backup':
                    _full = True

        if _full is True:
            ebLogInfo('*** Cluster environment detected as fully cabled system')
        else:
            ebLogInfo('*** Cluster environment detected as partially cabled system')

        return _full

    def mEnvTarget(self):
        """
        Try detect target environment - wether it's Prod or Dev/QA environment based on known QA/DEV domainname.
        Default is Prod in case we can not determine it we are dealing with a QA/DEV env.
        """
        _rc = None
        if self.mCheckConfigOption('target_env', 'prod'):
            _rc = True
        if self.mCheckConfigOption('target_env') in ['dev', 'qa']:
            _rc = False
        if _rc is None and self.mCheckConfigOption('target_env', 'discovery'):
            for _dom0, _ in self.mReturnDom0DomUPair():
                _dom0_mac = self.__machines.mGetMachineConfig(_dom0)
                _dom0_net_list = _dom0_mac.mGetMacNetworks()
                for _net in _dom0_net_list:
                    _priv = self.__networks.mGetNetworkConfig(_net)
                    _domain = _priv.mGetNetDomainName()
                    if _domain in [ 'us.oracle.com', 'usdv1.oraclecloud.com']:
                        _rc = False
                        break
                if _rc is not None:
                    break
        if _rc is None:
            ebLogInfo('*** TargetEnv discovery failed defaulting to PROD')
            _rc = True
        if self.__debug:
            if _rc is True:
                _target = 'PROD'
            else:
                _target = 'DEV'
            ebLogDebug('*** TARGET ENV DETECTED AS: %s' % _target)
        return _rc

    def mUpdateDBGIBPL(self):

        ebLogVerbose("mUpdateDBGIBPL: Updata all domU GI and DB home with BP Level.")

        _rc = 0
        #
        # Read Local/current last version
        #
        _c_dict = {}
        _c_dict['db'] = {}
        _c_dict['gi'] = {}
        for _dbhv in list(self.mGetDBConfig().keys()):
            _entry = self.mGetDBConfig()[_dbhv]
            _bp_doy =  _entry[1]
            _date = datetime.datetime.strptime(_bp_doy,'%Y.%j').strftime('%Y-%m-%d')
            _c_dict['db'][_entry[0]] = [_bp_doy, 'NA', _date]

        for _dbhv in list(self.mGetGridConfig().keys()):
            _entry = self.mGetGridConfig()[_dbhv]
            if self.mGetGiMultiImageSupport():
                _bp_doy =  _entry[3] # date
                if _bp_doy == '0':
                    _date = 0
                else:
                    _date = datetime.datetime.strptime(_bp_doy,'%y%m%d').strftime('%Y-%m-%d')
                _c_dict['gi'][_entry[2]] = [_bp_doy, 'NA', _date]
                _c_dict['gi'][_entry[1]] = [_bp_doy, 'NA', _date] #GIchange
            else:
                _bp_doy = _entry[1]
                _date = datetime.datetime.strptime(_bp_doy,'%Y.%j').strftime('%Y-%m-%d')
                _c_dict['gi'][_entry[0]] = [_bp_doy, 'NA', _date]

        _gihc = self.__clusters.mGetCluster()
        _gi_home = _gihc.mGetCluHome()
        _dir = _gi_home.split('/', 2)[1]
        _cluver = _gihc.mGetCluVersion()
        # check if version has date field
        if len(_cluver.split('.')) == 5:
           _cluver = '.'.join(_cluver.split('.')[:-1])

        #
        # Updata all domU GI and DB home with current version
        #
        for _dom0, _domU in self.mReturnDom0DomUPair():
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domU)
            #
            # Fetch GI MOUNT/VERSION
            #
            _cmd = f"mount | grep '/{_dir}/app.*/grid.*'"
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            _out = _o.readlines()
            if not _out or len(_out) == 0:
                ebLogError('*** GI mount point not found')
                _rc = -1
                break
            else:
                for _mount in _out:
                    _path = _mount.split(' ')[2]
                    if self.__cmd == 'vmgi_reshape':
                        _version = _cluver
                    else:
                        _version = _path.split('/')[-2]
                    _bpl = _c_dict['gi'][_version][0]
                    ebLogInfo('*** Update GI BPL for : %s %s %s' % (_version, _path, _bpl))
                    #
                    # Update BP Level
                    #
                    _cmd = 'echo '+_bpl+' > '+_path+'/.bpl'
                    _node.mExecuteCmdLog(_cmd)
            #
            # Fetch DB MOUNT/VERSION
            #
            _cmd = f"mount | grep '{_dir}/app.*/dbhome.*'"
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            _out = _o.readlines()
            if not _out or len(_out) == 0:
                ebLogError('*** DB mount point not found')
                _rc = -1
                break
            else:
                for _mount in _out:
                    _path = _mount.split(' ')[2]
                    if self.__cmd == 'vmgi_reshape':
                        _version = _cluver
                    else:
                        _version = _path.split('/')[-2]
                    _bpl = _c_dict['db'][_version][0]
                    ebLogInfo('*** Update DB BPL for : %s %s %s' % (_version, _path, _bpl))
                    #
                    # Update BP Level
                    #
                    _cmd = 'echo '+_bpl+' > '+_path+'/.bpl'
                    _node.mExecuteCmdLog(_cmd)

            _node.mDisconnect()

        return _rc

    def mCheckImagesVersion(self):

        _rc = 0
        _v_dict = {}
        _v_dict['gi']  = {}
        _v_dict['db']  = {}
        _v_dict['sys'] = {}

        ebLogVerbose("mCheckImagesVersion: Checking Images Version.")

        def _date_to_doy(aDate):
            try:
                _dt  = datetime.datetime.strptime(aDate,'%Y-%m-%d')
                _doy = _dt.timetuple().tm_yday
                _y   = aDate.split('-')[0]
            except:
                _doy = 0
                _y   = 0
            return str(_y)+'.'+str(_doy)

        def _doy_to_date(aDoy):

            try:
                _doy  = aDoy.strip()
                _dt   = datetime.datetime.strptime(_doy,'%Y.%j')
                _date = datetime.datetime.strftime(_dt,'%Y-%m-%d')
            except:
                _data = '0-0-0'
            return str(_date)

        for _dom0, _domU in self.mReturnDom0DomUPair():
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domU)
            #
            # Fetch GI MOUNT/VERSION
            #
            _cmd = "mount | grep '/u01/app.*/grid.*'"
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            _out = _o.readlines()
            if not _out or len(_out) == 0:
                ebLogError('*** GI mount point not found')
                _rc = ebError(0x723)
                break
            else:
                for _mount in _out:
                    _path = _mount.split(' ')[2]
                    _version = _path.split('/')[-2]
                    #
                    # Fetch BP Level
                    #
                    _date = None
                    _doy  = None
                    _cmd = 'cat '+_path+'/.bpl'
                    _i2, _o2, _e2 = _node.mExecuteCmd(_cmd)
                    _out2 = _o2.readlines()
                    if len(_out2):
                        _date = _doy_to_date(_out2[0])
                        _doy  = _out2[0].split('.')[-1].strip()
                        _doy  = _out2[0].strip()
                    #
                    # Try to guess BPL if not found using .signature file
                    #
                    if _date is None:
                        _cmd = 'ls '+_path+'/.patch_storage/NApply'
                        _i2, _o2, _e2 = _node.mExecuteCmd(_cmd)
                        _out2 = _o2.readlines()
                        if not len(_out2):
                            _cmd = 'ls '+_path+'/inventory/backup'
                            _i2, _o2, _e2 = _node.mExecuteCmd(_cmd)
                            _out2 = _o2.readlines()
                        _date_d = {}
                        for _dir in _out2:
                            _date = _dir.split('_')[0]
                            _doy  = _date_to_doy(_date)
                            _date_d[_doy] = _date
                    else:
                        _date_d = {}
                        _date_d[_doy] = _date

                    if len(_date_d) > 1:
                        ebLogError('*** MULTIPLE BP_LEVEL/DATA detected %s for GI: %s' % (str(list(_date_d.keys())),_version))
                    elif len(_date_d) == 0:
                        ebLogError('*** NO BP_LEVEL/DATA detected for GI: %s' % (_version))
                    if _date_d:
                        _bp_date = sorted(_date_d.keys())[-1]
                        if _version in list(_v_dict['gi'].keys()):
                            assert(_v_dict['gi'][_version][1]==_path)
                        _v_dict['gi'][_version] = [_bp_date, _path, _date_d[_bp_date]]
            #
            # Fetch DB MOUNT/VERSION
            #
            _cmd = "mount | grep '/u01/app.*/dbhome.*'"
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            _out = _o.readlines()
            if not _out or len(_out) == 0:
                ebLogError('*** DB mount point not found')
                _rc = ebError(0x0724)
                break
            else:
                for _mount in _out:
                    _path = _mount.split(' ')[2]
                    _version = _path.split('/')[-2]
                    #
                    # Fetch BP Level
                    #
                    _date = None
                    _doy  = None
                    _cmd = 'cat '+_path+'/.bpl'
                    _i2, _o2, _e2 = _node.mExecuteCmd(_cmd)
                    _out2 = _o2.readlines()
                    if len(_out2):
                        _date = _doy_to_date(_out2[0])
                        _doy  = _out2[0].split('.')[-1].strip()
                        _doy  = _out2[0].strip()

                    if _date is None:
                        _cmd = 'ls '+_path+'/.patch_storage/NApply'
                        _i2, _o2, _e2 = _node.mExecuteCmd(_cmd)
                        _out2 = _o2.readlines()
                        if not len(_out2):
                            _cmd = 'ls '+_path+'/inventory/backup'
                            _i2, _o2, _e2 = _node.mExecuteCmd(_cmd)
                            _out2 = _o2.readlines()
                        _date_d = {}
                        for _dir in _out2:
                            _date = _dir.split('_')[0]
                            _doy  = _date_to_doy(_date)
                            _date_d[_doy] = _date
                    else:
                        _date_d = {}
                        _date_d[_doy] = _date

                    if len(_date_d) > 1:
                        ebLogError('*** MULTIPLE BP_LEVEL/DATA detected %s for DB: %s' % (str(list(_date_d.keys())),_version))
                    elif len(_date_d) == 0:
                        ebLogError('*** NO BP_LEVEL/DATA detected for DB: %s' % (_version))
                    if _date_d:
                        _bp_date = sorted(_date_d.keys())[-1]
                        if _version in list(_v_dict['db'].keys()):
                            assert(_v_dict['db'][_version][1]==_path)
                        _v_dict['db'][_version] = [_bp_date, _path, _date_d[_bp_date]]
            if _rc:
                return _rc, False, {}
            #
            # Fetch IMAGE VERSION
            #
            _cmd = "imageinfo | grep 'Image version'"
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            _out = _o.readlines()
            if not _out or len(_out) == 0:
                ebLogError('*** imageinfo not found')
                _rc = ebError(0x0722)
                break
            else:
                for _mount in _out:
                    _version = _mount.split(' ')[2].strip()
                    _v_dict['sys'][_domU] = _version

            _node.mDisconnect()

        if _rc:
            return _rc, False, {}
        #
        # DUMP GI, DB version and corresponding BP_LEVEL
        #
        ebLogDebug('***************************************')
        ebLogDebug('***  VMs INSTALLED IMAGES VERSION   ***')
        ebLogDebug('***************************************')
        for _version in list(_v_dict['gi'].keys()):
            ebLogDebug('*** GI VERSION: %s BP_LEVEL: %s PATH: %s DATE: %s' % tuple([_version]+_v_dict['gi'][_version]))
        for _version in list(_v_dict['db'].keys()):
            ebLogDebug('*** DB VERSION: %s BP_LEVEL: %s PATH: %s DATE: %s' % tuple([_version]+_v_dict['db'][_version]))
        for _domU in list(_v_dict['sys'].keys()):
            ebLogDebug('*** %s imageinfo: %s' % (_domU, _v_dict['sys'][_domU]))
        #
        # Check if request DB version is available in VM
        #
        _found = False
        for _version in list(_v_dict['db'].keys()):
            _sversion = ''.join(_version.split('.'))[:3]
            if self.__db_version == _sversion:
                _found = True
        if _found is False:
            ebLogError('*** DB Version requested: %s but corresponding DB klone image not found in VM' % (str(self.__db_version)))
            return ebError(0x0725), True, {}
        _found = False
        if self.__db_version:
            for _version in list(_v_dict['gi'].keys()):
                _sversion = ''.join(_version.split('.'))[:3]
                if self.__db_version == _sversion or (self.__db_version == '112' and _sversion == '121'):
                    _found = True
            if _found is False:
                if self.__db_version == '122':
                    ebLogError('*** DB Version requested is 12.2 but GI 12.2 klone image not found in VM')
                    return ebError(0x0726), True, {}
                else:
                    ebLogError('*** DB Version requested is %s but GI 12.1 klone image not found in VM' % (self.__db_version))
                    return ebError(0x0729), True, {}
        #
        # Enforce starterDB 11.2 and 12.1 to use only 12.1 GI - xxx/MR: This constraint may be removed in the future
        #
        if self.__db_version and self.__db_version != '122':
            for _version in list(_v_dict['gi'].keys()):
                _sversion = ''.join(_version.split('.'))[:3]
                if _sversion == '122':
                    ebLogError('*** DB Version %s not supported with 12.2 GI for starterDB' % (self.__db_version))
                    return ebError(0x0727), True, {}
        #
        # Dump Local/current last version
        #
        _c_dict = {}
        _c_dict['db'] = {}
        _c_dict['gi'] = {}
        for _dbhv in list(self.mGetDBConfig().keys()):
            _entry = self.mGetDBConfig()[_dbhv]
            _bp_doy =  _entry[1]
            _date = datetime.datetime.strptime(_bp_doy,'%Y.%j').strftime('%Y-%m-%d')
            _c_dict['db'][_entry[0]] = [_bp_doy, 'NA', _date]

        for _dbhv in list(self.mGetGridConfig().keys()):
            _entry = self.mGetGridConfig()[_dbhv]
            _bp_doy =  _entry[1]
            _date = datetime.datetime.strptime(_bp_doy,'%Y.%j').strftime('%Y-%m-%d')
            _c_dict['gi'][_entry[0]] = [_bp_doy, 'NA', _date]

        ebLogDebug('***************************************')
        ebLogDebug('*** EXACLOUD CURRENT IMAGES VERSION ***')
        ebLogDebug('***************************************')
        for _version in list(_c_dict['gi'].keys()):
            ebLogDebug('*** GI VERSION: %s BP_LEVEL: %s PATH: %s DATE: %s' % tuple([_version]+_c_dict['gi'][_version]))
        for _version in list(_c_dict['db'].keys()):
            ebLogDebug('*** DB VERSION: %s BP_LEVEL: %s PATH: %s DATE: %s' % tuple([_version]+_c_dict['db'][_version]))
        #
        # Check if VMs images version are up to date
        #
        _upgrade = False
        for _version in list(_v_dict['gi'].keys()):
            if _v_dict['gi'][_version][0] < _c_dict['gi'][_version][0]:
                ebLogWarn('*** GI %s VERSION IN VM: %s CURRENT: %s --- UPGRADE REQUIRED ---' % (_version, _v_dict['gi'][_version][0], _c_dict['gi'][_version][0]))
                _upgrade = True
            elif _v_dict['gi'][_version][0] > _c_dict['gi'][_version][0]:
                ebLogWarn('*** GI %s VERSION IN VM: %s CURRENT: %s --- NEWER VERSION IN VM ---' % (_version, _v_dict['gi'][_version][0], _c_dict['gi'][_version][0]))
        for _version in list(_v_dict['db'].keys()):
            if _v_dict['db'][_version][0] < _c_dict['db'][_version][0]:
                ebLogWarn('*** DB %s VERSION IN VM: %s CURRENT: %s --- UPGRADE REQUIRED ---' % (_version, _v_dict['db'][_version][0], _c_dict['db'][_version][0]))
                _upgrade = True
            elif _v_dict['db'][_version][0] > _c_dict['db'][_version][0]:
                ebLogWarn('*** DB %s VERSION IN VM: %s CURRENT: %s --- NEWER VERSION IN VM ---' % (_version, _v_dict['db'][_version][0], _c_dict['db'][_version][0]))

        return _rc, _upgrade, _v_dict

    def mUpdateNTPServers(self):

        ebLogVerbose("mUpdateNTPServers: Update NTP Servers.")

        if self.mHasNatAndCustomerNet():
            for _, _domU in self.mReturnDom0DomUPair():
                _domU_mac = self.__machines.mGetMachineConfig(_domU)
                _ntpsl = _domU_mac.mGetNtpServers()
                ebLogInfo("*** BM_NTPSL: %s" % (str(_ntpsl)) )
                return

        # dns_entry
        if self.mCheckConfigOption('target_env', 'prod'):
            _ntpsl = self.mCheckConfigOption("ntp_list_prod")
        elif self.mCheckConfigOption('target_env') in [ 'dev', 'qa', 'dbqa', 'cqa']:
            _ntpsl = self.mCheckConfigOption("ntp_list_qadev")
        elif self.mCheckConfigOption('target_env') in ['discovery']:
            if self.mEnvTarget() is True:
                _ntpsl = self.mCheckConfigOption("ntp_list_prod")
            else:
                _ntpsl = self.mCheckConfigOption("ntp_list_qadev")
        else:
            _ntpsl = self.mCheckConfigOption("ntp_list_prod")

        if _ntpsl is None or _ntpsl == [] or self.__exacm is True:
            ebLogInfo('*** NTP Servers kept to XML default')
            return

        for _, _domU in self.mReturnDom0DomUPair():
            _domU_mac = self.__machines.mGetMachineConfig(_domU)
            _domU_mac.mSetNtpServers(_ntpsl)

        ebLogInfo('*** NTP servers: %s' % (str(_ntpsl)))

    def mUpdateCharacterSet(self, aOptions):
        '''Find and update the correct characterset node for oeda 16.274'''
        if aOptions.jsonconf is not None and 'dbParams' in list(aOptions.jsonconf.keys()):
            dbParams = aOptions.jsonconf['dbParams']
            if 'charset' in dbParams and 'ncharset' in dbParams:
                domU1 = self.mReturnDom0DomUPair()[0][1]
                found = False
                dbnode = None
                for dbn in self.__config.mGetConfigAllElement('databases/database'):
                    machines = dbn.findall('machines/machine')
                    for machinen in machines:
                        if machinen.get('id').startswith(domU1) and 'pdb' not in dbn.get('id'):
                            found = True
                            dbnode = dbn
                            break
                    if found:
                        break
                try:
                    dbnode.find('characterset').text = dbParams['charset']
                except AttributeError:
                    ebLogWarn('*** XML don\'t have characterset node')

    def mUpdatePrivNetworks(self):

        for _, domu in self.mReturnDom0DomUPair():
            domu_conf = self.__machines.mGetMachineConfig(domu)
            domu_conf_net = domu_conf.mGetMacNetworks()

            priv1_net = None
            priv2_net = None
            client_net = None

            for net_id in domu_conf_net:
                net_conf = self.__networks.mGetNetworkConfig(net_id)
                net_type = net_conf.mGetNetType()
                ebLogInfo('*** network name: %s' % (net_conf.mGetNetName()))
                if net_type == "client":
                    client_net = net_conf

                elif net_type == "private":
                    pkey_name = net_conf.mGetPkeyName()

                    if self.mIsKVM():
                        _int_name = net_conf.mGetInterfaceName()
                        _master = net_conf.mGetNetMaster()
                        if _int_name == "stre0":
                            priv1_net = net_conf
                        elif _int_name == "stre1":
                            priv2_net = net_conf
                        elif priv1_net is None and _master == "re0":
                            priv1_net = net_conf
                        elif priv2_net is None and _master == "re1":
                            priv2_net = net_conf
                    else:
                        if pkey_name == "stib0":
                            priv1_net = net_conf
                        elif pkey_name == "stib1":
                            priv2_net = net_conf
            if self.mIsKVM():
                if priv1_net is None or priv2_net is None:
                    ebLogWarn('*** !!!! PRIVATE STORAGE INTERFACE NOT FOUND IN XML ***')
                else:
                    ebLogInfo('*** PrivNetwork [CC] hostname patching for: %s %s' % (priv1_net.mGetNetMaster(), priv2_net.mGetNetMaster()))
            client_hostname = client_net.mGetNetHostName()
            priv1_net.mSetNetHostName(client_hostname + "-priv1")
            priv2_net.mSetNetHostName(client_hostname + "-priv2")

    def formStepList(self, aCreate, aOptions=None):
        """
        Modify step list for create service and delete service
        :param aCreate: Bool var to select create service or delete service
        :param aOptions: Options context; choose runstep or undostep for create and delete respectively.
        Default is full run.
        Three different formats available with runstep and undostep:
        1. stepStart - stepEnd : Runs from stepStart to stepEnd
        2. step+ : Runs from step till end
        3. step : Runs just the entered step
        """
        ebLogVerbose("formStepList: aCreate = %s" % aCreate)

        _options = aOptions

        if aCreate:
            getstep = _options.runstep

            step_list = [OSTP_PREVM_INSTALL, OSTP_CREATE_VM, OSTP_POSTVM_INSTALL, OSTP_PREGI_INSTALL, OSTP_CREATE_USER,
                             OSTP_SETUP_CELL, OSTP_CREATE_CELL, OSTP_CREATE_GDISK, OSTP_INSTALL_CLUSTER, OSTP_INIT_CLUSTER,
                             OSTP_POSTGI_INSTALL, OSTP_END_INSTALL]
            gi_step_list = [OSTP_CREATE_USER, OSTP_SETUP_CELL, OSTP_CREATE_CELL, OSTP_CREATE_GDISK,
                                OSTP_INSTALL_CLUSTER, OSTP_INIT_CLUSTER]

        else:
            getstep = _options.undostep

            step_list = [OSTP_PREGI_DELETE, OSTP_INIT_CLUSTER, OSTP_INSTALL_CLUSTER, OSTP_CREATE_GDISK, OSTP_CREATE_CELL,
                             OSTP_SETUP_CELL, OSTP_CREATE_USER, OSTP_POSTGI_DELETE, OSTP_PREVM_DELETE, OSTP_CREATE_VM,
                             OSTP_POSTVM_DELETE, OSTP_END_INSTALL]
            gi_step_list = [OSTP_INIT_CLUSTER, OSTP_INSTALL_CLUSTER, OSTP_CREATE_GDISK, OSTP_CREATE_CELL,
                             OSTP_SETUP_CELL, OSTP_CREATE_USER]

        ebLogInfo('*** Step List Altered ***')

        if getstep.find('-')+1:
            getstep_list = getstep.split('-')
            _startStep = getstep_list[0]
            _endStep = getstep_list[1]
            if _startStep.isdigit() and _endStep.isdigit():
                _startStep = int("".join(_startStep))-1
                _endStep = int("".join(_endStep))
                if aCreate:
                    if _startStep < _endStep and step_list.index(OSTP_PREVM_INSTALL) <= _startStep and step_list.index(OSTP_END_INSTALL) >= _endStep-1:
                        step_list = step_list[_startStep:_endStep]
                    else:
                        return [-1], [-1]
                else:
                    if _startStep < _endStep and step_list.index(OSTP_PREGI_DELETE) <= _startStep and step_list.index(OSTP_END_INSTALL) >= _endStep-1:
                        step_list = step_list[_startStep:_endStep]
                    else:
                        return [-1], [-1]

            else:
                return [-1], [-1]

        elif getstep.find('+')+1:
            getstep_list = list(getstep)
            getstep_list.remove('+')
            getstep = "".join(getstep_list)
            if getstep.isdigit():
                _startStep = int(getstep)-1
                if aCreate:
                    if step_list.index(OSTP_PREVM_INSTALL)<=_startStep and step_list.index(OSTP_END_INSTALL)>= _startStep:
                        step_list = step_list[_startStep:]
                    else:
                        return [-1], [-1]
                else:
                    if step_list.index(OSTP_PREGI_DELETE)<=_startStep and step_list.index(OSTP_END_INSTALL)>= _startStep:
                        step_list = step_list[_startStep:]
                    else:
                        return [-1], [-1]
            else:
                return [-1], [-1]

        else:
            if getstep.isdigit():
                tempVar = int(getstep)-1
                if aCreate:
                    if step_list.index(OSTP_PREVM_INSTALL) <= tempVar and step_list.index(OSTP_END_INSTALL) >= tempVar:
                        _stepToRun = step_list[tempVar]
                        step_list = []
                        step_list.append(_stepToRun)
                    else:
                        return [-1], [-1]
                else:
                    if step_list.index(OSTP_PREGI_DELETE) <= tempVar and step_list.index(OSTP_END_INSTALL) >= tempVar:
                        _stepToRun = step_list[tempVar]
                        step_list = []
                        step_list.append(_stepToRun)
                    else:
                        return [-1], [-1]
            else:
                return [-1], [-1]

        gi_step_list = list(set(step_list) & set(gi_step_list))

        return step_list, gi_step_list

    def mGetAsmSysPasswordForAdbs(self, aWalletKey, aDomU=None):
        
        _wallet_key = aWalletKey
        _passwd = None

        if aDomU:
            _domu = aDomU
        else:
            _domu_list = [ _domu for _ , _domu in self.mReturnDom0DomUPair()]
            _domu = _domu_list[0]

        _grid_home = getGridHome(_domu)

        _key_store = _grid_home + "/bin/mkstore"
        _wallet_loc = "/u01/app/oracle/admin/cprops/cprops_wallet"

        try:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domu)
            _cmd_timeout = 300
            _cmd  = '{0} -wrl {1} -viewEntry {2} | grep {2}'.format(_key_store, _wallet_loc, _wallet_key)
            _, _o, _e = _node.mExecuteCmd(_cmd, aTimeout=_cmd_timeout)
            _ret = _node.mGetCmdExitStatus()
            if _ret == 0 and _o:
                _out = _o.readlines()
                if _out and len(_out):
                    _out = _out[0].strip()
                    if '=' in _out:
                        _passwd = _out.split('=')[1].strip()

            _node.mDisconnect()

        except:
            ebLogError(f"Failed to get ASM SYS password from {_wallet_loc} wallet")

        return _passwd

    def mGetSysPassword(self, aSrcDomU):
        """
            Return sys password for specified DOMU
        """
        _srcdomU = aSrcDomU
        _node = exaBoxNode(get_gcontext())
        _node.mConnect(_srcdomU)

        # Get grid sys password
        _cmd = "/var/opt/oracle/ocde/rops get_cprops_key sys"

        _, _o, _e = _node.mExecuteCmd(_cmd)
        _out = _o.readlines()
        if _out and len(_out):
            _pswd = _out[0].strip()
            if _pswd == '0':
                ebLogWarn('*** Unable to get password from rops command. Fallback to default password')
                _options = get_gcontext().mGetConfigOptions()
                _pswd = _options["oeda_pwd"]
        else:
            ebLogWarn('*** Unable to get password from rops command. Fallback to default password')
            _options = get_gcontext().mGetConfigOptions()
            _pswd = _options["oeda_pwd"]

        return _pswd


    def mUpdateOedaDefaultDBPwd(self, aOedaPath, aPasswd):
        """
            update default db pwd set in oeda before invoking oedacli cmd
        """
        # getting the location of the file
        _prop_file = aOedaPath + '/properties/es.properties'
        ebLogInfo('property file location {0}'.format(_prop_file))
        self.mUpdateOedaUserPswd(aOedaPath, "non-root", aPasswd)
        _sed_cmd = "/bin/sed -i 's/{0}=.*$/{0}={1}/g' {2}"

        _cell_list = self.mReturnCellNodes()
        _num_cells = len(list(_cell_list.keys()))
        """
        35634247: With new OEDA build, configuring quorum devices will be
        handled by OEDA which requires quorum flag in es.properties
        if self.__enable_quorum and len(self.mGetOrigDom0sDomUs()) == 1 and _num_cells < 5:
            _cmd_str = _sed_cmd.format('ENABLEQUORUMDISK', 'false', _prop_file)
            _cmd_str_list = shlex.split(_cmd_str)
            subprocess.call(_cmd_str_list)
            _cmd_str = _sed_cmd.format('ENABLEQUORUMDISKINSSC', 'false', _prop_file)
            _cmd_str_list = shlex.split(_cmd_str)
            subprocess.call(_cmd_str_list)
            ebLogInfo('*** OEDA PROPERTIES UPDATED TO REFLECT QUORUMDISK DISABLED')
        """
    #
    # mRemoveDatabaseMachineXmlDomU(): Removes /opt/oracle.SupportTools/onecommand/databasemachine.xml from DomU
    # and place it in /EXAVMIMAGES/GuestImages/<domU>/ on Dom0 for use by ops
    #
    def mRemoveDatabaseMachineXmlDomU(self):
        _filename = '/opt/oracle.SupportTools/onecommand/databasemachine.xml'
        _dom0path = '/EXAVMIMAGES/GuestImages/'
        ebLogInfo("Started Removing databasemachine.xml operation")

        for _dom0, _domU in self.mReturnDom0DomUPair():
            try:
                with connect_to_host(_domU, get_gcontext(), username='root') as _node:
                    _pathdir = '/tmp/' + _domU.split('.')[0]

                    if not os.path.exists(_pathdir):
                        os.mkdir(_pathdir)

                    _pathcopy = _pathdir +"/databasemachine.xml"

                    if not _node.mFileExists(_filename):
                        return
                    
                    try:
                        _node.mCopy2Local(_filename,_pathcopy)
                        ebLogInfo("Trying Copy2Local(%s,%s) "%(_filename,_pathcopy))
                            #remove databasemachine.xml from domU
                        _cmd_str = "rm -f " + _filename

                        _i, _o, _e = _node.mExecuteCmd(_cmd_str)
                        if _e:
                            ebLogError("Errors occured while executing %s, %s"%(_cmd_str,str(_e)))
                    except:
                        ebLogError("*** mRemoveDatabaseMachineXml error: Error copying databasemachine.xml from DomU %s to local"%(_domU))
                        raise ExacloudRuntimeError(0x0801, 0xA, "Error Copying databasemachine.xml from DomU %s to local"%(_domU))

                    if _node.mFileExists(_filename):
                        raise ExacloudRuntimeError(0x0801, 0xA, "Error executing %s on host %s"%(_cmd_str, _domU))
            except:
                ebLogError("*** mRemoveDatabaseMachineXml error: Error while connecting to host %s"%(_domU))
                raise ExacloudRuntimeError(0x0801, 0xA, "Error while connecting to Host %s"%(_domU))

            try:
                with connect_to_host(_dom0, get_gcontext(), username='root') as _node:
                    _dom0location = _dom0path + _domU + "/databasemachine.xml"
                    try:
                        _node.mCopyFile(_pathcopy, _dom0location)
                        ebLogInfo("CopyFile(%s,%s) done"%(_pathcopy,_dom0location))
                    except:
                        ebLogError("*** mRemoveDatabaseMachineXml error: Error copying databasemachine.xml from local to host %s"%(_dom0))
                        raise ExacloudRuntimeError(0x0801, 0xA, "Error Copying databasemachine.xml from local to host %s"%(_dom0))
            except:
                ebLogError("*** mRemoveDatabaseMachineXml error: Error while connecting to host %s"%(_domU))
                raise ExacloudRuntimeError(0x0801, 0xA, "Error while connecting to Host %s"%(_domU))

            #removing databasemachine.xml from temporary location
            _cmd_str = "/bin/rm -rf " + _pathdir
            _rc, _i, _o, _e = self.mExecuteLocal(_cmd_str)
            if _e:
                ebLogError("Errors occured while executing %s"%(_cmd_str))


    # Stepwise execution for provisioning or de-provisioning.
    def mStepwiseMgmtVMGI(self, aAction=True, aOptions=None, aMode=VMGI_MODE):

        if aAction:
            # Provisioning (as per the steps)
            aOptions.undo = False

            if aMode == GI_MODE:
                _steplist = [ESTP_CREATE_USER,ESTP_CREATE_STORAGE,
                ESTP_INSTALL_CLUSTER,ESTP_POSTGI_INSTALL,ESTP_POSTGI_NID]

            elif aMode == VM_MODE:
                _steplist = [ESTP_PREVM_CHECKS,ESTP_PREVM_SETUP,
                ESTP_CREATE_VM,ESTP_POSTVM_INSTALL]

            else:
                # Assuming aMode=VMGI_MODE
                _steplist = [ESTP_PREVM_CHECKS,ESTP_PREVM_SETUP,
                ESTP_CREATE_VM,ESTP_POSTVM_INSTALL,ESTP_CREATE_USER,
                ESTP_CREATE_STORAGE,ESTP_INSTALL_CLUSTER,ESTP_POSTGI_INSTALL,
                ESTP_POSTGI_NID]

        else:
            # De-provisioning (as per the steps)
            aOptions.undo = True

            if aMode == GI_MODE:
                _steplist = [ESTP_POSTGI_INSTALL,ESTP_INSTALL_CLUSTER,
                ESTP_CREATE_STORAGE,ESTP_CREATE_USER]

            elif aMode == VM_MODE:
                _steplist = [ESTP_POSTVM_INSTALL,ESTP_CREATE_VM,
                ESTP_PREVM_SETUP]

            else:
                # Assuming aMode=VMGI_MODE
                _steplist = [ESTP_POSTGI_INSTALL,ESTP_INSTALL_CLUSTER,
                ESTP_CREATE_STORAGE,ESTP_CREATE_USER,ESTP_POSTVM_INSTALL,
                ESTP_CREATE_VM,ESTP_PREVM_SETUP]

            if not self.__run_all_undo_steps and ESTP_INSTALL_CLUSTER in _steplist:
                _steplist.remove(ESTP_INSTALL_CLUSTER)

        aOptions.steplist = ','.join(str(_step) for _step in _steplist)
        stepdriver = csDriver(self, aOptions)
        return stepdriver.handleRequest()

    def patchKVMGuestCfg(self, aOptions=None, aGIHome=None, aDom0DomUPair=None):
        ebLogVerbose('patchKVMGuestCfg: Started.')
        _subfactor = 1
        _cores = 0
        _num_computes = len(self._dom0U_list)
        if not aOptions:
            aOptions = self.mGetArgsOptions()

        _dir = "u01"
        _gridhome = aGIHome
        if _gridhome:
            _dir = _gridhome.split('/', 2)[1]

        _edv_states = {}
        if self.mIsExaScale() and self.mCheckConfigOption("exascale_edv_enable", "True"):
            _edv_info = get_hosts_edv_from_cs_payload(aOptions.jsonconf)
            _edv_states = get_hosts_edv_state(_edv_info)

        _u02_name = self.mCheckConfigOption('u02_name') if self.mCheckConfigOption('u02_name') else 'u02_extra'

        if aDom0DomUPair:
            _ddp = aDom0DomUPair
        else:
            _ddp = self.mReturnDom0DomUPair()

        _node_recovery = False
        if 'node_recovery_flow' in list(aOptions.jsonconf.keys()) and aOptions.jsonconf['node_recovery_flow'] == True:
            _node_recovery = True

        def _mBuildAttachU02(aDom0, aDomu):
            _dom0 = aDom0
            _domU = aDomu

            _device_path = ""
            _utils = self.mGetExascaleUtils()
            if _utils.mIsEDVImageSupported(aOptions):
                _dom0_short_name = _dom0.split('.')[0]
                _domU_short_name = _domU.split('.')[0]
                _u02_vol_name = _domU_short_name + "_u02"
                _disk_u02_size = self.mGetu02Size()
                _utils.mDetachU02(_dom0, _domU, _u02_name, _u02_vol_name, aOptions)
                _device_path = _utils.mCreateU02Volume(_dom0_short_name, _u02_vol_name, _disk_u02_size, aOptions)

            #
            # Fetch current dyndep stack version
            #
            _dyndep_version = self.__dyndep_version
            #
            # Check below is done to allow gi_install step to run independently
            #
            if self.__dyndep_version is None or self.__dyndep_version == '0.0':
                _dyndep_version, _ = self.mDyndepFilesList()
                self.__dyndep_version = _dyndep_version

            #
            _host_node = exaBoxNode(get_gcontext())
            _host_node.mConnect(aHost=_dom0)

            if self.mGetCmd() in ['vmbackup'] and aOptions and aOptions.vmbackup_operation in ["restore"] and _node_recovery:
                #
                # Attach /u02 disk
                #
                _new_ref_link = f"/EXAVMIMAGES/GuestImages/{_domU}/{_u02_name}.img"
                _ret = mAttachVDiskToKVMGuest(self, _dom0, _domU, _new_ref_link, "/u02", False, _gridhome)
                if _ret != 0:
                    ebLogError("*** Unable to attach U02 Extra vDisk to domu: {0}".format(_domU))
                    return _ret
            else:
                #
                # Build /u02 disk
                #
                _ret = mBuildAndAttachU02DiskKVM(self, _dom0, _domU, _host_node, _u02_name, _gridhome, aEDVStates=_edv_states, aDevicePath=_device_path)
                if _ret != 0:
                    ebLogError(f"*** Error while creating /u02 disk for DomU: {_domU}")
                    return _ret

            # See if DBHomes are still here,
            # grep pattern: (<whatever>/db<NOT ANY / to not collide if domU hostname starts with db>.img)
            _host_node.mExecuteCmd("/opt/exadata_ovm/vm_maker --list --disk-image --domain {} | grep '/db[^/]*\.img'".format(_domU))
            _db_homes_attached = (_host_node.mGetCmdExitStatus() == 0)
            if self.IsZdlraProv():
                #oeda needs db images to be already available.
                ebLogInfo("*** Not unmounting db images for ZDLRA !")
            elif not _node_recovery and _db_homes_attached:
                _exadiskobj = exaBoxKvmDiskMgr(self)
                _exadiskobj.mUnmountOedaDbHomes(_dom0,_domU, _host_node)
                ebLogInfo("*** Removed OEDA DBHomes from domU: {}".format(_domU))
            else:
                ebLogInfo("*** OEDA DBHomes were already removed from domU: {}".format(_domU))

            _host_node.mDisconnect()

            if _dir == "u01":
                _guest_node = exaBoxNode(get_gcontext())
                _guest_node.mConnect(aHost=_domU)
                _cmd ='chown -fR oracle.oinstall /u02'
                _guest_node.mExecuteCmd(_cmd)
                _guest_node.mDisconnect()

        _plist = ProcessManager()
        for _dom0, _domU in _ddp:
            _p = ProcessStructure(_mBuildAttachU02, [_dom0, _domU])
            _p.mSetMaxExecutionTime(60*60)
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()

        ebLogInfo("KVM Guests succesfully patched!!!.")
        return 0

    #
    # The function executes the steps required post VM installation.
    # Note: The function is used by vmgi_install and vmgi_reshape (add node)
    # flow. So, please make sure both the flows remain intact on changing
    # the code in this function.
    #
    def mAddPostVMInstallSteps(self, aStepList, aOptions):

        step_list = aStepList

        #
        # Filesystem Encryption in XEN domUs /u01 filesystem
        #
        if (isEncryptionRequested(aOptions, 'domU') and not self.mIsOciEXACC()
                                                    and not self.mIsKVM()):
            _step_time = time.time()
            self.mUpdateStatusCS(True, OSTP_POSTVM_INSTALL, step_list,
                                 aComment='Filesystem Encryption on /u01')
            for _dom0, _domU in self.mReturnDom0DomUPair():
                encryptionSetupDomU(self, _dom0, _domU, '/u01')
            self.mLogStepElapsedTime(_step_time, 'Filesystem Encryption on /u01')

        #
        # POST-VM Update SSH Keys from OEDA staging area
        #
        # TODO : Copy ssh keys to repo
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_POSTVM_INSTALL, step_list, 'Save OEDA SSH Keys')
        self.mLogStepElapsedTime(_step_time, 'POSTVM INSTALL : Save OEDA SSH Keys')
        #
        # POST-VM Reset SSH Host Key
        #
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_POSTVM_INSTALL, step_list, 'Reset Cluster SSH Keys')
        self.mResetClusterSSHKeys(aOptions)
        self.mLogStepElapsedTime(_step_time, 'POSTVM INSTALL : Reset Cluster SSH Keys')
        #
        # POST-VM Add ECRA NAT IP in domU /etc/hosts
        #
        if self.isBM() or self.mIsOciEXACC():
            _step_time = time.time()
            self.mUpdateStatusCS(True, OSTP_POSTVM_INSTALL, step_list, aComment='Add ECRA NAT-Ip')
            self.mAddEcraNatOnDomU()
            self.mLogStepElapsedTime(_step_time, 'POSTVM INSTALL : Add ECRA NAT-Ip')
        #
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_POSTVM_INSTALL, step_list, 'Post VM Patching')
        self.mPostVMCreatePatching(aOptions)
        self.mLogStepElapsedTime(_step_time, 'POST VM Patching')

        if self.__cmd != 'vmgi_reshape':
            #
            # POST-VM Check Cell for write-back cache option - if not set then enable the option
            #
            self.mAcquireRemoteLock()
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_POSTVM_INSTALL, step_list, 'Cell Patching (Write-Back Cache)')
            self.mPostVMCellPatching(aOptions)
            self.mLogStepElapsedTime(_step_time, 'Cell Patching (Write-Back Cache)')
            self.mReleaseRemoteLock()

        #
        # POST-VM Dom0 additional updates to network configuration when needed
        #
        self.mAcquireRemoteLock()
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_POSTVM_INSTALL, step_list, 'Update Dom0 Network Config')
        self.mDom0PostVMCreateNetConfig(aMode=False)       # POST CSVC
        self.mLogStepElapsedTime(_step_time, 'Update Dom0 Network Config')
        self.mReleaseRemoteLock()
        #
        # POST-VM Check sshd config in VMs for client interface - if not present add it to the list of interface to Listen
        #
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_POSTVM_INSTALL, step_list, 'Patching SSHD Config')
        self.mPatchSSHDConfig()
        self.mLogStepElapsedTime(_step_time, 'Patching SSHD Config')

        #
        # POST-VM Save Cluster Configuration
        #
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_POSTVM_INSTALL, step_list, 'Save Cluster Configuration')
        self.mSaveClusterConfiguration()
        self.mLogStepElapsedTime(_step_time, 'Save Cluster Configuration')
        #
        # POST-VM Change Min Free Kb
        #
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_POSTVM_INSTALL, step_list, 'Change Min Free Kb')
        self.mChangeMinFreeKb()
        self.mLogStepElapsedTime(_step_time, 'POSTVM INSTALL : Change Min Free Kb')

        if self.__cmd != 'vmgi_reshape':
            #
            # POST-VM Secure Steps (by default enabled)
            #
            if not self.mCheckConfigOption('secure_ssh_all', 'False'):
                self.mAcquireRemoteLock()
                _step_time = time.time()
                self.mUpdateStatusOEDA(True, OSTP_POSTVM_INSTALL, step_list, 'Secure Cell SSH config')
                self.mSecureCellsSSH()
                self.mLogStepElapsedTime(_step_time, 'Secure Cell SSH config')
                self.mReleaseRemoteLock()
        #
        # POST-VM Secure DomU Pwd - NOTE: At this stage Oracle/Grid are not yet created so expect and disregard errors
        #
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_POSTVM_INSTALL, step_list, 'Secure DomU Passwords')
        self.mSecureDOMUPwd()

        # send the list of nodes which need to be secured.
        self.mSecureDOMUSsh(self.mReturnDom0DomUPair())

        if self.__ociexacc and self.isATP():
            ebUserUtils.mPushSecscanKey(self)
        
        ebUserUtils.mAddSecscanSshd(self)
        self.mLogStepElapsedTime(_step_time, 'Secure DomU Passwords')
        #
        # POST-VM ATP Secure SCAN listener
        #
        if self.isATP():
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_POSTVM_INSTALL, step_list, 'ATP: Secure Listeners')
            self.mATPSecureListeners()
            self.mLogStepElapsedTime(_step_time, 'ATP: Secure Listeners')

        if self.__ociexacc:
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_POSTVM_INSTALL, step_list, 'OCIEXACC: Secure Infiniband')
            _domUs = list(map(operator.itemgetter(1),self.mReturnDom0DomUPair()))
            ExaCCIB_DomU(_domUs).mSecureDomUIB()
            self.mLogStepElapsedTime(_step_time, 'OCIEXACC: Secure Infiniband')

        #
        # POST-VM add GI and DB BPL
        #
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_POSTVM_INSTALL, step_list, 'Update GI and DB BPL')
        self.mUpdateDBGIBPL()
        self.mLogStepElapsedTime(_step_time, 'Update GI and DB BPL')
        #
        # POST-VM Update final status
        #
        self.mUpdateStatusOEDA(True, OSTP_POSTVM_INSTALL, step_list, 'Finalizing Create VM operation')
        ebLogInfo('*** Exacloud Operation Successful : POST VM Installation')

        #
        # POST-VM Create/Update nat-rules file to support NAT RULES recreation via dom0_iptables_setup.sh script
        #
        self.mCreateNatIptablesRulesFile()

    def mCreateNatIptablesRulesFile(self):
        if self.__ut:
            return

        # Oracle linux 8 has deprecated the dom0_iptables_setup.sh script
        for _dom0, _domU in self.mReturnDom0DomUPair():

            if self.mIsHostOL8(_dom0):
                continue

            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)

            def obtain_nat_ip(_dom_u):
                domu_conf = self.__machines.mGetMachineConfig(_dom_u)
                domu_conf_net = domu_conf.mGetMacNetworks()
                for _net_id in domu_conf_net:
                    _net_conf = self.__networks.mGetNetworkConfig(_net_id)
                    _net_id_prefix = _net_id.split('_')[-1]
                    if _net_id_prefix == "client":
                        _domu_nat = _net_conf.mGetNetNatAddr()
                        ebLogInfo(f"DomU: {_dom_u} \n NAT IP: {_domu_nat}")
                        return _domu_nat

            # Create directory /opt/exacloud/network/ if needed
            _node.mExecuteCmd(f'/bin/mkdir -p /opt/exacloud/network/{_domU}')
            _iptables_file = '/etc/sysconfig/iptables'

            # Read PREROUTING rules from _iptables_file
            ebLogInfo(f"*** Reading PREROUTING rules from {_iptables_file} from Dom0: {_dom0} ***")
            _cmd_read = f'/bin/cat {_iptables_file} | /bin/grep "A PREROUTING -d {obtain_nat_ip(_domU)}"'
            _, _o, _e = _node.mExecuteCmd(_cmd_read)
            _out = _o.read()
            _output = _out.replace("-A ", "")

            # Set PREROUTING rules in _iptables_nat_rules_file
            _iptables_nat_rules_file = f'/opt/exacloud/network/{_domU}/nat-rules'
            ebLogInfo(f"*** Filling {_iptables_nat_rules_file} file with PREROUTING rule in {_dom0} ***")
            _cmd_write = f"/bin/echo '{_output}' > '{_iptables_nat_rules_file}'"
            _node.mExecuteCmdLog(_cmd_write)
            ebLogInfo(f"*** Filled {_iptables_nat_rules_file} file with PREROUTING rule in {_dom0} ***")

            _node.mDisconnect()

    def mDeleteNatIptablesRulesFile(self):
        if self.__ut:
            return

        for _dom0, _domU in self.mReturnDom0DomUPair():
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)

            # Delete directory /opt/exacloud/network/{_domU}
            _dir_to_delete = f'/opt/exacloud/network/{_domU}'
            _node.mExecuteCmd(f'/bin/rm -rf {_dir_to_delete}')
            ebLogInfo(f"*** Deleted {_dir_to_delete} directory from Dom0: {_dom0} ***")

            _node.mDisconnect()

    #
    #This function saves the newly added dom0/domu machine details to the xml & Saves the config xml in clusters/CC'/
    #
    def mUpdateInMemoryXmlConfig(self, aConfigXml, aOptions):
        _configxml = aConfigXml
        _options = aOptions

        _config = exaBoxClusterConfig(self.mGetCtx(), _configxml)
        self.mSetConfig(_config)

        self.mSetHeaders(ebCluHeaderConfig(self.__config))
        self.mSetMachines(ebCluMachinesConfig(_config))
        self.mSetClusters(ebCluClustersConfig(_config, _options))
        self.mSetScans(ebCluClusterScansConfig(self.__config))
        self.mSetDBHomes(ebCluDatabaseHomesConfig(_config))
        self.mSetDatabases(ebCluDabasesConfig(_config))
        self.mSetStorage(ebCluStorageConfig(self, _config))
        self.mSetNetworks(ebCluNetworksConfig(_config))
        self.mSetSwitches(ebCluSwitchesConfig(self.__config))
        self.mSetEsracks(ebCluEsRacksConfig(self.__config))
        self.mSetUsers(ebCluUsersConfig(self.__config))
        self.mSetGroups(ebCluGroupsConfig(self.__config))
        self.mSetIloms(ebCluIlomsConfig(_config))
        self.mSetClusterName(self.mGetHeaders().mGetHeaderCustomerName())
        self.mSetStorageDesc(ebCluStorageDesc(self.__config))

        # Saves the config xml in clusters/CC'/
        self.mSaveXMLClusterConfiguration()
        self.mBuildClusterId(aForce=True)

    # The function executes the steps preparatory for VM installation.
    #
    # Note: THe function is used by vmgi_install and vmgi_reshape (add node)
    # flow. So, please make sure both the flows remain intact on changing
    # the code in this function.
    #
    def mAddPreVMInstallSteps(self, aStepList, aOptions):
        _step_list = aStepList
        #
        # PRE-VM dom0 resources check
        #
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_PREVM_INSTALL, _step_list, 'Check Dom0 Resources')

        # Activate resources
        for _dom0, _ in self.mReturnDom0DomUPair():
            with connect_to_host(_dom0, self.mGetCtx()) as _node:
                if self.mIsHostOL8(_dom0):
                    self.mEnableDom0Service('nftables', _node, _dom0)
                else:
                    self.mEnableDom0Service('iptables', _node, _dom0)

        self.mCheckDom0Resources(aOptions)
        self.mRemoveXleaveFromNtpConf()
        self.mCheckDiskResources()
        self.mConfigureArp()
        self.mLogStepElapsedTime(_step_time, 'PREVM INSTALL : Check Dom0 Resources completed')
        #
        # Disable RDS IB module if option set
        #
        if self.mCheckConfigOption ('disable_ib_rds', 'True'):
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_POSTGI_INSTALL, _step_list, 'Disabing Infiniband RDS Module')

            if not self.mIsKVM():
                # Lock are preserved even during reboot, this ensures that no other dom0 operation is ongoing
                with self.remote_lock():
                    self.mDisableDom0IBRdsModule()
            else:
                ebLogInfo("Skip mDisableDom0IBRdsModule for KVM")

            self.mLogStepElapsedTime(_step_time, 'Disabling Infiniband RDS Module')

        # TODO: Need to revisit for OCI and ExaBM
        if self.__cmd != 'vmgi_reshape':

            #
            # PRE-VM ebtables setup
            #
            self.mAcquireRemoteLock()
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_PREVM_INSTALL, _step_list, 'Setup Ebtables on Dom0')
            self.mCleanupEbtablesOnDom0()
            self.mSetupEbtablesOnDom0()
            self.mHandlerSetupNATIptablesOnDom0()

            ## BUG 28598302
            if self.isATP() and self.__exabm and self.mCheckClusterNetworkType():
                AtpAddiptables2Dom0(None, self.__ATP, self.mReturnDom0DomUPair(), self.mGetMachines(), self.mGetNetworks()).mExecute()
            ## BUG 28598302 END
            self.mLogStepElapsedTime(_step_time, 'PREVM INSTALL : ebtable setup completed')
            self.mReleaseRemoteLock()

            #
            # PRE-VM pkeys setup
            #
            self.mAcquireRemoteLock()
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_PREVM_INSTALL, _step_list, 'Check PKEYS on IB-Switches')

            self.mInjectSSHMasterKey()

            if not self.mIsKVM():
                self.mCheckSwitchFreeSpace()
            else:
                ebLogInfo('*** Skip mCheckSwitchFreeSpace for KVM.')

            if self.mIsKVM():  # RoCE
                if self.__ociexacc:
                    if self.mIsRoCEQinQ():
                        ebLogInfo('RoCE must have been configured during CPS reimage. Verifying setup is correct.')
                        self.mCheckCPSQinQSetup()
                    else:
                        _exacc_roce_cps = ExaCCRoCE_CPS(self.__debug, True)
                        _exacc_roce_cps.mSetupCPSRoCE()
            else:  # Infiniband
                # TODO Infiniband code should go in a class to cache GUIDs
                _allGuids = self.mGetAllGUID()
                _pkeys = self.mCheckPkeysConfig(_allGuids, True)
                if self.__ociexacc and _pkeys is not None:
                    _exacc_ib_cps = ExaCCIB_CPS(_allGuids, _pkeys, self.__debug, True)
                    _dom0s, _, _cells, _ = self.mReturnAllClusterHosts()
                    _exacc_ib_cps.mSetupIBSwitches(_dom0s, _cells)
                    _exacc_ib_cps.mSetupCPSIB()

            self.mCheckCellCompliance()
            self.mCheckCellsStatus()
            self.mCheckCellsPkeyConfig()
            self.mPatchDom0BM()
            self.mLogStepElapsedTime(_step_time, 'PREVM INSTALL : PKEYS checks completed')
            self.mReleaseRemoteLock()

            #
            # PRE-VM system image check
            #
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_PREVM_INSTALL, _step_list, 'Check System Image')

            #
            # PRE-VM Ensure Custom OS is valid, can be downloaded and XML is patched
            #
            _custom_img_ver = hasDomUCustomOS(self)

            # If _custom_img_ver is None, fn mCheckSystemImage will take care.
            with self.remote_lock():
                self.mCheckSystemImage(_custom_img_ver)
            
            self.mLogStepElapsedTime(_step_time, 'PREVM INSTALL : Check System Image completed')
        else:
            # cmd == 'vmgi_reshape'
            #Setup NAT iptables on reshaped dom0
            self.mAcquireRemoteLock()
            self.mHandlerSetupNATIptablesOnDom0()
            self.mReleaseRemoteLock()

        #
        # PRE-VM network config setup
        #
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_PREVM_INSTALL, _step_list, 'Check Network Discovery')
        #EXACC: PATCH THE XML WITH CLIENT/BACKUP/DR NETWORK EHTERNET SLAVES & ENABLE SKIPPASSPROPERTY FLAG
        _enable_skip_paasproperty = self.mCheckConfigOption('enable_skip_passproperty')
        _use_xml_patching = True if _enable_skip_paasproperty.lower() == "true" else False
        self.mOEDASkipPassProperty(aOptions, aUseXMLPatching=_use_xml_patching)

        self.mLogStepElapsedTime(_step_time, 'PREVM INSTALL : Network Discovery checks completed')

        #
        # PRE-VM Scripts
        #
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_PREVM_INSTALL, _step_list, 'Running External PREVM Scripts')
        self.mRunScript(aType='*',aWhen='pre.vm_install')
        self.mLogStepElapsedTime(_step_time, 'PREVM INSTALL : Running External Scripts')
        #
        # PRE-VM reclaim dom0 local disk space from unused LVs
        #
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_PREVM_INSTALL, _step_list, 'Running reclaimdisks.sh -extend-vgexadb')
        self.mRunReclaimdisks('extend-vgexadb')
        self.mLogStepElapsedTime(_step_time, 'PREVM INSTALL : reclaim disks space from unused LVs on Dom0')

        if self.mIsKVM():
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_PREVM_INSTALL, _step_list, 'Resize logical volume EXAVMIMAGES')
            self.mResizeExaVMImages()
            self.mLogStepElapsedTime(_step_time, 'PREVM INSTALL : resize EXAVMIMAGES from unused LVs on Dom0')

        #
        # PRE-VM Update/Copy vmetrics and xend-config.sxp
        #
        #BEGIN KVM CHANGE
        if not self.mIsKVM():
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_PREVM_INSTALL, _step_list, 'Update vmetrics and xend-config.sxp on Dom0')
            self.mUpdateVmetrics('vmexacs')
            self.mStopVmetrics()
            self.mXendConfig()
            self.mLogStepElapsedTime(_step_time, 'PREVM INSTALL : Update vmetrics and xend-config.sxp on Dom0')
        else:
            ebLogInfo("Skip updating xend-config.sxp for KVM")
            ebLogWarn("*** TBD: Adding vmetrics for KVM ")
        #END KVM CHANGE

        # Secure Dom0 access before copying images to dom0s
        if not self.mCheckConfigOption('secure_ssh_all', 'False'):  
            _step_time = time.time()         
            self.mSecureDom0SSH()           
            self.mLogStepElapsedTime(_step_time, 'Secure DOM SSH') 

        #
        # PRE-VM Dom0 additional updates to network configuration when needed
        #
        self.mAcquireRemoteLock()
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_PREVM_INSTALL, _step_list, 'Update Dom0 Network Config')
        self.mDom0PostVMCreateNetConfig(aMode=True)        # PRE CSVC
        self.mLogStepElapsedTime(_step_time, 'Update Dom0 Network Config')
        self.mReleaseRemoteLock()
        #
        # PRE-VM Create PreVm Keys Oeda WorkDir
        #
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_PREVM_INSTALL, _step_list, 'Create PreVm Keys Oeda WorkDir')
        self.mCreatePreVmKeysOedaWorkDir()
        self.mLogStepElapsedTime(_step_time, 'Create PreVm Keys Oeda WorkDir')


    # This function executes the steps required post GI installation.
    #
    # Note: THe function is used by vmgi_install and vmgi_reshape (add node)
    # flow. So, please make sure both the flows remain intact on changing
    # the code in this function.
    #
    def mAddPostGIInstallSteps(self, aStepList, aOptions):
        _step_list = aStepList
        #
        # Copy SSH Keys again in case they were rotated
        #
        _step_time = time.time()
        self.mLogStepElapsedTime(_step_time, 'Saving OEDA SSH Keys')

        if self.__cmd != 'vmgi_reshape':
            #
            # Add ACFS GridDisks only if they have not been created before (e.g. no delete/overwrite)
            #
            self.mAcquireRemoteLock()
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_POSTGI_INSTALL, _step_list, 'Create ACFS Grid Disks')
            self.__storage.mCreateACFSGridDisks()
            self.mLogStepElapsedTime(_step_time, 'Create ACFS Grid Disks')
            self.mReleaseRemoteLock()
        #
        # Install / Configure any extra RPMs
        #
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_POSTGI_INSTALL, _step_list, 'Running Extra RPMS and Config')
        self.mExtraRPMsConfig(aOptions)
        self.mSetupBDCSTree()
        self.mLogStepElapsedTime(_step_time, 'Running Extra RPMS and Config')
        #
        # POSTGI - Run External Scripts
        #
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_POSTGI_INSTALL, _step_list, 'Running External POSTGI Scripts')
        self.mRunScript(aType='*',aWhen='post.gi_install')
        self.mLogStepElapsedTime(_step_time, 'Running External Scripts')
        
        if self.__cmd != 'vmgi_reshape':
            #
            # POSTGI - Set IORM Objective to auto
            #
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_POSTGI_INSTALL, _step_list, 'Setting IORM Objective to auto')
            _ioptions = aOptions
            _ioptions.jsonconf['objective'] = "auto"
            _ioptions.resmanage = "setobj"
            _iormobj = ebCluResManager(self, _ioptions)
            _iormobj.mClusterIorm(_ioptions)
            self.mLogStepElapsedTime(_step_time, 'Setting IORM Objective to auto')

            #Drop pmemlogs for adbs env
            self.mDropPmemlogs(aOptions)

        #
        # POSTGI - Security Hardening for OCI
        #
        if self.__exabm:
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_POSTGI_INSTALL, _step_list, 'OCI Security Hardening')
            self.mHardenOCISecurity()
            self.mLogStepElapsedTime(_step_time, 'OCI Security Hardening')

        #
        # POSTGI - Secure SSH Ciphers
        #
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_POSTGI_INSTALL, _step_list, 'Secure SSH Ciphers')
        self.mSecureSSHCiphers()
        self.mLogStepElapsedTime(_step_time, 'Secure SSH Ciphers')

        #
        #
        # POSTGI - Run script of fedramp
        if self.__fedramp:
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_POSTGI_INSTALL, _step_list, 'OCI Fedramp Hardening')
            self.mFedrampConfig()
            self.mLogStepElapsedTime(_step_time, 'OCI Fedramp Hardening')

        #
        # POSTGI - Disable QoSM in the ATP-Dedicated GI install
        #
        if self.isATP():
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_POSTGI_INSTALL, _step_list, 'Disable QoSM in the ATP')
            self.mDisableQoSM()
            self.mLogStepElapsedTime(_step_time, 'Disable QoSM in the ATP')

        #
        # POSTGI - Enable Not Expire Password
        #
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_POSTGI_INSTALL, _step_list, 'Disable Password Expiration')
        self.mDisablePasswordExpiration()
        self.mLogStepElapsedTime(_step_time, 'Disable Password Expiration')

        #
        # POSTGI - Lockdown Dom0/Cells
        #
        if (self.__disable_dom0_cell_lockdown is not None) and (self.__disable_dom0_cell_lockdown == "True") :
            ebLogInfo('*** Skipping Dom0/Cells Lockdown')
        else :
            ebLogInfo('*** Starting Dom0/Cells Lockdown')
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_POSTGI_INSTALL, _step_list, 'Lockdown Dom0/Cells')
            self.mSetupLockdown()
            self.mLogStepElapsedTime(_step_time, 'Lockdown Dom0/Cells')
            ebLogInfo('*** Completed Dom0/Cells Lockdown')

        #
        # POSTGI - Remove rsp files form tmp
        #
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_POSTGI_INSTALL, _step_list, 'Removing Grid rsp files')
        self.mRemoveRspFiles()
        self.mLogStepElapsedTime(_step_time, 'Removing Grid rsp files')

        #
        # POSTGI - OCIEXACC: Reload dnsmasq service
        #
        if self.__ociexacc:
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_POSTGI_INSTALL, _step_list, 'Restarting dnsmasq service on CPS')
            self.mRestartDnsmasq()
            self.mLogStepElapsedTime(_step_time, 'Restarting dnsmasq service on CPS')

    # Prepare domU for DBCS agent MTLS communication:
    # 1 - Create the directory structure '/opt/oracle/dcs/auth'
    # 2 - Give 700 permission to this folder
    # 3 - Change ownership to opc/opc to this folder
    # 4 - Copy security certificate to this folder (source path will vary depending
    #     on headend type)
    #       VPN: '/etc/nginx/dbcscert/dbcsagent.pfx'
    #       WSS: '/opt/oci/exacc/websocket/wsclient/certs/client/dbcsagent.pfx'
    # 5 - Give 600 permission to this file
    # 6 - Change ownership to opc/opc to this file
    def mSetupDomUsForSecureDBCSCommunication (self):
        # Identify certificate directory based on VPN vs WebSocket headend
        _cert_local_dir = '/etc/nginx/dbcscert'
        _ocps_jsonpath = self.mCheckConfigOption('ocps_jsonpath')
        # For WebSocket connection, certificate will be in a different path
        if _ocps_jsonpath and os.path.exists(_ocps_jsonpath):
            with open(_ocps_jsonpath, 'r') as fd:
                _ocps_json = json.load(fd)
            if  'ociAdminHeadEndType' in _ocps_json and _ocps_json['ociAdminHeadEndType'].upper() == 'WSS':
                _cert_local_dir = '/opt/oci/exacc/websocket/wsclient/certs/client'
                ebLogInfo('*** WSS headend identified, taking certificate from {0}'.format(_cert_local_dir))
            else:
                ebLogInfo('*** OCI admin headend type is not WSS. Taking VPN certificate from {0}'.format(_cert_local_dir))
        else:
            ebLogInfo('*** Missing OCPS token file. Taking VPN certificate from {0}'.format(_cert_local_dir))

        _cert_remote_dir = '/opt/oracle/dcs/auth'
        _cert_filename = 'dbcsagent.pfx'
        _dbcs_agent_user = 'opc'
        _dbcs_agent_group = 'opc'
        _local_certificate = os.path.join(_cert_local_dir, _cert_filename)

        # If local file does not exist, it makes no sense to continue setting up the environment.
        if not os.path.isfile(_local_certificate):
            ebLogWarn('*** Local MTLS certificate {0} does not exist. Skipping additional security setup'.
                      format(_local_certificate))
            return

        for _, _domU in self.mReturnDom0DomUPair():
            _nodeU = exaBoxNode(get_gcontext())
            _nodeU.mConnect(aHost=_domU)

            # Create certificate directory and give proper permissions/ownership
            _cmd_str = 'mkdir -p {0}; chmod 700 {0}; chown {1}:{2} {0}'.\
                format(_cert_remote_dir, _dbcs_agent_user, _dbcs_agent_group)
            _nodeU.mExecuteCmdLog (_cmd_str)
            _rc = _nodeU.mGetCmdExitStatus()
            if _rc:
                ebLogWarn('*** Failed to setup MTLS certificate directory {0} on {1}'.format(_cert_remote_dir, _domU))
                _nodeU.mDisconnect()
                continue # No need to stay here
            else:
                ebLogInfo('*** MTLS certificate directory {0} properly setup on {1}'.format(_cert_remote_dir, _domU))

            # Upload certificate file to domU
            _remote_certificate = os.path.join(_cert_remote_dir, _cert_filename)
            ebLogInfo('*** Copying {0} at {1} on {2}'.format(_local_certificate, _remote_certificate, _domU))
            _nodeU.mCopyFile(_local_certificate, _remote_certificate)

            # Grant proper permissions/ownership on the remote certificate file
            _cmd_str = 'chmod 600 {0}; chown {1}:{2} {0}'.\
                format(_remote_certificate, _dbcs_agent_user, _dbcs_agent_group)
            _nodeU.mExecuteCmdLog(_cmd_str)
            _rc = _nodeU.mGetCmdExitStatus()
            if _rc:
                ebLogWarn('*** Failed to assign permission/ownership for MTLS certificate {0} on {1}'.
                          format(_remote_certificate, _domU))

                # Remove certificate on the remote host to avoid security breaches
                ebLogDebug('*** Removing MTLS certificate {0} on {1} to avoid security breaches.'.
                           format(_remote_certificate, _domU))
                _cmd_str = 'rm -f {0}'.format(_remote_certificate)
                _nodeU.mExecuteCmdLog(_cmd_str)
            else:
                ebLogInfo('*** MTLS certificate {0} successfully configured on {1}'.
                          format(_remote_certificate, _domU))

            _nodeU.mDisconnect()
    
    # Updates grid.ini with parameters - nodelist, sid, dbname, oracle_home
    def mUpdateGridINI(self, aDomUList):

        _domu_list = aDomUList
        _file = "/var/opt/oracle/creg/grid/grid.ini"
        ebLogInfo("Updating grid.ini in the VMs in ZDLRA env.")
        for _domu in _domu_list:
            _path, _sid = self.mGetGridHome(_domu)
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domu)
            _node.mExecuteCmdLog(f'/usr/bin/touch {_file}')

            _node.mExecuteCmd(f"/usr/bin/grep 'oracle_home=' {_file}")
            if _node.mGetCmdExitStatus() != 0:
                ebLogInfo(f"Updating oracle_home={_path} in {_domu}:grid.ini")
                _node.mExecuteCmdLog(f"/usr/bin/echo 'oracle_home={_path}' >> {_file}")
                if _node.mGetCmdExitStatus() != 0:
                    ebLogError(f"Unable to set oracle_home in {_domu}:grid.ini")
            else:
                ebLogError(f"oracle_home already present in {_domu}:grid.ini")

            _node.mExecuteCmd(f"/usr/bin/grep 'sid=' {_file}")
            if _node.mGetCmdExitStatus() != 0:
                ebLogInfo(f"Updating sid={_sid} in {_domu}:grid.ini")
                _node.mExecuteCmdLog(f"/usr/bin/echo 'sid={_sid}' >> {_file}")
                if _node.mGetCmdExitStatus() != 0:
                    ebLogError(f"Unable to set sid in {_domu}:grid.ini")
            else:
                ebLogError(f"sid already present in {_domu}:grid.ini")

            _node.mExecuteCmd(f"/usr/bin/grep 'dbname=' {_file}")
            if _node.mGetCmdExitStatus() != 0:
                ebLogInfo(f"Updating dbname=+ASM in {_domu}:grid.ini")
                _node.mExecuteCmdLog(f"/usr/bin/echo 'dbname=+ASM' >> {_file}")
                if _node.mGetCmdExitStatus() != 0:
                    ebLogError(f"Unable to set dbname in {_domu}:grid.ini")
            else:
                ebLogError(f"dbname already present in {_domu}:grid.ini")

            _node.mExecuteCmd(f"/usr/bin/grep 'nodelist=' {_file}")
            if _node.mGetCmdExitStatus() != 0:
                _, _o, _ = _node.mExecuteCmd(f'{_path}/bin/olsnodes')
                if _node.mGetCmdExitStatus() == 0:
                    _list = _o.read().strip().split('\n')
                    _nodelist = " ".join(_list)
                    ebLogInfo(f"Updating nodelist={_nodelist} in {_domu}:grid.ini")
                    _node.mExecuteCmdLog(f"/usr/bin/echo 'nodelist={_nodelist}' >> {_file}")
                    if _node.mGetCmdExitStatus() != 0:
                        ebLogError(f"Unable to set nodelist in {_domu}:grid.ini")
                else:
                    ebLogError(f"{_domu}: Unable to get result from olsnodes")
            else:
                ebLogError(f"nodelist already present in {_domu}:grid.ini")

            _node.mExecuteCmdLog(f"/usr/bin/chown oracle:oinstall {_file}")
            _node.mDisconnect()

    def mAddAgentWallet(self, aWalletBundlePath: str = ""):
        """
        This function orchestrates the copy of both DBCS and CPS wallets.
        To do so it extracts both wallets from aWalletBundlePath (which
        should be the path where both wallets are located)
        param aWalletBundlePath: String representing the path where both
              wallets should exist encoded in base64.
              This path is fixed to "/opt/oci/exacc/config_bundle", however
              I left this as optional for unittest purposes.
        raises ExacloudRuntimeError
        """

        def executeOrRaise(aNode: exaBoxNode, aDomU: str, aCmd: str) -> None:
            """
            Helper function
            This function will execute aCmd on an already connected aNode
            If cmd returns non-zero code, it will raise ExacloudRuntimeError
            and will close aNode connection
            """
            _, _o, _e = _node.mExecuteCmd(aCmd)
            _rc = _node.mGetCmdExitStatus()
            if _rc:
                _messages = '\n'.join(_o.readlines())+'\n'.join(_e.readlines())
                _err_msg = (f"Failed to create wallet in {aDomU}. "
                            f"Command '{aCmd}' failed with exit code {_rc}, "
                            f"stdout and sterr: {_messages}")
                ebLogError(_err_msg)
                raise ExacloudRuntimeError(0x098, 0xA, _err_msg)

        _domU = None

        def copyWallet(aNode: exaBoxNode, aEncodedWallet: str,
                aRemoteDir: str, aUserPermission:str,
                aGroupPermission:str) -> None:
            """
            This function copies aEncodedWallet to domU in aRemoteDir as
            cwallet.sso and sets proper permissions/ownership in addition
            to decode (base64) aEncodedWallet.
            param aNode: an already connected node to a domU host
            param aEncodedWallet: Filepath of desired wallet to copy on domU
            param aRemoteDir: Path in DomU where well copy wallet
            raises ExacloudRuntimeError:
            returns: None
            """
            _agent_wallet = aEncodedWallet
            _node = aNode
            if not os.path.isfile(_agent_wallet):
                _err_msg = (f"Wallet {os.path.basename(_agent_wallet)} "
                             "does not exists")
                ebLogError(_err_msg)
                raise ExacloudRuntimeError(0x098, 0xA, _err_msg)

            _remote_file_path = os.path.join(aRemoteDir, "cwallet.sso")
            _remote_file_base64 = _remote_file_path + ".b64"

            # Make sure remote dir exists
            _cmd = f"/usr/bin/mkdir -p {aRemoteDir}"
            executeOrRaise(_node, _domU, _cmd)

            # Assign proper permissions to remote dir
            _cmd = f"/usr/bin/chmod 700 {aRemoteDir}"
            executeOrRaise(_node, _domU, _cmd)

            # Assign proper ownership to remote dir
            _cmd = ("/usr/bin/chown "
                   f"{aUserPermission}:{aGroupPermission} "
                   f"{aRemoteDir}")
            executeOrRaise(_node, _domU, _cmd)

            # Copy base64 wallet to domU
            _node.mCopyFile(_agent_wallet, _remote_file_base64)

            # Decode base64 to final wallet
            _cmd = (f"/usr/bin/base64 -d {_remote_file_base64} > "
                    f"{_remote_file_path}")
            executeOrRaise(_node, _domU, _cmd)

            # Shred encoded wallet (no longer needed)
            _pass = self.mCheckConfigOption('vmerase_pass')
            if _pass is None:
                _pass = 3
            else:
                try:
                    _pass = int(_pass[:_pass.lower().find('pass')])
                except:
                    _pass = 3
            _cmd = (f"/usr/bin/shred {_remote_file_base64} "
                    f"-vun {_pass}")
            executeOrRaise(_node, _domU, _cmd)

            # Assign proper permissions to remote wallet file
            _cmd = f"/usr/bin/chmod 700 {_remote_file_path}"
            executeOrRaise(_node, _domU, _cmd)

            # Assign proper ownership to remote wallet file
            _cmd = ("/usr/bin/chown "
                   f"{aUserPermission}:{aGroupPermission} "
                   f"{_remote_file_path}")
            executeOrRaise(_node, _domU, _cmd)

            ebLogInfo("Succesfully added wallet "
                     f"{os.path.basename(aEncodedWallet)} on '{_domU}'")

        _bundle_wallets_path = aWalletBundlePath
        if not _bundle_wallets_path:
            _bundle_wallets_path = "/opt/oci/config_bundle"
        _dbcs_wallet = "DBCSAgentWallet.b64"
        _cps_wallet = "CPSAgentWallet.b64"
        _dbcs_target_dir = "/opt/oracle/dcs/auth"
        _cps_target_dir = "/var/opt/oracle/dbaas_acfs/dbagent/dbagent_wallet"
        _dbcs_local_dir = os.path.join(_bundle_wallets_path, _dbcs_wallet)
        _cps_local_dir = os.path.join(_bundle_wallets_path, _cps_wallet)
        _dbcs_user_owner = "opc"
        _dbcs_group_owner = "opc"
        _cps_user_owner = "oracle"
        _cps_group_owner = "oinstall"

        if os.path.isfile(_dbcs_local_dir) and os.path.isfile(_cps_local_dir):
            for _, _domU in self.mReturnDom0DomUPair():
                _node = exaBoxNode(get_gcontext())
                try:
                    _node.mConnect(aHost=_domU)
                    copyWallet(_node, _dbcs_local_dir, _dbcs_target_dir,
                            _dbcs_user_owner, _dbcs_group_owner)
                    copyWallet(_node, _cps_local_dir, _cps_target_dir,
                            _cps_user_owner, _cps_group_owner)
                finally:
                    _node.mDisconnect()
        else:
            ebLogInfo("DBCS and/or CPS agent wallets don't "
                     f"exist in '{_bundle_wallets_path}'. This is no-op")


    # Copying Domu certificates and key during Create Service flow and Add compute flow
    # if copy fails, then fail CS and Add compute flow respectively.
    def mSetupDomUsForSecurePatchServerCommunication(self):

        def mLoadConfig(file_path):
            with open(file_path, 'r') as f:
                config = json.load(f)
            return config

        def mGetCertKeyPath(config, component_name):
            """
            Retrieves basePath, resourceClass, certPath, and keyPath templates for the given component name.

            Returns:
                Tuple of (basePath, resourceClass, certPath, keyPath).
                Raises ExacloudRuntimeError if component or mapping is missing.
            """
            # Get basePath
            base_path = config.get("basePath")
            if not base_path:
                ebLogError(f"*** Fatal Error *** : basePath missing in exacc cert configuration file.")
                raise ExacloudRuntimeError(0x0820, 0xA, 'basePath missing in exacc cert configuration file.')

            # Find the component's resourceClass
            resource_class = None
            for comp in config.get("components", []):
                if comp.get("name") == component_name:
                    resource_class = comp.get("resourceClass")
                    break
            if resource_class is None:
                ebLogError(f"*** Fatal Error *** : Component {component_name} not found.")
                raise ExacloudRuntimeError(0x0820, 0xA, f"Component {component_name} not found.")

            # Find the certPath and keyPath templates by resourceClass
            cert_path = key_path = None
            for rtype in config.get("resourceTypes", []):
                if rtype.get("resourceClass") == resource_class:
                    cert_path = rtype.get("certPath")
                    key_path = rtype.get("keyPath")
                    break
            if cert_path is None or key_path is None:
                ebLogError(f"*** Fatal Error *** : certPath/keyPath not found for resourceClass {resource_class}.")
                raise ExacloudRuntimeError(0x0820, 0xA, f"certPath/keyPath not found for resourceClass {resource_class}.")

            return base_path, resource_class, cert_path, key_path

        _cluster_name = self.mGetClusterName()
        _domuclient_cert_path = f"/opt/oci/exacc/certs/domuclient/domuclient_{_cluster_name}.crt"
        _domuclient_key_path = f"/opt/oci/exacc/certs/domuclient/domuclient_{_cluster_name}.key"

        if self.mIsCaSignedCerts():
            _cert_config_path = "/opt/oci/exacc/certs/config/exacc_certs.conf"
            if os.path.exists(_cert_config_path):
                _cert_config = mLoadConfig(_cert_config_path)
                _base_path, _resource_class, _cert_path_val, _key_path_val = mGetCertKeyPath(_cert_config, "domuclient")
                _domuclient_cert_path = _base_path + "/" + _cert_path_val
                _domuclient_key_path = _base_path + "/" + _key_path_val
            else:
                ebLogError(f"Configuration file {_cert_config_path} doesn't exist in CPS")
                raise ExacloudRuntimeError(0x0820, 0xA, f'Configuration file {_cert_config_path} does not exist in CPS')

        _cert_source_destination_mapping = {
            f'{_domuclient_cert_path}' : '/etc/pki/oracle/domuidentity/domuclient_cert.pem',
            f'{_domuclient_key_path}' : '/etc/pki/oracle/domuidentity/domuclient.key',
            '/etc/pki/ociexacc/cacert.jks' :  '/etc/pki/oracle/domuidentity/domuclient_ca.jks',
            '/etc/pki/ociexacc/cacert.pem' :  '/etc/pki/oracle/domuidentity/domuclient_ca.pem',
            '/etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem' : '/etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem',
            '/etc/pki/java/cacerts' : '/etc/pki/java/cacerts'
        }
        
        try: 
            for source_file in _cert_source_destination_mapping:
                # If source file does not exist, it makes no sense to continue setting up the environment.
                if not os.path.isfile(source_file):
                    ebLogError(f"*** {source_file} does not exist. Skipping additional security setup")
                    raise ExacloudRuntimeError(0x0820, 0xA, 'Certificate/Key Missing!')

            for _, _domU in self.mReturnDom0DomUPair():
                with connect_to_host(_domU, get_gcontext(), username='root') as _nodeU:
                    for source_file , dest_file in _cert_source_destination_mapping.items():
                        dest_directory_path = os.path.dirname(dest_file)
                        if _nodeU.mFileExists(dest_directory_path) is False:
                            ebLogTrace(f"Directory path : {dest_directory_path} does not exist , creating the path")
                            _nodeU.mExecuteCmd(f'mkdir -p {dest_directory_path}')
                        # Copy certificate file to domU
                        ebLogInfo(f"*** Copying {source_file} to {dest_file} on {_domU}")
                        _nodeU.mCopyFile(source_file, dest_file)
                        if not _nodeU.mFileExists(dest_file):
                            ebLogError(f"*** Fatal Error *** : Error while copying the certificate/key {source_file} to domU {_domU}")
                            raise ExacloudRuntimeError(0x0820, 0xA, 'Certificate/Key Copy failed!')
                        
        except Exception as e:
            ebLogError(f' Error while copying certificate to domU: {e}')
            raise ExacloudRuntimeError(0x0820, 0xA, e)

    # This function executes the steps post GI NID installation.
    #
    # Note: THe function is used by vmgi_install and vmgi_reshape (add node)
    # flow. So, please make sure both the flows remain intact on changing
    # the code in this function.
    #
    def mAddPostGINIDSteps(self, aStepList, aOptions, aGridHome=None):
        _step_list = aStepList
        ebLogInfo('*** Setting up NID requirements')
        _step_time = time.time()
        _gridhome = aGridHome
        # REQ 1 FOR NID (Create ASM Diskgroups)
        self.mAcquireRemoteLock()

        # Do OEDA update, ASM is missing in oratab. Here we are adding the entry
        self.mAddOratabEntry(_gridhome)

        # We now expand the filesystem to 5G before moving ahead with POSTGNID_STEP to address Bug 38087148
        # Resize Domu filesystems
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_POSTGI_NID, _step_list, 'Resize FS')
        expand_domu_filesystem(self)
        self.mLogStepElapsedTime(_step_time, 'Resize FS')        

        if self.__cmd != 'vmgi_reshape':
            self.mUpdateStatusOEDA(True, OSTP_POSTGI_NID, _step_list, 'Create ASM Diskgroups')
            lCmd = "/bin/bash install.sh -cf {0} -s {1} {2}".format(self.__remoteconfig, \
                                                                            self.mFetchOedaStep(OSTP_CREATE_ASM), \
                                                                            self.mGetOEDAExtraArgs())
            ebLogInfo('Running: ' + lCmd)
            _out = self.mExecuteCmdLog2(lCmd, aCurrDir=self.__oeda_path)
            _rc = self.mParseOEDALog(_out)

            if not _rc:
                ebLogError('*** Fatal Error *** : Aborting current job - please review errors log and try again.')
                _error_str = '*** Exacloud Operation Failed : Create ASM Diskgroups failed during Create Service'
                ebLogError(_error_str)
                raise ExacloudRuntimeError(0x0410, 0xA, _error_str, aStackTrace=False)

        self.mReleaseRemoteLock()
        self.mLogStepElapsedTime(_step_time, 'Create ASM Diskgroups')

        ebLogInfo("*** DBCS nid requirements ")
        _step_time = time.time()

        self.mUpdateStatusOEDA(True, OSTP_POSTGI_NID, _step_list, 'OCDE NID configuration script')

        # BUG 27941250 Needed for creating acfs diskgroup as a part of ocde init
        self.mSaveCellInformation()

        _exadata_model =  self.mGetExadataCellModel()
        _exadata_model_gt_X7 = False
        _compare_exadata = self.mCompareExadataModel(_exadata_model, 'X7')
        if _compare_exadata >= 0:
            _exadata_model_gt_X7 = True

        if _exadata_model_gt_X7 is True:
            self.mCreateAcfsDirs()

        #
        # Check if install_tfa is set
        #
        if self.mCheckConfigOption('install_tfa', 'True') and not self.mIsAdbs():
            self.mExecuteOnAllNodes("echo 'installTFA' > /u02/opt/install_tfa", "*** Creating install_tfa indicator for exacc")

        # Indication for exacc that this is ATP instance
        if self.isATP():
            self.mExecuteOnAllNodes("echo 'ATP' > /u02/opt/atp", "*** Creating ATP indicator for exacc")

        #Wait for CRS/ASM/ACFS to be up
        # self.mReturnDom0DomUPair() returns new dom0-domU pairs in vmgi_reshape flow
        # self.mReturnDom0DomUPair() returns all dom0-domU pairs in the cluster in 
        # provisioning flow
        _dpairs = self.mReturnDom0DomUPair()

        _domu_list = [ _domu for _ , _domu in _dpairs]

        self.mCheckCrsIsUp(_domu_list[0], _domu_list)
        self.mCheckAsmIsUp(_domu_list[0], _domu_list, aCheckACFS=_exadata_model_gt_X7)

        # run the installation of dbaastools_exa_main.rpm and rpm updates
        if self.__cmd != 'vmgi_reshape':
            _rc, _cmd = self.mRunScript(aType='*', aWhen='post.gi_rpm', aStatusAbort=True)
            if _rc:
                ebLogError('*** Error ('+str(_rc)+') catched during scripts execution for cmd: '+_cmd)
                raise ExacloudRuntimeError(0x0116, 0xA, 'OCDE Step: RPM configuration error')

        # Execute dbaasapi to create cloud properties file (cprops.ini)
        # Bug28849638: We need to run DBAASAPI before OCDE init.
        # When cprops.ini is created, the atp_enabled entry is created
        # We need atp_enabled=True to stop using ATP indicators inside VM
        if not self.mIsAdbs():
            ebLogInfo("*** DBAAS API Params setup ****")
            _step_time = time.time()
            self.mExecuteDbaaSApi()
            if self.mIsOciEXACC() and self.mIsFedramp():
                self.mEnsureFedRampCpropsIni(self.mReturnDom0DomUPair(), aOptions)
            self.mLogStepElapsedTime(_step_time, 'DBAAS API setup')

            # ER 28621699: Install ATP rpm
            if self.isATP():
                ebLogInfo("*** Installing DBAAS Tools Exa ATP rpm ***")
                self.mUpdateRpm('dbaastools_exa_atp.x86_64.rpm')

        # Run OCDE init setup in new nodes
        if not self.IsZdlraProv() and not self.mIsAdbs():
            # run ocde init setup
            if self.isATP():
                _rc, _cmd = self.mRunScript(aType='*', aWhen='post.gi_nid_atp', aStatusAbort=True)
            else:
                _rc, _cmd = self.mRunScript(aType='*', aWhen='post.gi_nid', aStatusAbort=True)
            if _rc:
                ebLogError('*** Error ('+str(_rc)+') catched during scripts execution for cmd: '+_cmd)
                raise ExacloudRuntimeError(0x0116, 0xA, 'OCDE Step: NID configuration error')

        # install nosql
        if self.mCheckConfigOption('install_nosql', 'True'):
             _domUs = list(map(operator.itemgetter(1),self.mReturnDom0DomUPair()))
             _rackSize = self.mGetRackSize()
             _nosql = ebNoSqlInstaller(_domUs,_rackSize)
             _nosql.mRunInstall()

        if self.isATP():
            self.mExecuteOnAllNodes("rm /u02/opt/atp", "*** Removing ATP indicator")

        if self.mCheckConfigOption('install_tfa', 'True') and not self.mIsAdbs():
            self.mExecuteOnAllNodes("rm /u02/opt/install_tfa", "*** Removing install_tfa indicator")

        self.mLogStepElapsedTime(_step_time, 'OCDE NID configuration script')

        # For pre-provisioning, enable TFA blackout after TFA installation
        if self.__cmd == 'vmgi_preprov':
            self.mEnableTFABlackout(True, "Pre-Provision blackout", aOptions)

        # At this point opc user is available & hence create the priv/pub keys for opc user & inject the pub key
        # into the /home/opc/.ssh/authorized_keys and save the keys in the exacloud keys dir.
        ebLogInfo("*** Manage OPC user keys ****")
        self.mAddUserPubKey("opc")
        if not self.mIsAdbs():
            self.mCopySAPfile()

        #
        # POSTGI - Disable TFA if grid_tfa_enabled is not True( Dev Env )
        if not self.mCheckConfigOption('grid_tfa_enabled','True') and self.isATP() == False:
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_POSTGI_NID, _step_list, 'Disabing TFA')
            self.mDisableTFA()
            self.mLogStepElapsedTime(_step_time, 'Disabling TFA')

        #Run the mAtpConfig after create the OPC user
        if self.isATP():
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_POSTGI_NID, _step_list, 'ATPConfig and install of host-update-yaarp-clients rpm')
            self.mAtpConfig()
            self.mLogStepElapsedTime(_step_time, 'ATPConfig and install of host-update-yaarp-clients')

        #
        # ER 31923304: Configure temporary directories exclusions
        #
        self.mAddOracleFolderTmpConf()
        #
        # Add Extra config for GI
        #
        _step_time = time.time()
        self.mUpdateStatusOEDA(True, OSTP_POSTGI_NID, _step_list, 'Apply Extra Srvcrl Config')
        self.mApplyExtraSrvctlConfig()
        self.mLogStepElapsedTime(_step_time, 'Apply Extra Srvcrl Config')
        #
        # ER 30138555: MTLS Authentication For ExaCC DBCS Agent
        # Prepare domUs for MTLS communication
        #
        if self.mIsOciEXACC():
            self.mSetupDomUsForSecureDBCSCommunication()
            # fedramp check placed inside class
            _obj = ebCopyDBCSAgentpfxFile(self)
            _obj.mCopyDbcsAgentpfxFiletoDomUsForFedramp()
            #
            # ER 32161016: Copy DBCS/CPS agent wallets
            #
            self.mAddAgentWallet()

        #
        # ER 34691634: Install Suricata rpm as part of Node subsetting Operation
        #
        if self.mCheckConfigOption('ociexacc', 'True') and self.isATP():
            try:
                self.mUpdateStatusOEDA(True, OSTP_POSTGI_NID, _step_list, 'Installing Suricata rpm')
                _hostlist = [_domu for _, _domu in self.mReturnDom0DomUPair()]
                self.mInstallSuricataRPM(_hostlist,"domu")
            except Exception as e: 
                ebLogWarn(f"*** mInstallSuricataRPM failed with Exception: {str(e)}")
                ebLogInfo(f"*** mInstallSuricataRPM failed with Exception: {str(e)}")

        #
        # ER 27371691: Install DBCS agent rpm
        #
        if not self.mIsAdbs():
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_POSTGI_NID, _step_list, 'Installing DBCS Agent rpm')

            _majorityVersion = self.mGetMajorityHostVersion(ExaKmsHostType.DOMU)
            if _majorityVersion in ["OL7", "OL8"]:
                if self.mIsExabm() or self.mCheckConfigOption("force_install_dbcs_agent", "exacs"):
                    self.mUpdateRpm('dbcs-agent.OL7.x86_64.rpm')
                else:
                    if self.mIsOciEXACC() or self.mCheckConfigOption("force_install_dbcs_agent", "exacc"):
                        self.mUpdateRpm('dbcs-agent-exacc.OL7.x86_64.rpm')
                    else:
                        ebLogInfo('Skipping installing OL7 dbcs agent')
            else:
                if self.mIsExabm() or self.mCheckConfigOption("force_install_dbcs_agent", "exacs"):
                    self.mUpdateRpm('dbcs-agent.OL6.x86_64.rpm')
                else:
                    if self.mIsOciEXACC() or self.mCheckConfigOption("force_install_dbcs_agent", "exacc"):
                        self.mUpdateRpm('dbcs-agent-exacc.OL6.x86_64.rpm')
                    else:
                        ebLogInfo('Skipping installing OL6 dbcs agent')

            self.mLogStepElapsedTime(_step_time, 'Install DBCS Agent rpm')

        # DBCS No-Auth mode only in DEV/QA
        if not self.mEnvTarget() and self.mCheckConfigOption('force_dbcsagent_auth', 'True') and not self.mIsAdbs():
            self.mEnableNoAuthDBCS()

        if self.__exacm is True:
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_POSTGI_NID, _step_list, 'EXACM copy ssh key patch script')
            self.mCopyExacmPatchKeyScript()
            self.mLogStepElapsedTime(_step_time, 'EXACM copy ssh key script')

        # install ATP Namespace rpm, if enable_namespace is True
        if self.__exabm and self.isATP():

            _majorityVersion = self.mGetMajorityHostVersion(ExaKmsHostType.DOMU)
            if _majorityVersion in ["OL7", "OL8"]:
                _atp_config = self.mCheckConfigOption('atp')
                if _atp_config:
                    if 'enable_namespace' in _atp_config and _atp_config['enable_namespace'] == 'True':

                        self.mUpdateStatusOEDA(True, OSTP_POSTGI_NID, _step_list, 'Installing ATP Namespace rpm')
                        self.mSetupNamespace()
                        _step_time = time.time()
                        self.mUpdateRpm('atp-namespace.x86_64.rpm')
                        self.mLogStepElapsedTime(_step_time, 'Install ATP Namespace rpm')

        if self.isATP():
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, OSTP_POSTGI_INSTALL, _step_list, 'Set CSS Misscount in ATP')
            _miscnt = get_gcontext().mGetConfigOptions().get("css_misscount", "")
            if _miscnt:
                self.mDomUCSSMisscountHandler(aMode = False, aMisscount = _miscnt)
            else:
                ebLogInfo("*** Setting CSS Misscount as value set in exabox.conf")
            self.mLogStepElapsedTime(_step_time, 'Set CSS Misscount in ATP')

        #Set OraInventory Permissions to drwxrwx---
        self.mSetOraInventoryPermissions()

        # Store the interconnect IP's of the VM's in cluster_interconnect.dat
        self.mStoreDomUInterconnectIps()

        if self.mIsKVM():
            for _, _domU in self.mReturnDom0DomUPair():
                self.mMakeFipsCompliant(aOptions, aHost=_domU)
                #reboot will happen in mStartVMExacsService !

        #
        # Attach virtio serial device to KVM Guest.
        # Update the chasis information to GuestVM.
        #
        if self.mIsKVM():
            self.mUpdateVmetrics('vmexacs_kvm')
            self.mStartVMExacsService(aOptions)

        if self.IsZdlraProv():
            self.__ZDLRA.mUpdateHugePages(aOptions)

        for _, _domu in self.mReturnDom0DomUPair():                
            _node = exaBoxNode(get_gcontext())                     
            _node.mConnect(aHost=_domu)                            
            if _node.mFileExists('/etc/oracle/cell/network-config/cellkey.ora'):                                        
                _node.mExecuteCmd("chmod 640 /etc/oracle/cell/network-config/cellkey.ora")                              
            _node.mDisconnect()                                    

    def mExecuteFileFedramp(self, aHost, aLocalFile, aRemoteFile):

        ebLogInfo("*** In mExecuteFileFedramp: Copy and execute '{0}' on '{1}'".format(aRemoteFile, aHost))

        _node = exaBoxNode(get_gcontext())
        _node.mConnect(aHost)

        if not _node.mCompareFiles(aLocalFile, aRemoteFile, self._hash_file_cache):
            _node.mExecuteCmd('mkdir -p {}'.format(os.path.dirname(aRemoteFile)))
            _node.mCopyFile(aLocalFile, aRemoteFile)
            _cmdstr = 'chmod 740 {}'.format(aRemoteFile)
            _node.mExecuteCmdLog(_cmdstr)

        _node.mExecuteCmdLog(aRemoteFile)
        _node.mDisconnect()

    def mRegisterCronJob(self, aDom0s, aDomUs, aCells, aSwitches):
        # read cronjob.conf
        _cronjobs = None
        try:
            _file = open('config/cronjob.conf')
            _cronjobs = json.loads(_file.read())
            _file.close()
        except:
            ebLogError('*** Cannot access/read cronjob.conf file')
            return

        for _, _details in list(_cronjobs.items()):
            _local_file  = _details["local_file"]
            _remote_file = _details["remote_file"]
            _min_period  = _details["min_period"]
            _host_types  = _details["host_types"]
            _install_cmd = _details["install_cmd"]
            _exec_cmd    = _details["exec_cmd"]
            _hosts = []
            # make a list of target hosts
            for _host_type in _host_types:
                _host_type = _host_type.lower()
                if _host_type == "dom0":
                    _hosts += aDom0s
                elif _host_type == "domu":
                    _hosts += aDomUs
                elif _host_type == "cell":
                    _hosts += aCells
                elif _host_type == "switch":
                    _hosts += aSwitches
            for _host in _hosts:
                ebLogInfo('*** Register cronjob for {0} on {1}'.format(
                            _local_file, _host))
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(_host)
                # copy and install rpm / copy script and change file mode
                if not _node.mCompareFiles(_local_file, _remote_file, self._hash_file_cache):
                    _node.mExecuteCmd(
                            'mkdir -p {}'.format(os.path.dirname(_remote_file)))
                    _node.mCopyFile(_local_file, _remote_file)
                    _node.mExecuteCmdLog(_install_cmd)
                # register cronjob
                _cronjob = '*/{0} * * * * {1}'.format(_min_period, _exec_cmd)
                _cmd = 'cat <(crontab -l | grep -v "{0}") <(echo "{1}") | crontab'.format(_exec_cmd, _cronjob)
                _node.mExecuteCmdLog(_cmd)
                _node.mDisconnect()

    def mEnableNoAuthDBCS(self):

        for _, _domu in self.mReturnDom0DomUPair():

            ebLogInfo("Enable No-Auth mode in DBCS on {0}".format(_domu))

            _node = exaBoxNode(get_gcontext())
            _node.mConnect(_domu)

            if self.__exacm or self.__ociexacc:
                _node.mExecuteCmdLog("/opt/oracle/dcs/bin/agent-noauth.sh ExaCC")
            else:
                _node.mExecuteCmdLog("/opt/oracle/dcs/bin/agent-noauth.sh")

            _node.mDisconnect()


    def mFedrampConfig(self, aTargetType=None):

        ebLogInfo("*** Entering to mFedrampConfig")

        #Get the host file list
        if not self.mIsKVM():
            _dom0s, _domUs, _cells, _switches = self.mReturnAllClusterHosts()
        else:
            _dom0s, _domUs, _cells, _ = self.mReturnAllClusterHosts()
            _switches = []

        _hosts = _dom0s + _cells + _switches

        # In case fedramp configuration changes are performed
        # on individual targets. By default all configuration
        # changes are done.

        if aTargetType == "cells":
            _hosts = _cells
        elif aTargetType == "ibswitch":
            _hosts = _switches
        elif aTargetType == "dom0":
            _hosts = _dom0s
        elif aTargetType == "domu":
            _hosts = _domUs

        if aTargetType == "ibswitch" or aTargetType is None:
            #Copy and execute the switch script
            if aTargetType is not None:
                ebLogInfo("Fedramp configuration changes related to %s would be performed." % aTargetType)
            _local_file  = 'scripts/fedramp/ExadataStigFix_IBSwitch_SREv01.sh'
            _remote_file = '/opt/exacloud/scripts/fedramp/ExadataStigFix_IBSwitch_SREv01.sh'
            for _switch in _switches:
                self.mExecuteFileFedramp(_switch, _local_file, _remote_file)

        if aTargetType == "cells" or aTargetType is None:
            #Copy and execute the cell script
            if aTargetType is not None:
                ebLogInfo("Fedramp configuration changes related to %s would be performed." % aTargetType)
            _local_file  = 'scripts/fedramp/ExadataStigFix_cell_SREv01.sh'
            _remote_file = '/opt/exacloud/scripts/fedramp/ExadataStigFix_cell_SREv01.sh'
            for _cell in _cells:
                self.mExecuteFileFedramp(_cell, _local_file, _remote_file)

        if aTargetType == "dom0" or aTargetType is None:
            #Copy and execute the dom0 script
            if aTargetType is not None:
                ebLogInfo("Fedramp configuration changes related to %s would be performed." % aTargetType)
            if self.mIsKVM():
                _local_file  = 'scripts/fedramp/ExadataStigFix_cell_SREv01.sh'
                _remote_file = '/opt/exacloud/scripts/fedramp/ExadataStigFix_cell_SREv01.sh'
                for _dom0 in _dom0s:
                    self.mExecuteFileFedramp(_dom0, _local_file, _remote_file)
            else:
                _local_file  = 'scripts/fedramp/ExadataStigFix20190526_SREv02.sh'
                _remote_file = '/opt/exacloud/scripts/fedramp/ExadataStigFix20190526_SREv02.sh'
                for _dom0 in _dom0s:
                    self.mExecuteFileFedramp(_dom0, _local_file, _remote_file)

        #Generate the message of SSH Connections
        for _host in _hosts:

            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_host)

            ebLogInfo("*** Generate SSH banner on the host '{0}'".format(_host))

            if not _node.mCompareFiles('scripts/fedramp/ssh_message.txt', '/etc/issue.net', self._hash_file_cache):
                _cmd = 'cp /etc/issue.net /opt/exacloud/scripts/fedramp/ssh_message_old.txt'
                _node.mExecuteCmdLog(_cmd)
                _node.mCopyFile('scripts/fedramp/ssh_message.txt', '/etc/issue.net')

            _cmd = """
                    awk 'BEGIN { present=0 }
                    {
                        if($0 ~ /Banner/) {
                            printf \"%s\\n\", \"Banner /etc/issue.net\";
                            present=1
                        }
                        else {
                            print $0
                        }
                    }
                    END {
                        if (present == 0) {
                            printf \"%s\\n", \"Banner /etc/issue.net\"
                        }
                    }' /etc/ssh/sshd_config > /etc/ssh/sshd_config_cp
                    """
            _node.mExecuteCmdLog(_cmd)

            _cmd = 'mv /etc/ssh/sshd_config /opt/exacloud/scripts/fedramp/sshd_config_old'
            _node.mExecuteCmdLog(_cmd)

            _cmd = 'mv /etc/ssh/sshd_config_cp /etc/ssh/sshd_config'
            _node.mExecuteCmdLog(_cmd)

            _cmd = "service sshd restart"
            _node.mExecuteCmdLog(_cmd)
            _node.mDisconnect()

        # Register multiple fedramp scripts to cronjob of target hosts
        # Info is registered in exacloud/config/cronjob.conf
        #  - install remote_file rpm of working version in this stage
        #  - register exec_cmd to cronjob
        #  - weekly HouseKeeper will install new rpm if exists
        #  - e.g. RPM file is in exacloud/misc/fedramp/rpmwatch-<version>.noarch.rpm
        #  - e.g. build.sh copies rpm to images/ then create a symlink as rpmwatch-latest.noarch.rpm
        self.mRegisterCronJob(_dom0s, _domUs, _cells, _switches)

    def mConfigurePasswordlessAllUsers(self):

        if self.mIsExaScale():

            _userNames = []
            for _userId in self.mGetUsers().mGetUsers():
                _userConfig = self.mGetUsers().mGetUser(_userId)
                _userName = _userConfig.mGetUserName()
                _userNames.append(_userName)


            # Get a single list of Dom0, DomU hosts
            _plist = ProcessManager()
            for _userName in set(_userNames):

                ebLogInfo(f"Spawning process to configure passwordless on user {_userName}")
                _p = ProcessStructure(
                    self.mConfigurePasswordLessDomU, [_userName], _userName)
                _p.mSetMaxExecutionTime(60*15) # 15 minutes
                _p.mSetJoinTimeout(5)
                _p.mSetLogTimeoutFx(ebLogWarn)
                _plist.mStartAppend(_p)

            _plist.mJoinProcess()

    def mConfigurePasswordLessDomU(self, aUser):

        _domUs = [x[1] for x in self.mReturnElasticAllDom0DomUPair()]

        if aUser == 'root':
            _home_dir = '/root'
        else:
            _home_dir = f'/home/{aUser}'

        _keyType = "rsa"

        _exakms = get_gcontext().mGetExaKms()
        if _exakms.mGetDefaultKeyAlgorithm() == "ECDSA":
            _keyType = "ecdsa"

        for _actualDomU in _domUs:

            #Create key on actual domu
            ebLogInfo(f'Create key for user: {aUser} on {_actualDomU}')

            try:
                _node = exaBoxNode(get_gcontext())
                #Lets connect with default root user, since non-root users would be not be available.
                #root user does have permission to setup keys for other users.
                _node.mConnect(aHost=_actualDomU)
                node_exec_cmd_check(_node, f'/bin/su - {aUser} -c "/bin/mkdir -p {_home_dir}/.ssh"')
                node_exec_cmd_check(_node, f'/bin/su - {aUser} -c "/bin/chown `id -u {aUser}`:`id -g {aUser}` {_home_dir}/.ssh"')
                node_exec_cmd_check(_node, f'/bin/su - {aUser} -c "/bin/chmod 700 {_home_dir}/.ssh"')

                if not _node.mFileExists(f'{_home_dir}/.ssh/id_{_keyType}') and not _node.mFileExists(f'{_home_dir}/.ssh/id_{_keyType}.pub'):

                    if _keyType == "ecdsa":
                        _cmd = f'/bin/su - {aUser} -c "/bin/ssh-keygen -t ecdsa -b 384 -q -C \'USER_KEY\' -N \'\' -f {_home_dir}/.ssh/id_{_keyType}"'
                        node_exec_cmd_check(_node, _cmd)
                    else:
                        _cmd = f'/bin/su - {aUser} -c "/bin/ssh-keygen -q -C \'USER_KEY\' -N \'\' -f {_home_dir}/.ssh/id_{_keyType}"'
                        node_exec_cmd_check(_node, _cmd)

                _cmd = f'/bin/su - {aUser} -c "/bin/chmod 600 {_home_dir}/.ssh/id_{_keyType}*"'
                node_exec_cmd_check(_node, _cmd)
                _cmd = f'/bin/su - {aUser} -c "/bin/chown `id -u {aUser}`:`id -g {aUser}` {_home_dir}/.ssh/id_{_keyType}*"'
                node_exec_cmd_check(_node, _cmd)

                _i, _o, _e = _node.mExecuteCmd(f'/bin/su - {aUser} -c "/bin/cat {_home_dir}/.ssh/id_{_keyType}.pub"')
                _keyContent = _o.readlines()[0].strip()
                _node.mExecuteCmd(f'/bin/su - {aUser} -c "/bin/echo {_keyContent}  >> {_home_dir}/.ssh/authorized_keys"')
                node_exec_cmd_check(_node, f'/bin/su - {aUser} -c "/bin/chmod 600 {_home_dir}/.ssh/authorized_keys"')
                node_exec_cmd_check(_node, f'/bin/su - {aUser} -c "/bin/chown `id -u {aUser}`:`id -g {aUser}` {_home_dir}/.ssh/authorized_keys"')

                # Reset expiration date
                node_exec_cmd(_node, f'/usr/bin/chage --lastday `date +"%Y-%m-%d"` {aUser}')

                #Add self fingerprints
                node_exec_cmd(_node, f'/bin/su - {aUser} -c "/bin/ssh-keygen -R localhost"')
                node_exec_cmd(_node, f'/bin/su - {aUser} -c "/bin/ssh-keygen -R {_actualDomU}"')
                node_exec_cmd(_node, f'/bin/su - {aUser} -c "/bin/ssh-keygen -R {_actualDomU.split(".")[0]}"')
                node_exec_cmd_check(_node, f'/bin/su - {aUser} -c "/bin/ssh-keyscan -H localhost >> {_home_dir}/.ssh/known_hosts"')
                node_exec_cmd_check(_node, f'/bin/su - {aUser} -c "/bin/ssh-keyscan -H {_actualDomU} >> {_home_dir}/.ssh/known_hosts"')
                node_exec_cmd_check(_node, f'/bin/su - {aUser} -c "/bin/ssh-keyscan -H {_actualDomU.split(".")[0]} >> {_home_dir}/.ssh/known_hosts"')

                #Inject the key to the rest of the DomUs
                for _rest in _domUs:
                    if _rest == _actualDomU:
                        continue

                    ebLogInfo(f'Share key with {_rest}')

                    try:
                        _restnode = exaBoxNode(get_gcontext())
                        _restnode.mConnect(aHost=_rest)
                        node_exec_cmd(_restnode, f'/bin/su - {aUser} -c "/bin/mkdir -p {_home_dir}/.ssh"')
                        node_exec_cmd(_restnode, f'/bin/su - {aUser} -c "/bin/chmod 700 {_home_dir}/.ssh"')
                        _restnode.mExecuteCmd(f'/bin/su - {aUser} -c "/bin/echo {_keyContent}  >> {_home_dir}/.ssh/authorized_keys"')
                        node_exec_cmd_check(_restnode, f'/bin/su - {aUser} -c "/bin/chmod 600 {_home_dir}/.ssh/authorized_keys"')
                        node_exec_cmd_check(_restnode, f'/bin/su - {aUser} -c "/bin/chown `id -u {aUser}`:`id -g {aUser}` {_home_dir}/.ssh/authorized_keys"')
                    except Exception as exp:
                        _msg = f'::mConfigurePasswordLessDomU failed for user {aUser} on {_actualDomU}: {exp}'
                        ebLogError(_msg)
                        raise ExacloudRuntimeError(aErrorMsg=_msg) from exp
                    finally:
                        _restnode.mDisconnect()

                    #Add the access to the rest of the hosts
                    #Get client IP of the domu
                    _domU_mac = self.__machines.mGetMachineConfig(_rest)
                    _domU_net_list = _domU_mac.mGetMacNetworks()
                    for _net in _domU_net_list:
                        _priv = self.__networks.mGetNetworkConfig(_net)
                        if _priv.mGetNetType() == 'client':
                            _client_ip = _priv.mGetNetIpAddr()
                            node_exec_cmd_check(_node, f'/bin/su - {aUser} -c "/bin/ssh-keygen -R {_client_ip}"')
                    node_exec_cmd_check(_node, f'/bin/su - {aUser} -c "/bin/ssh-keygen -R {_rest}"')
                    node_exec_cmd_check(_node, f'/bin/su - {aUser} -c "/bin/ssh-keygen -R {_rest.split(".")[0]}"')
                    
                    _cmd = f'/bin/cat < /dev/null > /dev/tcp/{_rest}/22'
                    _rc, _out, _err = node_exec_cmd(_node, _cmd)
                    if _rc:
                        ebLogError(_err)
                        _msg = f'::mConfigurePasswordLessDomU failed for user {aUser} on {_actualDomU}: unable to reach {_rest} using ssh port'
                        raise ExacloudRuntimeError(0x10, 0xA, _msg)
                    _cmd_add_host_key = f'/bin/su - {aUser} -c "/bin/ssh-keyscan -T 30 -H {_rest} >> {_home_dir}/.ssh/known_hosts"'
                    _rc, _out, _err = node_exec_cmd(_node, _cmd_add_host_key)
                    if _rc:
                        _cmd_add_host_key = f'/bin/su - {aUser} -c "/bin/ssh-keyscan -vv -T 30 -H {_rest} >> {_home_dir}/.ssh/known_hosts"'
                        _rc, _out, _err = node_exec_cmd(_node, _cmd_add_host_key, log_stdout_on_error=True)
                        if _rc:
                            ebLogInfo(f'Output from command execution: {_out}')
                            ebLogError(f'Debug info from command execution:\n{_err}')
                            _msg = f'::mConfigurePasswordLessDomU failed for user {aUser} on {_actualDomU}: Failed to add ssh host key for {_rest}'
                            raise ExacloudRuntimeError(0x10, 0xA, _msg)
                    _cmd_add_host_key = f'/bin/su - {aUser} -c "/bin/ssh-keyscan -T 30 -H {_rest.split(".")[0]} >> {_home_dir}/.ssh/known_hosts"'
                    _rc, _out, _err = node_exec_cmd(_node, _cmd_add_host_key)
                    if _rc:
                        _cmd_add_host_key = f'/bin/su - {aUser} -c "/bin/ssh-keyscan -vv -T 30 -H {_rest.split(".")[0]} >> {_home_dir}/.ssh/known_hosts"'
                        _rc, _out, _err = node_exec_cmd(_node, _cmd_add_host_key, log_stdout_on_error=True)
                        if _rc:
                            ebLogInfo(f'Output from command execution: {_out}')
                            ebLogError(f'Debug info from command execution:\n{_err}')
                            _msg = f'::mConfigurePasswordLessDomU failed for user {aUser} on {_actualDomU}: Failed to add ssh host key for {_rest.split(".")[0]}'
                            raise ExacloudRuntimeError(0x10, 0xA, _msg)
            except Exception as exp:
                _msg = f'::mConfigurePasswordLessDomU failed for user {aUser} on {_actualDomU}: {exp}'
                ebLogError(_msg)
                raise ExacloudRuntimeError(aErrorMsg=_msg) from exp
            finally:
                _node.mDisconnect()

    #
    # ER 27595242
    #
    def mAddUserPubKey(self, aUser):

        ebLogInfo(f'*** mAddUserPubKey: Manage keys for {aUser} user to access from exacloud')

        _authkeys = f'/home/{aUser}/.ssh/authorized_keys'
        _comment = 'EXACLOUD KEY'

        _exakms = get_gcontext().mGetExaKms()

        for _dom0, _domU in self.mReturnDom0DomUPair():
            try:
                with connect_to_host(_domU, self.mGetCtx()) as _node:
                    # Pregenerate key for user
                    _cparam = {"FQDN": _domU, "user": aUser}
                    _entry = _exakms.mGetExaKmsEntry(_cparam)

                    if _entry:
                        _exakms.mDeleteExaKmsEntry(_entry)

                    _entry = _exakms.mBuildExaKmsEntry(
                        _domU,
                        aUser,
                        _exakms.mGetEntryClass().mGeneratePrivateKey(),
                        ExaKmsHostType.DOMU
                    )

                    _exakms.mInsertExaKmsEntry(_entry)
                    _pubkey = _entry.mGetPublicKey(_comment)

                    # Add new key
                    ebLogInfo(f'*** mAddUserPubKey add the {aUser} user pub_key into authorized_keys on {_domU}')
                    ebLogInfo(f'Public key to add: {_pubkey}')

                    node_write_text_file(_node, _authkeys, f'{_pubkey}\n', append=True)
                    node_exec_cmd_check(_node, f'/bin/chown -R `id -u {aUser}`:`id -g {aUser}` {_authkeys}')
                    node_exec_cmd_check(_node, f'/bin/chmod 600 {_authkeys}')

            except Exception as exp:
                _msg = f'::mAddUserPubKey failed for user {aUser} on {_domU}: {exp}'
                ebLogError(_msg)
                raise ExacloudRuntimeError(aErrorMsg=_msg) from exp

    # Function to execute simple shell commands on all nodes.
    # Caution: Avoid doing complex operations as this offers no error / exception handling.
    def mExecuteOnAllNodes(self, aCmd, aMsg=None):
        if aMsg is not None:
            ebLogInfo(aMsg)
        for _, _domu in self.mReturnDom0DomUPair():
            _nodeU = exaBoxNode(get_gcontext())
            _nodeU.mConnect(aHost=_domu)
            _nodeU.mExecuteCmdLog(aCmd)
            _nodeU.mDisconnect()

    def mAddOratabEntry(self, aGridHome=None):

        if aGridHome:
            _location = aGridHome
        else:
            if self.mGetGiMultiImageSupport():
                _giversion = self.mGetVersionGiMultiImages()
                _gihv = self.mGetGridConfig()[_giversion]
                _location = os.path.join('/u01/app', _gihv[2], 'grid')
            else:
                _giversion = self.mGetVersionGi()
                _gihv = self.mGetGridConfig()[_giversion]
                _location = os.path.join('/u01/app', _gihv[0], 'grid')

        _dpairs = self.mReturnDom0DomUPair()
        _domu_list = [ _domu for _ , _domu in _dpairs]

        #mCheckAsmIsUp will also bring up asm, if found to be down.
        self.mCheckAsmIsUp(_domu_list[0], _domu_list, aCheckACFS=False, aGridHome=_location)

        for _dom0, _domU in _dpairs:
            _node = exaBoxNode(get_gcontext())
            _node.mSetUser('grid')
            _node.mConnect(_domU)

            cmd = r"export ORACLE_HOME={0}; {0}/bin/srvctl status asm -node {1} -detail | /bin/grep -m1 'ASM instance'"
            ebLogTrace(f"Using command: {cmd} to extract ASM instance name.")
            _i, _o, _e = _node.mExecuteCmd(cmd.format(_location,_domU))
            if _node.mGetCmdExitStatus():
                #We would not be getting here, since asm is already made sure to be up and running with usage of mCheckAsmIsUp above
                msg = 'ASM is not running after "Step9_Initialize_Cluster_Software"'
                _node.mDisconnect()
                raise ExacloudRuntimeError(0x0516, 0x0A, msg)

            _asm_extract_op = _o.read().strip()
            ebLogTrace(f"Full ouput: {_asm_extract_op}")
            _words = _asm_extract_op.split(" ")
            _asmstr = None
            for _word in _words:
                if _word.startswith("+ASM"):
                    _asmstr = _word
                    break
            if _asmstr is None:
                msg = f"Failed to extract ASM instance name. svrctl output:{_asm_extract_op}"
                _node.mDisconnect()
                raise ExacloudRuntimeError(0x0516, 0x0A, msg)
            cmd = "grep ^+ASM /etc/oratab"
            _node.mExecuteCmd(cmd)
            if _node.mGetCmdExitStatus():
                ebLogInfo('Adding entry - %s:%s:N to oratab' % (_asmstr, _location))
                cmd = "echo '{}:{}:N' >> /etc/oratab"
                _node.mExecuteCmd(cmd.format(_asmstr, _location))
                _node.mDisconnect()
            else:
                ebLogInfo('*** ASM is already in oratab')
                _node.mDisconnect()

    # Adds an entry (to /tmp/operations) in dom0 related to
    # operations (create service and delete service) being executed along
    # with the Exacloud versions and OEDA versions. Thereby keeping track of
    # the OEDA and Exacloud versions being used on the racks.
    # aOperation - Operation being executed (Create Service / Delete Service)
    # aPhase - phase of the operation (started / ended)
    def mDom0UpdateCurrentOpLog(self, aOptions, aOperation, aPhase, aList=None):

        _ebCore = exaBoxCoreInit(aOptions)
        if self.isBaseDB() or self.isExacomputeVM():
            # Since basedb has only one vm
            _ddu = self.mReturnDom0DomUPair()
            _clusterName = _ddu[0][1]
        else:
            _clusterName = self.__clusters.mGetCluster().mGetCluName()
        # Get the Exacloud version
        _exacloudVersion = "%s (%s)" %(_ebCore.mGetVersion())
        # Get the OEDA version
        _oedaVersion = get_gcontext().mGetOEDAVersion().rstrip()
        # Get the user executing the operation.
        _uname = pwd.getpwuid(os.getuid()).pw_name
        _time = time.strftime("%a %b %d %H:%M:%S %Z %Y")
        # Get the Hostname of the Control Plane
        _rc, _, _o, _ = self.mExecuteLocal("/bin/hostname")
        _hostname = _o.strip()

        if aList:
            _list = aList
        else:
            _list = self.mReturnDom0DomUPair()
        if aOptions is not None and aOptions.jsonconf is not None and 'poolsize' in list(aOptions.jsonconf.keys()):
            _cos = "enabled"
        else:
            _cos = "disabled"

        _steplist = ""
        if aOptions.steplist:
            _steplist = aOptions.steplist

        for _dom0, _domU in _list:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            _entry = _uname + " : " + aOperation + " " + _steplist + \
                     " " + aPhase + " on dom0: " + _dom0 + " on domU: " + _domU + \
                     " with Exacloud version: " + _exacloudVersion + \
                     " and OEDA version: " + _oedaVersion + " with Cos: " + _cos + \
                     " and Control plane hostname: " + _hostname
            _cmd = "mkdir -p /opt/exacloud/clusters/operations"
            _node.mExecuteCmd(_cmd)

            #sample filename format: "cluster-dom0-slcs16adm04.txt"
            _commonFname = "cluster-dom0-" + _dom0.split('.')[0]
            ebLogVerbose("mDom0UpdateCurrentOpLog: Add OEDA and Exacloud " + \
                    "version to " + _commonFname + ".txt and " + _clusterName + ".txt under /opt/exacloud/clusters/operations/ in Dom0")
                                       # Converts time to the Dom0 timezone.
            _cmd = "echo \"[ `date -d \"" + _time  + "\"` ] " + _entry + "\" >> /opt/exacloud/clusters/operations/" + _clusterName + ".txt"
            _node.mExecuteCmd(_cmd)
            _cmd = "echo \"[ `date -d \"" + _time  + "\"` ] " + _entry + "\" >> /opt/exacloud/clusters/operations/" + _commonFname + ".txt"
            _node.mExecuteCmd(_cmd)
            _node.mDisconnect()

    def mGetOedaStepTable(self):
        aOptions = self.mGetArgsOptions()
        stepdriver = csDriver(self, aOptions)
        return stepdriver.mGetOedaTable()

    def mBuildOedaStepsTable(self, aPath):

        #In different services the oeda steps are different. but in all the service steps are more than 2
        _oeda_step_num = 2
        gOedaSTable = self.mGetOedaStepTable()

        # Fetch step and compute translation table accordingly
        lCmd = "/bin/bash install.sh -cf {0} -l {1}".format(self.__remoteconfig, \
                                                            self.mGetOEDAExtraArgs())
        fin, fout, ferr = self.mExecuteCmd(lCmd, aCurrDir=aPath)
        out = fout.readlines()
        _lv = list(gOedaSTable.values())
        if out and len(out) >= _oeda_step_num:
            for _l in out:
                if '. ' in _l:
                    _k, _v = _l[:-1].split('. ')
                    for _x in _lv:
                        if _v == _x[1]:
                            _x[2] = _k
                            gOedaSTable[_x[0]] = _x
        else:
            ebLogError('::mExecuteOEDAStep can not retrieve OEDA steps')
            return -1

        self.__oeda_stable = gOedaSTable
        return 0

    def mFetchOedaStep(self,aStep):
        ebLogVerbose("mFetchOedaStep: aStep = %s" % aStep)
        if type(aStep) == type(0):
            aStep = str(aStep)

        if not self.__oeda_stable:
            ebLogError('::mFetchOedaStep can not retrieve OEDA steps')
            return -1
        
        if self.mIsXS():
            return self.__oeda_stable[aStep][0]

        return self.__oeda_stable[aStep][2]

    def mFetchOedaString(self,aStep):

        ebLogVerbose("mFetchOedaString: aStep = %s" % aStep)
        if type(aStep) == type(0):
            aStep = str(aStep)

        if not self.__oeda_stable:
            ebLogError('::mFetchOedaStep can not retrieve OEDA string')
            return -1

        return self.__oeda_stable[aStep][1]

    def mRunScript(self, aType='*', aWhen='*', aStatusAbort=False, aParallel=True):

        _dom0U = self.mReturnDom0DomUPair()
        _elastic_op = False
        # check for a  SI Elastic DB operation
        if aWhen in ['posdb_nid', 'dg_repeat_setup']:
            _elastic_op,_nodelist = self.mCheckNodeListParam()
            if _elastic_op is True:
                _dom0U = self.mCheckNodeList(_nodelist)

        _dom0S = {}
        _domUS = {}
        for _d0,_du in _dom0U:
            _dom0S[_d0] = _d0
            _domUS[_du] = _du
        _str_dom0S = ' '.join(list(_dom0S.values()))
        _str_domUS = ' '.join(list(_domUS.values()))
        _str_dbName    = self.__dbname
        if self.__dbname_cfg:
            _str_dbNameCfg_remoteloc = '/tmp/'+self.__dbname_cfg.split('/')[-1]
        else:
            _str_dbNameCfg_remoteloc = None

        if self.__standbydb:
            _str_standbydb = self.__standbydb
        else:
            _str_standbydb = ""

        if self.__create_nid_starterdb:
            _str_create_nid_starterdb = "-nid_starterdb"
        else:
            _str_create_nid_starterdb = ""

        # Add DdeleteDbHome Parameter on Delete DB
        if self.mCheckDBConfigOption(self.__options, "deleteDbHome", "yes"):
            _str_delete_db_home = "-deletedbhome"
        else:
            _str_delete_db_home = ""

        if self.__dbaas_api_payload:
            _str_dbaas_api_payload = '/tmp/dbcs.{0}{1}'.format(str(aWhen), str(self.__uuid))
        else:
            _str_dbaas_api_payload = ""

        _rc  = 0
        _cmd = ''
        #
        # Copy dbname.cfg file to all DomUs during certain steps
        #
        if _str_dbNameCfg_remoteloc is not None and aWhen in ['post.db_install', 'dg_repeat_setup']:
            try:
                #Only copy to domUs specified for SI Elastic DB
                if _elastic_op is True:
                    self.mCopyFileToDomus(self.__dbname_cfg, _str_dbNameCfg_remoteloc,aMode='600',aDomUList=_dom0U,aUser="opc")
                else:
                    _pair = None
                    for _dom0, _domU in self.mReturnDom0DomUPair():
                        _pair = [[_dom0, _domU]]
                        break

                    self.mCopyFileToDomus(self.__dbname_cfg, _str_dbNameCfg_remoteloc,aMode='600',aDomUList=_pair,aUser="opc")
                for _, _domU in self.mReturnDom0DomUPair():
                    _node = exaBoxNode(get_gcontext())
                    _node.mConnect(aHost=_domU)
                    _node.mExecuteCmd("chown root:root {0}".format(_str_dbNameCfg_remoteloc))
                    _node.mDisconnect()
            except:
                ebLogError('::mCopyFileToDomus failed VM not up and running ?')
            finally:
                # Since config is now in domus cleanup temp from local
                self.mExecuteLocal('/bin/rm -f '+self.__dbname_cfg)

        # for dbpatch it requires
        _str_dbpatch = self.__dbpatch

        _parallelArgs = {}
        _sl = ebScriptsEngineFetch(aType,aWhen)
        # Process each Script in ScriptSets
        for _s in _sl:
            for _x in _s:
                ebLogInfo('Processing ScriptSet: '+_x.mGetName())
                # Process each Action in a Script
                _al = _x.mFetchActions()
                for _a in sorted(_al.keys()):
                    _prio = _al[_a].mGetPrio()
                    _where = _al[_a].mGetWhere()
                    _cmd = _al[_a].mGetCmd()
                    _script = _al[_a].mGetScript()
                    _mode = _al[_a].mGetMode()
                    _singleNode = _al[_a].mGetSingleNode()
                    _cmd = _cmd.replace('%DOM0_LIST%', _str_dom0S)
                    _cmd = _cmd.replace('%DOMU_LIST%', _str_domUS)
                    _cmd = _cmd.replace('%DB_NAME%', str(_str_dbName))
                    _cmd = _cmd.replace('%DB_NAME_CFG%', str(_str_dbNameCfg_remoteloc))
                    _cmd = _cmd.replace('%ENCODED_JSON%', str(_str_dbpatch))
                    _cmd = _cmd.replace('%STANDBYDB%', str(_str_standbydb))
                    _cmd = _cmd.replace('%NID_STARTERDB%', str(_str_create_nid_starterdb))
                    _cmd = _cmd.replace('%DBAAS_CFG%', str(_str_dbaas_api_payload))
                    _cmd = _cmd.replace('%DELETE_DB_HOME%', str(_str_delete_db_home))

                    if _script:
                        _idx = _x.mGetScriptPath().rfind('/')
                        _fps = _x.mGetScriptPath()[:_idx]+'/'+_script
                    if _where == 'domu':
                        _dom0List = []
                        for _ent in _dom0U:
                            if not _ent[1] in _dom0List:

                                if self.__cmd == "vmgi_reshape" and aWhen != "post.gi_rpm":
                                    _gridhome, _, _ora_base = self.mGetOracleBaseDirectories(aDomU=_ent[1])
                                    _dir = _ora_base.split('/', 2)[1]
                                    if _dir == "u02":
                                        _cmd = _cmd.replace('u01', 'u02')

                                _dom0List.append(_ent[1])

                                if _ent[1] not in _parallelArgs:
                                    _parallelArgs[_ent[1]] = []

                                _args = {}
                                _args["host"] = _ent[1]
                                _args["cmd"] = _cmd

                                if _script:
                                    _args["script"] = _script
                                    _args["fps"] = _fps

                                _parallelArgs[_ent[1]].append(_args)
                                _rc = 0

                            if _singleNode:
                                break
                    elif _where == 'dom0':
                        _dom0List = []
                        for _ent in _dom0U:
                            if not _ent[0] in _dom0List:
                                _dom0List.append(_ent[0])

                                if _ent[0] not in _parallelArgs:
                                    _parallelArgs[_ent[0]] = []

                                _args = {}
                                _args["host"] = _ent[0]
                                _args["cmd"] = _cmd

                                if _script:
                                    _args["script"] = _script
                                    _args["fps"] = _fps

                                _parallelArgs[_ent[0]].append(_args)
                                _rc = 0

                            if _singleNode:
                                break
                    if _rc and aStatusAbort:
                        break
                if _rc and aStatusAbort:
                    break
            if _rc and aStatusAbort:
                break

        # Real execute of commands
        def mExecuteStep(aHostname, aArgList, aStatusAbort, aMode, aRcStatus):

            with connect_to_host(aHostname, get_gcontext()) as _node:

                _rc = 0

                for _args in aArgList:

                    if _rc == 0:

                        ebLogInfo(f'Executing on Host({aHostname}): {_args["cmd"]}')

                        if "script" in _args:
                            _node.mCopyFile(_args["fps"], _args["script"])

                        _cmd = _args["cmd"]
                        _node.mExecuteCmdLog(_cmd)
                        _rc = _node.mGetCmdExitStatus()

                        if not (_rc and aStatusAbort and (aMode is None or aMode != 'ignore_errors')):
                            _rc = 0

                aRcStatus[aHostname] = _rc


        if aParallel:

            # Parallel execute
            _plist = ProcessManager()
            _rc_status = _plist.mGetManager().dict()

            for _host, _args in _parallelArgs.items():

                _p = ProcessStructure(mExecuteStep, [_host, _args, aStatusAbort, _mode, _rc_status], _host)
                _p.mSetMaxExecutionTime(30*60) # 30 minutes
                _p.mSetJoinTimeout(5)
                _p.mSetLogTimeoutFx(ebLogWarn)
                _plist.mStartAppend(_p)

            _plist.mJoinProcess()

        else:

            _rc_status = {}
            for _host, _args in _parallelArgs.items():
                mExecuteStep(_host, _args, aStatusAbort, _mode, _rc_status)


        _rc = 0
        for _host, _rcs in _rc_status.items():
            if _rcs != 0:
                _rc = _rcs

        #
        # Remove OCDE configuration file /tmp (e.g. for password) during post.db_install
        #
        # TODO: to test/validate
        #
        if _str_dbNameCfg_remoteloc and aWhen in ['post.db_install']:
            #Local copy may have already been clear.
            self.mExecuteLocal('/bin/rm -f '+self.__dbname_cfg)
            self.mExecuteCmd('/bin/rm -f '+_str_dbNameCfg_remoteloc)

            #Remote clear
            for _dom0, _domu in self.mReturnDom0DomUPair():
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_domu)
                _node.mExecuteCmd('rm -f {0}'.format(_str_dbNameCfg_remoteloc))
                _node.mDisconnect()

            ebLogInfo('*** post.db_install VM cleanup...')

        return _rc, _cmd

    def mCheckElasticSupportStarterDB(self,aNodeList):
        _local_db_pair = self.mCheckNodeList(aNodeList)
        _domU=_local_db_pair[0][1]
        _node = exaBoxNode(get_gcontext())
        _node.mConnect(aHost=_domU)
        _cmd_str = 'touch /var/opt/oracle/ocde/.firstdb'
        # first node on the list for ocde
        _node.mExecuteCmdLog(_cmd_str)
        _node.mDisconnect()

    def mCheckInstallDB(self,aDom0U):
        ebLogVerbose("mCheckInstallDB: Checking if Database is installed.")

        for _dom0,_domU in aDom0U:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domU)
            _cmd = "cat /etc/oratab | grep %s | awk -F : '{print $2}' "%(self.__dbname)
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            _out = _o.readlines()
            if not _out or len(_out) == 0:
                return False
            _oh = _out[0].strip()
            _cmd_oh = "export ORACLE_HOME=%s; %s/bin/srvctl status database -d %s | grep %s "%(_oh,_oh,self.__dbname,_domU.split('.')[0])
            _i, _o, _e = _node.mExecuteCmd("sh -c \"" + _cmd_oh + "\"")
            _out = _o.readlines()
            if not _out or len(_out) == 0:
                return False
        return True

    def mCheckDBIsUp(self, aNode, aExascale=False):
        """
        Checks the status of ora.database.type resources (if TARGET is online STATE must match)
        """

        _ret = False

        # expecting the following output
        # NAME=ora.suol7d18.db
        # TYPE=ora.database.type
        # TARGET=ONLINE
        # STATE=ONLINE
        # or below output for Data Guard case
        # TARGET=ONLINE
        # STATE=INTERMEDIATE
        # ROLE=PHYSICAL_STANDBY
        # usr_ora_open_mode=mount

        _target = None
        _state = None
        _name = None
        _role = None
        _open_mode = None

        _node = aNode
        _disconnect = False

        if isinstance(_node, str):
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=aNode)
            _disconnect = True

        _db_List = self.mGetActiveDbInstances(_node.mGetHostname())
        if len(_db_List) == 0:
          ebLogWarn(f'*** There are NO active database instances detected on [{_node}]".')
          if aExascale:
              return True #for the exascale mValidateGuest call
          return False

        _domU = _node.mGetHostname()
        _path, _, _ = self.mGetOracleBaseDirectories(_domU)
        if not _path:
            _path = self.mGetClusters().mGetCluster().mGetCluHome()

        # Check first if any DB are created
        _cmd = "{}/bin/crsctl stat res -w 'TYPE = ora.database.type'".format(_path)

        _in, _out, _err = _node.mExecuteCmd(_cmd)
        _rc = _node.mGetCmdExitStatus()
        if _rc:
            ebLogError("*** Non zero return-code ({}) for command:{}"
                       "(stdout:{}, stderr:{}), please check CRS status on {}"
                       .format(_rc,_cmd, _out and _out.read(),
                               _err and _err.read(), aNode))
            if _disconnect:
                _node.mDisconnect()
            return False
        _output = _out.readlines()
        if not _output:
            ebLogInfo("*** mCheckDBIsUp: No DB created yet on: {}, returning True".format(aNode))
            if _disconnect:
                _node.mDisconnect()
            return True

        _cmd = ("{0}/bin/crsctl stat res -attr TYPE,TARGET,STATE,ROLE,USR_ORA_OPEN_MODE -p -n `hostname -s` | grep -A4 -B1 ora.database.type".format(_path))

        _in, _out, _err = _node.mExecuteCmd(_cmd)
        _output = _out.readlines()


        if _output:
            for _line in _output:
                if _line.strip().lower().startswith('name'):
                    _name = _line.strip().split('=')[1]
                elif _line.strip().lower().startswith('target'):
                    _target = _line.strip().split('=')[1]
                elif _line.strip().lower().startswith('state'):
                    _state = _line.strip().split('=')[1]
                elif _line.strip().lower().startswith('role'):
                    _role = _line.strip().split('=')[1]
                elif _line.strip().lower().startswith('usr_ora_open_mode'):
                    _open_mode = _line.strip().split('=')[1]

                    if _target == 'ONLINE' and _role == 'PHYSICAL_STANDBY' and _state == 'INTERMEDIATE' and _open_mode == 'mount':
                        ebLogVerbose("*** DB :%s target is:%s, state is:%s, role is:%s, mode is:%s"%(_name,_target,_state,_role,_open_mode))
                        _ret = True
                    elif _target == 'ONLINE' and _state != 'ONLINE':
                        ebLogError("*** DB :%s not up as target is :%s  and state is:%s"%(_name,_target,_state))
                        if _disconnect:
                            _node.mDisconnect()
                        return False 
                    else:
                        _ret = True
        if _disconnect:
            _node.mDisconnect()
        return _ret

    #verify if nodelist param is present for SI elastic DB
    def mCheckNodeListParam(self):
        _aOptions = self.__options
        if _aOptions.jsonconf is not None and 'dbParams' in list(_aOptions.jsonconf.keys()):
            _dbParams = _aOptions.jsonconf['dbParams']
            if 'nodelist' in _dbParams and len(_dbParams['nodelist'].strip()) != 0:
                _nodelist = _dbParams['nodelist'].strip()
                return True,_nodelist
            else:
                return False,''
        else:
            return False,''


    #verify a single node with available doms
    def mCheckSingleNode(self,aSingleNode):

        if len(aSingleNode.strip()) == 0:
            return False,None
        for _dompair in self.mReturnDom0DomUPair():
            if _dompair[1].split('.')[0] == aSingleNode:
                return True,_dompair
        return False,None

    #verify a list of nodes for SI elastic DB
    def mCheckNodeList(self, nodelist):
        _localDom0DomUPair = []
        for _singlenode in nodelist.split(','):
            _result,_dompair = self.mCheckSingleNode(_singlenode)
            if _result is True:
                if _dompair not in _localDom0DomUPair:
                    _localDom0DomUPair.append(_dompair)
            else:
                _error_str = 'Value [%s] in "nodelist" param is invalid'% _singlenode
                raise ExacloudRuntimeError(0x0512, 0x0A, _error_str)
        return _localDom0DomUPair

    def mRunCmdScript(self, aCmd):
        '''Run a single script when it have all the logic of a given step

        Args:
            cmd (str): The 'when' identifier of the script
        '''
        _step_time = time.time()
        emsg = '*** Error ({0}) catched during scripts execution for cmd: {1}'
        abort_runscript = partial(self.mRunScript, aType='*', aStatusAbort=True)
        _rc, _cmd = abort_runscript(aWhen=aCmd)
        if _rc:
            ebLogError(emsg.format(_rc, _cmd))
        self.mUpdateStatus(aCmd)
        tmsg = 'Execution of {0} script'.format(_cmd)
        self.mLogStepElapsedTime(_step_time, tmsg)
        return _rc, _cmd

    def mCheckDBNameOnCells(self):

        ebLogVerbose("mCheckDBNameOnCells: Checking Database name on Cells.")

        _, _, _cells, _ = self.mReturnAllClusterHosts()
        for _cell in _cells:
            _node = exaBoxNode(get_gcontext(), Cluctrl = self)
            _node.mConnect(aHost=_cell)
            _cmdstr = "cellcli -e LIST DATABASE;"
            _i,_o,_e = _node.mExecuteCmdCellcli(_cmdstr)
            _databases = _o.read().split()
            if self.__dbname.upper() in _databases:
                _node.mDisconnect()
                ebLogError('*** DATABASE %s already defined on the cell side.' % (self.__dbname))
                raise ExacloudRuntimeError(0x0112, 0xA, 'DATABASE %s already defined on the cell side.' % (self.__dbname), Cluctrl = self)
            _node.mDisconnect()
            break

    def mHandlerUnlockAll(self):
        aOptions = self.mGetArgsOptions()
        return self.mSetupLockdown(False, aOptions)

    def mHandlerSetupLockdown(self):
        aOptions = self.mGetArgsOptions()
        return self.mSetupLockdown(True, aOptions)

    def mSetupLockdown(self,aMode=True,aOptions=None):

        ebLogVerbose("mSetupLockdown: aMode = %s" % aMode)

        if not self.mIsKVM():
            _dom0s, _domUs, _cells, _switches = self.mReturnAllClusterHosts()
        else:
            if self.mIsXS():
                _cells = []
                _dom0s, _domUs, _, _ = self.mReturnAllClusterHosts()
            else:
                _dom0s, _domUs, _cells, _ = self.mReturnAllClusterHosts()
            _switches = []

        ebLogDebug('*** {0}, {1}, {2}, {3}'.format(str(_dom0s),str(_domUs),str(_cells),str(_switches)))

        _cluhosts = []

        _lockdown_dom0   = True
        _lockdown_cell   = True
        _lockdown_switch = True
        _lockdown_domu   = False

        _excluded_hosts = [] 
        if aOptions and aOptions.jsonconf:
            jconf = aOptions.jsonconf
            # Get execuled hosts list
            _excluded_hosts = jconf.get('INGESTION_HW_FAILURE', [])

            _lockdown_dom0   = False
            _lockdown_cell   = False
            _lockdown_switch = False
            _lockdown_domu   = False

            if "lockdown_dom0" in jconf.keys() and str(jconf["lockdown_dom0"]).lower() == "true":
                _lockdown_dom0 = True

            if "lockdown_switch" in jconf.keys() and str(jconf["lockdown_switch"]).lower() == "true":
                _lockdown_switch = True

            if "lockdown_cell" in jconf.keys() and str(jconf["lockdown_cell"]).lower() == "true":
                _lockdown_cell = True

            if self.mIsXS():
                _lockdown_cell = False

            if "lockdown_domu" in jconf.keys() and str(jconf["lockdown_domu"]).lower() == "true":
                _lockdown_domu = True

            if "lockdown_by_type" in jconf.keys():

                _typeLock = jconf["lockdown_by_type"].lower()
                if _typeLock == "dom0":
                    _cluhosts = _dom0s
                elif _typeLock == "cell":
                    _cluhosts = _cells
                elif _typeLock == "switch":
                    _cluhosts = _switches
                elif _typeLock == "domu":
                    _cluhosts = _domUs

            if "lockdown_nodes" in jconf.keys():
                _nodes = jconf["lockdown_nodes"]
                if isinstance(_nodes, list):
                    _cluhosts = list(set(map(lambda x: str(x), _nodes)))
                else:
                    _cluhosts = [str(_nodes)]

            if _cluhosts:
                ebLogInfo("Change nodes by: {0}".format(_cluhosts))

        # Add configuration
        if not _cluhosts:

            if _lockdown_dom0:
                _cluhosts += _dom0s
            if _lockdown_cell:
                _cluhosts += _cells
            if _lockdown_switch:
                    if not self.mIsKVM():
                        _cluhosts += _switches
            if _lockdown_domu:
                _cluhosts += _domUs

            _cluhosts = list(set(map(lambda x: str(x), _cluhosts)))

        # Execute the operation
        for _host in _cluhosts:
            if _host in _excluded_hosts:
                continue

            if (_host in _switches or _host in _domUs) and not ( aOptions is not None and aOptions.force is True):
                ebLogWarn('*** Switch/DOMU (%s) lock/unlock is skipped by default (use --force to update sshd configuration)' % (_host))
                continue

            if not self.mPingHost(_host):
                ebLogInfo('*** Host: %s is not respoding skipping SSH connect' % (_host))
                continue

            _node = exaBoxNode(get_gcontext())
            #
            # Enforce SSH key to avoid use of password login
            #
            _cparam = {"FQDN": _host, "user": "root"}
            _entry = get_gcontext().mGetExaKms().mGetExaKmsEntry(_cparam)

            if not _entry:
                ebLogWarn('*** Using password authentication for lock/unlock operations is not recommended.')
                ebLogWarn('*** Please setup SSH Keys files for this cluster')
                raise ExacloudRuntimeError(0x0107, 0xA,'Lock/Unlock aborted due to missing SSH key to access: %s' % (_host))

            ebLogInfo('*** Connecting to: %s' % (_host))
            try:
                _node.mConnect(aHost=_host)
            except:
                ebLogInfo('*** Unable to connect to host: %s (skipping lock/unlock node)' % (_host))
                continue

            if self.__debug:
                _node.mExecuteCmdLog('grep "^PasswordAuthentication" /etc/ssh/sshd_config')
                _node.mExecuteCmdLog('grep "^PermitRootLogin" /etc/ssh/sshd_config')
                _node.mExecuteCmdLog('grep "EXACLOUD" .ssh/authorized_keys')
                _node.mExecuteCmdLog('grep "OEDA" .ssh/authorized_keys')

            #
            # To avoid access lock down due to other tooling - should be remove eventually
            #
            _cmd3_str_on = "/bin/sed -i 's/^auth\\s*required\\s*pam_tally2.so/#&/g' /etc/pam.d/login"
            _cmd4_str_on = "/bin/sed -i 's/^auth\\s*required\\s*pam_tally2.so/#&/g' /etc/pam.d/sshd"
            #
            # Restart SSHD service
            #
            _cmd_restart_sshd = "service sshd restart"
            if aMode:
                _sshd_props = {
                    'PasswordAuthentication': 'no',
                    'PermitRootLogin': 'without-password'
                }
                ebLogInfo('*** (disable password authentication) && (Allow root login w/o passwd)')
                node_update_key_val_file(_node, '/etc/ssh/sshd_config',
                                         _sshd_props, sep=' ')
                node_exec_cmd_check(_node, _cmd_restart_sshd)
            else:
                ebLogInfo('*** (enable password authentication) && (Remove root login w/o passwd)')
                _sshd_props = {
                    'PasswordAuthentication': 'yes',
                    'PermitRootLogin': 'yes'
                }
                node_update_key_val_file(_node, '/etc/ssh/sshd_config',
                                         _sshd_props, sep=' ')
                if _node.mFileExists('/etc/pam.d/login'):
                    node_exec_cmd_check(_node, _cmd3_str_on)
                if _node.mFileExists('/etc/pam.d/sshd'):
                    node_exec_cmd_check(_node, _cmd4_str_on)
                node_exec_cmd_check(_node, _cmd_restart_sshd)
                #
                # Reset to default password (only temporary)
                #
                if aOptions is not None and aOptions.resetpwd:

                    _coptions_for_pwd = get_gcontext().mGetConfigOptions()
                    _default_pwd = b64decode(_coptions_for_pwd["default_pwd"]).decode('utf8')

                    ebLogInfo('*** Reset root password')
                    _cmdstr = """echo 'root:%s' | chpasswd >& /dev/null""" % (_default_pwd)
                    _node.mExecuteCmdLog("sh -c \"" + _cmdstr + "\"")

            _node.mDisconnect()

        if self.__ociexacc:
            ebLogInfo('*** Lock accounts on cells')
            self.mLockCellUsers(aMode=aMode)


    def mFetchEgressIpsFromPayload(self, aJsonConf):

        _jconf = aJsonConf
        _egressIps = []

        # Fetch egress ips from payload
        # Reference bug: 36873685
        # The payload will include '[servers][whitelist_cidr]'
        # in all VM Create/Remove operations for us to consume
        if "ecra" in _jconf and "whitelist_cidr" in _jconf["ecra"]:

            for _server in _jconf["ecra"]["whitelist_cidr"]:

                _isIp = True
                try:
                    IPv4Network(_server)
                except:
                    _isIp = False

                if _isIp:
                    _egressIps.append(_server)
                    continue

                _cmd = f"/usr/bin/nslookup {_server}"
                _rc, _, _o, _e = self.mExecuteLocal(_cmd)

                if _rc == 0:
                    _hostname = re.search("Address:\s+(([0-9]{1,3}\.){3}[0-9]{1,3})\s", _o)

                    if _hostname:
                        _egressIps.append(_hostname.group(1))
                    else:
                        ebLogInfo(f"Could not resolve {_server}, {_o}, {_e}")
                else:
                    ebLogInfo(f"Could not resolve {_server}, {_o}, {_e}")

        return _egressIps

    def mAddNatEgressIPs(self, aOptions):

        ebLogInfo("Running mAddNatEgressIPs")
        _ips = self.mFetchEgressIpsFromPayload(aOptions.jsonconf)

        if not _ips:
            ebLogInfo("No nategressips provided in ECRA payload [ecra>servers>ip]")
            return

        for _dom0, _domU in self.mReturnDom0DomUPair():

            with connect_to_host(_dom0, get_gcontext()) as _node:

                _cmd = f'/usr/bin/virsh dumpxml {_domU}'
                _cmd = f'{_cmd} | /bin/grep bridge | /bin/grep -v bond'
                _cmd = f'{_cmd} | /bin/grep source | /bin/grep -oP "vmeth[0-9]+"'

                _, _out, _ = node_exec_cmd(_node, _cmd)
                _interface = _out.strip()

                if not _interface:
                    ebLogInfo(f"Missing vmeth interface in dom0-domU {_dom0} - {_domU}")
                    continue

                _routeFile = f"/etc/sysconfig/network-scripts/route-{_interface}"

                if not _node.mFileExists(_routeFile):
                    ebLogInfo(f"Missing route file in dom0-domU {_routeFile}")
                    continue

                for _ip in _ips:

                    ebLogInfo(f"Configuring the IP: {_ip} in dom0-domU: {_dom0} - {_domU}")

                    _cmd = f"/bin/cat {_routeFile} | grep '{_ip}'"
                    _, _, _ = node_exec_cmd(_node, _cmd)

                    if _node.mGetCmdExitStatus() == 0:
                        ebLogInfo(f"IP : '{_ip}' is already configured in route file : {_routeFile}")
                        continue

                    else:

                        _cmd = f"/bin/cat {_routeFile}"
                        _, _o, _ = _node.mExecuteCmd(_cmd)
                        _newRule = _o.readlines()[0]

                        _split = _newRule.split(" ")
                        _split[0] = _ip
                        _newRule = " ".join(_split).strip()

                        _cmd = f"/bin/echo '{_newRule}' >> {_routeFile}"
                        node_exec_cmd(_node, _cmd)

                        ebLogInfo(f"New IP configured in route file '{_ip}' {_routeFile}")

                ebLogInfo(f"Performing restart of interface {_interface}")

                node_exec_cmd(_node, f"/usr/sbin/ifdown {_interface}")
                node_exec_cmd(_node, f"/usr/sbin/ifup {_interface}")

                ebLogInfo(f"Configured all the servers ips in dom0: {_dom0}")

    def mCustomerNetworkXMLUpdate(self,aOptions,aJConf=None):

        ebLogInfo('*** mCustomerNetworkXMLUpdate: CustomerNetwork XML Update ...')
        if aJConf is not None:
            _jconf = aJConf
        else:
            _jconf = aOptions.jsonconf

        _nodesubset_conf = ''
        if self.__exabm:
            _customer_conf = _jconf['customer_network']
        elif self.__ociexacc:
            # Fallback to be removed once payload key is clear
            if 'network' in list(_jconf.keys()):
                _customer_conf = _jconf['network']
            else:
                _customer_conf = _jconf['customer_network']

            if "node_subset" in list(_jconf.keys()):
                _nodesubset_conf = _jconf['node_subset']
        else:
            raise ExacloudRuntimeError(0x0740, 0xA,'Network JSON Update requires either exabm or ociexacc flag in configuration')

        try:
            _timeZone = _customer_conf['timezone']
            for _, _domU in self.mReturnDom0DomUPair():
                _domU_mac = self.__machines.mGetMachineConfig(_domU)
                _domU_mac.mSetMacTimeZone(_timeZone)
                self.__timeZone = _timeZone
            ebLogInfo('*** Customer Network TimeZone set to: %s' % (_timeZone))
        except:
            pass

        _nw_utils = NetworkUtils()
        _nodes_list = _customer_conf['nodes']
        _nb_nodes = len(_nodes_list)
        _dr_net = {}
        _dr_vip_net = {}
        _client_net_list  = []
        _backup_net_list  = []
        _dr_net_list      = []
        _vip_net_list     = []
        _dr_vip_net_list  = []
        _dom0_net_dict    = {}
        ebLogInfo('*** Cutomer Network Configuration for %d nodes detected' % (_nb_nodes))
        # TO BE REMOVED, OCICCv1 may not pass dom0_oracle_name and rely on alpha order
        _tmp_ociexaccv1_activated = False
        _tmp_ociexaccv1_dom0domU = list(sorted(self.mReturnDom0DomUNATPair(),\
                                        key = operator.itemgetter(0)))
        for _node in _nodes_list:
            if 'client' not in list(_node.keys()) or 'backup' not in list(_node.keys()):
                raise ExacloudRuntimeError(0x0740, 0xA,'Client or Backup configuration not found in Customer Network JSON')
            _client_net = _node['client']
            _backup_net = _node['backup']
            _admin_net = _node.get('admin') or {}
            _vip_net    = _node['vip']
            if 'dr' in list(_node.keys()):
                _dr_net = _node['dr']
                _dr_net_list.append(_dr_net)
                self.mSetDRNetPresent(True)
            else:
                # Set it to empty otherwise in case of wrong payload, this gets info of the previous node.
                _dr_net = {}
            if 'drVip' in list(_node.keys()):
                _dr_vip_net = _node['drVip']
                _dr_vip_net_list.append(_dr_vip_net)
            else:
                # Set it to empty otherwise in case of wrong payload, this gets info of the previous node.
                _dr_vip_net = {}
            _client_net_list.append(_client_net)
            _backup_net_list.append(_backup_net)
            _vip_net_list.append(_vip_net)
            if 'dom0' not in list(_node.keys()):
                if 'dom0_oracle_name' in list(_client_net.keys()):
                    _dom0_net_key = _client_net['dom0_oracle_name']
                #to be removed in v2
                elif self.__ociexacc:
                    if _nodesubset_conf:
                        _participating_nodes = _nodesubset_conf['participating_computes']
                        _node_alias = _client_net['compute_node_alias']
                        _dom0_net_key = ""
                        for _item in _participating_nodes:
                            if _node_alias == _item['compute_node_alias']:
                                _dom0_net_key = _item['compute_node_hostname']
                                break
                        if not _dom0_net_key:
                            continue
                    else:
                        _tmp_ociexaccv1_activated = True
                        _dom0_net_key = _tmp_ociexaccv1_dom0domU[0][0] #dom0
                    ebLogInfo('*** TEMPORARY WORKAROUND - OCIEXACC V1 - matched client net to dom0: {}'.format(_dom0_net_key))
                else:
                    raise ExacloudRuntimeError(0x0740, 0xA,'DOM0 Key configuration not found in Network JSON (w/ CLIENT/BACKUP IN CONFIG)')
            else:
                _dom0_net_key = _node['dom0']
            if len(_dom0_net_key.split('.')) == 1:  # Append DOM0 domainname

                for _dom0 , _ in self.mReturnDom0DomUPair():
                    _dot_pos = _dom0.find('.')
                    _dom0_domain = _dom0[_dot_pos:]
                    if _dom0_net_key == _dom0[:_dot_pos]:
                        break

                _dom0_net_key = _dom0_net_key + _dom0_domain
                ebLogDebug('*** _dom0_net_key domainame added for: %s' % (_dom0_net_key))
            if 'domu_oracle_name' in list(_client_net.keys()):
                _domu_net_key = _client_net['domu_oracle_name']
            elif self.__ociexacc:
                #Get NatHostname of DomU matching dom0
                if _nodesubset_conf:
                    _domu_net_key = ""
                    for _item in _tmp_ociexaccv1_dom0domU:
                        if _dom0_net_key == _item[0]:
                            _domu_net_key = _item[1]
                            break
                    if not _domu_net_key:
                            continue
                else:
                    _tmp_ociexaccv1_activated = True
                    _domu_net_key = _tmp_ociexaccv1_dom0domU[0][1]

                ebLogInfo('*** TEMPORARY WORKAROUND - OCIEXACC V1 - matched client net to domU NAT: {}'.format(_domu_net_key))
            else:
                raise ExacloudRuntimeError(0x0744, 0xA,'DOMU Key configuration not found in Network JSON')

            # Pop processed dom0/domU at the end
            if _tmp_ociexaccv1_activated:
                _tmp_ociexaccv1_dom0domU.pop(0)

            _dom0_net_dict[_dom0_net_key] = [_client_net, _backup_net, _admin_net, _dr_net, _vip_net, _dr_vip_net, _domu_net_key]

        #
        # Build DomU list assuming one vip per DomU in cluster - _nb_domu should always match _nb_nodes
        #
        _cluster   = self.__clusters.mGetCluster()          # software/clustes/cluster
        _clu_vips  = _cluster.mGetCluVips()
        _clu_scans = _cluster.mGetCluScans()
        if (len(list(_clu_vips.keys())) != _nb_nodes):
            ebLogWarn("ClusterVips in XML not matching with nodes in JSON Payload")

        #
        # DOM0 driven XML Patching / On BM the DomU->Dom0 mapping is important for Cavium/MAC configuration/dependency
        #
        for _dom0_key in list(_dom0_net_dict.keys()):
            _net_master_client = None
            _net_slaves_client = None
            _net_master_backup = None
            _net_slaves_backup = None
            _client_net, _backup_net, _admin_net, _dr_net, _vip_net, _dr_vip_net, _domu_key = _dom0_net_dict[_dom0_key]

            _dom0_conf = self.__machines.mGetMachineConfig(_dom0_key)
            #Allow XMLs with more than one VM in machines to be used
            _domu_ids  = _dom0_conf.mGetMacMachines()
            _domu_id   = None
            # Locate good VM with Nat hostname
            for _domu in _domu_ids:
                _domu_nets = self.__machines.mGetMachineConfig(_domu).mGetMacNetworks()
                _host = None
                for _net_id in _domu_nets:
                    _net_conf = self.__networks.mGetNetworkConfig(_net_id)
                    _domu_nat = _net_conf.mGetNetNatHostName(aFallBack=True)
                    if _domu_nat == _domu_key:
                        _host = _domu_nat
                        break
                if _host:
                    _domu_id = _domu
                    break
            if not _domu_id:
                raise ExacloudRuntimeError(0x0740, 0xA,'domu_oracle_name in payload must match a domU NAT hostname')

            _domu_conf = self.__machines.mGetMachineConfig(_domu_id)

            ebLogDebug('@@@ Processing Dom0: %s :: DomU: %s' % (_dom0_key, _domu_id))
            #
            # Fetch corresponding VIP entry (always assume there is one for each DomU
            #
            _clu_vip_conf = None
            for _clu_vip in list(_clu_vips.values()):
                if _clu_vip.mGetCVIPMachines()[0] == _domu_id:
                    _clu_vip_conf = _clu_vip
                    break
            assert(_clu_vip_conf is not None)

            ebLogDebug('@@@ Found VIP Entry: %s for DomU: %s' % (_clu_vip_conf.mGetCVIPName(), _domu_id))
            #
            # Patch VIP Configuration
            #
            _clu_vip_conf.mSetCVIPName(_vip_net['hostname'])
            _clu_vip_conf.mSetCVIPDomainName(_vip_net['domainname'])
            _vip_single_stack, _vip_ipv6 = _nw_utils.mGetIPv4IPv6Payload(_vip_net)
            # IPv6 VIP should be added using oedacli and not by direct patching of xml
            # as per discussion with oeda/oeda spec.
            if _vip_single_stack:
                _clu_vip_conf.mSetCVIPAddr(_vip_single_stack)
            if _nw_utils.mIsIPv6(_vip_single_stack):
                self.mSetIPv6SingleStackPresent(True)

            ebLogDebug('@@@ DomU VIP updated: %s %s %s' % (_domu_conf.mGetMacHostName(), _clu_vip_conf.mGetCVIPName(), _clu_vip_conf.mGetCVIPAddr()))
            #
            # Fetch Client/Backup network config for each DomU
            #
            _domu_hostname = _domu_conf.mGetMacHostName()
            _domu_networks = _domu_conf.mGetMacNetworks()
            # List of network support for BM gen1 - any network in this list will trigger an error
            _valid_netlist = [ \
                'backup', 'client', 'private' \
            ]  # privX is IB Storage / clusterprivX is IB cluster
            # With Worflow, ECRA can sends a Patched XML with a network payload add support that it will contain an admin net
            if self.__ociexacc and self.isATP():
                _valid_netlist.append('admin')

            if self.__ociexacc and self.mIsDRNetPresent():
                _valid_netlist.append('other')

            _domu_net_client_single_stack = None
            _domu_net_backup_single_stack = None
            _domu_net_client_v6 = None
            _domu_net_backup_v6 = None
            _client_net_type = _nw_utils.mClassifyStack(_client_net)
            _backup_net_type = _nw_utils.mClassifyStack(_backup_net)
            for _net_id in _domu_networks:
                _net_conf = self.__networks.mGetNetworkConfig(_net_id)
                if _net_conf.mGetNetType() not in _valid_netlist:
                    raise ExacloudRuntimeError(0x0741, 0xA, 'Unsupported Network in DOMU %s / %s' % (_domu_hostname, _net_id))
                #
                # NET/Update only required for client/backup
                #
                if _net_conf.mGetNetType() == 'backup' and _backup_net_type == 'single':
                    _domu_net_backup_single_stack = _net_conf
                elif _net_conf.mGetNetType() == 'client' and _client_net_type == 'single':
                    _domu_net_client_single_stack = _net_conf
                elif _net_conf.mGetNetType() == 'backup' and _backup_net_type == 'dual':
                    if _net_conf.mGetNetIpAddr() and not _nw_utils.mIsIPv6(_net_conf.mGetNetIpAddr()):
                        _domu_net_backup_single_stack = _net_conf
                    elif _net_conf.mGetNetIpAddr() and _nw_utils.mIsIPv6(_net_conf.mGetNetIpAddr()):
                        _domu_net_backup_v6 = _net_conf
                elif _net_conf.mGetNetType() == 'client' and _client_net_type == 'dual':
                    if _net_conf.mGetNetIpAddr() and not _nw_utils.mIsIPv6(_net_conf.mGetNetIpAddr()):
                        _domu_net_client_single_stack = _net_conf
                    elif _net_conf.mGetNetIpAddr() and _nw_utils.mIsIPv6(_net_conf.mGetNetIpAddr()):
                        _domu_net_client_v6 = _net_conf
            if _domu_net_backup_single_stack is None or _domu_net_client_single_stack is None:
                raise ExacloudRuntimeError(0x0741, 0xA, 'DOMU %s / Client or Backup network config not found' % (_domu_hostname))
            #
            # DUMP Current Client/Backup Net Config
            #
            if self.__debug:
                _domu_net_client_single_stack.mDumpConfig()
                _domu_net_backup_single_stack.mDumpConfig()
                if _domu_net_client_v6:
                    _domu_net_client_v6.mDumpConfig()
                if _domu_net_backup_v6:
                    _domu_net_backup_v6.mDumpConfig()
            #
            # Fetch Dom0 domainname / netmask (in case not available in JSON payload)
            #
            _dom0_nat_domainname = None
            _dom0_nat_mask = None
            _mac_conf_list = self.__machines.mGetMachineConfigList()
            for _mac_conf in list(_mac_conf_list.keys()):
                _mac_vml = _mac_conf_list[_mac_conf].mGetMacMachines()
                if len(_mac_vml) != 0:
                    _dom0_hostname = _mac_conf_list[_mac_conf].mGetMacHostName()
                    _dom0_network_list = _mac_conf_list[_mac_conf].mGetMacNetworks()
                    for _dom0_net_id in _dom0_network_list:
                        _dom0_tmp_net_conf = self.__networks.mGetNetworkConfig(_dom0_net_id)
                        if _dom0_tmp_net_conf.mGetNetType() != 'admin':
                            continue
                        if _dom0_key and _dom0_key != _dom0_hostname:
                            continue
                        _dom0_net_conf = self.__networks.mGetNetworkConfig(_dom0_net_id)
                        _dom0_nat_domainname =  _dom0_net_conf.mGetNetDomainName()
                        _dom0_nat_mask = _dom0_net_conf.mGetNetMask()
            if 'natdomainname' not in _client_net and 'domainname' not in _admin_net and _dom0_nat_domainname is None:
                raise ExacloudRuntimeError(0x0741, 0xA, 'DOMU %s / Client NAT DomaineName not found' % (_domu_hostname))
            if 'natmask' not in _client_net and 'netmask' not in _admin_net and _dom0_nat_mask is None:
                raise ExacloudRuntimeError(0x0741, 0xA, 'DOMU %s / Client NAT NetMask not found' % (_domu_hostname))
            if 'domainname' in _admin_net:
                _dom0_nat_domainname = _admin_net['domainname']
            elif 'natdomainname' in _client_net:
                _dom0_nat_domainname = _client_net['natdomainname']
            if 'netmask' in _admin_net:
                _dom0_nat_mask = _admin_net['netmask']
            elif 'natmask' in _client_net:
                _dom0_nat_mask = _client_net['natmask']
            #
            # CLIENT - Expanded Update/Patching
            #
            _client_json_fields = ['domainname', 'hostname']
            _client_either_or_fields = {'ip':'ipv6', 'netmask': 'v6netmask', 'gateway': 'v6gateway'}
            if not self.__ociexacc:
                _client_json_fields.append('mac')
            for _field in _client_json_fields:
                if _field not in list(_client_net.keys()):
                    raise ExacloudRuntimeError(0x0740, 0xA, 'Client JSON Payload incomplete missing field: %s' % (_field))
            for _key, _value in _client_either_or_fields.items():
                if _key not in list(_client_net.keys()) and _value not in list(_client_net.keys()):
                    raise ExacloudRuntimeError(0x0740, 0xA, f'Client JSON Payload incomplete missing field: {_key}/{_value}.')

            #
            # Handle case where nathostname/natip are not part of the payload (e.g. already in the XML)
            #
            if ('hostname' not in _admin_net or 'ip' not in _admin_net) and \
               ('nathostname' not in _client_net or 'natip' not in _client_net):
                _nathostname_xml = _domu_net_client_single_stack.mGetNetNatHostName(aFallBack=False)
                _natip_xml = _domu_net_client_single_stack.mGetNetNatAddr(aFallBack=False)
                if _nathostname_xml is None or _natip_xml is None:
                    raise ExacloudRuntimeError(0x0740, 0xA, 'Client NAT hostname/ip are missing')
            else:
                _nathostname_xml = None
                _natip_xml = None
            #
            # Update DomU Client Networks
            #
            _domu_net_client_single_stack.mSetNetHostName(_client_net['hostname'])
            _client_ip_single_stack, _client_ipv6 = _nw_utils.mGetIPv4IPv6Payload(_client_net)
            # IPv6 host IP should be added using oedacli and not by direct patching of xml
            # as per discussion with oeda/oeda spec.
            if _client_ip_single_stack:
                _domu_net_client_single_stack.mSetNetIpAddr(_client_ip_single_stack)
            if _client_ipv6 and _domu_net_client_v6:
                _domu_net_client_v6.mSetNetIpAddr(_client_ipv6)
            if _nw_utils.mIsIPv6(_client_ip_single_stack):
                self.mSetIPv6SingleStackPresent(True)

            _client_netmask_single_stack, _client_netmaskv6 = _nw_utils.mGetIPv4IPv6Payload(_client_net, key_single_stack='netmask', key_dual_stack='v6netmask')
            # IPv6 netmask should be added using oedacli and not by direct patching of xml
            # as per discussion with oeda/oeda spec. Also, v6netmask will be a prefix for IPv6 case.
            if _client_netmask_single_stack:
                _domu_net_client_single_stack.mSetNetMask(_client_netmask_single_stack)
            if _client_netmaskv6 and _domu_net_client_v6:
                _domu_net_client_v6.mSetNetMask(_client_netmaskv6)

            _client_gateway_single_stack, _client_gatewayv6 = _nw_utils.mGetIPv4IPv6Payload(_client_net, key_single_stack='gateway', key_dual_stack='v6gateway')
            # IPv6 gateway should be added using oedacli and not by direct patching of xml
            # as per discussion with oeda/oeda spec.
            if _client_gateway_single_stack:
                _domu_net_client_single_stack.mSetNetGateWay(_client_gateway_single_stack)
            if _client_gatewayv6 and _domu_net_client_v6:
                _domu_net_client_v6.mSetNetGateWay(_client_gatewayv6)

            _domu_net_client_single_stack.mSetNetDomainName(_client_net['domainname'])
            # Added data from oeda network discovered
            _network_discovered = self.mGetNetworkDiscovered()
            if _network_discovered and 'client_net' in _network_discovered:
                _net_master_client = _network_discovered['client_net']['bond_master']
                _net_slaves_client = _network_discovered['client_net']['bond_slaves']
                _domu_net_client_single_stack.mSetNetMaster(_network_discovered['client_net']['bond_master'])
                _domu_net_client_single_stack.mSetNetSlave(_network_discovered['client_net']['bond_slaves'])
            if not self.__ociexacc:
                _domu_net_client_single_stack.mSetMacAddr(_client_net['mac'].lower())         # Enforce lower case for vm.cfg compliance
            # NAT
            _nat_addr = None
            _nat_host = None
            if _nathostname_xml is None or _natip_xml is None:

                if "ip" in _admin_net and "hostname" in _admin_net or \
                   "natip" in _client_net and "nathostname" in _client_net:

                    _domu_net_client_single_stack.mSetNatHostName(_admin_net.get('hostname', _client_net['nathostname']))
                    _nat_host = _admin_net.get('hostname', _client_net['nathostname'])

                    _domain = ""
                    if "domainname" in _admin_net:
                        _domu_net_client_single_stack.mSetNatDomainName(_admin_net['domainname'])
                        _domain = _admin_net['domainname']
                    elif "natdomainname" in _client_net:
                        _domu_net_client_single_stack.mSetNatDomainName(_client_net['natdomainname'])
                        _domain = _client_net['natdomainname']

                    if _admin_net.get("ip", _client_net["natip"]) == "discover":

                        _nathost = _admin_net.get('hostname', _client_net['nathostname'])

                        if _domain:
                            _nathost = f"{_nathost}.{_domain}"

                        _cmd = f"/usr/bin/nslookup {_nathost}"
                        _rc, _, _o, _e = self.mExecuteLocal(_cmd)

                        if _rc == 0:
                            _hostname = re.search("Address:\s+(([0-9]{1,3}\.){3}[0-9]{1,3})\s", _o)

                            if _hostname:
                                _domu_net_client_single_stack.mSetNatAddr(_hostname.group(1))
                                _nat_addr = _hostname.group(1)

                    else:
                        _domu_net_client_single_stack.mSetNatAddr(_admin_net.get("ip", _client_net["natip"]))
                        _nat_addr = _admin_net.get("ip", _client_net["natip"])

                    # Validate that client 'natip' is a valid ip
                    __natip = _domu_net_client_single_stack.mGetNetNatAddr()
                    try:
                        ip = ip_address(__natip)
                        ebLogTrace(f"Valid IP: {ip} for client natip parameter")
                    except ValueError:
                        _msg = f"Value {__natip} is not a valid IP"
                        ebLogError(_msg)
                        raise ExacloudRuntimeError(0x0740, 0xA, _msg)

            _domu_net_client_single_stack.mSetNatDomainName(_dom0_nat_domainname)
            _domu_net_client_single_stack.mSetNatMask(_dom0_nat_mask)

            # Set VLANID.  If "vlantag" is missing in the payload or its value
            # is "null", set VLANID to an empty value (represented by a None
            # python value).
            _domu_net_client_single_stack.mSetNetVlanId(_client_net.get('vlantag'))

            if _admin_net.get('vlantag'):
                _domu_net_client_single_stack.mSetNetVlanNatId(_admin_net.get('vlantag'))
            elif _client_net.get('natvlantag'):
                _domu_net_client_single_stack.mSetNetVlanNatId(_client_net.get('natvlantag'))

            if _admin_net.get('netgateway'):
                _domu_net_client_single_stack.mSetNetNatGateway(_admin_net.get('netgateway'))
            elif _client_net.get('natgateway'):
                _domu_net_client_single_stack.mSetNetNatGateway(_client_net.get('natgateway'))

            _egressIps = self.mFetchEgressIpsFromPayload(_jconf)

            if _egressIps:
                _egressArgs = ",".join(_egressIps)

                _cmd = [
                    "ALTER NETWORK",
                    {"nategressipaddresses": _egressArgs},
                    {"ID": _domu_net_client_single_stack.mGetNetId()}
                ]

                self.__extraXmlPatchingCommands.append(_cmd)


            _domu_net_client_single_stack.mUpdateNatLookup()
            # Patch hostname in machine (DEFAULT)
            _domu_new_hostname = _client_net['hostname']+'.'+_client_net['domainname']
            _domu_conf.mSetMacHostName(_domu_new_hostname)

            def __lacp_enabled(
                    net_payload: Mapping[str, Any],
                    net_type: str) -> bool:
                if not self.mIsOciEXACC():
                    return False  # LACP only supported in ExaCC

                if "network_types" in net_payload:
                    _net_info = net_payload["network_types"].get(net_type)
                    if _net_info and "bonding_mode" in _net_info:
                        if _net_info.get("bonding_mode").lower() == "lacp":
                            return True
                        else:
                            return False

                # If payload doesn't have attribute "lacp" fallback to global
                # configuration.
                if net_type == CLIENT:
                    config_param = "customer_net_client_lacp"
                elif net_type == BACKUP:
                    config_param = "customer_net_backup_lacp"
                else:
                    config_param = "customer_net_dr_lacp"

                return self.mCheckConfigOption(config_param, "True")

            _domu_net_client_single_stack.mSetNetLacp(
                __lacp_enabled(_customer_conf, net_type=CLIENT))
            
            #Setting accelaratedNetwork flag if needed. Feature and env checks will be done inside the function. 
            ebCluAcceleratedNetwork.addAcceleratedNetworkOedaAction(self, _domu_net_client_single_stack.mGetNetId(), _dom0_key, _domu_conf.mGetMacHostName(), _client_net.get("network_virtualization"), self.__extraXmlPatchingCommands)

            _mtu_set = None
            client_mtu = _client_net.get("mtu")
            if client_mtu and not clujumboframes.useExacloudJumboFramesAPI(self):
                _domu_net_client_single_stack.mSetNetMtu(int(client_mtu))
                _mtu_set = int(client_mtu)

            if _vip_ipv6:
                # Add ipv6 vip info using oedacli command
                self.mSetIPv6DualStackPresent(True)
                _dict_oeda_args = {}
                _dict_oeda_args["NAME"] = _vip_net['hostname']
                _dict_oeda_args["DOMAINNAME"] = _vip_net['domainname']
                _dict_oeda_args["IP"] = _vip_ipv6
                _dict_oeda_args["IPADDR_TYPE"] = "IPV6"
                _dict_oeda_args["NETMASK"] = _client_netmaskv6
                _dict_oeda_where = {"HOSTNAME": _domu_new_hostname}
                _cmd = [
                    "ADD VIP",
                    _dict_oeda_args,
                    _dict_oeda_where
                ]
                self.__extraXmlPatchingCommands.append(_cmd)

            # Patch the ipv6 client network information using oedacli
            if _client_ipv6 and _client_netmaskv6 and _client_gatewayv6:
                self.mSetIPv6DualStackPresent(True)
                _dict_oeda_args = {}
                _dict_oeda_args["NETWORKTYPE"] = "client"
                if _net_master_client:
                    _dict_oeda_args["MASTER"] = _net_master_client
                else:
                    _dict_oeda_args["MASTER"] = _domu_net_client_single_stack.mGetNetMaster()
                if _net_slaves_client:
                    _dict_oeda_args["SLAVE"] = _net_slaves_client
                else:
                    _dict_oeda_args["SLAVE"] = _domu_net_client_single_stack.mGetNetSlave()
                _dict_oeda_args["HOSTNAME"] = _client_net['hostname']
                _dict_oeda_args["DOMAINNAME"] = _client_net['domainname']
                _dict_oeda_args["IP"] = _client_ipv6
                _dict_oeda_args["NETMASK"] = _client_netmaskv6
                _dict_oeda_args["GATEWAY"] = _client_gatewayv6
                # Add all NAT entries
                if not self.__ociexacc:
                    _dict_oeda_args["MAC"] = _client_net['mac'].lower()
                if _mtu_set:
                    _dict_oeda_args["MTU"] = str(_mtu_set)
                _dom0_nat_mask = _domu_net_client_single_stack.mGetNetNatMask()
                if _dom0_nat_mask:
                    _dict_oeda_args["NATNETMASK"] = _dom0_nat_mask
                _dom0_nat_domainname = _domu_net_client_single_stack.mGetNetNatDomainName()
                if _dom0_nat_domainname:
                    _dict_oeda_args["NATDOMAINNAME"] = _dom0_nat_domainname
                _nat_host = _domu_net_client_single_stack.mGetNetNatHostName()
                if _nat_host:
                    _dict_oeda_args["NATHOSTNAME"] = _nat_host
                _nat_addr = _domu_net_client_single_stack.mGetNetNatAddr()
                if _nat_addr:
                    _dict_oeda_args["NATIP"] = _nat_addr
                _nat_gateway = _domu_net_client_single_stack.mGetNetNatGateway()
                if _nat_gateway and _nat_gateway != "UNDEFINED":
                    _dict_oeda_args["NATGATEWAY"] = _nat_gateway
                _nat_vlan = _domu_net_client_single_stack.mGetNetVlanNatId()
                if _nat_vlan and _nat_vlan != "UNDEFINED":
                    _dict_oeda_args["NATVLANID"] = _nat_vlan
                if _egressIps:
                    _egressArgs = ",".join(_egressIps)
                    _dict_oeda_args["nategressipaddresses"] = _egressArgs
                if 'vlantag' in _client_net:
                    _vlan = _client_net.get('vlantag')
                    if _vlan:
                        _dict_oeda_args["VLANID"] = _vlan
                _dict_oeda_where = {"HOSTNAME": _domu_new_hostname}
                _cmd = [
                    "ADD NETWORK",
                    _dict_oeda_args,
                    _dict_oeda_where
                ]
                self.__extraXmlPatchingCommands.append(_cmd)

            #
            # BACKUP - Expanded Update/Patching
            #
            _backup_json_fields = ['domainname', 'hostname']
            _backup_either_or_fields = {'ip':'ipv6', 'netmask': 'v6netmask', 'gateway': 'v6gateway'}
            if not self.__ociexacc:
                _backup_json_fields.append('mac')

            for _field in _backup_json_fields:
                if _field not in list(_backup_net.keys()):
                    raise ExacloudRuntimeError(0x0740, 0xA,
                                               'Backup JSON Payload incomplete missing field: %s' % (_field))
            for _key, _value in _backup_either_or_fields.items():
                if _key not in list(_backup_net.keys()) and _value not in list(_backup_net.keys()):
                    raise ExacloudRuntimeError(0x0740, 0xA, f'Backup JSON Payload incomplete missing field: {_key}/{_value}.')

            _domu_net_backup_single_stack.mSetNetHostName(_backup_net['hostname'])

            _backup_ip_single_stack, _backup_ipv6 = _nw_utils.mGetIPv4IPv6Payload(_backup_net)
            # IPv6 host IP should be added using oedacli and not by direct patching of xml
            # as per discussion with oeda/oeda spec.
            if _backup_ip_single_stack:
                _domu_net_backup_single_stack.mSetNetIpAddr(_backup_ip_single_stack)
            if _backup_ipv6 and _domu_net_backup_v6:
                _domu_net_backup_v6.mSetNetIpAddr(_backup_ipv6)
            if _nw_utils.mIsIPv6(_backup_ip_single_stack):
                self.mSetIPv6SingleStackPresent(True)

            _backup_netmask_single_stack, _backup_netmaskv6 = _nw_utils.mGetIPv4IPv6Payload(_backup_net, key_single_stack='netmask', key_dual_stack='v6netmask')
            # IPv6 netmask should be added using oedacli and not by direct patching of xml
            # as per discussion with oeda/oeda spec. Also, v6netmask will be IPv6 prefix.
            if _backup_netmask_single_stack:
                _domu_net_backup_single_stack.mSetNetMask(_backup_netmask_single_stack)
            if _backup_netmaskv6 and _domu_net_backup_v6:
                _domu_net_backup_v6.mSetNetMask(_backup_netmaskv6)

            _backup_gateway_single_stack, _backup_gatewayv6 = _nw_utils.mGetIPv4IPv6Payload(_backup_net, key_single_stack='gateway', key_dual_stack='v6gateway')
            # IPv6 netmask should be added using oedacli and not by direct patching of xml
            # as per discussion with oeda/oeda spec.
            if _backup_gateway_single_stack:
                _domu_net_backup_single_stack.mSetNetGateWay(_backup_gateway_single_stack)
            if _backup_gatewayv6 and _domu_net_backup_v6:
                _domu_net_backup_v6.mSetNetGateWay(_backup_gatewayv6)

            _domu_net_backup_single_stack.mSetNetDomainName(_backup_net['domainname'])
            if _network_discovered  and 'backup_net' in _network_discovered:
                _net_master_backup = _network_discovered['backup_net']['bond_master']
                _net_slaves_backup = _network_discovered['backup_net']['bond_slaves']
                _domu_net_backup_single_stack.mSetNetMaster(_network_discovered['backup_net']['bond_master'])
                _domu_net_backup_single_stack.mSetNetSlave(_network_discovered['backup_net']['bond_slaves'])
            if not self.__ociexacc:
                _domu_net_backup_single_stack.mSetMacAddr(_backup_net['mac'].lower())

            # Set VLANID.  If "vlantag" is missing in the payload or its value
            # is "null", set VLANID to an empty value (represented by a None
            # python value).
            _domu_net_backup_single_stack.mSetNetVlanId(_backup_net.get('vlantag'))

            if _backup_net.get('natvlantag'):
                _domu_net_backup_single_stack.mSetNetVlanNatId(_backup_net.get('natvlantag'))

            if _backup_net.get('natgateway'):
                _domu_net_backup_single_stack.mSetNetNatGateway(_backup_net.get('natgateway'))

            _egressIps = self.mFetchEgressIpsFromPayload(_jconf)

            if _egressIps:
                _egressArgs = ",".join(_egressIps)

                _cmd = [
                    "ALTER NETWORK",
                    {"nategressipaddresses": _egressArgs},
                    {"ID": _domu_net_backup_single_stack.mGetNetId()}
                ]

                self.__extraXmlPatchingCommands.append(_cmd)

            _domu_net_backup_single_stack.mUpdateNatLookup()

            _domu_net_backup_single_stack.mSetNetLacp(
                __lacp_enabled(_customer_conf, net_type=BACKUP))

            #Setting accelaratedNetwork flag if needed. Feature and env checks will be done inside the function. 
            ebCluAcceleratedNetwork.addAcceleratedNetworkOedaAction(self, _domu_net_backup_single_stack.mGetNetId(), _dom0_key, _domu_conf.mGetMacHostName(), _backup_net.get("network_virtualization"), self.__extraXmlPatchingCommands)

            backup_mtu = _backup_net.get("mtu")
            _mtu_set = None
            if backup_mtu and not clujumboframes.useExacloudJumboFramesAPI(self):
                _domu_net_backup_single_stack.mSetNetMtu(int(backup_mtu))
                _mtu_set = int(backup_mtu)

            # Patch the ipv6 backup network information using oedacli
            if _backup_ipv6 and _backup_netmaskv6 and _backup_gatewayv6:
                self.mSetIPv6DualStackPresent(True)
                _dict_oeda_args = {}
                _dict_oeda_args["NETWORKTYPE"] = "backup"
                if _net_master_backup:
                    _dict_oeda_args["MASTER"] = _net_master_backup
                else:
                    _dict_oeda_args["MASTER"] = _domu_net_backup_single_stack.mGetNetMaster()
                if _net_slaves_backup:
                    _dict_oeda_args["SLAVE"] = _net_slaves_backup
                else:
                    _dict_oeda_args["SLAVE"] = _domu_net_backup_single_stack.mGetNetSlave()
                _dict_oeda_args["HOSTNAME"] = _backup_net['hostname']
                _dict_oeda_args["DOMAINNAME"] = _backup_net['domainname']
                _dict_oeda_args["IP"] = _backup_ipv6
                _dict_oeda_args["NETMASK"] = _backup_netmaskv6
                _dict_oeda_args["GATEWAY"] = _backup_gatewayv6
                if _egressIps:
                    _egressArgs = ",".join(_egressIps)
                    _dict_oeda_args["nategressipaddresses"] = _egressArgs
                if 'vlantag' in _backup_net:
                    _vlan = _backup_net.get('vlantag')
                    if _vlan:
                        _dict_oeda_args["VLANID"] = _vlan
                if _mtu_set:
                    _dict_oeda_args["MTU"] = str(_mtu_set)
                if not self.__ociexacc:
                    _dict_oeda_args["MAC"] = _backup_net['mac'].lower()
                _dict_oeda_where = {"HOSTNAME": _domu_new_hostname}
                _cmd = [
                    "ADD NETWORK",
                    _dict_oeda_args,
                    _dict_oeda_where
                ]
                self.__extraXmlPatchingCommands.append(_cmd)

            if self.__ociexacc and self.mIsDRNetPresent():
                _dr_payload_oeda_mapping = {"domainname": "DOMAINNAME", "hostname": "HOSTNAME", "ip": "IP", "netmask":"NETMASK", "gateway": "GATEWAY"}
                _dict_oeda_args = {}
                # Get MASTER and SLAVE info from network discovery
                for _payload_key, _oeda_key in _dr_payload_oeda_mapping.items():
                    if _payload_key not in _dr_net:
                        raise ExacloudRuntimeError(0x0740, 0xA, 'DR JSON Payload incomplete missing field: %s' % (_payload_key))
                    _dict_oeda_args[_oeda_key] = _dr_net[_payload_key]
                if "vlantag" in _dr_net:
                    _dict_oeda_args["VLANID"] = _dr_net["vlantag"]
                _dict_oeda_args["NETWORKTYPE"] = "OTHER"
                _dict_oeda_args["MASTER"] = "bondeth2"
                if __lacp_enabled(_customer_conf, net_type=DR):
                    _dict_oeda_args["LACP"] = "true"
                else:
                    _dict_oeda_args["LACP"] = "false"
                _dict_oeda_where = {"HOSTNAME": _domu_new_hostname}
                # DR: This patching for DR network is required here because in the cluster xml,
                # we don't have the DR network entry originally - it is coming from the payload
                _cmd = [
                    "ADD NETWORK",
                    _dict_oeda_args,
                    _dict_oeda_where
                ]
                self.__extraXmlPatchingCommands.append(_cmd)
                _dr_vip_config = ebCluDRVipConfig()
                for _dr_vip_key in ["domainname", "hostname", "ip"]:
                    if _dr_vip_key not in list(_dr_vip_net.keys()):
                        raise ExacloudRuntimeError(0x0740, 0xA, 'DR VIP JSON Payload incomplete missing field: %s' % (_dr_vip_key))
                _dr_vip_config.mSetDRVIPDomainName(_dr_vip_net["domainname"])
                _dr_vip_config.mSetDRVIPName(_dr_vip_net["hostname"])
                _dr_vip_config.mSetDRVIPAddr(_dr_vip_net["ip"])
                self.mSetDRVips({_domu_new_hostname: _dr_vip_config})
            #
            # DUMP Updated Client/Backup Net Config
            #
            if self.__debug:
                _domu_net_client_single_stack.mDumpConfig()
                _domu_net_backup_single_stack.mDumpConfig()
            #
            # Check Global State (flush domU cached list).
            #
            self.mReturnDom0DomUPair(aForce=True)

        # Update DR SCAN
        if "drScan" in list(_customer_conf.keys()):
            _scan_name = _customer_conf['drScan']['hostname']
            _scan_ips  = _customer_conf['drScan']['ips']
            _scan_port = None
            if "port" in _customer_conf['drScan']:
                _scan_port = _customer_conf['drScan']['port']
            _dr_scan_config = ebCluDRScanConfig()
            _dr_scan_config.mSetScanName(_scan_name)
            _dr_scan_config.mSetScanIps(_scan_ips)
            _dr_scan_config.mSetScanPort(_scan_port)
            self.mSetDRScans(_dr_scan_config)
        #
        # Update SCAN (Assume only one scan entry)
        #
        _scan_port = None

        if 'scans' in list(_customer_conf.keys()):
            _jconf_scan_name = _customer_conf['scans'][0]['hostname']
            _jconf_scan_ips, _jconf_scan_v6_ips = _nw_utils.mGetIPv4IPv6Scans(_customer_conf['scans'][0])

            if "port" in _customer_conf['scans'][0]:
                _scan_port = _customer_conf['scans'][0]['port']

        else:
            _jconf_scan_name = _customer_conf['scan']['hostname']
            _jconf_scan_ips, _jconf_scan_v6_ips = _nw_utils.mGetIPv4IPv6Scans(_customer_conf['scan'])

            if "port" in _customer_conf['scan']:
                _scan_port = _customer_conf['scan']['port']

        _scan_name = _clu_scans[0]
        _scan_conf = self.__scans.mGetScan(_scan_name)
        _scan_conf.mSetScanName(_jconf_scan_name)
        _scan_conf_ip_list = _scan_conf.mGetScanIpsList()

        if _scan_port:
            _scan_conf.mSetScanPort(_scan_port)

        for _scan_idx in range(0,len(_scan_conf_ip_list)):
            _scan_ip  = _scan_conf_ip_list[_scan_idx]
            _scan2_ip = _jconf_scan_ips[_scan_idx]
            _scan_ip.text = _scan2_ip
        if _jconf_scan_v6_ips:
            # IPv6 Scan IPs should be added using oedacli
            self.mSetIPv6DualStackPresent(True)
            _dict_oeda_args = {}
            _dict_oeda_args["SCANNAME"] = _jconf_scan_name
            if _scan_port:
                _dict_oeda_args["SCANPORT"] = str(_scan_port)
            _dict_oeda_args["SCANIPS"] = ",".join(_jconf_scan_v6_ips)
            _dict_oeda_args["NETMASK"] = _client_netmaskv6
            _dict_oeda_args["GATEWAY"] = _client_gatewayv6
            _dict_oeda_args["SCANTYPE"] = "ipv6"
            _dict_oeda_args["SUBNET"] = str(IPv6Network(f"{_client_ipv6}/{_client_netmaskv6}", strict=False).network_address)
            _dict_oeda_where = {"CLUSTERID": self.__clusters.mGetCluster().mGetCluId()}
            _cmd = [
                "ADD SCAN",
                _dict_oeda_args,
                _dict_oeda_where
            ]
            self.__extraXmlPatchingCommands.append(_cmd)

        if not self.mIsOciEXACC():
            # Update DNS/NTP values from original xml
            try:
                _dom0s, _, _cells, _ = self.mReturnAllClusterHosts()
                _cluhosts = _dom0s + _cells
                for _host in _cluhosts:
                    _dict_oeda_args = {}
                    _mac = self.__machines.mGetMachineConfig(_host)
                    _dns_servers = _mac.mGetDnsServers()
                    _ntp_servers = _mac.mGetNtpServers()
                    if _dns_servers:
                        _dns_servers = ','.join(_dns_servers)
                        _dict_oeda_args["DNSSERVERS"] = _dns_servers
                    if _ntp_servers:
                        _ntp_servers = ','.join(_ntp_servers)
                        _dict_oeda_args["NTPSERVERS"] = _ntp_servers
                    if _dns_servers or _ntp_servers:
                        _dict_oeda_where = {"HOSTNAME": _host}
                        _cmd = [
                            "ALTER MACHINE",
                            _dict_oeda_args,
                            _dict_oeda_where
                        ]
                        self.__extraXmlPatchingCommands.append(_cmd)
            except Exception as ex:
                ebLogWarn("DNS and NTP server information could not be added to the list of oeda xml patching commands"\
                    " for DOM0s and Cells.")
            try:
                for _ilom in self.__iloms.mGetIlomsList():
                    _dict_oeda_args = {}
                    _ilomCfg = self.__iloms.mGetIlomConfig(_ilom)
                    _dns_servers = _ilomCfg.mGetDnsServers()
                    _ntp_servers = _ilomCfg.mGetNtpServers()
                    if _dns_servers:
                        _dns_servers = ','.join(_dns_servers)
                        _dict_oeda_args["DNSSERVERS"] = _dns_servers
                    if _ntp_servers:
                        _ntp_servers = ','.join(_ntp_servers)
                        _dict_oeda_args["NTPSERVERS"] = _ntp_servers
                    if _dns_servers or _ntp_servers:
                        _dict_oeda_where = {"ILOMNAME": _ilomCfg.mGetIlomName()}
                        _cmd = [
                            "ALTER ILOM",
                            _dict_oeda_args,
                            _dict_oeda_where
                        ]
                        self.__extraXmlPatchingCommands.append(_cmd)
            except Exception as ex:
                ebLogWarn("DNS and NTP server information could not be added to the list of oeda xml patching commands"\
                    " for ILOMs.")
        #
        # Update DNS/NTP to values in payload
        #
        _ntp_conf = _customer_conf['network_services']['ntp']
        _dns_conf = _customer_conf['network_services']['dns']
        if not _ntp_conf or not _ntp_conf[0]:
            raise ExacloudRuntimeError(0x0744, 0xA, 'JSON Payload incomplete missing field: _ntp_conf')
        if not _dns_conf or not _dns_conf[0]:
            raise ExacloudRuntimeError(0x0744, 0xA, 'JSON Payload incomplete missing field: _dns_conf')
        _ntp_conf = [x for x in _ntp_conf if str(x).strip()]
        _dns_conf = [x for x in _dns_conf if str(x).strip()]
        for _, _domU in self.mReturnDom0DomUPair():
            _domU_mac = self.__machines.mGetMachineConfig(_domU)
            _domU_mac.mSetNtpServers(_ntp_conf)
            _domU_mac.mSetDnsServers(_dns_conf)

    def mUpdateDRNetworkSlaves(self, aOptions):
        _jconf = aOptions.jsonconf
        if 'network' in list(_jconf.keys()):
            _customer_conf = _jconf['network']
        else:
            _customer_conf = _jconf['customer_network']
        _nodes_list = _customer_conf['nodes']
        for _node in _nodes_list:
            if 'dr' not in list(_node.keys()):
                return
        for _dom0, _domU in self.mReturnDom0DomUPair():
            _net_info = self.mGetNetworkSetupInformation(aNetworkType="dr", aDom0=_dom0)
            _dr_slaves = _net_info['dr']['bond_slaves'].split()
            _bridge = _net_info['dr']['bridge']
            self.mPatchDRNetworkSlaves(_domU, _dr_slaves, aBridge=_bridge)
        _patchconfig = self.mGetPatchConfig()
        self.mUpdateInMemoryXmlConfig(_patchconfig, aOptions)
        ebLogInfo('ebCluCtrl: Saved patched Cluster Config for DR (Updated DR slaves in xml): ' + _patchconfig)
        self.mCopyFile(_patchconfig, self.__remoteconfig)

    def mKvmRoceXMLUpdate(self, aOptions, aJConf=None):

        # Apply new patching of KVM-Roce
        ebLogInfo('*** KVM-Roce XML Update ...')
        if aJConf is not None:
            _jconf = aJConf
        else:
            _jconf = aOptions.jsonconf

        _kvmroce_conf = _jconf['kvmroce']
        ebLogInfo(_kvmroce_conf)

        # Patch DomU compute vlan id
        if "computeVlanId" in _kvmroce_conf:
            _vlanIdDomU = _kvmroce_conf['computeVlanId']

            for _, _domU in self.mReturnDom0DomUPair():
                _domu_conf = self.__machines.mGetMacIdFromMacHostName(_domU)
                _domu_conf = self.__machines.mGetMachineConfig(_domu_conf)

                if _domu_conf is None:
                    ebLogWarn("Missing {0} from XML".format(_domU))
                    continue

                _domu_networks = _domu_conf.mGetMacNetworks()

                for _net_id in _domu_networks:
                    _net_conf = self.__networks.mGetNetworkConfig(_net_id)

                    if _net_conf.mGetNetType() == "private":

                        if _net_conf.mGetInterfaceName().startswith("str"):
                            if "storageVlanId" in _kvmroce_conf:
                                _vlanIdStorage = _kvmroce_conf['storageVlanId']
                                _net_conf.mSetNetVlanId(_vlanIdStorage)
                                ebLogInfo("Change VlanId on {0} to {1}".format(_net_id, _vlanIdStorage))

                        if _net_conf.mGetInterfaceName().startswith("cl"):
                            _net_conf.mSetNetVlanId(_vlanIdDomU)
                            ebLogInfo("Change VlanId on {0} to {1}".format(_net_id, _vlanIdDomU))

        # Patch the compute networks ips
        if "computeNetwork" in _kvmroce_conf:
            _computeNetwork = _kvmroce_conf['computeNetwork']

            for _domU, _computeNetworkSingle in list(_computeNetwork.items()):

                _domu_conf = self.__machines.mGetMacIdFromMacHostName(_domU)
                _domu_conf = self.__machines.mGetMachineConfig(_domu_conf)

                if _domu_conf is None:
                    ebLogWarn("Missing {0} from XML".format(_domU))
                    continue

                _domu_networks = _domu_conf.mGetMacNetworks()

                # Apply change on storageIps
                if "storageIps" in _computeNetworkSingle:
                    _storageIps = _computeNetworkSingle['storageIps']

                    # Change the ips of the networks
                    _privateNetworkCounter = 0
                    for _net_id in _domu_networks:

                        _net_conf = self.__networks.mGetNetworkConfig(_net_id)
                        if _net_conf.mGetNetType() == "private" and _net_conf.mGetInterfaceName().startswith("str"):

                            try:
                                _ipAddress = _storageIps[_privateNetworkCounter]
                                _net_conf.mSetNetIpAddr(_ipAddress)
                                ebLogInfo("Change IP on {0} to {1}".format(_net_id, _ipAddress))

                                if "StorageNetmask" in _kvmroce_conf:
                                    _net_conf.mSetNetMask(_kvmroce_conf["StorageNetmask"])
                                    ebLogInfo("Change Netmask on {0} to {1}".format(_net_id, _kvmroce_conf["StorageNetmask"]))

                            except IndexError as e:
                                ebLogWarn("Could not patch {0}".format(_net_id))
                                ebLogWarn(e)

                            _privateNetworkCounter += 1

                # Apply change on clusterInterconnectIps
                if "clusterInterconnectIps" in _computeNetworkSingle:
                    _clusterIps = _computeNetworkSingle['clusterInterconnectIps']

                    # Change the ips of the networks
                    _privateNetworkCounter = 0
                    for _net_id in _domu_networks:

                        _net_conf = self.__networks.mGetNetworkConfig(_net_id)
                        if _net_conf.mGetNetType() == "private" and _net_conf.mGetInterfaceName().startswith("cl"):

                            try:
                                _ipAddress = _clusterIps[_privateNetworkCounter]
                                _net_conf.mSetNetIpAddr(_ipAddress)
                                ebLogInfo("Change IP on {0} to {1}".format(_net_id, _ipAddress))

                                if "ComputeNetmask" in _kvmroce_conf:
                                    _net_conf.mSetNetMask(_kvmroce_conf["ComputeNetmask"])
                                    ebLogInfo("Change Netmask on {0} to {1}".format(_net_id, _kvmroce_conf["ComputeNetmask"]))

                            except IndexError as e:
                                ebLogWarn("Could not patch {0}".format(_net_id))
                                ebLogWarn(e)

                            _privateNetworkCounter += 1

        # Patch the storage networks ips
        if "storageVlanId" in _kvmroce_conf:
            _vlanIdStorage = _kvmroce_conf['storageVlanId']
            _, _, _cells, _ = self.mReturnAllClusterHosts()

            for _cell in _cells:

                _cell_conf = self.__machines.mGetMacIdFromMacHostName(_cell)
                _cell_conf = self.__machines.mGetMachineConfig(_cell_conf)

                if _cell_conf is None:
                    ebLogWarn("Missing {0} from XML".format(_cell))
                    continue

                _cell_networks = _cell_conf.mGetMacNetworks()

                for _net_id in _cell_networks:
                    _net_conf = self.__networks.mGetNetworkConfig(_net_id)

                    if _net_conf.mGetNetType() == "private":
                        ebLogInfo("Change VlanId on {0} to {1}".format(_net_id, _vlanIdStorage))
                        _net_conf.mSetNetVlanId(_vlanIdStorage)

        if "storageNetwork" in _kvmroce_conf:
            _storageNetwork = _kvmroce_conf['storageNetwork']

            for _cell, _storageNetworkSingle in list(_storageNetwork.items()):

                _cell_conf = self.__machines.mGetMacIdFromMacHostName(_cell)
                _cell_conf = self.__machines.mGetMachineConfig(_cell_conf)

                if _cell_conf is None:
                    ebLogWarn("Missing {0} from XML".format(_cell))
                    continue

                _cell_networks = _cell_conf.mGetMacNetworks()

                # Apply changes on storageIps
                if "storageIps" in _storageNetworkSingle:
                    _storageIps = _storageNetworkSingle['storageIps']

                    # Change the ips of the networks
                    _clNetworkCounter = 0
                    for _net_id in _cell_networks:

                        _net_conf = self.__networks.mGetNetworkConfig(_net_id)
                        if _net_conf.mGetNetType() == "private" and _net_conf.mGetInterfaceName().startswith("str"):

                            try:
                                _ipAddress = _storageIps[_clNetworkCounter]
                                _net_conf.mSetNetIpAddr(_ipAddress)
                                ebLogInfo("Change IP on {0} to {1}".format(_net_id, _ipAddress))

                                if "StorageNetmask" in _kvmroce_conf:
                                    _net_conf.mSetNetMask(_kvmroce_conf["StorageNetmask"])
                                    ebLogInfo("Change Netmask on {0} to {1}".format(_net_id, _kvmroce_conf["StorageNetmask"]))

                            except IndexError as e:
                                ebLogWarn("Could not patch {0}".format(_net_id))
                                ebLogWarn(e)

                            _clNetworkCounter += 1

    @exakms_enable_fetch_clustername_decorator
    def mHandlerExaKmsMigrate(self):

        ebLogInfo("Running mExaKmsMigrate")

        _exakms = get_gcontext().mGetExaKms()

        # Backup the keys
        _uuid = str(self.__uuid).replace("-", "")
        _bkFolder = f"clusters/migrate_bk{_uuid}"
        ebLogInfo(f"Backup the keys on {_bkFolder}")

        if not os.path.exists(_bkFolder):
            os.makedirs(_bkFolder)

        _exakms.mSaveEntriesToFolder(
            _bkFolder,
            self.mGetExaKmsHostMap()
        )

        # Migrate the keys
        self.mHandlerRefreshExassh()

        # Validate the keys
        _rc = self.mValidateKeys()

        if _rc is not None and _rc != 0:
            return ebError(0x0108)

        # Delete backup
        ebLogInfo(f"Deleting backup: {_bkFolder}")
        self.mExecuteLocal(f"/bin/rm -rf {_bkFolder}")

        return 0

    @exakms_enable_fetch_clustername_decorator
    def mHandlerRefreshExassh(self):

        ebLogInfo("Running mRefreshExassh")

        _exakms = get_gcontext().mGetExaKms()
        _hostmap = self.mGetExaKmsHostMap()

        _fullFQDN = {}
        for _host in _hostmap.keys():
            _fullFQDN[_host.split(".")[0]] = _host

        _hostmapKeys = list(map(lambda x: x.split(".")[0], _hostmap.keys()))

        _keymap = {}
        _entries = _exakms.mSearchExaKmsEntries({}, aRefreshKey=True)

        for _entry in _entries:

            _append = False

            _nathost = _exakms.mGetEntryClass().mUnmaskNatHost(_entry.mGetFQDN())
            if _nathost.split(".")[0] in _fullFQDN:
                _append = True
                _nathost = _fullFQDN[_nathost.split(".")[0]]

            if _entry.mGetVersion() == "ExaKmsEntryKeysDBRSA":
                if _entry.mGetPkDB() == self.__key:
                    _append = True

            if _entry.mGetVersion() == "ExaKmsEntryOCIRSA":
                if _entry.mGetObjectName() == self.__key:
                    _append = True


            if _append:

                try:

                    # Delete Key
                    _exakms.mDeleteExaKmsEntry(_entry)

                    # Validate key is correct
                    _entry.mGetPrivateKey()

                    _mapkey = f"{_nathost}/{_entry.mGetUser()}"

                    if _mapkey not in _keymap:
                        _keymap[_mapkey] = _entry

                except:
                    pass

        for _mapkey, _entry in _keymap.items():

            _nathost = _mapkey.split("/")[0]
            _type = ExaKmsHostType.UNKNOWN

            if "-c" in _nathost or "-ilom" in _nathost:
                _type = ExaKmsHostType.ILOM

            if _nathost in _hostmap.keys():
                _type = _hostmap[_nathost]

            _updateEntry = _exakms.mBuildExaKmsEntry(
                _nathost,
                _entry.mGetUser(),
                _entry.mGetPrivateKey(),
                _type
            )

            _exakms.mInsertExaKmsEntry(_updateEntry)

    def mRotateVmKeys(self):

        ebLogInfo("Calling Rotate VM Keys")

        _exakmsEndpoint = ExaKmsEndpoint(None)
        for _, _domU in self.mReturnDom0DomUPair():
            _exakmsEndpoint.mSingleRotateKey(_domU)


    def mEnvInfo(self):

        _dict = {}
        ol7_supported = False
        ol8_supported = False

        # Shared env validation
        self.mCheckSharedEnvironment()

        for _dom0, _ in self.mReturnDom0DomUPair():
            with connect_to_host(_dom0, get_gcontext()) as _node:                
                _cmd = '/usr/local/bin/imageinfo -version'
                _, _o, _ = _node.mExecuteCmd(_cmd)
                _lines = _o.readlines()
                if len(_lines) != 0:
                    _dict['image_version'] = _lines[0].strip()

                    # Detect OL7
                    _srev   = ''.join(_dict['image_version'].split('.')[:5])
                    if _srev > '18200':
                        ol7_supported = True
                    if _srev > '22194':
                        ol8_supported = True
                _dict['is_shared_env'] = self.mGetSharedEnv()
            break

        # Put the GI Version
        self.mImageSeparationInit(ol7_supported)
        _dict['grids_version'] = list(self.__compat.keys())

        _hw = []                        
        _0s, _Us, _Cs, _Ss = self.mReturnAllClusterHosts()        

        _hosts = _0s + _Cs 
        if self.mIsExaScale():
            _hosts = _0s

        _basecmds = [
            "/bin/cat /etc/oracle-release",
            "/usr/local/bin/imageinfo -ver",
            "/usr/sbin/dmidecode -s system-serial-number"
        ]

        _toExecute = {}
        for _host in _hosts : 
            _toExecute[_host] = _basecmds

        _result = self.mExecuteCmdParallel(_toExecute)

        for _host, _status in _result.items():

            _hwInfo = {}
            _hwInfo['hostname'] = _host

            if _status[0]["rc"] == 0:
                _hwInfo['oracle-release'] = _status[0]["stdout"].strip()

            if _status[1]["rc"] == 0:
                _hwInfo['sw_version'] = _status[1]["stdout"].strip()

            if _status[2]["rc"] == 0:
                _hwInfo['node_serial_number'] = _status[2]["stdout"].strip()

            _hw.append(_hwInfo)

        _dict['hardware'] = _hw
        return _dict

    def mReadComputes(self):
        _computes = []

        for _machine in self.__machines.mGetMachineConfigList():
            _dictM = self.__machines.mGetMachineConfig(_machine).mGetDict()
            _type = _dictM["OsType"]
            if _type in ["LinuxDom0", "LinuxKVM"]:
                _computes.append(_dictM['HostName'])

        return _computes

    def mReadCellMachines(self):
        _cells = []

        for _machine in self.__machines.mGetMachineConfigList():
            _dictM = self.__machines.mGetMachineConfig(_machine).mGetDict()
            _type = _dictM["OsType"]
            if _type == "LinuxPhysical":
                _cells.append(_dictM['HostName'])

        return _cells

    def mHandlerClusterXmlInfo(self):

        _cluster = self.__clusters.mGetCluster()
        _clu_vips  = _cluster.mGetCluVips()
        _clu_scans = _cluster.mGetCluScans()
        _nw_utils = NetworkUtils()
        _result = {}

        _computes = []

        if self.__debug:
           ebLogInfo("%s: mHandlerClusterXmlInfo: Get Cluster Information from XML." % (datetime.datetime.now()))

        _mapping_vipv4_vipv6 = {}
        _mapping_hostname_vip = {}

        for _clu_vip in sorted(_clu_vips.values()):
            _machine = _clu_vip.mGetCVIPMachines()[0]
            if self.__debug:
               ebLogInfo("%s: mHandlerClusterXmlInfo: Get MachineConfig:%s" % (datetime.datetime.now(), _machine))
            _machine_config = self.__machines.mGetMachineConfig(_machine)

            _detail = {}
            _detail['virtual_ip'] = _clu_vip.mGetCVIPAddr()
            _detail['hostname'] = _machine_config.mGetMacHostName()
            if _detail['hostname'] not in _mapping_hostname_vip:
                _mapping_hostname_vip[_detail['hostname']] = _detail['virtual_ip']
            else:
                # We don't want multiple entries for same host
                # Have a mapping of vip IP address which is already present for the given hostname
                # to the vip IP address not present for the given hostname
                _mapping_vipv4_vipv6[_mapping_hostname_vip[_detail['hostname']]] = _detail['virtual_ip']
                continue

            _ctx = get_gcontext()
            _detail['nathostname'] = _detail['hostname']
            if _ctx.mCheckRegEntry('_natHN_' + _detail['hostname']):
                _detail['nathostname'] = _ctx.mGetRegEntry('_natHN_' + _detail['hostname'])

            for _network in _machine_config.mGetMacNetworks():
                if self.__debug:
                   ebLogInfo("%s: mHandlerClusterXmlInfo: Get NetworkConfig:%s" % (datetime.datetime.now(), _network))
                _neto = self.__networks.mGetNetworkConfig(_network)
                if _neto.mGetNetType() in ['admin', 'backup', 'client']:
                    if ':' not in _neto.mGetNetIpAddr():
                        _mask_cidr = self.mNetMaskToCIDR(_neto.mGetNetMask())
                        _splitted_ip = _neto.mGetNetIpAddr().split('.')
                        _splitted_mask = _neto.mGetNetMask().split('.')
                        _subnet = '.'.join([str(int(_pair[0]) & int(_pair[1])) for _pair in zip(_splitted_ip, _splitted_mask)])

                        _detail[_neto.mGetNetType() + '_vlan'] = _neto.mGetNetVlanId()
                        _detail[_neto.mGetNetType() + '_ip'] = _neto.mGetNetIpAddr()
                        _detail[_neto.mGetNetType() + '_network'] = _subnet + '/' + _mask_cidr
                        if _neto.mGetNetType() + '_network' not in _result:
                            _result[_neto.mGetNetType() + '_network'] = _subnet + '/' + _mask_cidr
                    else:
                        # Making vlan assignment separate though both vlans should be same
                        _detail[_neto.mGetNetType() + '_v6vlan'] = _neto.mGetNetVlanId()
                        _detail[_neto.mGetNetType() + '_ipv6'] = _neto.mGetNetIpAddr()
                        _detail[_neto.mGetNetType() + '_networkv6'] = str(ip_interface(_neto.mGetNetIpAddr() + '/' +\
                            _neto.mGetNetMask()).network.network_address) + '/' + _neto.mGetNetMask()
                        if _neto.mGetNetType() + '_networkv6' not in _result:
                            _result[_neto.mGetNetType() + '_networkv6'] = _detail[_neto.mGetNetType() + '_networkv6']

            # Single stack ipv6 support
            _property_list_ipv6 = ['v6vlan', 'ipv6', 'networkv6']
            _detail_keys = list(_detail.keys())
            _result_keys = list(_result.keys())
            for _key in _property_list_ipv6:
                for _detail_key in _detail_keys:
                    if _key in _detail_key and _detail_key.replace("v6", "") not in _detail_keys:
                        _detail[_detail_key.replace("v6", "")] = _detail[_detail_key]
                        del _detail[_detail_key]
            for _key in _property_list_ipv6:
                for _result_key in _result_keys:
                    if _key in _result_key and _result_key.replace("v6", "") not in _result_keys:
                        _result[_result_key.replace("v6", "")] = _result[_result_key]
                        del _result[_result_key]

            # If admin network is not present, try to calculate it from the XML
            # client section 'nat' fields
            if 'admin_ip' not in _detail:
                ebLogInfo(f"Populating Admin IP from Client section Nat Info")
                for _network in _machine_config.mGetMacNetworks():
                    _neto = self.__networks.mGetNetworkConfig(_network)
                    if _neto.mGetNetType() == "client":
                        _detail["admin_ip"] = _neto.mGetNetNatAddr()
                        _detail["admin_vlan"] = _neto.mGetNetVlanNatId()

                        # Try to get Admin Vlan
                        try:
                            int(_detail["admin_vlan"])
                        except Exception as e:
                            ebLogWarn(f"Admin Vlan {_detail['admin_vlan']} is not valid, assuming vlan 0")
                            _detail["admin_vlan"] = "0"

                        # Try to get Admin Network
                        try:
                            _admin_network = str(IPv4Network(
                                f"{_neto.mGetNetNatAddr()}/{_neto.mGetNetNatMask()}",
                                strict=False))
                        except Exception as e:
                            ebLogError(f"Failed to calculate Admin Network, error: {e}")
                            _admin_network = "UNDEFINED"

                        _detail["admin_network"] = _admin_network
                        if "admin_network" not in _result:
                            _result["admin_network"] = _admin_network

            # If still missing, duplicate client network on admin network
            if not self.mCheckConfigOption('info_without_admin_network_copy', 'True'):
                _fields_tocopy = ('_vlan','_ip','_network')
                for _src,_dst in (('client' + f,'admin' + f) for f in _fields_tocopy):
                    # only copy if admin_X does not exist and client_X exists
                    if _dst not in _detail and _src in _detail:
                        _detail[_dst] = _detail[_src]
                if 'admin_network' not in _result and 'client_network' in _result:
                    ebLogWarn(f"Adming network not found, using client info")
                    _result['admin_network'] = _result['client_network']

            _computes.append(_detail)

        for idx, _compute in enumerate(_computes):
            # There can be maximum 2 vips for a given compute
            # i.e. ipv4 vip and ipv6 vip
            for _vip1, _vip2 in _mapping_vipv4_vipv6.items():
                if _vip1 in _compute.get("virtual_ip", ""):
                    _computes[idx]["virtual_ip2"] = _vip2

        if self.__debug:
           ebLogInfo("%s: mHandlerClusterXmlInfo: Get ScanIps." % (datetime.datetime.now()))
        _scan_ips = []
        # TODO - Scan structure confirmation required
        for _scann in _clu_scans:
            _scano = self.__scans.mGetScan(_scann)
            _scan_ips += _scano.mGetScanIps()

        _result.update({'name' : self.__cluster_name, 'scan_ips' : _scan_ips, 'computes' : _computes})

        # Get total and available storage in GB's in /EXAVMIMAGES partition
        # and store it in the JSON result.
        if self.__debug:
           ebLogInfo("%s: mHandlerClusterXmlInfo: Get EXAVMStorage." % (datetime.datetime.now()))
        _total_storage, _available_storage = self.mHandlerGetEXAVMStorage()
        _result.update({'total_storage' : _total_storage, 'available_storage' : _available_storage})

        _reqobj = self.mGetRequestObj()
        if _reqobj is not None:
            _reqobj.mSetData(json.dumps(_result, sort_keys=True))
            _db = ebGetDefaultDB()
            if self.__debug:
               ebLogInfo("%s: mHandlerClusterXmlInfo: DB UpdateRequest." % (datetime.datetime.now()))
            _db.mUpdateRequest(_reqobj)

        ebLogInfo(json.dumps(_result, sort_keys=True, indent=4))

        ebLogInfo("%s: mHandlerClusterXmlInfo: Done !" % (datetime.datetime.now()))

    def mNetMaskToCIDR(self, aNetMask):
        return str(sum([bin(int(_num)).count('1') for _num in aNetMask.split('.')]))

    def mHandlerMonitorCluster(self):
        return self.mClusterCheckInfo(aMonitor=True)

    def mClusterCheckInfo(self, aOptions=None, aMonitor=False, aPingQMode=False,aVerbose=False):

        ebLogVerbose("mClusterCheckInfo: aMonitor = %s, aPingQMode = %s, aVerbose = %s" % (aMonitor, aPingQMode, aVerbose))

        _db = ebGetDefaultDB()
        if not _db.mCheckTableExist('status'):
            _db.mCreateClusterStatusTable()

        if not self.mIsKVM():
            _dom0s, _domUs, _cells, _switches = self.mReturnAllClusterHosts()
            _cluhosts = _dom0s + _domUs + _cells + _switches
        else:
            _dom0s, _domUs, _cells, _ = self.mReturnAllClusterHosts()
            _switches = []
            _cluhosts = _dom0s + _domUs + _cells

        _cluster_host_d = {}
        #
        # Collect info on all hosts
        #
        for _host in _cluhosts:
            if aVerbose:
                ebLogInfo('*** CheckInfo: %s' %(_host))
            _neto = self.__networks.mGetNetworkConfigByName(_host)
            _clunode = ebClusterNode()
            _cluster_host_d[_host] = _clunode
            _clunode.mSetClusterId(self.__key)
            _clunode.mSetHostname(_host)
            _clunode.mSetNetworkIp(_neto.mGetNetIpAddr())
            if _host in _dom0s:
                _clunode.mSetNodeType('dom0')
            elif _host in _domUs:
                _clunode.mSetNodeType('domu')
            elif _host in _cells:
                _clunode.mSetNodeType('cell')
            elif _host in _switches:
                _clunode.mSetNodeType('switch')
        #
        # Check if host is pingable
        #
        for _host in list(_cluster_host_d.keys()):

            _clunode = _cluster_host_d[_host]

            if aPingQMode:
                _pngcount = 1
            else:
                _pngcount = 4

            if not self.mPingHost(_host, _pngcount):
                _clunode.mSetPingable(False)
                _clunode.mSetSSHConnection(None)
                _clunode.mSetRootSSHDMode(None)
                _clunode.mSetPwdAuthentication(None)
                _clunode.mSetWeakPassword(None)
            else:
                _clunode.mSetPingable(True)
            #
            # Check if SSH and PWD status
            #
            if _clunode.mGetPingable():

                _node = exaBoxNode(get_gcontext())

                try:
                    _node.mConnect(aHost=_host)
                except:
                    ebLogWarn('*** CheckInfo failed to connect to: %s (pingable though)' % (_host))
                    _clunode.mSetSSHConnection(False)
                    continue

                _clunode.mSetSSHConnection(True)

                #
                # Check password login and w/o password login
                #
                _cmd_str  = 'grep "^PermitRootLogin without-password" /etc/ssh/sshd_config &> /dev/null'
                _cmd2_str = 'grep "^PasswordAuthentication no" /etc/ssh/sshd_config &> /dev/null'

                _node.mExecuteCmdLog(_cmd_str)
                if not _node.mGetCmdExitStatus():
                    _clunode.mSetRootSSHDMode(True)     # Close
                else:
                    _clunode.mSetRootSSHDMode(False)    # Open

                _node.mExecuteCmdLog(_cmd2_str)
                if not _node.mGetCmdExitStatus():
                    _clunode.mSetPwdAuthentication(False)       # Close
                else:
                    _clunode.mSetPwdAuthentication(True)        # Open
                #
                # Check strong or weak password (e.g. whether it is default password)
                #
                _cmd3_str = 'grep "^root:" /etc/shadow'
                _i, _o, _e = _node.mExecuteCmd(_cmd3_str)
                _out = _o.readlines()
                if _out:
                    _pwd = _out[0].split(':')[1]
                    if not _pwd[:2] in [ '$6', '$1' ]:
                        ebLogWarn('CheckInfo: %s - Unknown hash or invalid passwd entry' % _host)
                        ebLogWarn('CheckInfo: hash/pwd entry == %s' % (_pwd))
                        continue
                    _, _hash_t, _hash_d, _hash_v = _pwd.split('$')

                    _salt = '$'+_hash_t+'$'+_hash_d+'$'

                    _coptions_for_pwd = get_gcontext().mGetConfigOptions()
                    _default_pwd = b64decode(_coptions_for_pwd["default_pwd"]).decode('utf8')

                    if crypt.crypt(_default_pwd,_salt) == _salt+_hash_v:
                        _clunode.mSetWeakPassword(True)
                    else:
                        _clunode.mSetWeakPassword(False)

                #
                # Node specific checks/info
                #
                if _clunode.mGetNodeType() == 'switch':
                    _cmd4_str = 'smpartition list active no-page | head -10'
                    _i, _o, _e = _node.mExecuteCmd(_cmd4_str)
                    _out = _o.readlines()
                    if _out:
                        for _line in _out:
                            if _line.find('Default=') != -1:
                                _default = _line[len('Default='):-2]
                                _clunode.mSetSwitchDefault(_default)
                            elif _line.find('ALL_CAS=') != -1:
                                _all_cas = _line[len('ALL_CAS='):-2]
                                _clunode.mSetSwitchAllCas(_all_cas)

                # if aMonitor is set, pull more detailed cluster information
                if aMonitor:

                    _dict_data = {}

                    _cmd4_str = 'top -n 1 -b'
                    _i, _o, _e = _node.mExecuteCmd(_cmd4_str)
                    _out = _o.readlines()
                    _param_list = {
                        'Cpu(s)' : {'us'      : 'user_cpu',
                                    'sy'      : 'sys_cpu',
                                    'id'      : 'idle_cpu',
                                    'wa'      : 'wait_cpu'},
                        'Mem'    : {'total'   : 'total_memory',
                                    'used'    : 'used_memory',
                                    'free'    : 'free_memory',
                                    'buffers' : 'buffer_memory'},
                        'Swap'   : {'total'   : 'total_swap',
                                    'used'    : 'used_swap',
                                    'free'    : 'free_swap',
                                    'cached'  : 'cached_swap'}
                    }
                    if _out:
                        _top_stats = []
                        for _line in _out[2:5]:
                            _name, _row = _line.split(':', 1)
                            _row = _row.split(',')
                            for _value, _param in [x.replace('%', '% ').split() for x in _row]:
                                if _name in list(_param_list.keys()) and _param in _param_list[_name]:
                                    _key = _param_list[_name][_param]
                                    _top_stats.append((_key, _value))
                    _dict_data['top'] = _top_stats

                    if _clunode.mGetNodeType() == 'dom0':
                        _param_list = ['machine', 'release', 'nr_cpus', 'nr_nodes', 'cores_per_socket', 'threads_per_core', 'total_memory', 'free_memory']
                        _cmd5_str = 'xm info'
                        _i, _o, _e = _node.mExecuteCmd(_cmd5_str)
                        _out = _o.readlines()
                        if _out:
                            _xminfo_stats = []
                            for _line in _out:
                                _param, _value = [x.strip() for x in _line.split(':', 1)]
                                if _param in _param_list:
                                    _xminfo_stats.append((_param, _value))
                            _dict_data['xminfo'] = _xminfo_stats

                        _cmd6_str = 'xentop -b -i 1'
                        _i, _o, _e = _node.mExecuteCmd(_cmd6_str)
                        _out = _o.readlines()
                        if _out:
                            _rows = []
                            for _row in _out:
                                _rows.append(_row.split())
                            _dict_data['xentop'] = _rows

                    _clunode.mSetDictData(_dict_data)

                #
                # Reset HOST key
                #
                _local_cmd = "/bin/ssh-keygen -R {0}".format(_host)
                self.mExecuteLocal(_local_cmd, aStdOut=DEVNULL, aStdErr=DEVNULL)
                _node.mDisconnect()
            #
            # Record node status in DB
            #
            if not _db.mGetClusterStatus(_clunode.mGetHostname()):
                _db.mInsertNewClusterStatus(_clunode)
            else:
                _db.mUpdateClusterStatus(_clunode)

            ebLogInfo('*** %s : %s ' % (_host, _clunode.mToString()))

        return

    """
    ::mManageDatabaseHomes
    """
    def mManageDatabaseHomes(self,aOptions=None):

        ebLogInfo("mManageDatabaseHomes: Manage Database Homes.")

        if not ebCluCmdCheckOptions(self.__cmd, ['manage_dbhomes']):
            ebLogInfo("Skip ManageDatabaseHomes on cmd {0}".format(self.__cmd))
            return

        def _mReturnGiversionGihv():
            if self.mGetGiMultiImageSupport():
                _giversion = self.mGetVersionGiMultiImages()
            else:
                _giversion = self.mGetVersionGi()
    
            # in mutli gi support _gihv will contain
                # ["19.18.0.0.230117", "19.18.0.0", "19.0.0.0", "230117"]
            # in single gi support _gihv will contain
                # ["19.0.0.0", "2021.019", "019", "230117"]
            _gihv = self.mGetGridConfig()[_giversion]
            
            return _giversion, _gihv

        # Try to use the grid version that is actually running
        if self.__cmd == 'vmgi_reshape' or self.__cmd == "elastic_info":

            if self.mIsExaScale():

                # Delete compute
                if 'reshaped_node_subset' in list(aOptions.jsonconf.keys()) and \
                   'removed_computes' in list(aOptions.jsonconf['reshaped_node_subset'].keys()) and \
                   aOptions.jsonconf['reshaped_node_subset']['removed_computes']:

                    ebLogInfo("Skip ManageDatabaseHomes on cmd {0} in ExaScale".format(self.__cmd))
                    return
                _giversion, _gihv = _mReturnGiversionGihv()
                _gihc = self.__clusters.mGetCluster()
                if self.mGetGiMultiImageSupport():
                    # check if GI version supported by oeda
                    if mGetGridListSupportedByOeda(self, _giversion):
                        _set_cluster_version = _giversion
                    else:
                        # if oeda does not support this grid version skip the minor version
                        _set_cluster_version = _gihv[2]+'.'+_gihv[3]
                    _gihc.mSetCluVersion(_set_cluster_version)
                    ebLogTrace(f'GI version patched to XML : {_set_cluster_version}')
                else:
                    if len(_gihv[2]):
                        _gihc.mSetCluVersion(_gihv[0]+'.'+_gihv[3])    # conv. doy to date done via config
                    else:
                        _gihc.mSetCluVersion(_gihv[0])

                return

            if self.__cmd == 'vmgi_reshape':
                _options = self.mGetArgsOptions()
                _reshape = ebCluReshapeCompute(self, _options)
                _srcDomU = _reshape.mGetSrcDomU()
                _location, _giversion, _ = self.mGetOracleBaseDirectories(aDomU=_srcDomU)
                _grid_config = self.mGetGridConfig()
                if self.mGetGiMultiImageSupport():
                    _major_minor_version = _giversion[:5] # eg. 19.18
                    ebLogTrace(f'Major minor version derived : {_major_minor_version}')
                    _matchingGiFound = False
                    for _key in sorted(_grid_config.keys(), reverse=True):
                        if _key.startswith(_major_minor_version):
                            _gihv = _grid_config[_key]
                            _matchingGiFound = True
                            ebLogInfo(f"Grid version for vmgi_reshape patching: {_key}")
                            _giversion = _key
                            break
                    if not _matchingGiFound:
                        ebLogTrace(f'No matching GI found for add node , defaulting to latest avaialable')
                        # for add node flow if matching not found return the latest available GI
                        # ensure corresponding 19 or 23 GI latest is returned
                        _major_version = _major_minor_version.split('.')[0]
                        _giversion = self.mGetLatestGIFromMajorVersion(_major_version)
                        _gihv = self.mGetGridConfig()[_giversion]
                        ebLogTrace(f'vmgi_reshape flow - GI version:{_giversion}, gihv : {_gihv}')
                else:
                    # single GI image flow
                    if _giversion in _grid_config.keys():
                        _gihv = _grid_config[_giversion]
                    else:
                        _major_version = _giversion[:2]

                        for _key in sorted(_grid_config.keys(), reverse=True):
                            if _key.startswith(_major_version):
                                _gihv = _grid_config[_key]
                                break
            if self.__cmd == 'elastic_info':
                _giversion, _gihv = _mReturnGiversionGihv()
        else:
            _giversion, _gihv = _mReturnGiversionGihv()
      
        _gihc = self.__clusters.mGetCluster()

        #
        # Set Version and Path
        #
        if self.mGetGiMultiImageSupport():
            _set_cluster_version = ""
            _version = int(''.join(_giversion.split('.')[:4]))
            #'23.26.0.0.251021' -> 232600
            #"23.9.0.0.251021" -> 23900 which is less than 232600
            #"19.28.0.0.250715" -> 192800 which is less than 232600
            #23.26.1.0.260115" -> 232610 which is greater than 232600 (future releases)
            if _version >= 232600:
                _set_cluster_version = '.'.join(_giversion.split('.')[:4] + ['0']) #sets 23.26.0.0.0 for 26ai
                ebLogTrace(f"Cluster version prepared to patch XML: {_set_cluster_version} ")
                #for further versions, 23.26.1.0.date --> patches 23.26.1.0.0
            # check if GI version supported by oeda
            elif mGetGridListSupportedByOeda(self, _giversion):
                _set_cluster_version = _giversion
            else:
                # if oeda does not support this grid version skip the minor version
                _set_cluster_version = _gihv[2]+'.'+_gihv[3]
            _gihc.mSetCluVersion(_set_cluster_version)
            ebLogTrace(f'GI version patched to XML : {_set_cluster_version}')
            _gi_major_version = _gihv[2] # only major version
        else:
            if len(_gihv[2]):
                _gihc.mSetCluVersion(_gihv[0]+'.'+_gihv[3])     # conv. doy to date done via config
            else:
                _gihc.mSetCluVersion(_gihv[0])
            _gi_major_version = _gihv[0] # only major version

        if self.__cmd not in ["vmgi_reshape"]:
            _location = '/u01/app/'+_gi_major_version+'/grid'
        ebLogInfo(f"GI HOME PATH: {_location}")

        _stepList = ""
        if self.__options:
            _stepList = str(self.__options.steplist).upper()

        if not self.mIsExaScale() and \
           not "PREVM" in _stepList and \
           not "CREATE_VM" in _stepList and \
           self.__cmd != 'vmgi_reshape' and \
           self.__cmd != 'info':

            #Fetch the GI HOME LOCATION from domU if it is connectable
            _domU = self.mReturnDom0DomUPair()[0][1]
            _node = exaBoxNode(get_gcontext())
            if _node.mIsConnectable(aHost=_domU):
                _location, _, _ = self.mGetOracleBaseDirectories(aDomU=_domU)
                if not _location:
                    _location = '/u01/app/'+_gi_major_version+'/grid'
                ebLogInfo("Using GI Location from DomU {0}".format(_location))

        _gihc.mSetCluHome(_location)

        #
        # Add bundle patch list on patches
        #
        _dbver = _gi_major_version.replace(".", "")[0:2] # "19"
        if _gi_major_version.startswith("12"):
            _dbver = _gi_major_version.replace(".", "")[0:3]
        _patches = self.mGetOedaProperty("LINUX_BUNDLE_PATCH_LIST_{0}.*".format(_dbver))
        _patches = list(map(lambda x: x.split(":")[0], _patches.split(",")))
        _gihc.mSetPatches(_patches)

        _domu_list = []
        for _dom0, _domu in self.mReturnDom0DomUPair():
            _domu_list.append(_domu)


        ebLogInfo(f"***mManageDatabaseHomes: input cmd {self.__cmd}")

        # Remove the db section from the XML in all commands/flows/operations
        _dbhcfg = [ _dbhome for _dbhome in self.__dbhomes.mGetDBHomeConfigs()]
        ebLogInfo('*** mManageDatabaseHomes: service provisioning command !')
        for _dbh in _dbhcfg:

            ebLogInfo(f"*** Found DB Home {_dbh.mGetDBHomeName()} in XML")
            ebLogInfo(f"*** Deleting DB Home {_dbh.mGetDBHomeName()} from XML")
            self.__dbhomes.mRemoveDBHomeConfig(_dbh.mGetDBHomeConfig_ptr(), _dbh)

        return

    def mArpingHost(self, aIntf, aGateway, aCount=2, aTimeout=4):
        _intf = aIntf
        _gateway = aGateway
        _timeout = aTimeout

        _count = int(aCount)
        while _count:
            _cmd = '/sbin/arping -c 1 -w %s -I %s %s' %(_timeout, _intf, _gateway)
            _rc, _, _, _ = self.mExecuteLocal(_cmd)
            if _rc == 0:
                return True
            _count -= 1
            if self.__debug and _count:
                ebLogWarn('*** Arping failed retrying for gateway: %s' % (_gateway))

        return False

    def mPingHost(self,aHostname,aCount=4, aTimeout=0, aIPtype='4'):

        _host = aHostname
        _ctx = get_gcontext()
        if _ctx.mCheckRegEntry('_natHN_' + _host):
            _host = _ctx.mGetRegEntry('_natHN_' + _host)
            if self.__debug:
                ebLogDebug('*** CONN_PING_USING_NATHN: %s/%s' % (aHostname, _host))

        # ssh_post_fix
        if _ctx.mCheckRegEntry('ssh_post_fix') and \
           _ctx.mGetRegEntry('ssh_post_fix') == "True" and \
           _host in _ctx.mGetRegEntry('domU_set'):

                ebLogInfo('*** ssh_post_fix, Checking Ping in {0}'.format(_host))

                # Check the ping using vm ping endpoint
                for _dom0, _domU in self.mReturnDom0DomUPair():
                    if _domU == _host:
                        _node = exaBoxNode(get_gcontext())
                        _node.mConnect(aHost=_dom0)
                        _vmhandle = self.__CompRegistry.mGetComponent("vm_operations")
                        _vmhandle.mSetOVMCtrl(aCtx=get_gcontext(), aNode=_node)
                        _rc = _vmhandle.mDispatchEvent("ping", aOptions=None, aVMId=_domU)
                        if _rc == 0:
                            return True
                        else:
                            return False

        _count = int(aCount)
        # If IP address in passed as _host - detect if it is ipv6 address.
        # If a hostname is passed, : (colon) in hostname is invalid
        if ':' in _host:
            aIPtype = 6
        while _count:
            if aIPtype == '4':
                _ping_cmd = '/bin/ping'
            else:
                _ping_cmd = '/bin/ping -6'
            if aTimeout == 0:
                #use original code if no timeout given
                _cmd = f'{_ping_cmd} -c 1 ' + _host + ' '
            else:
                _cmd = f'{_ping_cmd} -c 1 -W '+str(aTimeout)+' '+_host + ' '
            _rc, _, _, _err = self.mExecuteLocal(_cmd)
            if _rc == 0:
                return True
            elif _err and len(_err) != 0:
                ebLogError(f"*** Ping failed with Error:  {str(_err)}")
                
            _count -= 1
            if self.__debug and _count:
                ebLogWarn('*** Ping Failed retrying for host: %s' % (_host))

        return False

    def mHandlerUnLockDBMUsers(self):
        return self.mLockDBMUsers(False)

    def mHandlerLockDBMUsers(self, aMode=True):
        return self.mLockDBMUsers(True)

    def mLockDBMUsers(self, aMode=True):

        def _mLockDBMUsers(aHost, aMode=True):
            #
            # Try to ping host first
            #
            if not self.mPingHost(_host):
                ebLogWarn('*** Host {0} is not pingable aborting LockDbmUsers for this host.'.format(_host))
                return

            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_host)
            for _user in ('dbmadmin', 'dbmmonitor', 'dbmsvc'):
                if aMode: #Lock
                    _node.mExecuteCmdLog('echo {0}:$(openssl rand -base64 32) | chpasswd'.format(_user)) # Random passwd
                    _node.mExecuteCmdLog('passwd -l {0}'.format(_user)) # Lock passwd
                    ebLogInfo('*** Locked {0} account on host {1}'.format(_user, _host))
                else: #Unlock
                    _default_pwd = self.mCheckConfigOption("default_pwd")
                    _pwd = b64decode(_default_pwd).decode('utf8')
                    _node.mExecuteCmdLog('passwd -u {0}'.format(_user)) # Unlock passwd
                    _node.mExecuteCmdLog('echo {0}:{1} | chpasswd'.format(_user, _pwd)) # Default passwd
                    ebLogInfo('*** Unlocked {0} account on host {1}'.format(_user, _host))
            _node.mDisconnect()

        # Get a single list of Dom0, DomU hosts
        _hostList = self.mReturnDom0DomUPair()
        _hostList = [_host for _pair in _hostList for _host in _pair]
        _plist = ProcessManager()

        for _host in _hostList:

            _p = ProcessStructure(_mLockDBMUsers, [_host, aMode], _host)
            _p.mSetMaxExecutionTime(60*15) # 15 minutes
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()


    def mSecureDOMUSsh(self, aList=None):

        if aList == None:
            _list = self.mGetOrigDom0sDomUs() 
        else:
            _list = aList

        for _, _host in _list:
            if not self.mPingHost(_host):
                ebLogInfo('*** (SECURE_DOMU_SSH) Host: %s is not responding' % (_host))
                continue
            _node = exaBoxNode(get_gcontext())

            ebLogInfo('*** (SECURE_DOMU_SSH) Connecting to: %s' % (_host))
            try:
                _node.mConnect(aHost=_host)
            except:
                ebLogInfo('*** (SECURE_DOMU_SSH) Unable to connect to host: %s' % (_host))
                continue
            _node.mExecuteCmdLog('grep "^PasswordAuthentication" /etc/ssh/sshd_config')
            _node.mExecuteCmdLog('grep "^PermitRootLogin" /etc/ssh/sshd_config')
            _node.mExecuteCmdLog('grep "^PubkeyAuthentication" /etc/ssh/sshd_config')
            _node.mExecuteCmdLog('grep "OEDA_PUB" .ssh/authorized_keys')
            #
            # OFF == (disable password authentication) && (Allow root login w/o passwd)
            #
            _cmd4_str_off = "/usr/bin/passwd -l root"
            _ssh_config_options = {
                'PasswordAuthentication': 'no',
                'PubkeyAuthentication': 'yes',
                'PermitRootLogin': 'without-password'
            }
            node_update_key_val_file(_node, '/etc/ssh/sshd_config', _ssh_config_options, sep=' ')

            _cmd_restart_sshd = "service sshd restart"
            _node.mExecuteCmdLog(_cmd4_str_off +  " ; " + _cmd_restart_sshd)
            _node.mDisconnect()

    def mHandlerDesecureDOMUPwd(self):
        return self.mSecureDOMUPwd(False)

    def mHandlerSecureDOMUPwd(self):
        return self.mSecureDOMUPwd(True)

    """
    ::mSecureDOMUPwd
    """
    def mSecureDOMUPwd(self,aMode=True):

        ebLogVerbose("mSecureDOMUPwd: aMode = %s" % aMode)

        #
        # Check if SPWD is disabled
        #
        if self.mCheckConfigOption('disable_spwd','True'):
            ebLogInfo('*** PWD rotation (sw/pwd) is disabled')
            return
        #
        # Check mode requested
        #
        if aMode:
            ebLogInfo('*** DOMU PWD strong password secure requested')
        else:
            ebLogInfo('*** DOMU PWD defaut/weak password requested')
        #
        # Check if Strong Pwd is available from the config - if not Generate Strong Password
        #
        _dpairs = self.mReturnDom0DomUPair()
        _spwd = self.mCheckConfigOption('root_spwd')
        if _spwd is None:
            _pchecks = ebCluPreChecks(self)
            _spwd = _pchecks.mGeneratePassword(self.__clusters.mGetCluster().mGetCluId(), self.__clusters.mGetCluster().mGetCluName(), _dpairs[0][1])
            _spwd = b64decode(_spwd).decode('utf8')

        #
        # Check if Weak Pwd is available from the config - else assume it's our terrible default password
        #
        _wpwd = self.mCheckConfigOption('root_wpwd')
        if _wpwd is None:
            default_pwd = self.mCheckConfigOption("default_pwd")
            _wpwd = default_pwd

        #
        # Check if SSH KEY file is available if not log a warning and skip RE/Desecure PWD
        #
        # w/ RC5 we rely on OEDA SSH Key

        for _dom0, _domU in _dpairs:
            #
            # Try to ping host first
            #
            if not self.mPingHost(_domU):
                ebLogWarn('*** Host (%s) is not pingable aborting SecureDOMUPwd for this host.' % (_domU))
                continue
            #
            # Try to connect to the _domU according the mode requested eg. De/Re Secure
            #
            _error = False
            _done  = False
            try:
                #
                # In order to avoid all the potential mess of getting locked out because of the timeout on
                # Authentication failure that can happen if we don't pick up the right password. We are using
                # SSH Key login by default.
                #
                with connect_to_host(_domU, get_gcontext()) as _nodeU:
                    pass
            except Exception as e:
                if str(e) == "Authentication failed.":
                    if aMode == False:
                        try:
                            with connect_to_host(_domU, get_gcontext(), password=_wpwd) as _nodeU:
                                ebLogWarn('*** Desecure DomU %s PWD already done !' % (_domU))
                                _done = True
                        except:
                            _error = True
                    else:
                        try:
                            with connect_to_host(_domU, get_gcontext(), password=_spwd) as _nodeU:
                                ebLogWarn('*** Resecure DomU %s PWD already done !' % (_domU))
                                _done = True
                        except:
                            _error = True
            #
            # Re/De Secure Already done
            #
            if _done:
                continue
            #
            # Check if we were able to connect else raise Exception
            #
            if _error:
                ebLogError('*** Critical Error while connecting to %s - Authentication Failed' % (_domU))
                raise Exception('Authentication Failed.')
            #
            # Proceed to change the PWD
            #
            if aMode == False:
                _spwd = _wpwd
                if self.__verbose:
                    ebLogWarn('*** Desecure DomU %s PWD' % (_domU))
            else:
                if self.__verbose:
                    ebLogWarn('*** Secure DomU %s PWD' % (_domU))
            with connect_to_host(_domU, get_gcontext()) as _nodeU:
                _cmdstr = """echo 'root:%s' | chpasswd >& /dev/null""" % (_spwd)
                _nodeU.mExecuteCmdLog("sh -c \"" + _cmdstr + "\"")

                _cmdstr = """echo 'oracle:%s' | chpasswd >& /dev/null""" % (_spwd)
                _nodeU.mExecuteCmdLog("sh -c \"" + _cmdstr + "\"")

                _cmdstr = """echo 'grid:%s' | chpasswd >& /dev/null""" % (_spwd)
                _nodeU.mExecuteCmdLog("sh -c \"" + _cmdstr + "\"")


    def mCopyVMxmltoGcvBackup(self, aDom0Node, aDomUName):

        if self.mIsKVM() and self.mIsExaScale():

            _tstamp = time.strftime("%Y%m%d-%H%M%S")
            _libvrt_xml = f"/etc/libvirt/qemu/{aDomUName}.xml"
            _gcv_backup_xml = f"/EXAVMIMAGES/GuestImages/{aDomUName}/backup/libvirt/{aDomUName}.xml"

            if aDom0Node.mFileExists(_gcv_backup_xml):
                #Lets save a copy of the gcv/backup xml before we overwrite it.
                _cmd = f"/bin/cp {_gcv_backup_xml} {_gcv_backup_xml}.{_tstamp}"
                aDom0Node.mExecuteCmdLog(_cmd)

            if aDom0Node.mFileExists(_libvrt_xml):
                ebLogInfo(f"mCopyVMxmltoGcvBackup: Copying  {_libvrt_xml} to {_gcv_backup_xml}")
                _cmd = f"/bin/cp {_libvrt_xml} {_gcv_backup_xml}"
                aDom0Node.mExecuteCmdLog(_cmd)


    @ebRecordReplay.mRecordReplayWrapper
    def mParallelDomUShutdown(self, aDom0DomUPair=None, force_on_timeout=False):

        def mShutdownDomU(aDom0Name, aDomUName, force_on_timeout):

            # For ExaCC with FS Encryption enabled, we must inject the
            # u02 passphrase to allow the VMs to reboot.
            # The socket data MUST be set before the DomUs go down,
            # otherwise the Dom0 tool won't know how to identify
            # the domU socket channel
            if self.mIsKVM() and self.mIsOciEXACC() and exacc_fsencryption_requested(self.mGetArgsOptions()):

                ebLogInfo(f"{aDomUName} -- Setting up u02 socket data before DomU shutdown")
                mSetLuksPassphraseOnDom0Exacc(
                        self, aDom0Name, aDomUName, aWait=False, aWaitSeconds=1200)

            # Do VM Reboot
            with connect_to_host(aDom0Name, get_gcontext()) as _node0:
                shutdown_domu(_node0, aDomUName, force_on_timeout=force_on_timeout)
                self.mCopyVMxmltoGcvBackup(_node0, aDomUName)

        _plist = ProcessManager()
        _rc_status = _plist.mGetManager().dict()

        if aDom0DomUPair:
            _dpairs = aDom0DomUPair
        else:
            _dpairs = self.mReturnDom0DomUPair()

        # Parallelize execution on dom0s
        for _dom0, _domU in _dpairs:
            _p = ProcessStructure(mShutdownDomU, [_dom0, _domU, force_on_timeout], _dom0)
            _p.mSetMaxExecutionTime(60*60) # 60 minutes
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()

    @ebRecordReplay.mRecordReplayWrapper
    def mParallelDomUStart(self, aDom0DomUPair=None):

        def mStartDomU(aDom0Name, aDomUName):

            # Do VM Reboot, if u02 is encrypted on ExaCC we must set up
            # the socket data channel and then wait for the boot
            # Ref: 36685993
            if self.mIsKVM() and self.mIsOciEXACC() and exacc_fsencryption_requested(self.mGetArgsOptions()):
                _wait_for_connectable = False
            else:
                _wait_for_connectable = True

            with connect_to_host(aDom0Name, get_gcontext()) as _node0:

                start_domu(_node0, aDomUName,
                        wait_for_connectable=_wait_for_connectable)

                if not _wait_for_connectable:
                    mSetLuksPassphraseOnDom0Exacc(
                        self, aDom0Name, aDomUName, aWait=False,
                        aWaitSeconds=600)
                    start_domu(_node0, aDomUName,
                        wait_for_connectable=True)


        _plist = ProcessManager()
        _rc_status = _plist.mGetManager().dict()

        if aDom0DomUPair:
            _dpairs = aDom0DomUPair
        else:
            _dpairs = self.mReturnDom0DomUPair()

        # Parallelize execution on dom0s
        for _dom0, _domU in _dpairs:
            _p = ProcessStructure(mStartDomU, [_dom0, _domU], _dom0)
            _p.mSetMaxExecutionTime(60*60) # 60 minutes
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()

    @ebRecordReplay.mRecordReplayWrapper
    def mParallelDomURestart(self):

        def mRestartDomU(aDom0Name, aDomUName):

            # Do VM Reboot
            with connect_to_host(aDom0Name, get_gcontext()) as _node0:
                shutdown_domu(_node0, aDomUName)
                self.mCopyVMxmltoGcvBackup(_node0, aDomUName)

                if (self.mIsOciEXACC() and self.mIsKVM() and
                        luksCharchannelExistsInDom0(aDom0Name, aDomUName)):
                    mSetLuksPassphraseOnDom0Exacc(
                            self, aDom0Name, aDomUName, aWait=False, aWaitSeconds=600)
                start_domu(_node0, aDomUName)

        _plist = ProcessManager()
        _rc_status = _plist.mGetManager().dict()

        # Parallelize execution on dom0s
        for _dom0, _domU in self.mReturnDom0DomUPair():
            _p = ProcessStructure(mRestartDomU, [_dom0, _domU], _dom0)
            _p.mSetMaxExecutionTime(60*60) # 60 minutes
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()

    @ebRecordReplay.mRecordReplayWrapper
    def mCheckSshd(self,aDomU,aTotalTime,aTimeout):
        _host    = aDomU
        _ctx     = get_gcontext()
        if _ctx.mCheckRegEntry('_natHN_' + _host):
            _host = _ctx.mGetRegEntry('_natHN_' + _host)
        _start_time   = time.time()
        _time_elapsed = 0
        _loop_count = 0

        _localNode = exaBoxNode(get_gcontext(), aLocal=True)
        try:
            _localNode.mConnect()

            while _time_elapsed < aTotalTime:

                #Check remote ssh port
                if _localNode.mCheckPortSSH(_host):
                    ebLogInfo('SSH port is alive on domU')
                    return True

                #Update time
                time.sleep(aTimeout)
                _time_elapsed = time.time() - _start_time
                if _loop_count % 10:
                    ebLogWarn('Waiting for VM: {0} to come up. Time elapsed: {1}s'.format(_host, _time_elapsed))
                _loop_count += 1

        finally:
            _localNode.mDisconnect()

        ebLogError("*** Timeout while waiting for ssh port")
        return False

    def mRestartVM(self,aDomU,aVMHandle=None,aNode=None,aNatName=None):
        _domu=aDomU
        _node=aNode
        if aVMHandle is None:
            if aNode is not None:
                _vmhandle = self.__CompRegistry.mGetComponent("vm_operations")
                _vmhandle.mSetOVMCtrl(aCtx=get_gcontext(), aNode=_node)
            else:
                raise ExacloudRuntimeError(0x0113, 0xA, 'NIMPL : Invalid parameters passed to mRestartVM ***')
        else:
            _vmhandle = aVMHandle

        ebLogInfo('*** Restarting VM: '+_domu)
        _options = self.mGetArgsOptions()
        _rc = _vmhandle.mDispatchEvent("start", aOptions=_options, aVMId=_domu, aCluCtrlObj=self)
        if _rc and _rc not in [ 0x0410,0x0454 ]:
            ebLogError('*** VM Restart did not work for: %s' % (_domu))
            raise ExacloudRuntimeError(0x0411, 0xA, 'VM was not able to restart')


        # Avoid using ssh resolution on ssh_post_fix
        _ctx = get_gcontext()
        if _ctx.mCheckRegEntry('ssh_post_fix') and _ctx.mGetRegEntry('ssh_post_fix') == "True":
            ebLogInfo('VM ' + _domu + ' is now up and running.')
            return 0

        #
        # Wait for the node to come back online
        #
        _retry_periods = 7
        _retry_interval = 30

        if self.mCheckConfigOption('vm_time_sleep_reboot') is not None:
            _retry_interval = self.mCheckConfigOption('vm_time_sleep_reboot') * 3

        _aTotalTime = _retry_periods * self.__timeout_ecops
        _natDomU = _domu
        if aNatName:
            _natDomU = aNatName
        if self.mCheckSshd(_natDomU,aTotalTime=_aTotalTime,aTimeout=_retry_interval):
            # even if port is open, wait one interval more for resiliency
            time.sleep(_retry_interval)
            if not self.isBaseDB() and not self.isExacomputeVM():
                _vmnode = exaBoxNode(get_gcontext())
                _vmnode.mConnect(aHost=_natDomU)
                _vmnode.mExecuteCmd("ip addr show | grep 'ib0\|ib1\|inet '")
                _vmnode.mDisconnect()
            ebLogInfo('VM ' + _domu + ' is now up and running.')
        else:
            _detail_error = "Error while restarting the VM: {}".format(_domu)
            ebLogError('*** ' + _detail_error)
            if _options and _options.jsonconf and 'node_recovery_flow' in list(_options.jsonconf.keys()) and _options.jsonconf['node_recovery_flow'] == True:
                self.mUpdateErrorObject(gReshapeError['ERROR_VM_NOT_CONNECTABLE'], _detail_error)
                raise ExacloudRuntimeError(0x0454, 0xA, 'VM failed to restart')

        if _rc == 0x0454:
            _detail_error = "Failed to enable AutoStart Params status for %s"%(_domu)
            ebLogError('*** ' + _detail_error)
            self.mUpdateErrorObject(gReshapeError['ERROR_AUTOSTART_PARAM_SETTING'], _detail_error)
            raise ExacloudRuntimeError(0x0454, 0xA, 'VM failed to enable autostart params')
        return 0

    def mManageVMCpusBursting(self, aVMCmd, aVMId, aOptions=None):
        if self.mIsKVM():
            _exacpueobj = exaBoxKvmCpuMgr(self)
            return _exacpueobj.mManageVMCpusBurstingKvm(aVMCmd, aVMId, aOptions)

        _vmcmd = aVMCmd
        _vmid  = aVMId
        _options = aOptions
        _maxcores = 0
        _maxvcpus = 0
        _data_d  = {}
        _data_d['version'] = '1.0'
        _data_d['comment'] = 'vcpu bursting enabling'
        _data_d['vms'] = {}

        ebLogVerbose("mManageVMCpusBursting: aVMCmd = %s, aVMId = %s" % (aVMCmd, aVMId))

        def _mUpdateRequestData(aDataD):
            _data_d = aDataD
            _reqobj = self.mGetRequestObj()
            if _reqobj is not None:
                _reqobj.mSetData(json.dumps(_data_d))
                _db = ebGetDefaultDB()
                _db.mUpdateRequest(_reqobj)

        if _options is not None:
            _jconf = aOptions.jsonconf

        if _vmcmd == 'enablebursting':
            if _jconf is None:
                _jconf = {}

        for _dom0, _domU in self.mReturnDom0DomUPair():

            if _vmid != '_all_' and _vmid != _domU:
                continue
            #
            # Initiaize _data_d
            #
            _data_d['vms'][_domU] = {}
            #
            # Fetch maxcores from Dom0
            #
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            _in, _out, _err = _node.mExecuteCmd('xm info | grep nr_cpus')
            _out = _out.readlines()
            if _out and len(_out):
                try:
                    _dat = _out[0].split(':')[1].strip()
                except ValueError:
                    ebLogError('*** Failed to split the output String :%s'%(_out))
                    return ebError(0x0430)
    
                # Remove Dom0 vcpu from count of vm vcpus
                _dom0vcpus = _node.mSingleLineOutput("xm li | grep Domain-0 | awk '{ print $4 }'")
                if _dom0vcpus:
                    _dat = str(int(_dat) - int(_dom0vcpus))

                ebLogInfo('*** MAX_VCPUS on dom0 %s : %s' % (_dom0, _dat))
                _maxcores = int(_dat)
            #
            # Update vm.cfg
            #
            _vmhandle = exaBoxOVMCtrl(aCtx=get_gcontext(), aNode=_node)
            _vmhandle.mReadRemoteCfg(_domU)
            _cfg = _vmhandle.mGetOVSVMConfig(_domU)
            if _cfg is None:
                _node.mDisconnect()
                ebLogError('*** DomU (%s) configuration is not available' % (_domU))
                _mUpdateRequestData(_data_d)
                return ebError(0x0430)
            #
            # Check vm.cfg settings : maxvcpus and current vCPUs
            #
            _cmd = 'cat /EXAVMIMAGES/GuestImages/'+_domU+'/vm.cfg | grep maxvcpus'
            _in, _out, _err = _node.mExecuteCmd(_cmd)
            _out = _out.readlines()
            if len(_out):
                _maxvcpus = int(_out[0][:-1].split('=')[1])
                ebLogInfo('*** MAX_VCPUS (CFG) for domU %s : %d' % (_domU, _maxvcpus))

            if _maxvcpus != _maxcores:
                #
                # Update maxvcpus
                #
                _cfg.mSetValue('maxvcpus', str(_maxcores))
                self.mSaveVMCfg(_node, _domU, _cfg.mRawConfig())
                ebLogInfo('*** MAX_VCPUS (CFG) for domU %s reset to: %d' % (_domU, _maxcores))
            else:
                ebLogInfo('*** vm.cfg maxvcpus already updated to maxcores')

            _data_d['vms'][_domU]['bursting_enabled'] = 'True'

            _node.mDisconnect()

        _data_d['bursting_enabled'] = 'True'
        _mUpdateRequestData(_data_d)

        return 0


    # ER 28096631
    # Enable/Disable CPU oversubscription on an existing service

    def mModifyService(self, aOptions=None, aSubfactor=None, aPoolsize=None, aDomU=None, aForcePinning=False):
        if self.mIsKVM():
            _exacpueobj = exaBoxKvmCpuMgr(self)
            _exacpueobj.mModifyServiceKvm(aOptions, aSubfactor, aPoolsize, aDomU, aForcePinning)
            return

        _jconf = None
        _cores = 0
        _subfactor = 2 #default (override in payload)
        _cos = True
        _num_computes = len(self._dom0U_list)

        #VGE: aForcePinning arg was added as the disable_vcpus_pinning flag
        # is only reliable during create service (where ATP flag is passed and image verified)
        # Also ONLY in multiVM+disableCOS:    the create service flows
        #      will RELY ON THIS FUNCTION to set pinning so vm.cfg
        #      will NOT have pinning (no cpus= line)
        #      but also NOT be expected to be vNuma
        if not aForcePinning and not self.mCheckConfigOption('force_pinning_on_resize', 'True'):

            for _dom0, _domU in self.mReturnDom0DomUPair():

                if (aDomU is not None) and (aDomU != _domU):
                    ebLogVerbose('*** Skipping CPU Pinning for %s' %(_domU))
                    continue

                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_dom0)

                _node.mExecuteCmd("grep '^cpus' /EXAVMIMAGES/GuestImages/{0}/vm.cfg".format(_domU))
                _rc = _node.mGetCmdExitStatus() != 0
                _node.mDisconnect()

                if _rc != 0:
                    ebLogInfo("*** vNuma Configuration as no Pinning set in vm.cfg, will not update Pinning")
                    return # BAIL OUT


        if not aOptions:
            aOptions = self.mGetArgsOptions()

        if aOptions is not None:
            _jconf = aOptions.jsonconf

        _ratio = self.mCheckConfigOption('core_to_vcpu_ratio')
        if _ratio is None:
            _ratio = 2
        else:
            _ratio = int(_ratio)

        if aPoolsize is not None:
            # Internal call. Ratio already accounted for since we
            # calculated it using VCPU column of 'xm list'
            _cores = str(aPoolsize)
        elif 'poolsize' in list(_jconf.keys()):
            # Call from upper layer
            _cores = str(_jconf['poolsize'])
            ebLogVerbose('*** mModifyService: Poolsize : %s' %(_cores))
            _cores = (int(_cores) * _ratio) # xen vcpus across all computes
            _cores = int(_cores/_num_computes)      # xen vcpus per compute : use this for range calculation
            _cores = str(_cores)
        else:
            ebLogError('*** json payload for CPU oversubscription command: poolsize not provided.')
            return ebError(0x0433)

        if aSubfactor is not None:
            _subfactor = int(aSubfactor)
        elif 'subfactor' in list(_jconf.keys()):
            _subfactor = int(_jconf['subfactor'])

        if _subfactor == 1:
            _cos = False

        _allocatable_cores = str(int(_subfactor) * int(_cores)) # Max allowed sum of all vcpus

        if _cos:
            ebLogVerbose('*** mModifyService: Subfactor : %d | vcores_per_compute : %s' %(_subfactor, _cores))
            ebLogVerbose('*** mModifyService: Sum of vcpus cannot be more than (allocatable_cores) : %s' %(_allocatable_cores))
        else:
            ebLogVerbose('*** mModifyService (non-cos): Subfactor : %d | vcores_per_compute : %s' %(_subfactor, _cores))

        # 5 minutes default timeout for vcpu-pin operation
        _vpt = self.mCheckConfigOption('timeout_vcpu_pin')
        if _vpt is not None:
            _timeout_vcpu_pin = int(_vpt)
        else:
            _timeout_vcpu_pin = 300

        for _dom0, _domU in self.mReturnDom0DomUPair():
            if (aDomU is not None) and (aDomU != _domU):
                ebLogVerbose('*** Skipping CPU Pinning for %s' %(_domU))
                continue
            ebLogVerbose('*** Attempting CPU Pinning for %s' %(_domU))
            _range_dict={}
            _cluster_list=[]
            _clusters_to_update=[]
            _pin_range_start = None

            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)

            ##########
            # STEP 1 #
            ##########
            # Get the cluster list from each dom0.
            # TO-DO : Once ECRA provides the cluster list, validate against it.
            #  i. if ECRA's list is subset of Exacloud's list we will use ECRA list
            #  ii. if ECRA's list is a superset of Exacloud's list we will error out
            #  iii. if they are same any list should be fine

            _cmd = "ls /EXAVMIMAGES/GuestImages/*/vm.cfg | awk -F '/' '{print $4}'"
            _, _out, _ = _node.mExecuteCmd(_cmd)
            if _out:
                _out = _out.readlines()
                for _line in _out:
                    _cluster_list.append(_line.strip())

            ##########
            # STEP 2 #
            ##########
            # Get the expected range with the start value after num_of_dom0_vcpus and end value would be the start+_cores-1

            _cmd = "xm li | grep Domain-0 | awk '{ print $4 }'"
            _, _out, _ = _node.mExecuteCmd(_cmd)
            if _out:
                _out = _out.readlines()
                _dom0vcpus = int(_out[0])
                ebLogInfo('*** DOM0 %s current VCPUS allocation: %d' % (_dom0, _dom0vcpus))
            else:
                _node.mDisconnect()
                _error_str = '*** Can not retrieve DOM0 vCPUS allocation'
                ebLogError(_error_str)
                raise ExacloudRuntimeError(0x0780, 0xA, _error_str)

            # In the non-COS case, we will generate a separate pinning range for each VM
            if not _cos:
                _domU_start_range = _dom0vcpus
                _pr_start_dict={}
                _pr_end_dict={}
                _pin_range_dict={}

            ##########
            # STEP 3 #
            ##########
            # Get the sum of vcpus of all VMs (excluding Dom0) and validate against the max allowed value

            _sum_vcpus = 0
            for _domU in _cluster_list:
                _cmd = "grep ^vcpus /EXAVMIMAGES/GuestImages/"+_domU+"/vm.cfg |  awk -F '=' '{print $2}'"
                _, _out, _ = _node.mExecuteCmd(_cmd)
                _out = _out.readlines()
                if _out:
                    _sum_vcpus += int(_out[0])
                    # In the non-COS case, each VM gets its own range
                    if not _cos:
                        _pr_start_dict[_domU] = _domU_start_range
                        _pr_end_dict[_domU] =  _pr_start_dict[_domU] + int(_out[0]) -1
                        _domU_start_range = _pr_end_dict[_domU] + 1
                        ebLogVerbose('*** DomU: %s expected pinning range: %s - %s' %(_domU, _pr_start_dict[_domU], _pr_end_dict[_domU]))
                        _pin_range_dict[_domU]="'%s-%s'" %(str(_pr_start_dict[_domU]),str(_pr_end_dict[_domU]))
                else:
                    _error_str = '*** Can not retrieve VCPUS allocation for (%s)' % (_domU)
                    ebLogError(_error_str)
                    _node.mDisconnect()
                    raise ExacloudRuntimeError(0x0780, 0xA, _error_str)

            if (_subfactor > 1) and (_sum_vcpus > int(_allocatable_cores)):
                _error_str = '*** Sum of vcpus (%s) > max(allocatable cores) (%s) on Dom0 (%s)' % (_sum_vcpus, _allocatable_cores, _dom0)
                ebLogError(_error_str)
                raise ExacloudRuntimeError(0x0780, 0xA, _error_str)
            ebLogInfo('*** Sum of vcpus (%s) : max(allocatable cores) (%s) on Dom0 (%s)' % (_sum_vcpus, _allocatable_cores, _dom0))

            if _cos:
                _pin_range_start = _dom0vcpus
                _pin_range_end = _pin_range_start + int(_cores) - 1

                _expected_range="'%s-%s'" %(str(_pin_range_start),str(_pin_range_end))
                ebLogInfo('*** expected range : <%s>' %(_expected_range))

            ##########
            # STEP 4 #
            ##########
            #validate the range & do a hot-update & on-disk update of the vm.cfgs as needed with the new pinning pool

            for _domU in _cluster_list:
                _cmd = "grep ^cpus /EXAVMIMAGES/GuestImages/"+_domU+"/vm.cfg |  awk -F '=' '{print $2}'"
                _, _out, _ = _node.mExecuteCmd(_cmd)
                _out = _out.readlines()
                if _out:
                    _cpurange = _out[0].lstrip().rstrip()
                    if _cos:
                        _range_dict[_domU] = _cpurange
                        if _cpurange != _expected_range:
                            _clusters_to_update.append(_domU)
                    else:
                        if (_cpurange != _pin_range_dict[_domU]) or (_domU == aDomU):
                            _clusters_to_update.append(_domU)
                            ebLogVerbose('*** Pin range of %s will be changed from %s to %s' %(_domU, _cpurange,  _pin_range_dict[_domU]))
                else:
                    _clusters_to_update.append(_domU)

            if len(_clusters_to_update) == 0:
                ebLogInfo('*** CPU over-subscription value is unchanged')
            else:
                ebLogInfo('*** CPU over-subscription value is modified')
                for _domU in _clusters_to_update:
                    ebLogVerbose('*** Updating vm.cfg for %s' %(_domU))
                    _vmhandle = exaBoxOVMCtrl(aCtx=get_gcontext(), aNode=_node)
                    _vmhandle.mReadRemoteCfg(_domU)
                    _cfg = _vmhandle.mGetOVSVMConfig(_domU)
                    if _cos:
                        _cfg.mSetValue('cpus', _expected_range)
                    else:
                        _cfg.mSetValue('cpus', _pin_range_dict[_domU])
                    self.mSaveVMCfg(_node, _domU, _cfg.mRawConfig())

                    # xm vcpu-pin (hot-update) to be done only for live VMs
                    _cmd = 'xm li %s' %(_domU)
                    _node.mExecuteCmd(_cmd)
                    _rc = _node.mGetCmdExitStatus()
                    if not _rc:
                        if _cos:
                            _cmd = 'xm vcpu-pin ' + _domU + ' all ' + _expected_range
                        else:
                            _cmd = 'xm vcpu-pin ' + _domU + ' all ' + _pin_range_dict[_domU]
                        _node.mExecuteCmdLog(_cmd)

            ##########
            # STEP 5 #
            ##########
            # Validate the pinning range for all live VMs only if there was an update.

            if len(_clusters_to_update) != 0:
                _total_time = 0
                _check_intv = 10
                _suc = 0

                if _cos:
                    _cmd = "xm vcpu-list | grep -v Domain-0 | grep -v ^Name | awk '{print $7}' |sort | uniq"
                    # Sometimes vcpu-pin takes a while to take effect, hence the wait over loop
                    while _total_time < _timeout_vcpu_pin:
                        time.sleep(_check_intv)
                        _, _out, _ = _node.mExecuteCmd(_cmd)
                        if _out:
                            _out = _out.readlines()
                            if (len(_out) ==1) and  (_out[0].strip() == _expected_range.strip('\'')):
                                ebLogInfo('*** CPU pinning range updated successfully on %s' % (_dom0))
                                _suc = 1
                                break
                        else:
                            _node.mDisconnect()
                            _error_str = '*** Can not retrieve vCPUS allocation'
                            ebLogError(_error_str)
                            raise ExacloudRuntimeError(0x0780, 0xA, _error_str)
                        _total_time += _check_intv
                    if _suc == 0:
                        _node.mDisconnect()
                        _error_str = '*** The pinning range is not same across clusters or incorrect range on %s' %(_dom0)
                        ebLogError(_error_str)
                        raise ExacloudRuntimeError(0x0780, 0xA, _error_str)
                else:
                    for _domU in _clusters_to_update:

                        # xm pinning range verification to be done only for live VMs
                        _cmd = 'xm li %s' %(_domU)
                        _node.mExecuteCmd(_cmd)
                        _rc = _node.mGetCmdExitStatus()
                        if _rc:
                            _suc = 1
                            ebLogVerbose('*** Skipping validation of pinning range for %s as VM is not up' %(_domU))
                            continue

                        ebLogVerbose('*** Validating pinning range for %s' %(_domU))
                        _total_time = 0
                        _check_intv = 10
                        _suc = 0
                        _cmd = "xm vcpu-list | grep -v Domain-0 | grep -v ^Name | grep "+_domU+" |awk '{print $7}' |sort | uniq"
                        while _total_time < _timeout_vcpu_pin:
                            time.sleep(_check_intv)
                            _, _out, _ = _node.mExecuteCmd(_cmd)
                            if _out:
                                _out = _out.readlines()
                                if (len(_out) ==1) and  (_out[0].strip() == _pin_range_dict[_domU].strip('\'')):
                                    ebLogInfo('*** CPU pinning range updated successfully on %s' % (_dom0))
                                    _suc = 1
                                    break
                            else:
                                _node.mDisconnect()
                                _error_str = '*** Can not retrieve vCPUS allocation'
                                ebLogError(_error_str)
                                raise ExacloudRuntimeError(0x0780, 0xA, _error_str)
                            _total_time += _check_intv
                    if _suc == 0:
                        _node.mDisconnect()
                        _error_str = '*** The pinning range is incorrect on %s' %(_dom0)
                        ebLogError(_error_str)
                        raise ExacloudRuntimeError(0x0780, 0xA, _error_str)

            _node.mDisconnect()

        return 0

    def mCheckDom0sPingable(self):
        _dom0s_pingable = []
        _dom0s_offline = []
        _dom0s, _, _, _ = self.mReturnAllClusterHosts()
        for _dom0 in _dom0s:
            if not self.mPingHost(_dom0):
                ebLogWarn('*** Dom0 (%s) is not pingable' % (_dom0))
                _dom0s_offline.append(_dom0)
            else:
                _dom0s_pingable.append(_dom0)
        return _dom0s_pingable, _dom0s_offline

    def mManageVMCpusCount(self, aVMCmd, aVMId, aOptions=None):
        if self.mIsKVM():
            _exacpueobj = exaBoxKvmCpuMgr(self)
            return _exacpueobj.mManageVMCpusCountKvm(aVMCmd, aVMId, aOptions)

        _vmcmd = aVMCmd
        _vmid  = aVMId
        _options = aOptions
        _jconf   = None
        _review  = False
        _cpu_bursting = None
        _bursting_enabled = None
        _subfactor = None
        _cos = False
        _allocatable_cores = 0
        _num_computes = len(self._dom0U_list)
        _clu_utils = ebCluUtils(self)
        _data_d  = {}
        _data_d['version'] = '1.0'
        _data_d['comment'] = 'vcpu data info and status'
        _data_d['bursting_ready'] = 'False'
        _data_d['bursting_enabled'] = 'False'
        _data_d['vms'] = {}
        _partial_update = False

        ebLogVerbose("mManageVMCpusCount: aVMCmd = %s, aVMId = %s" % (aVMCmd, aVMId))
        _stepSpecificDetails = _clu_utils.mStepSpecificDetails("reshapeDetails", 'ONGOING', "CPU reshape in progress", "","CPU")
        _clu_utils.mUpdateTaskProgressStatus([], 0, "CPU Reshape", "In Progress", _stepSpecificDetails)

        def _mUpdateRequestData(aDataD):
            _data_d = aDataD
            _reqobj = self.mGetRequestObj()
            if _reqobj is not None:
                _reqobj.mSetData(json.dumps(_data_d))
                _db = ebGetDefaultDB()
                _db.mUpdateRequest(_reqobj)

        if _options is not None:
            _jconf = aOptions.jsonconf
        #
        # Handle special case for cpustatus
        #
        if _vmcmd == 'cpustatus':
            if _jconf is None:
                _jconf = {}
            _review = True
        #
        # Payload (json) is mandatory
        #
        if _jconf is None:
            _detail_error = 'json payload for VMCpusCount command: %s not provided.' % (_vmcmd)
            ebLogError('*** ' + _detail_error)
            self.mUpdateErrorObject(gReshapeError['INVALID_INPUT_PARAMETER'],_detail_error)
            _mUpdateRequestData(_data_d)
            return ebError(0x0433)
        #
        # Fetch values from payload for cores, memory and disk (only cpus used for now)
        #
        _ratio  = 2
        _host_d = {}
        try:
            if 'vms' in list(_jconf.keys()):
                for _h in _jconf['vms']:
                    _host_d[_h['hostname']] = int(_h['cores']) * _ratio
                    # xxx/MR: Add support to deltacores if needed.
            elif "exaunitAllocations" in _jconf.keys():
                for _, _domU in self.mReturnDom0DomUPair():    
                    _host_d[_domU] = {}  
                    _host_d[_h['hostname']] = int(_jconf['exaunitAllocations']['cores']) * _ratio
        except Exception as e:
            _detail_error = 'CC:mManageVMCpusCount Exception:: %s - %s' % (e.__class__, e)
            ebLogError(_detail_error)
            self.mUpdateErrorObject(gReshapeError['INVALID_INPUT_PARAMETER'],_detail_error)
            _mUpdateRequestData(_data_d)
            return ebError(0x0430)
        #
        # Check if payload is valid
        #
        if not len(list(_host_d.keys())) and not _review:
            _detail_error = 'VM CPU core count/delta not found in payload. PL: %s' % (str(_jconf))
            ebLogError('*** ' + _detail_error)
            self.mUpdateErrorObject(gReshapeError['INVALID_INPUT_PARAMETER'],_detail_error)
            _mUpdateRequestData(_data_d)
            return ebError(0x0433)

        #
        # modify cluster with CPU oversubscription factor; check payload attributes
        #
        _subfactor = None
        if 'subfactor' in list(_jconf.keys()):
            _subfactor = int(_jconf['subfactor'])

        if _subfactor is not None:
            if _subfactor > 1:
                _cos = True
                if 'poolsize' in list(_jconf.keys()):
                    _poolsize = (_jconf['poolsize'])
                    _poolsize = (int(_poolsize) * int(_ratio)) # xen vcpus across all computes
                    _poolsize = int(_poolsize/_num_computes)                # xen vcpus per compute
                    _allocatable_cores = str(int(_subfactor) * int(_poolsize)) # Max allowed sum of all vcpus
                else:
                    ebLogError('*** json payload for CPU oversubscription command: poolsize not provided.')
                    return ebError(0x0433)

        #
        # Iterative/Sequential VCPU update (e.g. parallel support may be added in the future)
        #
        _maxcores = 0
        _maxvcores = 0

        _dom0s_pingable, _dom0s_offline = self.mCheckDom0sPingable()
        if len(_dom0s_pingable) == 0:
            _mUpdateRequestData(_data_d)
            _error_str = 'CPU resize failed as none of the Dom0s are pingable'
            ebLogError('*** ' + _error_str)
            self.mUpdateErrorObject(gReshapeError['ERROR_VM_CPU_RESIZE_PINGABLE'],_error_str)
            raise ExacloudRuntimeError(0x0430, 0xA, _error_str)
        
        _percentage_increase = 10.0    
        _percentageStepSize= 89.0/len(self.mReturnDom0DomUPair())
        _lastNode = []
        _stepSpecificDetails = _clu_utils.mStepSpecificDetails("reshapeDetails", 'ONGOING', "CPU reshape in progress", "","CPU")

        for _dom0, _domU in self.mReturnDom0DomUPair():
            _clu_utils.mUpdateTaskProgressStatus(_lastNode , _percentage_increase , "CPU Reshape", "In Progress", _stepSpecificDetails)
            _percentage_increase = _percentage_increase + _percentageStepSize
            _lastNode.append(_domU)

            if _vmid != '_all_' and _vmid != _domU:
                continue

            if _domU not in list(_host_d.keys()) and not _review:
                continue

            if _dom0 not in _dom0s_pingable:
                _partial_update = True
                ebLogWarn('*** VM CPU core count will be partially updated as dom0 %s is not pingable' % (_dom0))
                continue


            #
            # Initialize _data_d
            #
            _data_d['vms'][_domU] = {}
            #
            # Fetch new cores number for the VM
            #
            if not _review:
                _cores = int(_host_d[_domU])
            else:
                _cores = 0

            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            _in, _out, _err = _node.mExecuteCmd('xm info | grep nr_cpus')
            _out = _out.readlines()
            if _out and len(_out):
                try:
                    _dat = _out[0].split(':')[1].strip()
                except ValueError:
                    ebLogError('*** Failed to split the output String :%s'%(_out))
                    return ebError(0x0430)
                ebLogInfo('*** MAX_VCPUS on dom0 %s : %s' % (_dom0, _dat))
                _maxcores = int(_dat)
            # If cos enabled, consider virtual max cores
            if _cos:
                ebLogInfo('*** MAX_COS_VCPUS on dom0 %s : %s' % (_dom0, _allocatable_cores))
                _maxvcores = int(_allocatable_cores)
            #
            # If VM is down then vm.cfg will be modified and VCPU change will take place when VM is started
            #
            _vmhandle = exaBoxOVMCtrl(aCtx=get_gcontext(), aNode=_node)
            _vmhandle.mReadRemoteCfg(_domU)
            _cfg = _vmhandle.mGetOVSVMConfig(_domU)
            if _cfg is None:
                _node.mDisconnect()
                _detail_error = 'DomU (%s) configuration is not available' % (_domU)
                ebLogError('*** ' + _detail_error)
                _mUpdateRequestData(_data_d)
                self.mUpdateErrorObject(gReshapeError['ERROR_IMAGE_CFG_MISSING'],_detail_error)
                return ebError(0x0430)

            if not self.mPingHost(_domU):
                ebLogWarn('*** DomU (%s) is not pingable for command: %s.' % (_domU, _vmcmd))
                _is_pingable = False
            else:
                _is_pingable = True
            #
            # Fetch current vCPU for DomU
            #
            _in, _o, _err = _node.mExecuteCmd("xm li %s | grep %s | awk '{ print $4 }'" % (_domU,_domU))
            _out = _o.readlines()
            if _is_pingable:
                if _out and len(_out):
                    #_dat = _out[0].split(':')[1].strip()
                    _currvcpus = int(_out[0][:-1])
                    ebLogInfo('*** CURRENT_VCPUS for domU %s : %d' % (_domU, _currvcpus))
                #
                # Fetch current MAXVCPUs (not the one set in vm.cfg)
                #
                _in, _o, _err = _node.mExecuteCmd("xm li %s -l | grep '(vcpu' | tr -d ')' | awk '{ print $2 }'" % (_domU))
                _out = _o.readlines()
                if _out and len(_out):
                    _currmaxvcpus = int(_out[0][:-1])
                    ebLogInfo('*** CURRENT_MAXVCPUS for domU %s : %d' % (_domU, _currmaxvcpus))
            else:
                if _out:
                    ebLogWarn('*** DomU (%s) in an inconsistent state since it is running yet not pingable. Using config file for CPU update.' % _domU)

                _currvcpus = int(_cfg.mGetValue('vcpus'))
                _currmaxvcpus = 0
            #
            # _data_d update
            #
            _data_d['vms'][_domU]['currvcpus']    = _currvcpus
            _data_d['vms'][_domU]['currmaxvcpus'] = _currmaxvcpus
            #
            # Check if new VCPUs count is 2X more than current VCPUs
            #
            if _cores > _currvcpus * 2:
                ebLogWarn('*** NEW_VCPUS count %s for %s is 2 times higher than CURR_VCPUS %s' % (str(_cores),_domU,str(_currvcpus)))
            #
            # Fetch current vCPUs used by all VMs (including dom0)
            #
            _vcpusinuse = 0
            _vcpusleft  = 0
            _cmd = "xm li | awk '{ print $4 }'"
            #
            # For COS , exclude dom0 vCPUs
            #
            if _cos:
                _cmd = "xm li | grep -v Domain-0 | awk '{ print $4 }'"
            _in, _out, _err = _node.mExecuteCmd(_cmd)
            if _out:
                _out = _out.readlines()
                for _value in _out[1:]:
                    _vcpusinuse += int(_value)
                if _maxvcores:
                    _vcpusleft = _maxvcores - _vcpusinuse
                    ebLogInfo('*** COS Enabled CURRENT_VCPUS in dom0 %s : %d (%d left)' % (_domU, _vcpusinuse, _vcpusleft))
                else:
                    _vcpusleft = _maxcores - _vcpusinuse
                    ebLogInfo('*** CURRENT_VCPUS in dom0 %s : %d (%d left)' % (_domU, _vcpusinuse, _vcpusleft))
            _left = _vcpusleft
            _requested = (_cores - _currvcpus)
            _vcpusleft = _vcpusleft - (_cores - _currvcpus)
            if _vcpusleft < 0:
                _detail_error = 'vCPUs over-provisioning detected !!! (%d left / %d requested)' % (_left,_requested)
                ebLogError('*** ' + _detail_error)
                _node.mDisconnect()
                _mUpdateRequestData(_data_d)
                self.mUpdateErrorObject(gReshapeError['ERROR_VCPU_OVERSUBSCRIBED'],_detail_error)
                return ebError(0x0431)
            #
            # In cos enabled, domU total VCPU can not cross more than purchased cores (_ratio * physical cores)
            #
            if _cos:
                _maxvcoresdomu = (int(_maxvcores)/int(_ratio))
                if _maxvcoresdomu < _cores:
                    _detail_error = 'vCPUs over-provisioning detected !!! (max(allocatable cores) (%d) on DomU (%s) /  %d requested)' % (_maxvcoresdomu,_domU,_requested)
                    ebLogError('*** ' + _detail_error)
                    _node.mDisconnect()
                    _mUpdateRequestData(_data_d)
                    self.mUpdateErrorObject(gReshapeError['ERROR_VCPU_OVERSUBSCRIBED'],_detail_error)
                    return ebError(0x0431)
            #
            # Check vm.cfg settings : maxvcpus and current vCPUs
            #
            _cmd = 'cat /EXAVMIMAGES/GuestImages/'+_domU+'/vm.cfg | grep maxvcpus'
            _nodeU = exaBoxNode(get_gcontext())
            _nodeU.mConnect(aHost=_dom0)
            _in, _out, _err = _nodeU.mExecuteCmd(_cmd)
            if _out:
                _out = _out.readlines()
                _maxvcpus = int(_out[0][:-1].split('=')[1])
                ebLogInfo('*** MAX_VCPUS (CFG) for domU %s : %d' % (_domU, _maxvcpus))
            #
            # _data_d udpate
            #
            _data_d['vms'][_domU]['cfgmaxvcpus'] = _maxvcpus
            #
            # Check for discrepancy between current and vm.cfg maxvcpus values
            #
            if _currmaxvcpus != 0 and \
               _currmaxvcpus < _maxvcpus-4 and \
               _currmaxvcpus > _maxvcpus:
                _detail_error = 'MAX_VCPUS for domU in vm.cfg not applied to running VM (cfg:%s/current:%s)' % (_maxvcpus,_currmaxvcpus)
                ebLogWarn('*** ' + _detail_error)
                if not _review:
                    _data_d['vms'][_domU]['bursting_enabled'] = 'False'
                    _data_d['bursting_enabled'] = 'False'
                    _data_d['bursting_ready'] = 'False'
                    _nodeU.mDisconnect()
                    _node.mDisconnect()
                    _mUpdateRequestData(_data_d)
                    self.mUpdateErrorObject(gReshapeError['ERROR_VCPU_NOT_APPLIED'],_detail_error)
                    return ebError(0x0435)
                else:
                    _data_d['vms'][_domU]['bursting_enabled'] = 'False'
                    _bursting_enabled = 'False'
                    _data_d['bursting_enabled'] = 'False'
                    _data_d['bursting_ready'] = 'False'
            else:
                _data_d['vms'][_domU]['bursting_enabled'] = 'True'
                if _bursting_enabled is None:
                    _bursting_enabled = 'True'
                    _cpu_bursting = 'False'
                    _data_d['bursting_enabled'] = 'True'
            #
            # Check if VCPUS adjustment is correct
            #
            if _cores > _maxvcpus and not _review:
                _detail_error = 'domU MAX_VCPUS (%d) is lower than new number of cores requested: %d' %(_maxvcpus,_cores)
                ebLogError('*** ' + _detail_error)
                _nodeU.mDisconnect()
                _node.mDisconnect()
                _mUpdateRequestData(_data_d)
                self.mUpdateErrorObject(gReshapeError['ERROR_MAX_CPU_LESS'],_detail_error)
                return ebError(0x0434)

            if _cores == _currvcpus and not _review:
                ebLogWarn('*** domU cores already matching cores numbers in resize request')
                _nodeU.mDisconnect()
                _node.mDisconnect()
                continue
            #
            # set cpu-bursting enabled or disabled
            #
            if _cpu_bursting is None or _cpu_bursting != 'False':
                if (_currmaxvcpus and _currmaxvcpus == _maxcores) or (_currmaxvcpus == 0 and _maxvcpus == _maxcores):
                    _cpu_bursting = 'True'
                else:
                    _cpu_bursting = 'False'
                ebLogInfo('*** setting cpu_bursting to : %s (_currmaxcvpus: %d/ _maxcores: %d/ _cfgmaxvcpus: %d)' % (_cpu_bursting, _currmaxvcpus, _maxcores, _maxvcpus))
            #
            # Display vCPUs change requested
            #
            ebLogInfo('*** domU (%s) current vCPUs: %s new vCPUS count: %s' % (_domU, _currvcpus, _cores))
            #
            # Adjust vcpu on DomU
            #

            if not _review and _cos:
                #
                # modify cluster with CPU oversubscription factor; do update the vcpu count as needed
                #
                if _cos:
                    _cluster_list=[]
                    _sum_vcpus = 0

                    _cmd = "ls /EXAVMIMAGES/GuestImages/*/vm.cfg | awk -F '/' '{print $4}'"
                    _, _out, _ = _node.mExecuteCmd(_cmd)
                    if _out:
                        _out = _out.readlines()
                        for _line in _out:
                            _cluster_list.append(_line.strip())

                    for _vm in _cluster_list:
                        _cmd = "grep ^vcpus /EXAVMIMAGES/GuestImages/"+_vm+"/vm.cfg |  awk -F '=' '{print $2}'"
                        _, _out, _ = _node.mExecuteCmd(_cmd)
                        _out = _out.readlines()
                        if _out:
                            # sum all vcpus except the domU whose _cores is getting updated.
                            if _vm not in _domU:
                                _sum_vcpus += int(_out[0])
                        else:
                            _detail_error = 'Can not retrieve VCPUS allocation for (%s)' % (_domU)
                            ebLogError('*** ' + _detail_error)
                            _node.mDisconnect()
                            self.mUpdateErrorObject(gReshapeError['ERROR_FETCHING_VCPUS_ALLOC'],_detail_error)
                            raise ExacloudRuntimeError(0x0780, 0xA, _detail_error)

                    # We don't need the range check since modify_service should have
                    # taken care of range check for all clusters.
                    # Verification of the sum of vcpus <= allocatbale cores, should suffice
                    # _cores is the new cores value for the domU

                    if (int(_allocatable_cores) - _sum_vcpus) < _cores:
                        _detail_error = 'Number of cores is more than the maximum allowed value for %s' % (_domU)
                        ebLogError('*** ' + _detail_error)
                        _node.mDisconnect()
                        self.mUpdateErrorObject(gReshapeError['INVALID_ALLOC_SIZE'], _detail_error)
                        raise ExacloudRuntimeError(0x0780, 0xA, _detail_error)
                    else:
                        #
                        # vm.cfg vCPUs change (require reboot)
                        #
                        _cfg.mSetValue('vcpus', str(_cores))
                        self.mSaveVMCfg(_node, _domU, _cfg.mRawConfig())

                        #
                        # Dynamic vCPUs change
                        #
                        if _is_pingable:

                            # Retry logic
                            _maxRetries = 3
                            _retries = 0
                            _updateSuccess = False

                            while _retries < _maxRetries:

                                _cmd = f'/opt/exadata_ovm/exadata.img.domu_maker vcpu-set {_domU} {_cores}'

                                if _retries != 0:
                                    _cmd = f"{_cmd} --force"

                                _node.mExecuteCmdLog(_cmd)

                                if _node.mGetCmdExitStatus() == 0:
                                    _updateSuccess = True
                                    break
                                else:
                                    time.sleep(5)

                                _retries += 1

                            if not _updateSuccess:
                                _msg = f"VCPU-SET not applied in {_domU} with {_cores}"
                                raise ExacloudRuntimeError(0x0436, 0xA, _msg)

                    #COS ends here

            if not _review and not _cos:

                # 1. Read dom0 CPU usage
                # 2. Determine poolsize using _vcpusinuse and dom0 CPU usage
                # 3. Do vcpu-set operation for the current domU
                # 4. Call mModifyService with subfactor as 1 and poolsize to do
                # vcpu pinning in both single and multi-vm cases

                if 'poolsize' in list(_jconf.keys()):
                    # Rack where COS is enabled
                    _poolsize = str(_jconf['poolsize'])
                    ebLogVerbose('*** Poolsize received is %s' %(_poolsize))
                else:
                    # Rack where COS is not enabled

                    # Total number of cpus used by dom0
                    _dom0vcpus = 0

                    _cmd = "xm li | grep Domain-0 | awk '{ print $4 }'"
                    _, _out, _ = _nodeU.mExecuteCmd(_cmd)
                    if _out:
                        _out = _out.readlines()
                        _dom0vcpus = int(_out[0])
                        ebLogInfo('*** DOM0 %s current vCPUS allocation: %d' % (_dom0, _dom0vcpus))
                    else:
                        _detail_error ='Can not retrieve DOM0 vCPUS allocation'
                        ebLogError('*** ' + _detail_error)
                        self.mUpdateErrorObject(gReshapeError['ERROR_FETCHING_VCPUS_ALLOC'], _detail_error)
                        _nodeU.mDisconnect()
                        raise ExacloudRuntimeError(0x0780, 0xA, _detail_error)

                    # Determine poolsize
                    _poolsize = str(_vcpusinuse + _cores - _currvcpus - _dom0vcpus)
                    ebLogVerbose('*** Poolsize calculated is %s' %(_poolsize))

                # Ensure subfactor is 1
                _subfactor = 1

                #
                # Dynamic vCPUs change
                #
                if _is_pingable:
                   _maxRetries = 3
                   _retries = 0
                   _updateSuccess = False

                   while _retries < _maxRetries:

                      _cmd = f'/opt/exadata_ovm/exadata.img.domu_maker vcpu-set {_domU} {_cores} --force'
                      _nodeU.mExecuteCmdLog(_cmd)

                      if _nodeU.mGetCmdExitStatus() == 0:
                         _updateSuccess = True
                         break
                      else:
                         time.sleep(5)

                      _retries += 1

                   if not _updateSuccess:
                      _msg = f"VCPU-SET not applied in {_domU} with {_cores}"
                      raise ExacloudRuntimeError(0x0436, 0xA, _msg)

                else:
                  ebLogWarn(f'*** skipping vcpu-set execution on {_domU} with {_cores} which is not pingable')

                #
                # vm.cfg vCPUs change (require reboot). NOTE: mavcpus is _NOT_ updated
                #
                _cfg.mSetValue('vcpus', str(_cores))
                self.mSaveVMCfg(_node, _domU, _cfg.mRawConfig())

                # Invoke mModifyService() to do pinning. apply force pinning after resize
                self.mModifyService(aOptions, _subfactor, _poolsize, _domU, True)

            #
            # Check if vCPUs have been updated effectively.
            #
            if not _review and _is_pingable:
                _total_time = 0
                _check_itv  = 5

                # Default timeout of 900 seconds (15min)
                while _total_time < self.__timeout_vmcpu_resize:
                    time.sleep(_check_itv)
                    _in, _o, _err = _node.mExecuteCmd("xm li %s | grep %s | awk '{ print $4 }'" % (_domU,_domU))
                    _out = _o.readlines()
                    if _out and len(_out):
                        _currvcpus = int(_out[0][:-1])
                    if _currvcpus == _cores:
                        break
                    _total_time += _check_itv

                if _currvcpus != _cores:
                    _detail_error = 'domU cores update not successful current: %d - requested: %d' % (_currvcpus,_cores)
                    ebLogError('*** ' + _detail_error)
                    _nodeU.mDisconnect()
                    _node.mDisconnect()
                    _mUpdateRequestData(_data_d)
                    self.mUpdateErrorObject(gReshapeError['ERROR_VM_RESIZE_UPDATE_FAIL'], _detail_error)
                    return ebError(0x0430)
                else:
                    ebLogInfo('*** domU cores update successfull.')

            _nodeU.mDisconnect()

            _node.mDisconnect()

        if _partial_update:
            _mUpdateRequestData(_data_d)
            _dbNodeData = []
            for _nodeData in _dom0s_offline:
                _ndata = {}
                _ndata['hostname'] = _nodeData.strip()
                _dbNodeData.append(_ndata)
            _error_str = 'CPU resize failed to update all nodes as following Dom0s are not pingable %s'%(str(_dom0s_offline))
            ebLogError('*** ' + _error_str)
            self.mUpdateErrorObject(gPartialError['ERROR_VM_CPU_RESIZE_PARTIAL'], _error_str, _dbNodeData)
            raise ExacloudRuntimeError(0x0436, 0xA, _error_str)

        #
        # update _data_d and flush request.data field to DB
        #
        if _cpu_bursting is not None and _cpu_bursting == 'True':
            _data_d['bursting_ready'] = 'True'
        _mUpdateRequestData(_data_d)
        _stepSpecificDetails = _clu_utils.mStepSpecificDetails("reshapeDetails", 'DONE', "CPU Reshape completed", "","CPU")
        _clu_utils.mUpdateTaskProgressStatus(_lastNode, 100, "CPU Reshape", "DONE", _stepSpecificDetails)

        return 0

    def mManageVMMemory(self, aVMCmd, aVMId, aOptions=None):

        _vmcmd = aVMCmd
        _vmid = aVMId
        _options = aOptions
        _jconf = None
        _data_d = {}
        _data_d['version'] = '1.0'
        _data_d['comment'] = 'vmemory data info and status'
        _data_d['vms'] = {}
        _host_d = {}
        _detail_error = ''
        _min_hugepage_mem = ''
        _hugepage_value = None
        _clu_utils = ebCluUtils(self)
        ebLogVerbose("mManageVMMemory: aVMCmd = %s, aVMId = %s" % (aVMCmd, aVMId))
        _stepSpecificDetails = _clu_utils.mStepSpecificDetails("reshapeDetails", 'ONGOING', "VM memory reshape in progress", "","Memory")
        _clu_utils.mUpdateTaskProgressStatus([], 0, "VM Memory Reshape", "In Progress", _stepSpecificDetails)
        _lastNode = []

        def _mUpdateRequestData(aDataD):
            _data_d = aDataD
            _reqobj = self.mGetRequestObj()
            if _reqobj is not None:
                _reqobj.mSetData(json.dumps(_data_d))
                _db = ebGetDefaultDB()
                _db.mUpdateRequest(_reqobj)

        if _options is not None:
            _jconf = aOptions.jsonconf
        #
        # Payload (json) is mandatory
        #
        if _jconf is None:
            _detail_error = 'json payload for VMMemorySet command: %s not provided.' % (_vmcmd) 
            ebLogError('*** '+ _detail_error)
            _mUpdateRequestData(_data_d)
            self.mUpdateErrorObject(gReshapeError['INVALID_INPUT_PARAMETER'],_detail_error)
            return ebError(0x0445)
        #
        # Fetch new value for memory
        #
        try:

            if 'vms' in _jconf.keys():
                for _h in _jconf['vms']:
                    _host_d[_h['hostname']] = int(_h['gb_memory'])
            elif 'gb_memory' in _jconf.keys():
                for _, _h in self.mReturnDom0DomUPair():
                    _host_d[_h] = int(_jconf['gb_memory'])
            elif "exaunitAllocations" in _jconf.keys():   
                for _, _domU in self.mReturnDom0DomUPair():  
                    _host_d[_domU]= int(_jconf['exaunitAllocations']['memoryGb'])
            else:
                _detail_error = 'json payload for VMMemorySet command: %s does not have value for gb_memory.' % (_vmcmd)
                ebLogError('*** '+ _detail_error)
                _mUpdateRequestData(_data_d)
                self.mUpdateErrorObject(gReshapeError['INVALID_INPUT_PARAMETER'],_detail_error)
                return ebError(0x0446)
        except Exception as e:
            _detail_error = 'CC:mManageVMMemory Exception:: %s - %s' % (e.__class__, e)
            ebLogError('*** '+ _detail_error)
            _mUpdateRequestData(_data_d)
            self.mUpdateErrorObject(gReshapeError['INVALID_INPUT_PARAMETER'], _detail_error)
            return ebError(0x0447)

        #Precheck for Domu ping, bug 36202855
        for _dom0, _domU in self.mReturnDom0DomUPair():
           if not self.mPingHost(_domU):
              _detail_error = '*** DomU (%s) is not pingable.' % (_domU)
              ebLogError(_detail_error)
              self.mUpdateErrorObject(gReshapeError['ERROR_PING_FAILURE'], _detail_error)
              return ebError(0x0800)

        # Let's fetch current memory and check new mem size is valid or not
        ebLogInfo('*** check new mem size is valid or not')
        for _dom0, _domU in self.mReturnDom0DomUPair():

            if _domU not in _host_d.keys():
                continue

            # Get _memsize value of the domU from the payload.
            _memsizeGB = _host_d[_domU]
            _memsizeMB = _host_d[_domU] * 1024

            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            if not self.mIsKVM():
                _vmhandle = exaBoxOVMCtrl(aCtx=get_gcontext(), aNode=_node)
                _vmhandle.mReadRemoteCfg(_domU)
                _cfg = _vmhandle.mGetOVSVMConfig(_domU)
                if _cfg is None:
                    _node.mDisconnect()
                    _detail_error = 'DomU (%s) configuration is not available' % (_domU)
                    ebLogError('*** '+ _detail_error)
                    self.mUpdateErrorObject(gReshapeError['ERROR_IMAGE_CFG_MISSING'], _detail_error)
                    raise ExacloudRuntimeError(0x0448, 0xA, _detail_error)
                # _currvmem will be in MB
                _currvmem = int(_cfg.mGetValue('memory').strip('\' '))  # # Memory values are saved within single (') quotes in cfg
            else:
                _hv = getHVInstance(_dom0)
                _currvmem = _hv.mGetVMMemory(_domU, 'CUR_MEM')
            _first_node_memreshaped = False
            ebLogInfo('*** current memory size is :%s new mem size is :%s' % (_currvmem,_memsizeMB))
            if _memsizeMB != _currvmem and not self.mIsExaScale() and self.mPingHost(_domU):
                # Now, let's call dbasapi to check whether new memory size is allowed or not
                _dbaasobj = ebCluDbaas(self, _options)
                _dbaasData = {}
                _params = {}
                _params["dbname"] = "grid"
                _params["new_mem_size"] = str(_memsizeGB) + " GB"
                _uuid = self.mGetUUID()
                _params["infofile"] = "/var/opt/oracle/log/get" + "_" + _uuid + "_infofile.out"
                _dbaasobj.mExecuteDBaaSAPIAction("get", "precheck_memory_resize", _dbaasData, _domU, _params, _options)
                _stjson = _dbaasData["get"]
                _min_hugepage_mem = _stjson["min_reqd_hugepages_memory"]
                if int(_stjson["is_new_mem_sz_allowed"]) == 0:
                    _error_list = _stjson["error"]
                    _detail_error = ','.join(_error_list)
                    ebLogError('*** '+ _detail_error)
                    _data_d['vms'] = _stjson
                    _mUpdateRequestData(_data_d)
                    _node.mDisconnect()
                    self.mUpdateErrorObject(gReshapeError['ERROR_RESHAPE_PRECHECK'], _detail_error)
                    raise ExacloudRuntimeError(0x0450, 0xA, _detail_error)
                _node.mDisconnect()
                break # We need to calculate for 1 node only
            #If first node is reshaped, reuse its hugepages for other nodes
            elif _memsizeMB == _currvmem and not self.mIsExaScale() and self.mPingHost(_domU):
                _first_node_memreshaped = True                
                with connect_to_host(_domU, get_gcontext()) as _nodeU:
                    _, _hugepage_value = self.mGetSysCtlConfigValue(_nodeU, 'vm.nr_hugepages', aSkipValidation=True)
                    if _hugepage_value is None:
                        ebLogError(f"*** Could not get nr_hugepages value from node: {_domU}***".format(_domU))
                        raise ExacloudRuntimeError(0x0743, 0xA, f"Error while getting nr_hugepages settings from node: {_domU}")
                _node.mDisconnect()
                break

        _percentage_increase = 20.0    
        _stepSpecificDetails = _clu_utils.mStepSpecificDetails("reshapeDetails", 'ONGOING', "VM memory reshape in progress", "","Memory")
        _percentageStepSize= 79.0/len(self.mReturnDom0DomUPair())
        for _dom0, _domU in self.mReturnDom0DomUPair():
            _clu_utils.mUpdateTaskProgressStatus(_lastNode, _percentage_increase, "VM Memory Reshape", "In Progress", _stepSpecificDetails)
            _percentage_increase = _percentage_increase + _percentageStepSize
            _lastNode.append(_domU) 
            if _vmid != '_all_' and _vmid != _domU:
                continue

            if _domU not in _host_d.keys():
                continue

            # Get _memsize value of the domU from the payload.
            _memsizeGB = _host_d[_domU]
            _memsizeMB = _host_d[_domU] * 1024

            #
            # Initialize _data_d
            #
            _data_d['vms'][_domU] = {}
            _hv = getHVInstance(_dom0)
            _dat = _hv.getDom0FreeMem()
            ebLogInfo('*** Available Memory on dom0 %s : %s' % (_dom0, _dat))
            _free_mem_dom0 = int(_dat)

            #
            #    Our standard config is to have VM memory and maxmem configs same value
            #    This prohibits hot addition of the memory to VMs
            #    Due to this reason, vm.cfg will be modified
            #    If VM is down, we shall leave it in same state, else stop/start it
            #
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            if not self.mIsKVM():
                _vmhandle = exaBoxOVMCtrl(aCtx=get_gcontext(), aNode=_node)
                _vmhandle.mReadRemoteCfg(_domU)
                _cfg = _vmhandle.mGetOVSVMConfig(_domU)
                if _cfg is None:
                    _node.mDisconnect()
                    _detail_error = 'DomU (%s) configuration is not available' % (_domU)
                    ebLogError('*** '+ _detail_error)
                    self.mUpdateErrorObject(gReshapeError['ERROR_IMAGE_CFG_MISSING'], _detail_error)
                    _mUpdateRequestData(_data_d)
                    return ebError(0x0448)

            if not self.mPingHost(_domU):
                ebLogWarn('*** DomU (%s) is not pingable.' % (_domU))
                _is_pingable = False
            else:
                _is_pingable = True
            #
            # Fetch current memory for DomU.
            #
            # In KVM it is possible to read the VM's memory configuration even
            # if it is down (see ebKvmVmMgr.mGetVMMemory()) whereas in Xen the
            # VM must be up (see ebXenVmMgr.mGetVMMemory()).
            #
            if self.mIsKVM() or _is_pingable:
                _currvmem = _hv.mGetVMMemory(_domU, 'CUR_MEM')
                _currmaxvmem = _hv.mGetVMMemory(_domU, 'MAX_MEM')
            else:
                # Xen Hypervisor and DomU not pingable.  Read memory size from
                # the configuration (it is wrapped in single quotes and with
                # trailing whitespace).
                _currvmem = int(_cfg.mGetValue('memory').strip("' "))
                _currmaxvmem = _currvmem

            ebLogInfo('*** CURRENT_VMEM for domU {}: {}'.format(_domU, _currvmem))
            ebLogInfo('*** CURRENT_MAXVMEM for domU {}: {}'.format(_domU, _currmaxvmem))

            #
            # Fetch current CRS stack status
            #
            if _is_pingable and not self.mIsExaScale():
                _crs_prestatus = self.mCheckCrsUp(_domU)
                _db_prestatus = self.mCheckDBIsUp(_domU)

            #
            # _data_d update
            #
            _data_d['vms'][_domU]['_currvmem'] = _currvmem
            _data_d['vms'][_domU]['_currmaxvmem'] = _currmaxvmem

            # Skip further processing if configured memory is 0
            if _currvmem == 0:
                ebLogWarn('*** Skipping DomU {} with CURRENT_VMEM = 0'.format(_domU))
                continue

            #
            # Skip without doing anything if new memory value is same as current config
            #
            if _memsizeMB == _currvmem:
                #check if DomU is pingeable and only then proceed to check CRS and DB status
                if not _is_pingable:
                    ebLogInfo(f"*** Not Pingable; Moving to next DOMU: Current config - {_currvmem}; Requested config - {_memsizeMB}.")
                    continue
                if self.mIsExaScale():
                    ebLogInfo('*** Doing nothing; Moving to next DOMU: Current config - ' +
                              '%d; Requested config - %d.' % (_currvmem, _memsizeMB))
                    continue
                #check if crs and dbs are up
                if self.mCheckIfCrsDbsUp(_domU):
                    ebLogInfo('*** Doing nothing; Moving to next DOMU: Current config - ' +
                              '%d; Requested config - %d.' % (_currvmem, _memsizeMB))
                    continue
            #
            # Skip without doing anything if new memory value is 2% more/less than current config
            #
            _diff_inMB = abs(_memsizeMB - _currvmem)
            _percentage = 100 * float(_diff_inMB)/float(_currvmem)
            if _percentage < 2:
                #check if DomU is pingeable
                if not _is_pingable:
                    ebLogInfo(f"*** Not Pingable; Moving to next DOMU: Current config - {_currvmem}; Requested config - {_memsizeMB}.")
                    continue
                if self.mIsExaScale():
                    ebLogInfo('*** Doing nothing; Moving to next DOMU: Current config - ' +
                              '%d; Requested config - %d.' % (_currvmem, _memsizeMB))
                    continue
                #check if crs and dbs are up
                if self.mCheckIfCrsDbsUp(_domU):       
                    ebLogInfo('*** Doing nothing; Moving to next DOMU: Current config - ' +
                              '%d; Requested config - %d. Size difference less then 2 Percent' % (_currvmem, _memsizeMB))
                    continue

            #
            # Additional memory required on the VM should not me more than what is available on the hypervisor
            #
            if _memsizeMB > (_free_mem_dom0 + _currvmem):
                _node.mDisconnect()
                _detail_error = 'Not enough memory to hypervisor to perform the operation: Current config - ' + '%d; Requested config - %d; Available memory - %d' % (_currvmem, _memsizeMB, _free_mem_dom0)
                ebLogError('*** '+ _detail_error)
                self.mUpdateErrorObject(gReshapeError['INVALID_SIZE_PROVIDED'], _detail_error)
                _mUpdateRequestData(_data_d)
                return ebError(0x0450)
            
            if not self.isBaseDB() and not self.isExacomputeVM() and _is_pingable and not self.isATPCluster(_domU):
                # Update the parameter value of vm.nr_hugepages in sysctl.conf
                if not _first_node_memreshaped:
                    _rc = self.mUpdateHugePagesSysctlConf(_domU, _currvmem, _memsizeMB, _min_hugepage_mem)
                else:
                    with connect_to_host(_domU, get_gcontext()) as _nodeU:
                        _rc = 0 if self.mSetSysCtlConfigValue(_nodeU, "vm.nr_hugepages", _hugepage_value, aRaiseException=False, aInstantApply=False, aValidate=False) else -1
                if _rc != 0:
                    _detail_error = f'Failed to set Hugepage Value. Failure during sysctl.conf  vm.nr_hugepages update in {_domU}'
                    ebLogError('*** ' + _detail_error)
                    self.mUpdateErrorObject(gReshapeError['ERROR_HUGEPAGE_UPDATE_FAIL'], _detail_error)
                    _mUpdateRequestData(_data_d)
                    return ebError(0x0442)

            _memsizeMB_str = "'%d'" % (_memsizeMB)

            if not self.mIsKVM():
                ebLogInfo('*** Memory for %s set to %s' % (_domU, str(_memsizeMB)))
                _cfg.mSetValue('memory', _memsizeMB_str)

                ebLogInfo('*** Max Memory for %s set to %s' % (_domU, str(_memsizeMB)))
                _cfg.mSetValue('maxmem', _memsizeMB_str)

                self.mSaveVMCfg(_node, _domU, _cfg.mRawConfig())
            else:

                # Grab a lock on this dom0
                # Lock is only grabbed if check shared env is set!
                _dom0_lock = RemoteLock(self,
                        force_host_list=[_dom0])

                with _dom0_lock():
                    _memsizeMB_str = _memsizeMB_str + 'M'
                    _rc = _hv.mSetVMMemory(_domU, _memsizeMB_str)

                if _rc != 0:
                    _currvmem = _hv.mGetVMMemory(_domU, 'CUR_MEM')
                    _detail_error = 'domU memory update failed current: %d - requested: %d' % (_currvmem, _memsizeMB)
                    ebLogError('*** '+ _detail_error)
                    _node.mDisconnect()
                    _mUpdateRequestData(_data_d)
                    self.mUpdateErrorObject(gReshapeError['ERROR_VM_MEMORY_RESIZE_MISMATCH'], _detail_error)
                    return ebError(0x0451)
                        

            #
            # Bounce the DomU if it was up; else leave for the config to take effect when it it booted next time
            #
            if _is_pingable:
                if self.mIsExaScale():
                    # Shutdown and Restart Exascale VM 
                    self.mSingleDomURestart(_dom0,_domU,aOptions)
                else:
                    _crs_status, _db_status = self.mShutdownVMForReshape(_dom0,_domU,aOptions,_node)
                    self.mStartVMAfterReshape(_dom0,_domU, aOptions, _crs_status, _db_status, _node)

                #
                # Check if Memory has been updated effectively.
                #
                _nodeU = exaBoxNode(get_gcontext())
                _nodeU.mConnect(aHost=_dom0)
                _currvmem = _hv.mGetVMMemory(_domU, 'CUR_MEM')
                ebLogInfo('*** CURRENT_VMEM for domU %s : %d' % (_domU, _currvmem))
                _nodeU.mDisconnect()
                if _currvmem == _memsizeMB:
                    _node.mDisconnect()
                    continue

                #
                # Putting the post reshape check under 'False' block to avoid Bug 27196481
                # The issue is due to inherent xen bug causing an offset of 3 MB
                # We should restore the code post resolution of xen bug
                #
                if False:
                    if _currvmem != _memsizeMB:
                        _detail_error = 'domU memory update failed current: %d - requested: %d' % (_currvmem, _memsizeMB)
                        ebLogError('*** '+ _detail_error)
                        _node.mDisconnect()
                        _mUpdateRequestData(_data_d)
                        self.mUpdateErrorObject(gReshapeError['ERROR_VM_MEMORY_RESIZE_MISMATCH'], _detail_error)
                        return ebError(0x0451)
                    else:
                        ebLogInfo('*** domU memory update successful on host %s.' % (_domU))

                ebLogInfo('*** domU memory update successful on host %s.' % (_domU))

            _node.mDisconnect()

        _mUpdateRequestData(_data_d)
        if self.isBaseDB() or self.isExacomputeVM() or self.mIsXS() or self.mIsExaScale():
            return 0

        # Grid disk check removed from Memory reshape as part of 38317248
        if self.mCheckSubConfigOption('reshape_memory', 'offline_griddisk_check') and self.mCheckSubConfigOption('reshape_memory', 'offline_griddisk_check').lower() == "true":
            _check = ebCluStorageReshapePrecheck(self)
            _rc = _check.mGetOfflineCellDisks()
            _count = 0
            _timeout = int(self.mCheckConfigOption('disk_online_timeout'))
            while _rc != 0 and _count < _timeout:
                ebLogInfo('*** Waiting for all griddisks to be online')
                time.sleep(60)
                _rc = _check.mGetOfflineCellDisks()
                _count = _count + 1

            if _rc != 0:
                _detail_error = 'Griddisks failed to come online as part of memory reshape'
                ebLogError(f'*** {_detail_error}')
                raise ExacloudRuntimeError(0x0808, 0xA, _detail_error)

        _stepSpecificDetails = _clu_utils.mStepSpecificDetails("reshapeDetails", 'DONE', "VM memory reshape completed", "","Memory")
        _clu_utils.mUpdateTaskProgressStatus(_lastNode, 100, "VM Memory Reshape", "DONE", _stepSpecificDetails)
        return 0

    ##Use DBAASTOOL to determine if VM is ATP or not. The difference between isATP() and this function is that
    ## In isATP() we rely on data send as part of payload to mark if the cluster is ATP/ADBD
    ## Here we rely on the info got from the domU by running DBAASTOOL command.
    def isATPCluster(self,aDomU):        
        _node = exaBoxNode(get_gcontext())
        _node.mConnect(aHost=aDomU)
        _,_o,_ = _node.mExecuteCmd('/var/opt/oracle/ocde/rops atp_enabled')
        _rc = (_o and '1' in _o.read().strip()) # is ATP
        _node.mDisconnect()
        return _rc

    def mSingleDomURestart(self,aDom0, aDomU, aOptions):
        # Shutdown VM 
        _node0 = exaBoxNode(get_gcontext())
        _node0.mConnect(aHost=aDom0)
        _vmhandle = self.__CompRegistry.mGetComponent("vm_operations")
        _vmhandle.mSetOVMCtrl(aCtx=get_gcontext(), aNode=_node0)
        _domUcmd = 'shutdown'
        ebLogInfo('*** Shutting down %s' % (aDomU))
        _rc = _vmhandle.mDispatchEvent(_domUcmd, aOptions, aVMId=aDomU, aCluCtrlObj=self)
        if _rc not in [0, 0x0411, 0x0454]:
            _node0.mDisconnect()
            _detail_error = 'vmcmd: %s and vmid: %s - Could not be shut down' % (_domUcmd, aDomU)
            ebLogError('*** FATAL :: ' + _detail_error)
            self.mUpdateErrorObject(gReshapeError['ERROR_SHUTDOWN_FAILED'],_detail_error)
            raise ExacloudRuntimeError(0x0451, 0xA, _detail_error)
        elif _rc == 0x0454:
            _node0.mDisconnect()
            _detail_error = "Failed to disable AutoStart Params status for %s"%(aDomU)
            ebLogError('*** ' + _detail_error)
            self.mUpdateErrorObject(gReshapeError['ERROR_AUTOSTART_PARAM_SETTING'], _detail_error)
            raise ExacloudRuntimeError(0x0454, 0xA, 'VM failed to disable autostart params')
        
        # ReStart VM 
        if self.mIsOciEXACC() and self.mIsKVM() and luksCharchannelExistsInDom0(aDom0, aDomU):
            mSetLuksPassphraseOnDom0Exacc(self, aDom0,aDomU)
        _rc = self.mRestartVM(aDomU, aVMHandle=_vmhandle)
        if _rc != 0:
            _detail_error = 'vmid: %s - Could not be started' % (aDomU)
            ebLogError('*** FATAL :: ' + _detail_error)
            self.mUpdateErrorObject(gReshapeError['ERROR_SHUTDOWN_FAILED'],_detail_error)
            raise ExacloudRuntimeError(0x0451, 0xA, _detail_error)

        ebLogInfo('*** Waiting for machine %s to be pingable' % (aDomU))
        _total_time = 0
        _check_itv = 5
        _domU_pingable = False

        while _total_time < self.__timeout_vm_boot:
            if self.mPingHost(aDomU):
                _domU_pingable = True
                break
            _total_time += _check_itv

        if not _domU_pingable:
            _detail_error = 'vmid: %s - did not come up after boot' % (aDomU)
            ebLogError('*** FATAL :: ' + _detail_error)
            self.mUpdateErrorObject(gReshapeError['ERROR_SHUTDOWN_FAILED'],_detail_error)
            raise ExacloudRuntimeError(0x0451, 0xA, _detail_error)

    def mCheckIfCrsDbsUp(self, aDomU):
        #check if crs and dbs are up and all db instances which were running before shutdown are running
        _clu_utils = ebCluUtils(self)
        if not self.mCheckCrsUp(aDomU):
            _detail_error = "vmid: %s - CRS stack did not start after boot"%(aDomU)
            ebLogError('***  ' + _detail_error)
            self.mUpdateErrorObject(gReshapeError['ERROR_CRS_START'],_detail_error)
            raise ExacloudRuntimeError(0x0451, 0xA, _detail_error)
        if not self.mCheckDBIsUp(aDomU):
            _detail_error = "vmid: %s - Database did not start after boot"%(aDomU)
            _dbs_not_up = _clu_utils.getNotUpDbsList(aDomU)
            if _dbs_not_up:
                _detail_error = "vmid: %s - Database did not start after boot. DBs not up list : %s" % (aDomU, _dbs_not_up) 
            ebLogError('***  ' + _detail_error)
            self.mUpdateErrorObject(gReshapeError['ERROR_DB_START'],_detail_error)
            raise ExacloudRuntimeError(0x0451, 0xA, _detail_error)

        _db = ebGetDefaultDB()
        _dbs_not_up = _clu_utils.getNotUpDbsList(aDomU)
        if _dbs_not_up:
            _detail_error = " DBs instances are not running in %s for DBs %s"%(aDomU,_dbs_not_up)
            self.mUpdateErrorObject(gReshapeError['ERROR_DBS_NOT_RUNNING'],_detail_error)
            ebLogError('*** ' + _detail_error)
            ebLogError("*** ALL Database are not up after booting node %s. not proceeding further" % (aDomU))
            raise ExacloudRuntimeError(0x0437, 0xA, _detail_error)
        else:
            _db.mRemoveDBListByNode(aDomU)
        return True
    
    def mRestartDBInstance(self, aDomU, aDbInstList):
        """
            Restart database Instances for domU for each dbinstance
        """
        _node = exaBoxNode(get_gcontext())
        _node.mSetUser('root')
        _node.mConnect(aHost=aDomU)
        _path = None
        if self.IsZdlraProv():
            _path, _sid = self.mGetGridHome(aDomU)
            _cmd_prefix = 'ORACLE_HOME=%s;export ORACLE_HOME;ORACLE_SID=%s; export ORACLE_SID;PATH=$PATH:$ORACLE_HOME/bin;export PATH;' % (_path,_sid)
            _cmd = _cmd_prefix + '/usr/bin/su grid -c "$ORACLE_HOME/bin/srvctl config database"'
        else:
            _path, _, _  = self.mGetOracleBaseDirectories(aDomU)
            _cmd_pfx = 'ORACLE_HOME=%s;export ORACLE_HOME;' % (_path)
            _cmd = _cmd_pfx + "$ORACLE_HOME/bin/srvctl config database"
        _i, _o, _e = _node.mExecuteCmd(_cmd)
        _out = _o.readlines()
        _rc = _node.mGetCmdExitStatus()
        if not _out or len(_out) == 0 or _rc:
            ebLogError('*** Command %s execution failed with return code :%s'%(_cmd, _rc))
            _node.mDisconnect()
            return []
        ebLogInfo('*** DB name list of all Dbs:  %s ' % (_out))
        _dbName = _out
        for _dbname in _dbName:
            _dbname = _dbname.strip()
            _oh, _, _= self.mGetOracleBaseDirectories(aDomU, _dbname)
            for _dbinstance in aDbInstList:
                if _dbinstance[:-1] in _dbname:
                    ebLogInfo(f"*** Starting instance {_dbinstance} of DB {_dbname} on node {aDomU}")
                    _cmd_oh = '/usr/bin/su grid -c "export ORACLE_HOME=%s; %s/bin/srvctl start instance -d %s -i %s "'%(_oh,_oh,_dbname,_dbinstance)
                    _node.mExecuteCmdLog(_cmd_oh)
                    break
            
    def mGetActiveDbInstances(self, aDomU):
        """
            Return Dict of database Instances for domU for each db
        """
        _node = exaBoxNode(get_gcontext())
        _node.mSetUser('root')
        _node.mConnect(aHost=aDomU)
        _path = None
        if self.IsZdlraProv():
            _path, _sid = self.mGetGridHome(aDomU)
            _cmd_prefix = 'ORACLE_HOME=%s;export ORACLE_HOME;ORACLE_SID=%s; export ORACLE_SID;PATH=$PATH:$ORACLE_HOME/bin;export PATH;' % (_path,_sid)
            _cmd = _cmd_prefix + '/usr/bin/su grid -c "$ORACLE_HOME/bin/srvctl config database"'
        else:
            _path, _, _  = self.mGetOracleBaseDirectories(aDomU)
            _cmd_pfx = 'ORACLE_HOME=%s;export ORACLE_HOME;' % (_path)
            _cmd = _cmd_pfx + "$ORACLE_HOME/bin/srvctl config database"
        ebLogInfo(f'command to be executed:{_cmd}')
        _i, _o, _e = _node.mExecuteCmd(_cmd)
        _out = _o.readlines()
        _rc = _node.mGetCmdExitStatus()
        if not _out or len(_out) == 0 or _rc:
            ebLogError('*** Command %s execution failed with return code :%s'%(_cmd, _rc))
            _node.mDisconnect()
            return []
        ebLogInfo('*** DB name list of all Dbs:  %s ' % (_out))
        _dbName = _out
        _db_instances=[]
        _out_autorestart = []

        for _dbname in _dbName:
            _dbname = _dbname.strip()
            _oh, _, _= self.mGetOracleBaseDirectories(aDomU, _dbname)
            if _oh == "":
                ebLogWarn('*** Skip getting DB running instances for DB %s as oracle home path is not found'%(_dbname))
                continue
            _cmd_oh = '/usr/bin/su grid -c "export ORACLE_HOME=%s; %s/bin/srvctl status database -d %s | /usr/bin/grep %s "'%(_oh,_oh,_dbname,aDomU.split('.')[0])
            _i, _o, _e = _node.mExecuteCmd(_cmd_oh)
            _out = _o.readlines()
            ebLogInfo(f"*** Output of command {_cmd_oh}: \n{''.join(_out).strip()}\nError: {str(_e.readlines())}")
            if _out and 'not running' not in str(_out):
                _cmd_autorestart = (
                    "/usr/bin/su grid -c \"export ORACLE_HOME=%s; "
                    "%s/bin/srvctl config database -d %s | "
                    "awk -F': ' '/Management policy/ {print \\$2}'\""
                    % (_oh, _oh, _dbname)
                )
                _i, _o, _e = _node.mExecuteCmd(_cmd_autorestart)
                _out_status = _o.readlines()
                ebLogInfo(f'*** _out_status is {_out_status} ')
                if any('AUTOMATIC' in status for status in _out_status):
                    _out_autorestart = _out[0].strip().split(' ')
                    if len(_out_autorestart) > 1:
                        _db_instances.append(_out_autorestart[1])
                else:
                    ebLogInfo(f'*** Skipping adding db instance of {_dbname} for post restart check')
        ebLogInfo('*** DB running instances:  %s ' % (_db_instances))
        return _db_instances


    def mShutdownVMForReshape(self,aDom0, aDomU, aOptions,aNode):
        _db_List = self.mGetActiveDbInstances(aDomU)
        _db = ebGetDefaultDB()
        _dbList_str = ' '.join([str(_dbs) for _dbs in _db_List])
        _db.mSetDBlist(aDomU,_dbList_str)
        _crs_prestatus = self.mCheckCrsUp(aDomU)
        _db_prestatus = self.mCheckDBIsUp(aDomU)
        shutdown_domu(aNode, aDomU, force_on_timeout=True)
        self.mCopyVMxmltoGcvBackup(aNode, aDomU)

        return _crs_prestatus, _db_prestatus

    def mStartVMAfterReshape(self, aDom0, aDomU, aOptions, aCRS, aDB , aNode):

        _wait_condition = lambda domu_node:\
            (self.mCheckCrsUp(domu_node) if aCRS else True) and\
            (self.mCheckDBIsUp(domu_node) if aDB else True)

        if self.mIsOciEXACC() and self.mIsKVM() and luksCharchannelExistsInDom0(aDom0, aDomU):
            mSetLuksPassphraseOnDom0Exacc(self, aDom0, aDomU)

        try:
            start_domu(aNode, aDomU, wait_condition=_wait_condition)
            if not _wait_condition(aDomU):
                _crs_cmd_pfx, _, _ = self.mGetOracleBaseDirectories(aDomU = aDomU)
                with connect_to_host(aDomU, get_gcontext()) as _node:
                    _crs_cmd_pfx += '/bin/crsctl '
                    _node.mExecuteCmdLog(_crs_cmd_pfx + 'start crs')
                self.mCheckIfCrsDbsUp(aDomU) 
        except ExacloudRuntimeError as e:
            if e.mGetErrorCode() == 0x10 and e.mGetErrorType() == 0xA:
                self.mCheckIfCrsDbsUp(aDomU)
            else:
                raise

        if aDB:
            _db = ebGetDefaultDB()
            _dbs_up = _db.mGetDBListByNode(aDomU)
            if _dbs_up !='':
                _dbActiveList = self.mGetActiveDbInstances(aDomU)
                _dbList = _dbs_up.split(" ")
                ebLogInfo('*** the dbs active before last reboot are %s'%(_dbList))
                ebLogInfo('*** the dbs active now are %s'%(_dbActiveList))
                if not set(_dbList).issubset(set(_dbActiveList)):
                    _failed_dbList = list(set(_dbList) - set(_dbActiveList))
                    self.mRestartDBInstance(aDomU, _failed_dbList)
                    _dbActiveList = self.mGetActiveDbInstances(aDomU)
                    if not set(_dbList).issubset(set(_dbActiveList)):
                        ebLogInfo("*** ALL Database are up after booting %s" % (aDomU))
                        _db.mRemoveDBListByNode(aDomU)
                        return
                    _failed_dbList = list(set(_dbList) - set(_dbActiveList))
                    _detail_error = " DBs instances are not running in %s for DBs %s"%(aDomU,_failed_dbList)
                    self.mUpdateErrorObject(gReshapeError['ERROR_DBS_NOT_RUNNING'],_detail_error)
                    ebLogError('*** ' + _detail_error)
                    ebLogError("*** ALL Database are not up after booting node %s. not proceeding further" % (aDomU))
                    raise ExacloudRuntimeError(0x0437, 0xA, _detail_error)
                else:
                    ebLogInfo("*** ALL Database are up after booting %s" % (aDomU))
                    _db.mRemoveDBListByNode(aDomU)
            else:
                ebLogInfo("*** No Database up at the time of reboot. No check needed")

    def mGetUserkey(self, aOptions):
        _ssh_public_key = None
        _ssh_key = None
        if aOptions and aOptions.jsonconf:
            if "vm" in list(aOptions.jsonconf.keys()):
                _vm = aOptions.jsonconf['vm']
                if "sshkey" in list(_vm.keys()):
                    _ssh_public_key = _vm.get("sshkey")

        if _ssh_public_key:
            if not check_string_base64(_ssh_public_key):
                _err = ("sshkey given on payload is empty or invalid (base64 encoding)")
                ebLogError(_err)
                raise ExacloudRuntimeError(0x0823, 0xA, _err)
            else:
                _ssh_key = base64.b64decode(_ssh_public_key).decode()

        return _ssh_key

    def mValidatesshkey(self, aSshKey):                                                                                                                                         
        _ssh_key = aSshKey
        try:
            _ssh_key_algo, _ssh_key_content = _ssh_key.split()[:2]
            _ssh_key_decoded = decodestring(six.ensure_binary(_ssh_key_content))

            if _ssh_key_algo == "ssh-rsa":
                key_bytes = unpack('>I', _ssh_key_decoded[:4])[0]
                assert(key_bytes == 7)
                assert(_ssh_key_decoded[4:4+key_bytes] == six.ensure_binary(_ssh_key_algo))
        except Exception as e:
            ebLogError ("*** Error *** Ssh key validation failure. Exception: %s"  % (e))
            raise ExacloudRuntimeError(0x0406, 0xA, "VM_CMD SSH Key operation failed. SSH Key not provided or incorrect", aStackTrace=False)

        ebLogInfo('*** DONE Validating SSH KEY')

    def mManageVMSSHKeys(self, aVMCmd, aVMId, aOptions=None, aKey=None, aMode=True):
        ebLogVerbose("mManageVMSSHKeys: aVMCmd = %s, aVMId = %s, aKey = %s, aMode = %s" % (aVMCmd, aVMId, str(aKey), aMode))

        # Set the mManageVMSSHKeys as a non ssh_post_fix
        get_gcontext().mSetRegEntry('ssh_post_fix', 'False')
        _ssh_key = None

        # Use SSH Key passed as argument if any
        if aKey:
            _ssh_key = aKey

        # Fetch SSH key if provided in the json payload under the 'vm' section
        elif aOptions and aOptions.jsonconf:
            _ssh_public_key = aOptions.jsonconf.get("vm", {}).get("sshkey", False)

            # If sshkey is present under 'vm' section, make sure
            # is base64 encoded, otherwise raise exception
            if _ssh_public_key:

                if not check_string_base64(_ssh_public_key):
                    _err = ("sshkey given on payload is empty or invalid (base64 encoding)")
                    ebLogError(_err)
                    raise ExacloudRuntimeError(0x0823, 0xA, _err)
                else:
                    ebLogTrace("mManageVMSSHKeys - Exacloud using sshkey on payload under 'vm' field")
                    _ssh_key = base64.b64decode(_ssh_public_key).decode()

            # If not found under 'vm' section, check if present under legacy 'sshkey' section
            elif "sshkey" in aOptions.jsonconf:
                ebLogTrace("mManageVMSSHKeys - Exacloud using sshkey on payload under legacy 'ssheky' section")
                _ssh_key = aOptions.jsonconf['sshkey']

        # If SSH Key is not provided, use value from memory. This value
        # could be None at this point, below this check is done so that None
        # is only valid if aVMCmd is 'listKey', in other words we must have a valid
        # SSH Key for specific commands, such as addKey
        else:
            _ssh_key = self.__tools_key_public
            ebLogTrace("mManageVMSSHKeys - Exacloud using sshkey already in memory")

        # Check valid ssh key
        ebLogInfo('*** Validating SSH KEY VMId=%s VMCmd=%s' % (aVMId, aVMCmd))

        # Move None checking before .split to prevent crash
        # when no sshkey provided for ADBCS instances (aborting the add is fine)
        if _ssh_key is not None and self.__debug:
            ebLogInfo('*** PUB_KEY: %s' % (str(_ssh_key))) # [:20])+'...')
        elif _ssh_key is None and aVMCmd != 'listkey':
            ebLogWarn('*** SSH Key not provided aborting vmcmd: %s' % (str(aVMCmd)))
            return ebError(0x0406)

        self.mValidatesshkey(_ssh_key)

        #
        # Bring down all VMs in the cluter in parallel.
        #

        def sk_command(_dom0,_domu,_vmcmd,_options,_key,_status,_user,_users):
            #
            # umount VM FS and restart VM
            #
            def _umount_fs(_node,_vmid,_mode=True):
                if _mode:
                    _path = '/opt/exacloud/bin'
                    _cmd = _path + '/vmimg.sh umount ' + str(_vmid) + ' ' + _domu
                    ebLogInfo('*** _umount_fs: %s' % (_cmd))
                    _node.mExecuteCmd(_cmd)
                    self.mRestartVM(_domu,aVMHandle=_vmhandle)

            def _bail_out(_node,_vmnode,_vmid,_mode=True):
                _umount_fs(_node,_vmid,_mode)
                if not _mode:
                    _vmnode.mDisconnect()
                _node.mDisconnect()

            def _exec_cmd(_node,_vmnode,_mode,_cmd):
                if _mode:
                    _node.mExecuteCmdLog(_cmd)
                else:
                    _vmnode.mExecuteCmdLog(_cmd)
                if self.__debug:
                    ebLogWarn('*** _exec_cmd: mode: %s cmd: %s' % (str(_mode),_cmd))

            def _exec_cmd_rc(_node,_vmnode,_mode,_cmd):
                if _mode:
                    _node.mExecuteCmd(_cmd)
                    _rc = _node.mGetCmdExitStatus()
                else:
                    _vmnode.mExecuteCmd(_cmd)
                    _rc = _vmnode.mGetCmdExitStatus()
                if self.__debug:
                    ebLogWarn('*** _exec_cmd_rc: mode: %s rc: %s _cmd: %s' % (str(_mode),str(_rc),_cmd))
                return _rc
            #
            # set scoped vars
            #
            _status[_domu] = 0
            _mode = aMode

            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            #
            # Shutdown VM
            #
            if _mode:
                _vmhandle = self.__CompRegistry.mGetComponent("vm_operations")
                _vmhandle.mSetOVMCtrl(aCtx=get_gcontext(), aNode=_node)
                _rc = _vmhandle.mDispatchEvent("shutdown", _options, aVMId=_domu, aCluCtrlObj=self)
                if _rc not in [0, 0x0411, 0x0454]:
                    ebLogWarn('*** Error 0x%x VM %s did not shutdown properly' % (_rc, _domu))
                    _node.mDisconnect()
                    _status[_domu] = _rc
                    return
            #
            # Compute vmid
            #
            _vmid = re.sub('[a-zA-Z-]*','',_domu.split('.')[0])
            if _vmid is None or _vmid == '':
                '''
                hostname need not have a digit. Hence lets use the uuid.
                This _vmid is only used to derive a temporary mount point directory during some key operations.
                '''
                _vmid = self.mGetUUID()

            ebLogInfo('***mManageVMSSHKeys _vmid: %s' %_vmid)

            #
            # Copy vg/vm FS script
            #
            if _mode:
                _script = 'scripts/images/vmimg.sh'
                _path = '/opt/exacloud/bin'
                _cmd  = 'mkdir -p /opt/exacloud/bin'
                _node.mExecuteCmdLog(_cmd)
                _node.mCopyFile(_script,_path + '/vmimg.sh')
                _cmd = 'chmod u+x /opt/exacloud/bin/vmimg.sh'
                _node.mExecuteCmdLog(_cmd)
                #
                # set _base_path to either '/' or mount_point
                #
                _base_path   = '/mnt/vmfs_vm0'+str(_vmid)
                if _user == 'root':
                    _path_sshdir = '/mnt/vmfs_vm0'+str(_vmid)+'/root/.ssh'
                else:
                    _path_sshdir = '/mnt/vmfs_vm0'+str(_vmid)+'/home/'+_user+'/.ssh'
                #
                # Execute mount script
                #
                _cmd = _path + '/vmimg.sh mount ' + str(_vmid) + ' ' + _domu
                _node.mExecuteCmd(_cmd)
                _vmnode = None
            else:
                _vmnode = exaBoxNode(get_gcontext())
                _base_path   = '/'
                if _user == 'root':
                    _path_sshdir = '/root/.ssh'
                    _vmnode.mSetSudo(True)
                else:
                    _path_sshdir = '/home/'+_user+'/.ssh'
                try:
                    _vmnode.mConnect(_domu)
                    # Enforce user to prevent default root access and ensure mode settings
                    # Todo: MR
                except Exception as e:
                    _status[_domu] = 0x0411
                    ebLogWarn('*** Error VM %s not accessible or not running' % (_domu))
                    ebLogWarn(e)
                    _node.mDisconnect()
                    return

            ebLogInfo('*** SSH Keys OPS (%s) for VM %s/%s' % (_vmcmd,_vmid,_domu))
            #
            # Process CMD (resetkey, addkey, deletekey and listkey)
            #
            if _vmcmd in ['resetkey','rescuekey']:
                #
                # retrieve id/grp for _user
                #
                _cmd = """grep %s %s/etc/passwd | tr ':' ' ' | awk '{ print $3" "$4 }'""" % (_user,_base_path)
                _i, _o, _e = _node.mExecuteCmd(_cmd)
                _output = _o.readlines()
                _err = 1
                _opc_uid = 0
                _opc_gid = 0
                if _output:
                    try:
                        _opc_uid, _opc_gid = _output[0][:-1].split(' ')
                        if _opc_uid.isdigit() and _opc_gid.isdigit():
                            if self.__debug:
                                ebLogInfo('*** user: %s uid/gid: %s %s' % (_user,_opc_uid,_opc_gid))
                            _err = 0
                    except:
                        pass
                if _err:
                    ebLogError('*** VM %s user uid/gid not accessible. SSK key reset aborted' % (_user))
                    _status[_domu] = 0x0405
                    _bail_out(_node,_vmnode,_vmid,_mode)
                    return
                #
                # copy / move .ssh directory
                #
                _cmd  = 'mkdir -p '+_path_sshdir
                _cmd += ' ; cp -r '+_path_sshdir+' '+_path_sshdir+'.orig'
                _cmd += ' ; rm ' +_path_sshdir+'/authorized_keys'
                _cmd += ' ; chmod 600 ' +_path_sshdir+'/authorized_keys'
                _node.mExecuteCmdLog(_cmd)
                #
                # Reset authorized key file
                #
                _cmd  = 'echo "'+clusshkey.to_raw_str(_ssh_key)+'" >> '+_path_sshdir+'/authorized_keys 2> /dev/null'
                _cmd += ' ; chown -R '+str(_opc_uid)+':'+str(_opc_gid)+' '+_path_sshdir
                _cmd += ' ; chmod 600 ' +_path_sshdir+'/authorized_keys'
                _node.mExecuteCmdLog("sh -c \'" + _cmd + "\'")
                ebLogInfo('*** SSH Key added to authorized_keys')

            elif _vmcmd == 'addkey':
                #
                # Check if SSH key file is present (e.g. user already created ?)
                #
                _ssh_comment = ''
                _jconf = _options.jsonconf

                # Verify if sshcomment is present under the 'vm' field in the payload
                if _jconf and 'vm' in _jconf.keys():
                    _ssh_comment = _jconf.get("vm", {}).get("sshcomment", "")

                # If not present under 'vm' section, check for legacy 'sshcomment' section
                if not _ssh_comment and _jconf and 'sshcomment' in _jconf.keys():
                    _ssh_comment = _jconf['sshcomment']

                for _user in _users:
                    if _user == 'root':
                        _path_sshdir = '/root/.ssh'
                    else:
                        _path_sshdir = '/home/'+_user+'/.ssh'

                    _cmd = 'test -f '+_path_sshdir+'/authorized_keys'
                    _rc = _exec_cmd_rc(_node,_vmnode,_mode,_cmd)
                    if _rc:
                        ebLogError('*** VM %s user SSH key file not found' %(_user))

                        _cmd = 'test -d '+_path_sshdir
                        _rc = _exec_cmd_rc(_node,_vmnode,_mode,_cmd)
                        if _rc:
                            ebLogError('*** VM %s user SSH key file not available ! addkey aborted' %(_user))
                            _status[_domu] = 0x0405
                            _bail_out(_node,_vmnode,_vmid,_mode)
                            return

                        else:
                            _cmd = 'touch {0}/authorized_keys ; chmod 600 {0}/authorized_keys ; chown {1}:`id -g {1}` {0}/authorized_keys'.format(_path_sshdir, _user)
                            _exec_cmd_rc(_node,_vmnode,_mode,_cmd)
                    #
                    # Check if SSH key is already present
                    #
                    _cmd = 'grep -F "'+clusshkey.to_raw_str(_ssh_key)+'" '+_path_sshdir+'/authorized_keys 2> /dev/null'
                    _rc = _exec_cmd_rc(_node,_vmnode,_mode,_cmd)
                    if _rc:
                        #
                        # Add SSH Key to authorized_keys file
                        #
                        _ssh_keystr = str(_ssh_key) + ' ' + _ssh_comment if _ssh_comment else _ssh_key
                        _cmd = 'echo "'+clusshkey.to_raw_str(_ssh_keystr)+'" >> '+_path_sshdir+'/authorized_keys 2> /dev/null'
                        _exec_cmd(_mode,_vmnode,_mode,"sh -c \'" + _cmd + "\'")
                        _cmt = _ssh_keystr.split(' ')[-1]
                        ebLogInfo('*** SSH Key (%s) added to authorized_keys for user: %s' % (_cmt, _user))
                    else:
                        ebLogWarn('*** SSH Key already present in authorized_keys for user: %s' % (_user))

            elif _vmcmd == 'deletekey':
                #
                # Check if SSH key file is present (e.g. user already created ?)
                #
                _cmd = 'test -f '+_path_sshdir+'/authorized_keys'
                _rc = _exec_cmd_rc(_node,_vmnode,_mode,_cmd)
                if _rc:
                    ebLogError('*** VM %s user SSH key file not available ! deletekey aborted' % (_user))
                    _status[_domu] = 0x0405
                    _bail_out(_node,_vmnode,_vmid,_mode)
                    return
                #
                # Remove Key if present
                #
                _cmd = 'grep -F "'+clusshkey.to_raw_str(_ssh_key)+'" '+_path_sshdir+'/authorized_keys 2> /dev/null'
                _rc = _exec_cmd_rc(_node,_vmnode,_mode,_cmd)
                if not _rc:
                    #
                    # Delete SSH Key
                    #
                    _esckey = clusshkey.to_raw_str(_ssh_key).replace('/','\\/')
                    _cmd = 'sed \'/'+_esckey+'/d\' -i '+_path_sshdir+'/authorized_keys 2> /dev/null'
                    if self.__debug:
                        ebLogInfo('*** Delete SSH Key: %s' % (_cmd))
                    _exec_cmd(_node,_vmnode,_mode,_cmd)
                    _cmt = _ssh_key.split(' ')[-1]
                    ebLogInfo('*** SSH Key (%s) removed from authorized_keys' % (_cmt))
                else:
                    ebLogWarn('*** SSH Key not present in authorized_keys')

            elif _vmcmd == 'listkey':
                #
                # List SSH Key for _user
                #
                _cmd = 'cat '+_path_sshdir+'/authorized_keys'
                _exec_cmd(_node,_vmnode,_mode,_cmd)
            #
            # UnMount VM FS (including RestartVM if _mode is True)
            #
            _umount_fs(_node,_vmid,_mode)
            #
            # Cleanup
            #
            if not _mode:
                _vmnode.mDisconnect()
            _node.mDisconnect()
        #
        # Reset SSH Keys
        #
        _jconf = None
        _jconf_vml = []
        _jconf_ulist = ['opc']
        try:
            _jconf = aOptions.jsonconf
            _jconf_vml = _jconf['vms']
            _jconf_ulist = _jconf['users']
        except:
            pass
        _user = _jconf_ulist[0]
        _users = _jconf_ulist
        #
        # The VM list passed in the json payload takes precedence (over the default vmid)
        #
        if _jconf is not None and _jconf_vml == []:
            _jconf_vml = ['_all_']
        if _jconf is not None and len(_jconf):
            aVMId = 'None'
            ebLogInfo('*** VM List to update/reset SSH Keys : %s' % str(_jconf_vml))

        #Create the process structure
        _plist = ProcessManager()
        _rc_status = _plist.mGetManager().dict()

        for _dom0, _domu in self.mReturnDom0DomUPair():
            if _domu == aVMId or '_all_' in _jconf_vml or _domu in _jconf_vml or aVMId == '_all_':

                _p = ProcessStructure(sk_command, [_dom0,_domu,aVMCmd,aOptions,_ssh_key,_rc_status,_user,_users])
                _p.mSetMaxExecutionTime(30*60) #30 minutes timeout
                _p.mSetJoinTimeout(5)
                _p.mSetLogTimeoutFx(ebLogWarn)
                _plist.mStartAppend(_p)

        _plist.mJoinProcess()

        #
        # Check if SSH Keys have been updated
        #
        _failures = 0
        _domUs = [ _domu for _ , _domu in self.mReturnDom0DomUPair()]
        for _host, _rc in list(_rc_status.items()):
            ebLogInfo('*** SSH Key reset/update for %s rc: 0x%x ' % (_host, _rc))

            if _host in _domUs:
                if _rc == 0x0411:
                    raise ExacloudRuntimeError(_rc_status[_domu], 0xA, 'Exception on mManageVMSSHKeys, host: {0} not accessible'.format(_host))

            if _rc != 0:
                _failures += 1

        if len(list(_rc_status.keys())) == _failures:
            raise ExacloudRuntimeError(_rc_status[_domu], 0xA, 'Exception on mManageVMSSHKeys, too many failures')

        return _failures

    def mCleanupKeys(self, aOptions=None):

        ebLogVerbose("mCleanupKeys: Cleanup SSH keys.")

        _exakms = get_gcontext().mGetExaKms()

        _domUs = [ _domu for _ , _domu in self.mReturnDom0DomUNATPair()]

        for _domU in _domUs:

            _entries = _exakms.mSearchExaKmsEntries({"FQDN": _domU})

            for _entry in _entries:

                if not (self.mCheckConfigOption('keep_domu_opckey', 'True') and \
                   _entry.mGetUser() == 'opc'):
                    continue

                _exakms.mDeleteExaKmsEntry(_entry)


    def mValidateKeys(self, aOptions = None, aVerbose=True):

        if self.__cluster_path is None or self.__cluster_name is None and (not self.mIsKVM() or self.__ociexacc):
            ebLogWarn('*** Cluster path or name is node defined - Validating Keys canceled')
            return ebError(0x0107)

        # Filter/remap the domu on BM
        _dom0s, _domUs, _cells, _switches = self.mReturnAllClusterHosts()

        _domUs_filtered = []
        for _host in _domUs:
            _ctx = get_gcontext()
            if _ctx.mCheckRegEntry('_natHN_' + _host):
                _host = _ctx.mGetRegEntry('_natHN_' + _host)
                if self.__debug:
                    ebLogDebug('*** DOMU Key rotation done using: %s' % (_host))
            _domUs_filtered.append(_host)

            if not self.mPingHost(_host,3):
                if _host in _domUs_filtered:
                    ebLogInfo('*** DomU is not available - Skip ssh key validation')
                    continue
                else:
                    ebLogError('*** Mandatory Host {0} is not pingable - SSH key validation error'.format(_host))
                    return ebError(0x0107)

        # Validate keys
        _host_list = _dom0s + _domUs_filtered + _cells + _switches
        _user = "root"

        for _host in _host_list:

            _node = exaBoxNode(get_gcontext())

            if not self.mPingHost(_host,3):
                ebLogInfo(f'*** Host {_host} is not available - Skip ssh key validation')
                continue

            if _node.mIsConnectable(aHost=_host, aKeyOnly=True):
                ebLogInfo(f"Connection using key success for host: {_host}")

            else:

                if _host in _domUs_filtered:
                    ebLogWarn(f"*** Warning connection with key on host {_host} failed")

                else:
                    ebLogError(f"*** Error connection with key on host {_host} failed")
                    return ebError(0x0107)
        
        return 0

    def mIsIntelX9MDom0(self, aDom0: str) -> bool:
        """
        Returns True if the given Dom0 is an X9M Intel node and above.
        This is different to mGetExadataDom0Model because that function
        returns 'X9' for both Intel and AMD nodes, whereas in this function
        we only return true if the node is Intel only.

        :param aDom0: A Dom0 FQDN.
        :returns: bool representing if the given Dom0 is an Intel X9M.
        """
        with connect_to_host(aDom0, get_gcontext()) as _node:
            _, _model, _ = node_exec_cmd_check(_node, "/opt/oracle.cellos/exadata.img.hw --get model")
            return 'X9-2' in _model or  'X10-2' in _model or 'X11-2' in _model 

    def mGetExadataDom0Model(self, aDom0: str = None) -> Optional[str]:
        """ Connect to first Dom0 and get model (X5/X6/X7 etc) """

        _dom0_name = self.mReturnDom0DomUPair()[0][0]
        if aDom0 is not None:
            _dom0_name = aDom0
        _exa_model = None

        ebLogInfo('*** Getting Dom0 Exadata model')
        _exa_model = self.mGetNodeModel(aHostName=_dom0_name)
        ebLogInfo(f'*** Dom0 Exadata model: {_exa_model}')

        return _exa_model

    def mGetExadataDom0DisksCount (self):

        if self.__dom0_disks is not None:
            return self.__dom0_disks

        _count = 0
        _dpairs = self.mReturnDom0DomUPair()
        for _dom0, _ in _dpairs:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            _cmdstr = "/opt/MegaRAID/MegaCli/MegaCli64"
            if not _node.mFileExists("/opt/MegaRAID/MegaCli/MegaCli64"):
                _cmdstr = "/opt/MegaRAID/storcli/storcli64"
            _cmdstr = "{0} {1}".format(_cmdstr, '-cfgdsply -a0|grep \'Physical Disk:\'')
            _i, _o, _e = _node.mExecuteCmd(_cmdstr)
            _count = len(_o.readlines())
            _node.mDisconnect()
            break

        ebLogInfo('*** Dom0 Exadata Physical Disks count: %s'%_count)

        self.__dom0_disks = _count

        return self.__dom0_disks

    def mIsBaseSystem(self, aOptions=None):

        if not aOptions:
            aOptions = self.mGetArgsOptions()

        if aOptions is not None:
            _jconf = aOptions.jsonconf

        if _jconf is not None and 'rack' in _jconf.keys() and 'size' in _jconf['rack'].keys() and _jconf['rack']['size'] == "BASE-RACK":
            ebLogInfo("Base System Config detected")
            return True

        return False

    def mGetOHSize(self):
        return self.__ohsize

    def mSetOHSize(self, aVal):
        self.__ohsize = aVal

    def mGetAdditionalDisks(self):
        return self._additional_disks

    def mSetAdditionalDisks(self, aVal):
        self._additional_disks.append(aVal)

    def mGetu02Size (self):

        if self.__u02size is not None:
            return self.__u02size

        _source = ''
        _fs_sizes = get_max_domu_filesystem_sizes(self)
        if ebDomUFilesystem.U02 in _fs_sizes:
            _disk_u02_size = f"{math.ceil(_fs_sizes[ebDomUFilesystem.U02] / GIB)}G"
            _source = 'filesystems field in payload'
        elif self.__ohsize is not None:
            _disk_u02_size = self.__ohsize
            _source = 'oh_gbsize field in payload'
        elif self.mCheckConfigOption('disk_u02_size') is not None:
            _disk_u02_size = self.mCheckConfigOption('disk_u02_size')
            _source = 'disk_u02_size field from exabox.conf'
        elif self.__shared_env and not self.IsZdlraProv():
            if self.mCheckConfigOption('disk_u02_size_shared_env') is not None:
                _disk_u02_size = self.mCheckConfigOption('disk_u02_size_shared_env')
                _source = 'disk_u02_size_shared_env field from exabox.conf'
            else:
                _disk_u02_size = '60G'
                _source = 'default size for MVM env'
        else:
            _disk_u02_size = '200G'
            _source = 'default value for Single VM env'
            # Check the number of Physical disks attached to dom0
            if self.mGetExadataDom0DisksCount() == 8:
                # We have enough local storage to increase /u02 space
                _disk_u02_size = '1100G'
                _source = 'default value for Single VM with 8 HDD in Dom0'
            if mCompareModel(self.mGetExadataDom0Model(), 'X8') >= 0:
                _disk_u02_size = self.mGetX8u02Config()
                _source = 'x8_u02_size field from exabox.conf'
            if self.mIsBaseSystem() is True and (not self.__ociexacc and not self.__exacm):
                ebLogInfo('***Setting U02 disk size to 900G for base-system configuration***')
                # For reference, see bug #35229710
                _disk_u02_size = '900G'
                _source = 'default value for base-system env'

        if parse_size(_disk_u02_size) > 2 * TIB:
            _msg = (
                f"DomU u02 filesystem size set to {_disk_u02_size} "
                f"in {_source}, but max allowed size for u02 is 2048G."
            )
            ebLogError(_msg)
            raise ExacloudRuntimeError(0x10, 0xA, _msg)
        if parse_size(_disk_u02_size) < 40 * GIB:
            _msg = (
                f"DomU u02 filesystem size set to {_disk_u02_size} "
                f"in {_source}, but minimum allowed size for u02 is 40G."
            )
            ebLogError(_msg)
            raise ExacloudRuntimeError(0x10, 0xA, _msg)

        ebLogInfo(f'*** Dom0 Exadata u02 disk size : {_disk_u02_size} taken from {_source}')
        self.__u02size = _disk_u02_size
        return self.__u02size

    def mParseChasisInfo(self, aData):
        _data = aData
        _out = {}

        _stmp = NamedTemporaryFile(delete=False)
        _stmp.file.write(six.ensure_binary(_data))
        _stmp.file.close()

        _config = XMLProcessor(_stmp.name)
        _chassis = _config.find("chassis")
        for _item in _chassis.iter("entry"):
            _out[_item.get('name')] = _item.text

        os.unlink(_stmp.name)

        return _out

    def mUpdateChasisToXML(self, aDom0, aDomU, aConfigXML, aSysInfo):
        _dom0 = aDom0
        _domU = aDomU
        _configXML = aConfigXML
        _sysinfo = aSysInfo

        _stmp = NamedTemporaryFile(delete=False)
        _stmp.file.write(six.ensure_binary(_configXML))
        _stmp.file.close()

        _config = XMLProcessor(_stmp.name)

        _smbios_tag = False
        _os_elems = _config.findall("os")
        for _x in _os_elems:
            for _ret in _x.iter('smbios'):
                _smbios_tag = True

        if not _smbios_tag:
            _os_elem = _config.find('os')
            _smbios_elem = _config.make_element('smbios')
            _config.add_attribute(_smbios_elem, 'mode', 'sysinfo')
            _os_elem.append(_smbios_elem)
        else:
            ebLogInfo("smbios element already added to the XML.")

        _chassis_tag = False
        _sysinfo_elem = _config.findall("sysinfo")
        for _x in _sysinfo_elem:
            for _ret in _x.iter('chassis'):
                _chassis_tag = True

        if not _chassis_tag:
            _chassis_elem = _config.make_element('chassis')
            _config.make_sub_element(_chassis_elem, 'entry', _sysinfo['manufacturer'], {'name': 'manufacturer'})
            _config.make_sub_element(_chassis_elem, 'entry', _sysinfo['version'], {'name': 'version'})
            _config.make_sub_element(_chassis_elem, 'entry', _sysinfo['serial'], {'name': 'serial'})
            _config.make_sub_element(_chassis_elem, 'entry', 'OracleCloud.com', {'name': 'asset'})
        else:
            ebLogInfo("chassis element already added to the XML.")
            return None

        _sysinfo = _config.make_element('sysinfo')
        _sysinfo.set('type', 'smbios')
        _sysinfo.insert(0, _chassis_elem)
        _config.insert_after_element('os', _sysinfo)

        _dir = self.mGetBasePath() + '/clusters/' + self.__key + '/config/'
        if not os.path.exists(_dir):
            os.makedirs(_dir)

        _path = _dir + _domU + '.libvirt.xml'

        _config.writeXml(_path)

        os.unlink(_stmp.name)

        return _path

    def mStartVMExacsService(self, aOptions, aDom0DomUPair=None, aCheckCrsAsm=True):
        _ddpair = aDom0DomUPair
        _checkcrsasm = aCheckCrsAsm

        with self.remote_lock():
            self.mParallelDomUShutdown(aDom0DomUPair=_ddpair, force_on_timeout=True)
            self.mStartVMExacsServiceOnShutdown(aOptions, aDom0DomUPair=_ddpair)
            self.mParallelDomUStart(aDom0DomUPair=_ddpair)
            self.mStartVMExacsServiceAfterBoot(aOptions, aDom0DomUPair=_ddpair, aCheckCrsAsm=_checkcrsasm)

    def mStartVMExacsServiceOnShutdown(self, aOptions, aDom0DomUPair=None):
        '''
        1. Fetch the chasis information from dom0 virsh sysinfo
        3. Update the chasis information to GuestVM.
        4. Attach virtio serial device to GuestVM.
        '''

        if self.__cmd == 'vmgi_reshape':
            _dpairs = self.mReturnElasticNewDom0DomUPair()
        else:
            if aDom0DomUPair:
                _dpairs = aDom0DomUPair
            else:
                _dpairs = self.mReturnDom0DomUPair()

        for _dom0, _domU in _dpairs:

            ebLogTrace(f"*** VMExacsService for: '{_dom0}' - '{_domU}'")

            aOptions = self.mGetArgsOptions()

            with connect_to_host(_dom0, get_gcontext()) as _node:
                _vmhandle = ebVgLifeCycle()
                ebLogTrace(f"Fetching VM Handle:{_vmhandle}")
                _vmhandle.mSetOVMCtrl(aCtx=get_gcontext(), aNode=_node)

                _hv = _vmhandle.mGetVmCtrl().mGetHVInstance()
                ebLogTrace(f"Getting HV Instance:{_hv} for dom0:{_dom0}")

                ebLogTrace(f"Getting system info and reading XML: {_domU}")
                _sys_info = _hv.mGetSysInfo()
                _configXML = _hv.mReadRemoteXML(_domU)

                ebLogTrace(f"Parsing chassis info: {_domU}")
                _chasis_info = self.mParseChasisInfo(_sys_info)
                _updated_xml_path = self.mUpdateChasisToXML(_dom0, _domU, _configXML, _chasis_info)

                # Remove all the remains .backup files
                _node.mExecuteCmdLog("/bin/rm /etc/libvirt/qemu/*.backup.xml")

                #Copy the updated chassis_asset_tag information to dom0
                if _updated_xml_path:

                    # Create backup folder
                    _backupFolder = "/opt/exacloud/qemu_backup/"
                    _remoteOldBackup = "{0}/{1}.backup.xml".format(_backupFolder, _domU)
                    _remoteLocation = "{0}/{1}_ExacsServiceBackup.xml".format(_backupFolder, _domU)
                    _node.mExecuteCmdLog("/bin/mkdir -p {0}".format(_backupFolder))

                    # Create backup of file
                    _cmd = "/usr/sbin/vm_maker --dumpxml {0} > {1}".format(_domU, _remoteOldBackup)
                    _node.mExecuteCmdLog(_cmd)

                    # Undefine domain
                    _hv.mUnDefineXMLToGuest(_domU)

                    # Upload updated XML
                    _node.mCopyFile(_updated_xml_path, _remoteLocation)

                    # Define new XML
                    _hv.mDefineXMLToGuest(_remoteLocation)

                    _hv.mAutoStartVM(_domU, aEnabled=True)

                    # Clean Up of local environment
                    self.mExecuteLocal('/bin/rm -rf {0}'.format(_updated_xml_path))

                # Attach vmexacs_kvm_controller, vmexacs_kvm_channel to domU
                ebLogTrace(f"Attaching vmexacs_kvm controller/channel to: {_domU}")
                _hv.mAttachDevice(_domU, '/opt/oracle.vmexacs_kvm/vmexacs_kvm_controller.xml')
                _hv.mAttachDevice(_domU, '/opt/oracle.vmexacs_kvm/vmexacs_kvm_channel.xml')

    def mStartVMExacsServiceAfterBoot(self, aOptions, aDom0DomUPair=None, aCheckCrsAsm=True):

        def mStartVMExacsServiceAfterBootPerDom0(aDom0, aDomU):

            with connect_to_host(aDom0, get_gcontext()) as _node:

                # Start the vmexacs_kvm service
                ebLogTrace(f"Starting the vmexacs_kvm service for: '{_dom0}' - '{aDomU}'")
                _remotefile = "/etc/init.d/vmexacs_kvm"
                if _node.mFileExists(_remotefile) is False:
                    _cmd = "cp /opt/oracle.vmexacs_kvm/vmexacs_kvm.svc /etc/init.d/vmexacs_kvm"
                    _cmd += " ; chmod u+x /etc/init.d/vmexacs_kvm"
                    _node.mExecuteCmdLog(_cmd)
                _state = _node.mSingleLineOutput('systemctl is-enabled {}'.format('vmexacs_kvm'))
                ebLogInfo("Service {} is {} on dom0:{}".format('vmexacs_kvm', _state, _dom0))
                if _state == "disabled":
                    _cmd = "systemctl enable vmexacs_kvm ; systemctl start vmexacs_kvm "
                    _node.mExecuteCmdLog(_cmd)
                elif _state == "enabled" :
                    _cmd = "systemctl daemon-reload"
                    _node.mExecuteCmdLog(_cmd)

        if self.__cmd == 'vmgi_reshape':
            _dpairs = self.mReturnElasticNewDom0DomUPair()
        else:
            if aDom0DomUPair:
                _dpairs = aDom0DomUPair
            else:
                _dpairs = self.mReturnDom0DomUPair()

        # Run status in parallel
        _plist = ProcessManager()
        for _dom0, _domU in _dpairs:
            _p = ProcessStructure(mStartVMExacsServiceAfterBootPerDom0, [_dom0, _domU], _dom0)
            _p.mSetMaxExecutionTime(10*60) # 10 minutes
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)
        _plist.mJoinProcess()

        for _dom0, _domU in _dpairs:

            #chassis_asset_tag should contain the string OracleCloud.com
            with connect_to_host(_domU, get_gcontext()) as _node:
                _cmd = "cat /sys/devices/virtual/dmi/id/chassis_asset_tag"
                _, _o, _ = _node.mExecuteCmd(_cmd)
                if _o:
                    _out = _o.readlines()
                    if _out:
                        _chassis_tag = _out[0].strip()
                        if self.__debug:
                            ebLogDebug('*** chassis_asset_tag: %s' % (_chassis_tag))

            ebLogTrace(f"*** vmexacs_kvm service is started for:'{_dom0}' - '{_domU}'")

        _domu_list = [ _domu for _ , _domu in _dpairs]

        if aCheckCrsAsm is True:
            self.mCheckCrsIsUp(_domu_list[0], _domu_list)
            self.mCheckAsmIsUp(_domu_list[0], _domu_list)


    def mHandlerResizeExaVMImages(self):
        if not self.mIsKVM():                                                   
            ebLogWarn('*** Resizing /EXAVMIMAGES on XEN systems not supported') 
            return 0  
        return self.mResizeExaVMImages()

    def mResizeExaVMImages(self):
        _rc = 0
        def get_vg_free_space(aNode):
            _node = aNode
            _vg_free = ''

            _cmd = "vgs VGExaDb | awk '{ print $7 }'"
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            if _o:
                _out = _o.readlines()
                try:
                    _vg_free = _out[1].strip()
                    if _vg_free == '0':
                        _vg_free = ''
                except Exception as e:
                    ebLogError(f'Error in Volume Group : {_e.readlines()}, output from dom0:{_out}')
                    raise ExacloudRuntimeError(0x0437, 0xA, 'VM Image Resize failed: Volume Group Error')
                
            return _vg_free

        def extend_logical_volume(aNode):
            _node = aNode

            #Extend the logical volume & resize the file system
            _cmd = "lvextend -l +100%FREE -r /dev/VGExaDb/LVDbExaVMImages"

            _, _o, _e = _node.mExecuteCmd(_cmd)
            _ret = _node.mGetCmdExitStatus()
            if _ret:
                _error_str = '*** extending logical volume EXAVMIMAGES failed (lvextend )'
                _error_str = _error_str + str(_o.readlines()) + ' ' + str(_e.readlines())
                ebLogError(_error_str)
                _rc =  ebError(0x0206)

        def check_disk_space(aNode):
            _node = aNode

            _cmd = "df -h /EXAVMIMAGES/"

            if self.__debug:
                _node.mExecuteCmdLog(_cmd)

        _dpairs = self.mReturnDom0DomUPair()
        for _dom0, _ in _dpairs:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost = _dom0)
            _vg_free = get_vg_free_space(_node)
            if _vg_free:
                #1. Extend the logical volume that contains /EXAVMIMAGES to include the rest of the free space.
                extend_logical_volume(_node)
                #2. Mount the /EXAVMIMAGES file system and then view the disk space usage for this file system
                check_disk_space(_node)
            else:
                if self.__debug:
                    ebLogDebug('*** No Free space available in volume group VGExaDb')

            _node.mDisconnect()
        return _rc

    def mHandlerListOEDASSHKeys(self, aOptions=None):
        if not aOptions: 
            aOptions = self.mGetArgsOptions()

        _hostmap = self.mGetExaKmsHostMap()
        _exakms = get_gcontext().mGetExaKms()

        for _host, _type in _hostmap.items():

            _cparams = {"FQDN": _host}
            _entries = _exakms.mSearchExaKmsEntries(_cparams)

            for _entry in _entries:
                ebLogInfo(f"Found ExaKmsEntry: {_entry}")

        return 0

    def mGetMasterSshKey(self, aNodeType):
        """
        Return a dict with the master key content by type

        :param:aNodeType:could take only the values of "dom0", "cell", 'switch'
        :return: a dict with the key content
        :return:['private']: private key content of the master key
        :return:['public']: public key content of the master key
        :raises:ExacloudRuntimeError: On invalid option in aNodeType
        """

        if aNodeType not in ["dom0", "cell", 'switch']:
            raise ExacloudRuntimeError(0x0119, 0xA, "Invalid Master key type in aNodeType")

        _keyContent = {}
        _type = aNodeType

        if _type  == "switch":
            _type = "ib"

        _pubfile = 'config/masterkeys/id_rsa.{0}master.root.pub.dat'.format(_type)
        with open(_pubfile, 'r') as _f:
            _key_data = b64decode(b64decode(b64decode(_f.read()))).decode('utf8')
            _keyContent['public'] = _key_data.strip()

        _privfile = 'config/masterkeys/id_rsa.{0}master.root.dat'.format(_type)
        with open(_privfile, 'r') as _f:
            _key_data = b64decode(b64decode(b64decode(_f.read()))).decode('utf8')
            _keyContent['private'] = _key_data.strip()

        return _keyContent


    def mInjectSSHMasterKeySingle(self, aNode, aFilePrefix):

        #Get the SSH Master Key Data
        _file = 'config/masterkeys/id_rsa.{0}.root.pub.dat'.format(aFilePrefix)
        try:
            with open(_file, 'r') as _f:
                _key_data = b64decode(b64decode(b64decode(_f.read()))).decode('utf8').strip()
        except Exception as _error:
            ebLogWarn("*** Warning on keyfile of mInjectSSHMasterKeySingle: {0}".format(_error))
            return 1

        if not aNode.mFileExists(".ssh/authorized_keys"):

            aNode.mExecuteCmdLog("/bin/mkdir -p .ssh")
            aNode.mExecuteCmdLog("/bin/touch .ssh/authorized_keys")
            aNode.mExecuteCmdLog("/bin/chmod 700 .ssh")
            aNode.mExecuteCmdLog("/bin/chmod 600 .ssh/authorized_keys")

        aNode.mExecuteCmd(f"/bin/grep '{_key_data}' .ssh/authorized_keys")

        if aNode.mGetCmdExitStatus() != 0:
            aNode.mExecuteCmdLog(f"/bin/echo '{_key_data}' >> .ssh/authorized_keys")

        return 0

    def mInjectSSHMasterKey(self):
        # Produce list of target hosts
        if not self.mIsKVM():
            _dom0s, _domUs, _cells, _switches = self.mReturnAllClusterHosts()
        else:
            _dom0s, _domUs, _cells, _ = self.mReturnAllClusterHosts()
            _switches = []

        _devices = _dom0s + _cells + _switches

        for _device in _devices:

            ebLogInfo("*** Inject Master Key on {0}".format(_device))
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(_device)

            # Prevent host key verification failed
            if not self.mCheckConfigOption('remove_remote_known_hosts', 'False'):
                _node.mExecuteCmd("rm ~/.ssh/known_hosts")

            if _device in _switches:
                self.mInjectSSHMasterKeySingle(_node, "ibmaster")
            elif _device in _cells:
                self.mInjectSSHMasterKeySingle(_node, "cellmaster")
            elif _device in _dom0s:
                self.mInjectSSHMasterKeySingle(_node, "dom0master")

            _node.mDisconnect()

        return 0

    def mUpdateAllClusterHostsKeys(self, aCreateNew=True, aClusterId=None, aHost=None, aHostType=None, aRes={}):

        _dom0s = []
        _domUs = []
        _cells = []
        _switches = []
        _res = aRes

        # Host type
        _hosttype = aHostType
        if not _hosttype and "hosttype" in self.__options and self.__options.hosttype:
            _hosttype = self.__options.hosttype

        # Produce list of target hosts
        if not self.mIsKVM():
            _dom0s, _domUs, _cells, _switches = self.mReturnAllClusterHosts()
        else:
            _dom0s, _domUs, _cells, _ = self.mReturnAllClusterHosts()
            _switches = []

        # Filter/remap the domu on BM
        _domUs_filtered = []
        _ctx = get_gcontext()

        if aHost:
            _host_list = [aHost]

            if _hosttype == "DOM0":
                _dom0s = _host_list
            elif _hosttype == "DOMU":
                _host = aHost
                if _ctx.mCheckRegEntry('_natHN_' + _host):
                    _host = _ctx.mGetRegEntry('_natHN_' + _host)
                    if self.__debug:
                        ebLogDebug('*** DOMU Key rotation done using: %s' % (_host))
                _domUs_filtered.append(_host)
            elif _hosttype in ["CELLS", "CELL"]:
                _cells = _host_list
            elif _hosttype == "SWITCHES":
                _switches = _host_list
        else:

            for _host in _domUs:
                if _ctx.mCheckRegEntry('_natHN_' + _host):
                    _host = _ctx.mGetRegEntry('_natHN_' + _host)
                    if self.__debug:
                        ebLogDebug('*** DOMU Key rotation done using: %s' % (_host))
                _domUs_filtered.append(_host)

            if _hosttype == "DOM0":
                _host_list = _dom0s

            elif _hosttype == "DOMU":
                _host_list = _domUs_filtered

            elif _hosttype in ["CELLS", "CELL"]:
                _host_list = _cells

            elif _hosttype == "SWITCHES":
                _host_list = _switches

            else:
                _host_list = _dom0s + _domUs_filtered + _cells + _switches
                
        if self.__options.jsonconf and self.__options.jsonconf.get('KEY_ROTATION_NODES', []):
            # if nodes passed in payload , use them only
            _payload_host_list = self.__options.jsonconf.get('KEY_ROTATION_NODES', [])
            _matching_hosts = []
            # host list should be matching
            for _host in _payload_host_list:
                if _host in _host_list:
                    _matching_hosts.append(_host)
            # use the intersection of payload list and valid list
            _host_list = _matching_hosts

        # Iterate over host list
        _exakms = get_gcontext().mGetExaKms()
        
        # Get excluded nodes list
        _excluded_hosts = []
        if self.__options.jsonconf:
            _excluded_hosts = self.__options.jsonconf.get('INGESTION_HW_FAILURE', [])

        for _host in _host_list:
            if _host in _excluded_hosts:
                continue

            _res[_host] = "FAIL"
            if not self.mPingHost(_host):
                _res[_host] = "FAIL"
                ebLogWarn('*** Host: %s is not pingable - Cancel OEDA SSH key push to this node' % (_host))
                continue
            #
            # Generate new SSH Key (or Rotate existing SSH Key)
            #
            _name = _host.split('.')[0]
            if _host not in _switches:
                _comment = 'EXACLOUD KEY'
            else:
                _comment = 'EXACLOUD KEY '+self.mBuildClusterId(aClusterId)[:64]

            _cparam = {"FQDN": _host, "user": "root"}
            _entry = _exakms.mGetExaKmsEntry(_cparam)
            if not _entry:
                _entry = _exakms.mBuildExaKmsEntry(_host, "root", _exakms.mGetEntryClass().mGeneratePrivateKey())

            if aCreateNew == True:
                _entry.mSetPrivateKey(_exakms.mGetEntryClass().mGeneratePrivateKey())

            _key_data = _entry.mGetPublicKey(_comment)

            if _key_data is not None:

                _node = exaBoxNode(get_gcontext())
                #
                # Try to connect to the remote node using existing SSH Key (fallback to the default password the first time)
                #
                ebLogInfo('$$$ Trying to connect to host ' + _host)
                try:
                    _node.mConnect(_host)
                except:
                    if _host in _domUs_filtered:
                        _res[_host] = "FAIL"
                        ebLogWarn('*** SSH connection to DOMU %s failed due to invalid SSH Key (or bad default password if initial setup)' % (_host))
                        continue
                    raise ExacloudRuntimeError(0x0107,0xA,'Exception caught while trying SSH connection to host: %s' % (_host))

                if aCreateNew == True:

                    if _host in _dom0s:
                        _entry.mSetHostType(ExaKmsHostType.DOM0)
                    elif _host in _switches and not self.mIsKVM():
                        _entry.mSetHostType(ExaKmsHostType.SWITCH)
                    elif _host in _cells:
                        _entry.mSetHostType(ExaKmsHostType.CELL)
                    else:
                        _entry.mSetHostType(ExaKmsHostType.UNKNOWN)

                    _exakms.mInsertExaKmsEntry(_entry)
                    ebLogInfo('*** SSH Key inserted into KMS for the host: %s' % (_host))

                ebLogInfo('*** Updating root EXACLOUD SSH key on host: %s' % (_host))
                #
                # For switches only - since they may be shared w/ other environments
                # Keep existing EXACLOUD keys
                #
                if _host not in _switches:
                    #
                    # On non-switch, create authorized_keys file if not present
                    #
                    _create_str =  "test -e .ssh/authorized_keys || mkdir -p .ssh && touch .ssh/authorized_keys && chmod 700 .ssh && chmod 600 .ssh/authorized_keys"
                    _node.mExecuteCmdLog(_create_str)

                #
                # Clean up existing entry and add the new one
                #
                _cleanup_str = "ex '+g/.*%s.*/d' -scwq ~/.ssh/authorized_keys ;" % (_comment) + "echo -n '%s' >> ~/.ssh/authorized_keys" % (_key_data)
                _node.mExecuteCmdLog("sh -c \"" + _cleanup_str + "\"")


                #
                # On Cells and Dom0 only inject master
                #
                if _host in _cells:
                    self.mInjectSSHMasterKeySingle(_node, "cellmaster")

                if _host in _dom0s:
                    self.mInjectSSHMasterKeySingle(_node, "dom0master")
                #
                # On Switches also inject the SSH IBMaster Key
                #
                if _host in _switches and not self.mIsKVM():
                    self.mInjectSSHMasterKeySingle(_node, "ibmaster")

                # Validate we can access the remote node using the new SSH Key (or rotated one)
                #
                if self.__debug:
                    _node.mExecuteCmdLog('grep "%s" .ssh/authorized_keys ;' % (_comment))
                else:
                    _node.mExecuteCmd('grep "%s" .ssh/authorized_keys ;' % (_comment))
                _node.mDisconnect()
                _res[_host] = "PASS"

        return 0


    def mHandlerInjectExistingKeys(self, aOptions=None):
        """
        Inject appropriate SSH Keys required for pwd less connectivity, if they already exist
        Useful for boxes just re-imaged.
        :param aOptions:
        :return: NA
        """
        if not aOptions: 
            aOptions = self.mGetArgsOptions()

        ebLogVerbose("mInjectExistingKeys: Inject exising OEDA/EXACLOUD SSH Keys.")

        #
        #  Code shared by Create and InjectExisting keys
        #
        self.mUpdateAllClusterHostsKeys(aCreateNew=False)

        #
        # Validate the new SSH Keys from the global repo
        #
        _rc = self.mValidateKeys(aOptions)

        if _rc is not None and _rc != 0:
            ebLogError('*** Injection of existing OEDA SSH Key failed')
            return ebError(0x0108)

        ebLogInfo('*** Existing Exacloud SSH Keys successfully injected and tested ready for OEDA deployment')

        return 0

    def mHandlerRotateKeys(self):
        aOptions = self.mGetArgsOptions()
        return self.mCreateOEDASSHKeys(aOptions, True)

    def mCreateOEDASSHKeys(self, aOptions=None, aRotate=False):

        if aRotate is False:
            ebLogVerbose("mCreateOEDASSHKeys: Creating OEDA/EXACLOUD SSH keys.")
        else:
            ebLogVerbose("mRotateOEDASSHKeys: Rotate OEDA/EXACLOUD SSH Keys.")

        #
        #  Code shared by Create and InjectExisting keys
        #
        self.mUpdateAllClusterHostsKeys(aCreateNew=True)

        #
        # Validate the new SSH Keys from the global repo
        #
        _rc = self.mValidateKeys(aOptions)

        if _rc is not None and _rc != 0:
            ebLogError('*** Create OEDA SSH Key failed')
            return ebError(0x0108)

        ebLogInfo('*** Exacloud SSH Keys ready for OEDA deployment')

        return 0


    def mHandlerImportKeys(self):

        _clustersOedaPath = self.mCheckConfigOption('export_import_keys_folder')
        if not os.path.exists(_clustersOedaPath):
            os.makedirs(_clustersOedaPath)

        _exakms = get_gcontext().mGetExaKms()

        _exakms.mSaveEntriesToFolder(
            _clustersOedaPath,
            self.mGetExaKmsHostMap(),
            self.mExaKmsEntryExtraValidation
        )

        return 0


    def mExaKmsEntryExtraValidation(self, aKmsEntry):

        if not self.mCheckConfigOption('exakms_validate_import_export', "True"):
            return True

        if aKmsEntry.mGetHostType() == ExaKmsHostType.DOMU:

            _vmExists = False

            for _dom0, _clientHost in self.mReturnDom0DomUPair():

                _domU = _clientHost
                _ctx = get_gcontext()
                if _ctx.mCheckRegEntry('_natHN_' + _domU):
                    _domU = _ctx.mGetRegEntry('_natHN_' + _domU)

                if _domU.split(".")[0] == aKmsEntry.mGetFQDN().split(".")[0]:
                    try:
                        _vmExists = self.mCheckIfVMExists(_clientHost, _dom0)
                    except Exception as e:
                        ebLogInfo('***Unable to check if vm ' + _domU + 'exists. Error is ' + str(e))

                    break

            if not _vmExists:
                return True

        # Fetch FQDN from XML
        _fqdn = aKmsEntry.mGetFQDN()
        _dom0s, _domUs, _cells, _switches = self.mReturnAllClusterHosts()
        _allHosts = _dom0s + _domUs + _cells + _switches

        for _host in _allHosts:
            if _fqdn.split(".")[0] == _host.split(".")[0]:
                _fqdn = _host
                break

        _node = exaBoxNode(get_gcontext())

        _node.mSetUser(aKmsEntry.mGetUser())
        _node.mSetExaKmsEntry(aKmsEntry)

        return _node.mIsConnectable(aHost=_fqdn, aKeyOnly=True)


    def mHandlerExportKeys(self, aOptions=None):

        if not ebCluCmdCheckOptions(self.__cmd, ['validate_exakms_entries']):
            return

        _clustersOedaPath = self.mCheckConfigOption('export_import_keys_folder')
        if not os.path.exists(_clustersOedaPath):
            os.makedirs(_clustersOedaPath)

        _exakms = get_gcontext().mGetExaKms()

        _metrics = _exakms.mRestoreEntriesFromFolder(
            _clustersOedaPath,
            self.mGetExaKmsHostMap(),
            self.mExaKmsEntryExtraValidation
        )

        ebLogInfo("Exportkeys overall status")
        ebLogInfo(_metrics)

        ebLogInfo("Removing SSH keys from {} folder.".format(_clustersOedaPath))

        if os.path.exists(_clustersOedaPath):
            _exakms.mCleanUpKeysFolder(_clustersOedaPath, self.mGetExaKmsHostMap())

        return 0


    def mHandlerDeleteInfraNodeSSHAndDNS(self):

        def mGetRemovedNodes(aOptions):
            _removed_nodes = []
            if 'removed_cells' in aOptions.jsonconf.keys():
                _removed_nodes.extend(aOptions.jsonconf['removed_cells'])
            if 'removed_computes' in aOptions.jsonconf.keys():
                _removed_nodes.extend(_options.jsonconf['removed_computes'])
            return _removed_nodes


        def mRemoveExaKmsEntries(aFQDN):
            _exakms = get_gcontext().mGetExaKms()
            _entries = _exakms.mSearchExaKmsEntries({"FQDN" : aFQDN})
            for _entry in _entries:
                _rc = _exakms.mDeleteExaKmsEntry(_entry)
                ebLogInfo(f"Deleting {_entry}: {_rc}")

        try:
            _options = self.mGetArgsOptions()
            _dnsconfig = ebDNSConfig(_options)

            for _infra_node in mGetRemovedNodes(_options):
                _admin_fqdn = _infra_node['admin']
                _ilom_fqdn = _infra_node['ilom']

                mRemoveExaKmsEntries(_admin_fqdn)
                mRemoveExaKmsEntries(_ilom_fqdn)

                _dnsconfig.mDeleteInfraEntry(_admin_fqdn)
                _dnsconfig.mDeleteInfraEntry(_ilom_fqdn)

                if 'nats' in _infra_node:
                    for _nat_fqdn in _infra_node['nats']:
                        mRemoveExaKmsEntries(_nat_fqdn)
                        _dnsconfig.mDeleteInfraEntry(_nat_fqdn)

                if 'nat_vips' in _infra_node:
                    for _nat_vip in _infra_node['nat_vips']:
                        _dnsconfig.mDeleteInfraEntry(_nat_vip)

            _dnsconfig.mUpdateRemoteNode()
            self.mRestartDnsmasq()
            return 0
        except Exception as e:
            ebLogError(f'Error while trying to delete hardware node from cps')
            ebLogError(f'Exception: {e}')
            ebLogError(f'{traceback.format_exc()}')
            return 1

    def mHandlerDeleteKeys(self):

        _exakms = get_gcontext().mGetExaKms()

        _hostmap = self.mGetExaKmsHostMap()

        for _host, _type in _hostmap.items():

            _cparams = {"FQDN": _host}
            _entries = _exakms.mSearchExaKmsEntries(_cparams)

            for _entry in _entries:
                _rc = _exakms.mDeleteExaKmsEntry(_entry)
                ebLogInfo(f"Deleting {_entry}: {_rc}")

        return 0


    #
    # check if NID environment
    # getting the information from the domU
    #
    def mCheckNIDEnvironment(self):
        _str_nid_conf = "/u01/app/oracle/nid"
        _nid_count = 0

        for _, _domU in self.mReturnDom0DomUPair():
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domU)
            _rc = _node.mFileExists(_str_nid_conf)
            _node.mDisconnect()
            if _rc:
                _nid_count += 1

        if not _nid_count:
            ebLogWarn("** Old starter DB environment detected. DomUs were created without NID images **")
            return False

        ebLogInfo("*** NID ENVIRONMENT DETECTED !! ***")
        return True

    def mCheckSharedEnvironment(self) -> bool:

        # Cache shared_env status - set self.__shared_env to None to force discovery

        if self.__shared_env is not None:
            return self.__shared_env

        self.__shared_env = False

        if self.mIsClusterLessXML():
            ebLogInfo("Current operation is based on a special XML. Considering the environment as DEDICATED.")
            return self.__shared_env

        aOptions = self.__options
        # Add detection of shared_env by payload or ExaScale env.
        if self.__options is not None and self.__options.jsonconf is not None and (\
            'shared_env' in self.__options.jsonconf or \
            'storageType' in self.__options.jsonconf):

            self.__shared_env = True
            if str(self.__options.jsonconf.get('shared_env')).upper() == "TRUE":
                ebLogInfo('*** SHARED_ENVIRONMENT DETECTED BY PAYLOAD ***')
            elif str(self.__options.jsonconf.get('storageType')).upper() in  ["EXASCALE"]:
                ebLogInfo('*** SHARED_ENVIRONMENT DETECTED BY EXASCALE (EXADBXS) ***')
                self.__shared_env = True
                return self.__shared_env
            elif aOptions is not None and hasattr(aOptions, 'storageType') and \
               str(aOptions.storageType).upper() == "EXASCALE":
                ebLogInfo('*** SHARED_ENVIRONMENT DETECTED BY EXASCALE (EXADBXS) ***')
                self.__shared_env = True
                return self.__shared_env
            elif str(self.__options.jsonconf.get('storageType')).upper() in  ["XS"]:
                ebLogInfo('*** SHARED_ENVIRONMENT DETECTED BY XS ***')
            else:
                self.__shared_env = False
                ebLogInfo('*** DEDICATED ENVIRONMENT DETECTED BY PAYLOAD ***')

            # Setting Dom0 configuration
            self.mSetSharedEnvironment(self.__shared_env)
            return self.__shared_env

        if self.__mock_mode == 'TRUE':
            try:
                ebLogInfo('*** MOCK SHARED_ENVIRONMENT VERIFICATION ***')
                if self.__header.mGetHeaderFiletype().lower() == 'vswitch':
                    ebLogInfo('*** SHARED_ENVIRONMENT DETECTED ***')
                    self.__shared_env = True
                else:
                    ebLogInfo('*** DEDICATED ENVIRONMENT DETECTED ***')
            except Exception as e:
                ebLogInfo("*** MOCK error verify shared env:" + str(e))

            return self.__shared_env

        _str_shared_conf = '/opt/exacloud/clusters/shared_env_enabled'
        _d0_cnt = 0
        _d0_shf = 0
        for _dom0, _ in self.mReturnDom0DomUPair():
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            _rc = _node.mFileExists(_str_shared_conf)
            if _rc:
                _d0_shf += 1
            _d0_cnt += 1
            _node.mDisconnect()

        if _d0_cnt > 0 and _d0_shf == _d0_cnt:
            ebLogInfo('*** SHARED_ENVIRONMENT DETECTED ***')
            self.__shared_env = True
        elif _d0_shf:
            ebLogError('*** PARTIAL SHARED ENVIRONMENT DETECTED ***')
        elif self.__debug:
            ebLogInfo('*** DEDICATED ENVIRONMENT DETECTED ***')

        return self.__shared_env

    def mCheckVMCyclesAndReboot(self, aDom0DomUPair=None, aNodeRecovery=False):
        _node_recovery = aNodeRecovery
        if aDom0DomUPair:
            _ddpair = aDom0DomUPair
        else:
            _ddpair = self.mReturnDom0DomUPair()

        _mvcr = self.mCheckConfigOption('min_vm_cycles_reboot')
        if _mvcr is not None:
            _mvcr = int(_mvcr)
        else:
            _mvcr = 0

        for _dom0, _domu in _ddpair:
            _hv = getHVInstance(_dom0)
            if not self.mIsKVM():
                _cmd_str = '/opt/exadata_ovm/exadata.img.domu_maker remove-domain '+ _domu+' -force'
                _cmd_str2 = 'xm destroy ' + _domu
            _cmd_del_vmbkup = 'source /opt/python-vmbackup/bin/set-vmbackup-env.sh && vmbackup cleanall --vm '+_domu
            ebLogInfo('*** running vm delete using: ' + _cmd_str)
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            #
            # Retrieve (MAX) VM cycle id on the system if available (e.g. at least one VM needs to be running)
            #
            _currid = 0
            _currid = _hv.mRetrieveVMID(_domu)
            if _currid:
                ebLogInfo('*** CURRENT_VMID for domU %s : %d' % (_domu, _currid))
            else:
                ebLogWarn('*** VMID for domU: %s not found (VM down or already deleted)' % (_domu))

            #
            # Destroy VM and Remove Domain
            #
            _del_vmbkup = self.mCheckConfigOption('delete_vmbackup')
            if _del_vmbkup is not None and _del_vmbkup == 'True' and not self.mIsKVM() and not _node_recovery:
                _node.mExecuteCmdLog(_cmd_del_vmbkup)
            if not self.mIsKVM():
                _node.mExecuteCmdLog(_cmd_str2)
                _node.mExecuteCmdLog(_cmd_str)
            else:
                self.mAcquireRemoteLock()
                _hv.mDestroyVM(_domu)
                _hv.mDeleteVM(_domu)
                self.mReleaseRemoteLock()  

            #
            # Check loopback devices presence (losetup -a)
            #
            _cmd_str = 'losetup -a | grep /EXAVMIMAGES/GuestImages/' + _domu
            _, _out, _err = _node.mExecuteCmd(_cmd_str)
            if _out:
                _out = _out.readlines()
                if _err:
                    _err = _err.readlines()
                    if len(_err):
                        ebLogError('*** losetup errors reported: %s' % (str(_err)) )
                for _entry in _out:
                    ebLogError('*** Loopback devices still mounted for domU %s : %s' % (_domu, _entry.strip()))
            _node.mDisconnect()

            #
            # TODO: Bounce dom0 on non shared env. Use VM cycles
            #
            if not self.__shared_env:
                if _currid >= _mvcr:
                    self.mRebootNode(_dom0, aForce=True, aSkipShutdown=True)
                else:
                    ebLogInfo('** MCVR threshold not reached skipping dom0 reboot (%s/%s)' % (str(_currid), str(_mvcr)))
            else:
                ebLogInfo('*** ShareEnv detected DOM0 Reboot disabled')

    def mSetSharedEnvironment(self,aMode=True):

        ebLogVerbose("mSetSharedEnvironment: aMode = %s" % aMode)

        _str_shared_conf = '/opt/exacloud/clusters/shared_env_enabled'
        for _dom0, _ in self.mReturnElasticAllDom0DomUPair():
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            if aMode:
                _node.mExecuteCmdLog('mkdir -p /opt/exacloud/clusters')
                _node.mExecuteCmdLog('touch '+_str_shared_conf)
            else:
                _node.mExecuteCmdLog('rm '+_str_shared_conf)
            _node.mDisconnect()

    def mExecuteOEDAInstallCmd(self, aCmd, aOptions=None):

        pass

    def mCloneOEDAProperties(self):
        '''
        If necessary, make a recursive copy of all the property files of OEDA
        '''
        prop_path = self.__oeda_path+'/properties'
        if os.path.islink(prop_path):
            cmd_str = '/bin/mv ' + prop_path + ' ' + prop_path + '.orig;'
            cmd_str_2 = '/bin/cp -rL ' + prop_path + '.orig ' + prop_path + '; '
            self.mExecuteLocal(cmd_str)
            self.mExecuteLocal(cmd_str_2)

    def mSetOraInventoryPermissions(self):
        """ Update the oraInventory directory permissions to drwxrwx--- """
        _path = '/u01/app/oraInventory'
        for _, _domU in self.mReturnDom0DomUPair():
            _node = exaBoxNode(get_gcontext())
            _node.mSetUser('grid')
            _node.mConnect(aHost=_domU)
            _cmd_str = '/bin/chmod 770 %s' %(_path)
            _node.mExecuteCmd(_cmd_str)
            _node.mDisconnect()

    def mExecuteCmdPty(self, aFiledesc, aPasswd, aTimeOut=None):
        """
            Send password to the pseudo-terminal
        """
        _fd = aFiledesc
        _timeout = aTimeOut
        _poll_o = select.poll()
        _poll_o.register(_fd, select.POLLIN)
        _s_time = time.time()

        while True:
            _poll_rc = _poll_o.poll(0)
            if _poll_rc == []:
                continue
            _fd, _flags = _poll_rc[0]
            if _flags & select.POLLHUP:
                break
            elif _flags & select.POLLIN:
                _ol = os.read(_fd, 1024)
                if _ol is not None:
                    aPasswd = aPasswd + "\n"
                    os.write(_fd, six.ensure_binary(aPasswd))
            else:
                break
            _e_time = time.time() - _s_time
            if _timeout and _e_time >= _timeout:
                raise ExacloudRuntimeError(0x0117, 0xA, 'Timeout Exception')
            time.sleep(1)

    def mUpdateOedaUserPswd(self, aOedaPath, aUser, aPasswd):

        if not aPasswd:
            ebLogInfo("Empty password, skipping es.properties update.")
            return

        """
            update db/sys pwd  set in oeda before invoking oedacli cmd
        """
        if not os.path.exists(aOedaPath):
            raise ExacloudRuntimeError(0x0730, 0xA, "OEDA path doesn't exist. Aborting.", aStackTrace=False)
        _pid, _fd = pty.fork()
        if _pid == 0:
            try:
                _binary = os.path.join(aOedaPath, "genPasswordHash.sh")
                subprocess.run([_binary, f'-{aUser}'], shell=False, timeout=60, check=True)
            except Exception as e:
                # Parent PID after pty fork is completely isolated, stacktrace
                # is not reported and ebLogXXXX does not work here
                with CrashDump() as crash:
                    crash.ProcessException()
            os._exit(0)
        else:
            self.mExecuteCmdPty(_fd, aPasswd)
            os.close(_fd)
            os.waitpid(_pid, 0)

    def mUpdateOEDAProperties(self, aOptions, aSkipValidation=False):
        '''
        General method to update es.properties based on the given command

        Args:
            aCmd (str): Cluster control command
            aOptions (nsOpt): Global request options
        '''

        _clu_utils = ebCluUtils(self)
        ebLogTrace("mUpdateOEDAProperties: Update es.properties.")
        # Below method is to update the global es properties file in oeda/properties directory
        # if needed. We don't need to check for mIsNoOeda() since oeda request specific directory
        # is not needed for global es.properties

        _clu_utils.mUpdateGlobalEsProperties()

        if self.mIsNoOeda():
            return
        # Below code is to update properties specific to oeda request id folder.

        if not aSkipValidation and (aOptions.jsonconf is not None and 'dbParams' in aOptions.jsonconf.keys()):
            dbParams = aOptions.jsonconf['dbParams']
            if 'pga_memory_size' in dbParams or 'sga_memory_size' in dbParams:
                prop_file = self.__oeda_path + '/properties/es.properties'
                sed_cmd = "/bin/sed -i 's/{0}.*$/{0}={1}/g' {2}"
                cmd_list = []
                cmd_str = sed_cmd.format('ENABLEEXACLOUDPGASGAMEMORYSIZE', 'true', prop_file)
                cmd_list.append(cmd_str)
                for param in ('pga_memory_size', 'sga_memory_size'):
                    if param in dbParams:
                        tvalue = dbParams[param]
                        prop = 'EXACLOUD_' + param.upper()
                        cmd_list.append(sed_cmd.format(prop, tvalue, prop_file))
                for _cmd in  cmd_list :
                    self.mExecuteLocal(_cmd)

        #
        # DB Password update in es.properties
        #

        if not aSkipValidation and (aOptions.jsonconf is not None and 'dbParams' in aOptions.jsonconf.keys()):
            if self.IsZdlraProv() or self.__enable_nid_starterdb:
                ebLogInfo("*** Updating es.properties password for stater db nid ***")
                dbParams = aOptions.jsonconf['dbParams']
                if not self.IsZdlraProv() and (not "passwd" in dbParams or not dbParams['passwd']):
                    ebLogError('The create starter dbnid json must contain passwd parameter')
                    return ebError(0x507)

                # getting the location of the file
                prop_file = self.__oeda_path + '/properties/es.properties'
                ebLogInfo('property file location {0}'.format(prop_file))
                if not self.IsZdlraProv():
                    _passwd = self.mGetWalletViewEntry(dbParams['passwd'])
                    self.mUpdateOedaUserPswd(self.__oeda_path, "non-root", _passwd)

        #
        # CHARSET AND NCHARSET CMD
        #
        if not aSkipValidation and (aOptions.jsonconf is not None and 'dbParams' in aOptions.jsonconf.keys()):
            dbParams = aOptions.jsonconf['dbParams']
            if 'charset' in dbParams and 'ncharset' in dbParams:
                prop_file = self.__oeda_path + '/properties/es.properties'
                sed_cmd = "/bin/sed -i 's/^{0}=.*$/{0}={1}/g' {2}"

                cmd_str = sed_cmd.format('CHARSET', dbParams['charset'], prop_file)
                self.mExecuteLocal(cmd_str)

                cmd_str = sed_cmd.format('NLSCHARSET', dbParams['ncharset'], prop_file)
                self.mExecuteLocal(cmd_str)

        #
        # OEDA QUORUM support for 12.2 (Default is disabled until 12.1.2.3.x)
        # TEMPORARY WORKAROUND FOR PSTACK 18 OCTBP ISSUE (29233958)
        #
        _prop_file = self.__oeda_path + '/properties/es.properties'
        _sed_cmd = "/bin/sed -i 's/{0}.*$/{0}={1}/g' {2}"
        _cmd_list = []
        _cmd_str = _sed_cmd.format('MAXGIVERTODISABLEPSTACK', '18.0.0.0', _prop_file)
        _cmd_list.append(_cmd_str)


        #
        # Bug 29951971: DISABLE OEDA CVU CHECKS
        # Bug 32182391: DISABLE OEDA CVU CHECKS FOR ALL RELEASES
        #
        if self.mCheckConfigOption('skip_cvu_checks', 'True'):
            _cmd_str = _sed_cmd.format('SKIPCVUCHECKS', 'true', _prop_file)
            _cmd_list.append(_cmd_str)
            _cmd_str = _sed_cmd.format('SKIPCVUPOSTHWOSCHECKS', 'true', _prop_file)
            _cmd_list.append(_cmd_str)
            _cmd_str = _sed_cmd.format('SKIPCVUPEERNODECHECKS', 'true', _prop_file)
            _cmd_list.append(_cmd_str)
            _cmd_str = _sed_cmd.format('SKIPALLCVUADDNODECHECKS', 'true', _prop_file)
            _cmd_list.append(_cmd_str)

        if self.__enable_quorum:
            _cmd_str = _sed_cmd.format('ENABLEQUORUMDISK', 'true', _prop_file)
            _cmd_list.append(_cmd_str)
            _cmd_str = _sed_cmd.format('ENABLEQUORUMDISKINSSC', 'true', _prop_file)
            _cmd_list.append(_cmd_str)
            ebLogInfo('*** OEDA PROPERTIES UPDATED TO REFLECT QUORUMDISK ENABLED')
        else:
            _cmd_str = _sed_cmd.format('ENABLEQUORUMDISK', 'false', _prop_file)
            _cmd_list.append(_cmd_str)
            _cmd_str = _sed_cmd.format('ENABLEQUORUMDISKINSSC', 'false', _prop_file)
            _cmd_list.append(_cmd_str)
            ebLogInfo('*** OEDA PROPERTIES UPDATED TO REFLECT QUORUMDISK DISABLED')

        # This property is checked by oeda during add cell. They have made the default
        # behavior to not allow cell addition with different image version than the existing cells.
        # To bypass their validation, we need to set this to true
        _clu_utils.mSetPropertyValueOeda("ALLOWDIFFERENTIMAGEVERSONCELLS", "true", "false")
        # We still need to update the below if nooeda is not set in program arguments - it can happen that
        # the es.properties copied to the request specific directory may not still have the new property value.
        _clu_utils.mSetPropertyValueOeda("DISABLEVALIDATEDGSPACEFOR37371565", "true", "false")
        for _cmd_str in _cmd_list:
            self.mExecuteLocal(_cmd_str)

        if self.mGetEnableAsmss() and self.mGetEnableAsmss().lower() == "true":
            _cmd_str = _sed_cmd.format('ENABLEASMSCOPESECURITY', 'true', _prop_file)
        else:
            _cmd_str = _sed_cmd.format('ENABLEASMSCOPESECURITY', 'false', _prop_file)

        self.mExecuteLocal(_cmd_str)

        ebLogInfo('*** OEDA ROLLBACK temporary: Adding 19.4 to OEDA properties')
        _prop_file = self.__oeda_path + '/properties/s_LinuxXen.properties'
        ebGenerateDBGIProperties.GetCmdAddGIToOEDA(self)
        ebGenerateDBGIProperties.GetCmdAddDBToOEDA(self)

        if not self.mCheckConfigOption("enable_oeda_syslog_format"):
            _cmd_str  =  "/bin/sed -i 's/SETSYSLOGFORMAT=true/SETSYSLOGFORMAT=false/g' {0}".format(self.__oeda_path + '/properties/es.properties')
            self.mExecuteLocal(_cmd_str)

        if self.mIsKVM():
            _gi_size = None
            if aOptions and aOptions.jsonconf:
                if 'gi_home_partition_size' in list(aOptions.jsonconf.keys()):
                    _gi_size = aOptions.jsonconf['gi_home_partition_size']
            if not _gi_size:
                _gi_size = self.mCheckConfigOption('gi_home_partition_size')
            if not _gi_size:
                _gi_size = "50G"

            _oedaReqPath = self.mGetOEDARequestsPath()
            _propertyFile = "{0}/properties/s_LinuxKvm.properties".format(_oedaReqPath)
            ebLogInfo("Change grid home disksize to: {0}".format(_gi_size))
            self.mExecuteLocal("/bin/sed -i 's/VMGRIDDISKSPACE=.*/VMGRIDDISKSPACE={0}/g' {1}".format(_gi_size, _propertyFile))

        # Set the property ADDSCANIPTOHOSTSFILEONEXC to false to not add non resolvable scan name to /etc/hosts
        self.mSetScanPropertyFalseOeda()

        # Update properties for Encryption
        if self.mIsKVM() and ( isEncryptionRequested(aOptions, 'domU') or exacc_fsencryption_requested(aOptions)):
            addEncryptionProperties(self)

        #ENABLE CLUSTERS WITH EDV SUPPORT BY UPDATING FORCEEXCCLOUD=true in oeda/properties/es.properties
        _utils = self.mGetExascaleUtils()
        _utils.mEnableEDVProperty(aOptions)

    def mUpdateOEDAPropertiesFromFile(self, aPropertiesFile):
        """
            General method to update es.properties based on the parameters in exacloud/properties/OEDAProperties.json

            This method will only update the value of the properties that exist in es.properties
            or create new ones if they do not exist in es.properties.
            This method will not clear properties which are not in exacloud/properties/OEDAProperties.json
        """

        # Open JSON file with OEDA properties
        _OEDA_properties_file = aPropertiesFile
        try:
            _f = open(_OEDA_properties_file, 'r')
            _OEDA_properties = json.load(_f)
        except Exception as e:
            _msg = f"Failed to load OEDA properties from file {_OEDA_properties_file}: {e}"
            ebLogError(_msg)
            raise ExacloudRuntimeError(0x0750, 0xA, _msg) from e
        finally:
            _f.close()

        ebLogVerbose("mUpdateOEDA PropertiesFromFile: Update es.properties.")

        for _base_prop_file in list(_OEDA_properties.keys()):

            _prop_file = self.__oeda_path + f'/properties/{_base_prop_file}'
            for property_name, value in _OEDA_properties[_base_prop_file].items():
                # Search if property is already in the file
                _cmd = f"/bin/grep -w {property_name} {_prop_file}"
                _return_code, _, _, _ = self.mExecuteLocal(_cmd)
                if _return_code == 0:
                    _sed_cmd = f"/bin/sed -i 's/\<{property_name}\>.*$/{property_name}={value}/g' {_prop_file}"
                    self.mExecuteLocal(_sed_cmd)
                else:
                    _sed_cmd = f"/bin/sed -i '$a {property_name}={value}' {_prop_file}"
                    self.mExecuteLocal(_sed_cmd)
                    ebLogInfo(f"Added property {property_name}={value}")

    #
    # Note: Temporary workaround for the update of /etc/oratab w/ DB 12.2 Klone image.
    #
    def mPatchOratab122(self,aOptions=None):

        if self.__dbname is None:
            return
        if self.__db_version is None or self.__db_version != '122':
            return
        ebLogDebug('*** Patching Oratab for 12.2 DB')
        _dpairs = self.mReturnDom0DomUPair()
        for _, _domU in _dpairs:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domU)
            #
            # Get Node Index
            #
            _cmd = "cat /etc/oratab | grep '^+.*'"
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            _out = _o.readlines()
            if not _out or len(_out) == 0:
                _node.mDisconnect()
                ebLogWarn('*** ORATAB entry not found for grid')
                return
            _index  = _out[0][4]
            _dbname = self.__dbname
            #
            # Patch oratab
            #
            _cmdstr = 'cat /etc/oratab'
            _i, _o, _e = _node.mExecuteCmd(_cmdstr)
            _out = _o.readlines()
            if not _out or len(_out) == 0:
                ebLogWarn('*** ORATAB entry not found for grid')
                _node.mDisconnect()
                continue

            for _entry in  _out:
                _entry = _entry.strip()
                if not len(_entry) or _entry[0] == '#':
                    continue
                _oratab = _entry.split(':')
                _sid  = _oratab[0]
                _path = _oratab[1]
                ebLogInfo('*** sid: %s path: %s' % (_sid, _path))
            # Fetch 12.2 full version
            _fver = self.mGetDBConfig()['122'][0]
            # xxx/HACK: Add manually 12.2 entry in the oratab
            _cmd  = "sed 's/^%s.*//' -i /etc/oratab" % _dbname
            # xxx/MR - TODO: Compute location for 12.2.x.x based on db_homes from exabox.conf
            _str  = "; echo '%s:/u01/app/oracle/product/"+_fver+"/dbhome_1:N        # line added by Exacloud' >> /etc/oratab"
            _cmd += _str % _dbname
            _node.mExecuteCmdLog(_cmd)
            #
            # xxx/HACK: Setup auto restart for 12.2 DB
            #
            _coptions = get_gcontext().mGetConfigOptions()
            _fver = self.mGetGridConfig()['122'][0]
            _cmd = "/u01/app/"+_fver+"/grid/bin/srvctl stop database -d %s" % (_dbname)
            _node.mExecuteCmdLog(_cmd)
            _cmd = "/u01/app/"+_fver+"/grid/bin/srvctl start database -d %s" % (_dbname)
            _node.mExecuteCmdLog(_cmd)
            _node.mDisconnect()


    #
    # ER 27236335
    #
    def mXendConfig(self):
        if self.mCheckConfigOption('disable_xendconfig', 'True'):
            return
        _records = ['(enclosure-asset-tag \'OracleCloud.com\')', '(enable-dump yes)']  # records for future use : system-family, system-type
        _confile = '/etc/xen/xend-config.sxp'

        for _dom0, _ in self.mReturnDom0DomUPair():
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(_dom0)
            for _r in _records:
                ebLogInfo('*** Checking for '+_r+' in '+_confile+' on '+_dom0)
                _cmd = 'cat "%s" | grep "%s"' %(_confile, _r)
                _i, _o, _e = _node.mExecuteCmd(_cmd)
                _out = _o.readlines()
                if not _out or len(_out) == 0:
                    _cmd = 'echo "%s" >> "%s"' % (_r, _confile)
                    ebLogInfo('*** Appending '+_r+' in '+_confile+' on '+_dom0)
                    _i, _o, _e = _node.mExecuteCmd(_cmd)
                    _out = _o.readlines()
                    if _out:
                        _node.mDisconnect()
                        ebLogError('*** Unable to update '+_confile+' in '+_dom0)
                        return
                    _node.mExecuteCmdLog('/etc/init.d/xend restart')
            _node.mDisconnect()
        return


    #
    # ER 27409564 - run reclaimdisks on dom0
    #
    def mRunReclaimdisks(self, _option):
        if self.mIsKVM():
            ebLogWarn('*** Reclaim Disk is not supported on KVM target')
            return

        if _option is not None:
            _cmd = '/opt/oracle.SupportTools/reclaimdisks.sh -%s' %(_option)
        else:
            return

        for _dom0, _ in self.mReturnDom0DomUPair():
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(_dom0)
            ebLogInfo('*** Running '+_cmd+' on '+_dom0)
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            _out = _o.readlines()
            if _out:
                ebLogInfo('%s'%_out)
            _node.mDisconnect()
        return


    # ER 28461240
    def mStopVmetrics(self):
        if self.mIsKVM():
            ebLogWarn ("Vmetric not supported in KVM")
            return -1
        for _dom0, _ in self.mReturnDom0DomUPair():
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(_dom0)
            ebLogInfo('*** Only in cloud env : Stopping vmmetrics service on '+_dom0)
            _node.mExecuteCmd('cd /etc/init.d; service vmetrics stop')
            ebLogInfo('*** Checking status on '+_dom0)
            _fin, _fout, _ferr = _node.mExecuteCmd('cd /etc/init.d; service vmetrics status')
            _out = _fout.readlines()
            if 'stopped' not in _out[0]:
                ebLogError('*** failed to stop vmetrics service')
                ebLogError(_out[0])
            _node.mDisconnect()


    #
    # ERs 27194446, 27503421
    #
    def mUpdateVmetrics(self,aType):
        _type = aType
        if _type == 'vmexacs':
            if self.mCheckConfigOption('disable_vmexacs', 'True'):
                return
            _localfiles = ['vmexacs', 'vmexacs.conf', 'networkstatus.py']
        if _type == 'vmetrics':
            if self.mCheckConfigOption('disable_vmetrics', 'True'):
                return
            _localfiles = ['vmetrics', 'vmetrics.conf']
        if _type == 'vmexacs_kvm':
            if self.mCheckConfigOption('disable_vmexacs_kvm', 'True'):
                return
            _localfiles = ['vmexacs_kvm', 'vmexacs_kvm_channel.xml', 'vmexacs_kvm_controller.xml', 'vmexacs_kvm.svc']

        _localdir      = 'misc/'+_type+'/'
        _remoteInit    = '/etc/init.d/'+_type
        _remotePkgdir  = '/opt/oracle.'+_type+'/'

        ebLogInfo('*** Update/Install '+_type+' pkg')

        _localhash_dict={}
        for _lf in _localfiles:
            _out = subprocess.check_output(['sha256sum', _localdir+_lf]).decode('utf8')
            _local_hash=''
            if _out:
                _local_hash = _out.strip().split(' ')[0]
            else:
                ebLogError('*** Fail to compute sha256sum for %s' % (_lf))
            _localhash_dict[_lf] = _local_hash

        def _mUpdateVmetrics(dom0):
            with connect_to_host(dom0, get_gcontext()) as _node:
                if _type == 'vmexacs' and _node.mFileExists(_remoteInit) is False:
                    ebLogInfo('*** '+_type+' pkg not installed on '+_dom0)
                    _node.mCopyFile(_localdir+_type+'.svc', _remoteInit)
                    _node.mExecuteCmd('chmod 555 '+_remoteInit+';mkdir -p '+_remotePkgdir)
                    for _f in _localfiles:
                        _node.mCopyFile(_localdir+_f, _remotePkgdir+_f)
                        _node.mExecuteCmd('chmod 555 '+_remotePkgdir+_f)
                    ebLogInfo('*** Checking configuration on '+_dom0+' :'+_remoteInit)
                    _node.mExecuteCmd('chkconfig '+_type+' on')
                    ebLogInfo('*** Starting service on '+_dom0+' :'+_remoteInit)
                    _node.mExecuteCmd('cd /etc/init.d; service '+_type+' start')
                    ebLogInfo('*** Checking status on '+_dom0+' :'+_remoteInit)
                    _fin, _fout, _ferr = _node.mExecuteCmd('cd /etc/init.d; service '+_type+' status')
                    _out = _fout.readlines()
                    if 'running' not in _out[0]:
                        ebLogError('*** Failed to start '+_type+' service on '+_dom0)
                        ebLogError(_out[0])
                        return
                elif _type ==  'vmexacs_kvm' and _node.mFileExists(_remotePkgdir+_type) is False:
                    ebLogInfo('*** '+_type+' pkg not installed on '+_dom0)
                    _node.mCopyFile(_localdir+_type+'.svc', _remoteInit)
                    _node.mExecuteCmd('chmod 555 '+_remoteInit+';mkdir -p '+_remotePkgdir)
                    for _f in _localfiles:
                        _node.mCopyFile(_localdir+_f, _remotePkgdir+_f)
                        _node.mExecuteCmd('chmod 555 '+_remotePkgdir+_f)
                else:
                    ebLogInfo('*** '+_type+' pkg is installed on '+_dom0+', checking for any updates')
                    _remotehash_dict={}
                    for _f in _localfiles:
                        _fin, _fout, _ferr = _node.mExecuteCmd('sha256sum ' + _remotePkgdir+_f)
                        _out = _fout.readlines()
                        _remote_hash=''
                        if _out:
                            _remote_hash = _out[0].split(' ')[0]
                        else:
                            ebLogError('*** Fail to compute sha256sum for %s' % (_f))
                        _remotehash_dict[_f] = _remote_hash

                    if _localhash_dict == _remotehash_dict:
                        ebLogInfo('*** No update needed for '+_type+' pkg files on '+_dom0)
                    else:
                        ebLogInfo('*** '+_type+' file(s) needs to be updated on '+_dom0)
                        _updatelist=[]
                        for _k in list(_localhash_dict.keys()):
                            if _localhash_dict[_k] != _remotehash_dict[_k]:
                                _updatelist.append(_k)
                        if not self.mIsKVM():
                            ebLogInfo('*** Stopping service on '+_dom0)
                            _node.mExecuteCmd('cd /etc/init.d; service '+_type+' stop')
                            ebLogInfo('*** Checking status on '+_dom0+' :'+_remoteInit)
                            _fin, _fout, _ferr = _node.mExecuteCmd('cd /etc/init.d; service '+_type+' status')
                            _out = _fout.readlines()
                            if 'stopped' not in _out[0]:
                                ebLogError('*** failed to stop '+_type+' service')
                                ebLogError(_out[0])
                                return
                        else:
                            # Stop the vmexacs_kvm service
                            _cmd = "systemctl stop vmexacs_kvm; systemctl disable vmexacs_kvm"
                            _node.mExecuteCmdLog(_cmd)
                            _state = _node.mSingleLineOutput('systemctl is-enabled {}'.format('vmexacs_kvm'))
                            if _state == "enabled":
                                ebLogError('*** Failed to stop vmexacs_kvm service on '+_dom0)
                                return
                        for _f in _updatelist:
                            ebLogInfo('*** Copying file: '+_localdir+_f+' to : '+_dom0+':'+_remotePkgdir+_f)
                            _node.mCopyFile(_localdir+_f,_remotePkgdir+_f)
                        if not self.mIsKVM():
                            ebLogInfo('*** Checking configuration on '+_dom0+' :'+_remoteInit)
                            _node.mExecuteCmd('chkconfig '+_type+' on')
                            ebLogInfo('*** Starting service on '+_dom0+' :'+_remoteInit)
                            _node.mExecuteCmd('cd /etc/init.d; service '+_type+' start')
                            ebLogInfo('*** Checking status on '+_dom0+' :'+_remoteInit)
                            _fin, _fout, _ferr = _node.mExecuteCmd('cd /etc/init.d; service '+_type+' status')
                            _out = _fout.readlines()
                            if 'running' not in _out[0]:
                                ebLogError('*** failed to start '+_type+' service on '+_dom0)
                                ebLogError(_out[0])
                                return
                        else:
                            # Start the vmexacs_kvm service
                            _cmd = "systemctl enable vmexacs_kvm ; systemctl start vmexacs_kvm"
                            _node.mExecuteCmdLog(_cmd)
                            _state = _node.mSingleLineOutput('systemctl is-enabled {}'.format('vmexacs_kvm'))
                            if _state == "disabled":
                                ebLogError('*** Failed to start vmexacs_kvm service on '+_dom0)
                                return
        _plist = ProcessManager()
        for _dom0, _ in self.mReturnDom0DomUPair():
            _poolArgs = []
            _poolArgs = [_dom0]
            _p = ProcessStructure(_mUpdateVmetrics, _poolArgs)
            _p.mSetMaxExecutionTime(15*60) # 15 minutes
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)
        _plist.mJoinProcess()

    #
    # ER 27503421 - This is the domU part for vmexacs pkg
    #
    def mCopyVmexacsRpm(self):
        if self.mCheckConfigOption('disable_vmexacs', 'True'):
            return
        _localfile = 'misc/vmexacs/oraecscol.rpm'
        _remotefile = '/u02/opt/dbaas_images/oraecscol.rpm'
        _dpairs = self.mReturnDom0DomUPair()
        for _, _domU in _dpairs:
            _nodeU = exaBoxNode(get_gcontext())
            _nodeU.mConnect(aHost=_domU)
            if _nodeU.mFileExists(_remotefile) is False:
                ebLogInfo('*** Copy %s to %s on %s' %(_localfile, _remotefile, _domU))
                _nodeU.mCopyFile(_localfile, _remotefile)
                _cmd_str = 'chown -fR oracle.oinstall %s' %_remotefile
                _nodeU.mExecuteCmd(_cmd_str)
                _rc = _nodeU.mGetCmdExitStatus()
                if _rc:
                    ebLogError('*** Failed to chown of %s on %s' % (_remotefile, _domU))
                    _nodeU.mDisconnect()
                    return
                else:
                    ebLogInfo('*** Copied oraecscol.rpm to %s successfully' % (_domU))
            _nodeU.mDisconnect()


    #
    # BUG 27941250
    #
    def mSaveCellInformation(self):
        if self.mCheckConfigOption('disable_savecellinfo`', 'True'):
            return
        _f = 'config.json'
        _config_dir = '/opt/exacloud/cluster/'

        _cluster_dict = {}
        _cluster_dict['cellnodes'] = []

        # Bug 29667439 : Service subtype - exaopc, exacm or exabm
        _cluster_dict['subtype'] = []

        _cell_list = self.mReturnCellNodes()
        for _cell in _cell_list:
            _cluster_dict['cellnodes'].append(_cell)

        if self.__exabm:
            _cluster_dict['subtype'].append('exabm')
        elif self.__exacm:
            _cluster_dict['subtype'].append('exacm')
        else:
            _cluster_dict['subtype'].append('exaopc')

        _str_config = json.dumps(_cluster_dict, sort_keys=True, indent=4, separators=(',',' : '))

        #
        # Save configuration in /opt/exacloud/cluster/ in all domUs
        #
        _ntp = NamedTemporaryFile(delete=False)
        _ntp.file.write(six.ensure_binary(_str_config))
        _ntp.file.close()
        #
        # Save the cluster configuration on each domU of the VM RAC Cluster
        #
        _dpairs = self.mReturnDom0DomUPair()
        for _, _domU in _dpairs:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domU)
            _node.mExecuteCmd('mkdir -p '+_config_dir)
            _node.mCopyFile(_ntp.name, _config_dir+_f)
            ebLogInfo('*** Saving Cell information on domU: '+_domU+' in '+_config_dir+_f)
            _bin_chmod = node_cmd_abs_path_check(_node, "chmod")
            _out_set_perms = node_exec_cmd(
                _node, f"{_bin_chmod} 644 {_config_dir+_f}")
            ebLogTrace(_out_set_perms)
            _node.mDisconnect()
        # Cleanup (remove temporary file)
        os.unlink(_ntp.name)
        assert(os.path.exists(_ntp.name)==False)


    #
    # ER 30874841
    #
    def mCopySAPfile(self):
        _f = 'sapplatforminfo'
        _localdir = 'misc/sap/'
        _remotedir = '/var/opt/oracle/misc/'
        _dpairs = self.mReturnDom0DomUPair()
        for _, _domU in _dpairs:
            _nodeU = exaBoxNode(get_gcontext())
            _nodeU.mConnect(aHost=_domU)
            if _nodeU.mFileExists(_remotedir+_f) is False:
                #_nodeU.mExecuteCmd('mkdir -p '+_remotedir) # by now rpm ivh of dbaastools is done
                ebLogInfo('*** Copy %s to %s on %s' %(_localdir+_f, _remotedir+_f, _domU))
                _nodeU.mCopyFile(_localdir+_f, _remotedir+_f)
                _cmd_str = 'chown -fR oracle.oinstall %s' %(_remotedir+_f)
                _nodeU.mExecuteCmdLog(_cmd_str)
                _cmd_str = 'chmod 555 %s' %(_remotedir+_f)
                _nodeU.mExecuteCmd(_cmd_str)
            _nodeU.mDisconnect()

    #
    # ER 27555477
    #
    def mCopyCreateVIP(self):
        if self.mCheckConfigOption('disable_addvip2vcn', 'True'):
            return
        _f = 'addviptovcn.sh'
        _localdir = 'misc/sap/'
        _remotedir = '/opt/exacloud/sap/'
        _dpairs = self.mReturnDom0DomUPair()
        for _, _domU in _dpairs:
            _nodeU = exaBoxNode(get_gcontext())
            _nodeU.mConnect(aHost=_domU)
            if _nodeU.mFileExists(_remotedir+_f) is False:
                _nodeU.mExecuteCmd('mkdir -p '+_remotedir)
                ebLogInfo('*** Copy %s to %s on %s' %(_localdir+_f, _remotedir+_f, _domU))
                _nodeU.mCopyFile(_localdir+_f, _remotedir+_f)
                _cmd_str = 'chmod 555 %s' %(_remotedir+_f)
                _nodeU.mExecuteCmd(_cmd_str)
            _nodeU.mDisconnect()


    #
    # Bug 27216120, 27371691
    #
    def mUpdateRpm(self, aRpm, aUndo=False, aDom0DomUPair=None, aForce=False, aLocalPath='images/', aRemotePath='/u02/opt/dbaas_images/'):
        _rpm = aRpm
        if _rpm == 'dbaastools_exa_main.rpm' and self.mCheckConfigOption('disable_update_dbaas_rpm', 'True'):
            return
        if _rpm.startswith('dbcs-agent'):
            if self.mCheckConfigOption('disable_dbcs_agent_install', 'True'):
                ebLogInfo('*** disable_dbcs_agent_install is True. Skipping dbcs-agent rpm installation.')
                return
        _local_hash = 0
        _out = ''
        _operation = 'Update/Install'
        _package_name = ''
        _localfile = os.path.join(aLocalPath,_rpm)
        _remotefile = os.path.join(aRemotePath,_rpm)

        if os.path.exists(_localfile):
            _out = subprocess.check_output(['sha256sum', _localfile]).decode('utf8')
        else:
            ebLogError('*** {} not available'.format(_localfile))
            return

        if _out:
            _local_hash = _out.strip().split(' ')[0]
            _cmd_arr = ['/bin/rpm', '--qf', '"%{NAME}\n"', '-qp', _localfile]
            _out = subprocess.check_output(_cmd_arr).decode('utf8')
            _package_name = _out.replace('"', '').strip()
        else:
            ebLogError('*** Fail to compute sha256sum for {}'.format(_localfile))
            return

        if aUndo:
            _operation = 'Uninstall'
            ebLogInfo('*** Uninstall operation for {}'.format(_rpm))
            _cmd = '/bin/rpm -e ' + _package_name
        else:
            _cmd = '/bin/rpm --force -Uhv ' + _remotefile
            if "mysql" in _remotefile or "dcs" in _remotefile:
                _cmd = _cmd + f" > /tmp/{_rpm}-installoutput"

        def mSingleUpdateRPM(aDomU, aRPM, aPackageName, aOperation, aLocalFile, aRemoteFile, aLocalHash):

            _nodeU = exaBoxNode(get_gcontext())
            if not _nodeU.mIsConnectable(aHost=aDomU) and aUndo:
                ebLogWarn(f'*** mUpdateRpm: uninstall {aRpm}. domU: {aDomU} not connectable')
                return

            with connect_to_host(_domU, get_gcontext()) as _nodeU:

                ebLogInfo('*** verify if {} is installed on {}'.format(aRpm, aDomU))

                _is_rpm_installed = False
                _cmd_str = '/bin/rpm -qa | /bin/grep {}'.format(aPackageName)

                _nodeU.mExecuteCmdLog(_cmd_str)
                _rc = _nodeU.mGetCmdExitStatus()

                if _rc == 0:
                    _is_rpm_installed = True
                    ebLogInfo('*** {} is installed on {}'.format(aRPM, aDomU))
                else:
                    ebLogInfo('*** {} is not installed on {}'.format(aRPM, aDomU))

                _is_latestaRPM = False

                if not aUndo:
                    ebLogInfo('*** verify sha256sum of {} in {}'.format(aRemoteFile, aDomU))
                    _fin, _fout, _ferr = _nodeU.mExecuteCmd('sha256sum ' + aRemoteFile)
                    _out = _fout.readlines()
                    if _out:
                        _remote_hash = _out[0].strip().split(' ')[0]
                    else:
                        ebLogError('*** Fail to compute sha256sum for {} on {}'.format(aRemoteFile, aDomU))
                        _remote_hash = 0

                    if aLocalHash == _remote_hash:
                        _is_latestaRPM = True
                        ebLogInfo('*** No update to {} is needed on {}'.format(aRemoteFile, aDomU))
                    else:
                        ebLogInfo('*** Updating rpm {} in {} on {}'.format(aRPM,aRemoteFile,aDomU))
                        if aRPM.startswith('dbcs-agent'):

                            # Call to mCopyFile() requires <aRemoteFile> to be of the
                            # format - /u02/opt/dbaas_images/<filename>. But, since
                            # dbcs-agent.x86_64.rpm is not already present at this
                            # stage, create the file in the dir before calling
                            # mCopyFile()
                            _nodeU.mExecuteCmdLog('touch ' + aRemoteFile)

                        ebLogInfo('*** Copy {} on {}'.format(aRemoteFile, aDomU))
                        _nodeU.mCopyFile(aLocalFile, aRemoteFile)
                        _cmd_str = '/bin/chown -fR oracle.oinstall {}'.format(aRemoteFile)
                        _nodeU.mExecuteCmdLog(_cmd_str)

                if not _is_rpm_installed or not _is_latestaRPM or aForce:
                    ebLogInfo(f'*** RPM operation, cmd: [{_cmd}], on DomU: {aDomU}')
                    _nodeU.mExecuteCmdLog(_cmd)
                    _rc = _nodeU.mGetCmdExitStatus()
                    if _rc:
                        ebLogError('*** Failed to {} {} on {}'.format(aOperation, aRPM, aDomU))
                        return
                    else:
                        ebLogInfo('*** {}ed {} in {} successfully'.format(aOperation, aRPM, aDomU))
                else:
                    ebLogInfo('*** No operation was performed')

        # Execute RPM update in parallel
        if aDom0DomUPair:
            _dpairs = aDom0DomUPair
        else:
            _dpairs = self.mReturnDom0DomUPair()

        _plist = ProcessManager()

        for _, _domU in _dpairs:
            _p = ProcessStructure(mSingleUpdateRPM, [_domU, _rpm, _package_name, _operation, _localfile, _remotefile, _local_hash], _domU)
            _p.mSetMaxExecutionTime(60*60) # 60 minutes
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()

    #
    # ER 25900538
    #
    def mCopyOneoffZipToDomus(self, aOptions=None):
        _local_hash = 0
        _out = ''
        _localfile = 'images/ecsOneoffArchive.zip'
        _remotefile = '/u02/opt/dbaas_images/ecsOneoffArchive.zip'
        ebLogInfo('*** copy %s to %s on domUs' % (_localfile, _remotefile))
        # irrespective of oneoffs availability the ecsOneoffArchive.zip will have the exa_map file at the least
        if os.path.exists(_localfile):
            _out = subprocess.check_output(['sha256sum', _localfile]).decode('utf8')
        else:
            ebLogError('*** %s not available' %(_localfile))
            return

        if _out:
            _local_hash = _out.strip().split(' ')[0]
        else:
            ebLogError('*** Fail to compute sha256sum for %s' % (_localfile))
            return

        try:
            self.mCopyFileToDomus(_localfile, _remotefile, aMode='644', aDomUList=None)
        except:
            ebLogError('::mCopyFileToDomus failed VM not up and running ?')
            return

        _dpairs = self.mReturnDom0DomUPair()
        for _, _domU in _dpairs:
            _nodeU = exaBoxNode(get_gcontext())
            _nodeU.mConnect(aHost=_domU)
            ebLogInfo('*** verify sha256sum of %s in %s' % (_remotefile, _domU))
            _fin, _fout, _ferr = _nodeU.mExecuteCmd('sha256sum ' + _remotefile)
            _out = _fout.readlines()
            if _out:
                _remote_hash = _out[0].strip().split(' ')[0]
            else:
                ebLogError('*** Fail to compute sha256sum for %s on %s' % (_remotefile, _domU))
                _remote_hash = 0
            if _local_hash == _remote_hash:
                ebLogInfo('*** Copied %s to %s on %s successfully' % (_localfile, _remotefile, _domU))
            else:
                ebLogError('*** Failed to copy %s to %s on %s' % (_localfile, _remotefile, _domU))
            _nodeU.mDisconnect()


    #
    # Bug 27098559
    #
    def mCreateAcfsDirs(self,aOptions=None):
        ebLogInfo('*** Create ACFS dirs and symlinks')

        _cmd = '/bin/mkdir -p /acfs01/dbaas_acfs; /bin/mkdir -p /acfs01/acfs; /bin/mkdir -p /acfs01/app_acfs; /bin/chown -fR oracle.oinstall /acfs01/dbaas_acfs /acfs01/acfs /acfs01/app_acfs'
        domu_list = [ _domu for _ , _domu in self.mReturnDom0DomUPair()]
        domu = domu_list.pop(0)
        _node = exaBoxNode(get_gcontext())
        _node.mConnect(aHost=domu)
        _node.mExecuteCmdLog(_cmd)
        _node.mDisconnect()

        _cmd = '/bin/mkdir -p /var/opt/oracle ; /bin/mkdir -pm 755 /scratch; /bin/chown -f oracle.oinstall /var/opt/oracle ; ln -sf /acfs01/dbaas_acfs /var/opt/oracle/dbaas_acfs; ln -sf /acfs01/acfs /scratch/acfs; ln -sf /acfs01/app_acfs /u02/app_acfs'
        for _, _domu in self.mReturnDom0DomUPair():
            _nodeU = exaBoxNode(get_gcontext())
            _nodeU.mConnect(aHost=_domu)
            _nodeU.mExecuteCmdLog(_cmd)
            _nodeU.mDisconnect()

    #
    # OneOff patch/fix for bug 23715436
    #
    def mBug23715436(self):

        ebLogInfo('*** BUG 23715436: 12.1 - 23196995/24310686 11.2 - 23207383/24302480 one-off patch')
        _dpairs = self.mReturnDom0DomUPair()
        for _, _domU in _dpairs:
            _node = exaBoxNode(get_gcontext())
            _node.mSetUser('oracle')
            _node.mConnect(aHost=_domU)
            #
            # Get Node Index
            #
            _cmd = "cat /etc/oratab | grep '^+.*'"
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            _out = _o.readlines()
            if not _out or len(_out) == 0:
                _node.mDisconnect()
                ebLogWarn('*** ORATAB entry not found for grid')
                return
            _index  = _out[0][4]
            #
            # Get dbname - assume one dbhome in /etc/oratab at this stage
            #
            try:
                _jconf = self.__options.jsonconf
            except:
                _jconf = None
            # DB Payloads checks
            if _jconf is not None and 'dbParams' in list(_jconf.keys()) and 'dbname' in list(_jconf['dbParams'].keys()):
                _cmd = "cat /etc/oratab | grep -v '^#' | grep '^%s:'" % _jconf['dbParams']['dbname']
                ebLogInfo('*** Fetching DBHOME for DBNAME: %s' %(_jconf['dbParams']['dbname']))
            else:
                _cmd = "cat /etc/oratab | grep -v '^#' | grep 'dbhome_1'"
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            _out = _o.readlines()
            if not _out or len(_out) == 0:
                ebLogWarn('*** DBHOME not found in /etc/oratab')
                # DB has been deleted - get home from json payload
                if self.__db_version is None:
                    _node.mDisconnect()
                    ebLogWarn('*** DB Version not found skipping rollback')
                    return
                if self.__db_version == '121':
                    _dbname, _dbpath = 'unknown', '/u01/app/oracle/product/12.1.0.2/dbhome_1'
                elif self.__db_version == '112':
                    _dbname, _dbpath = 'unknown', '/u01/app/oracle/product/11.2.0.4/dbhome_1'
                else:
                    _node.mDisconnect()
                    ebLogWarn('*** DB Version not supported skipping rollback')
                    return
            else:
                _dbname, _dbpath, _  = _out[0].split(':')
                if len(_out) > 1:
                    ebLogWarn('*** Multiple DB Homes not supported using: %s/%s' % (_dbname,_dbpath))
                else:
                    ebLogInfo('*** Running one-off patch rollback against: %s/%s' % (_dbname,_dbpath))
            #
            # GET OPATCH LIST
            #
            _cmd = "export ORACLE_SID=%s%s ; export ORACLE_HOME=%s ; PATH=$PATH:$ORACLE_HOME/bin ; export PATH ;" % (_dbname, _index, _dbpath)
            _cmd += "cd $ORACLE_HOME ; OPatch/opatch lspatches"
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            _out = _o.readlines()
            _patchid = None
            _libpath = None
            _cmdrollback = None
            if _out and len(_out):
                for _e in _out:
                    # 11.2 patch OCT
                    if _e.find('24693827') != -1:
                        _libpath = '/u01/app/oracle/product/11.2.0.4/dbhome_1/lib/libvsn11.a.pre24693827'
                        _libpath_new = '/u01/app/oracle/product/11.2.0.4/dbhome_1/lib/libvsn11.a'
                        _cmdrollback = 'cp '+_libpath+ ' ' +_libpath_new
                        if _node.mFileExists(_libpath):
                            _patchid = '24693827'
                            ebLogWarn('*** Found patch %s' % ('24693827'))
                    # 11.2 patch APR
                    if _e.find('23207383') != -1:
                        _libpath = '/u01/app/oracle/product/11.2.0.4/dbhome_1/lib/libvsn11.a.pre23207383'
                        _libpath_new = '/u01/app/oracle/product/11.2.0.4/dbhome_1/lib/libvsn11.a'
                        _cmdrollback = 'cp '+_libpath+ ' ' +_libpath_new
                        if _node.mFileExists(_libpath):
                            _patchid = '23207383'
                            ebLogWarn('*** Found patch %s' % ('23207383'))
                    # 11.2 patch JUL
                    if _e.find('24302480') != -1:
                        _libpath = '/u01/app/oracle/product/11.2.0.4/dbhome_1/lib/libvsn11.a.pre24302480'
                        _libpath_new = '/u01/app/oracle/product/11.2.0.4/dbhome_1/lib/libvsn11.a'
                        _cmdrollback = 'cp '+_libpath+ ' ' +_libpath_new
                        if _node.mFileExists(_libpath):
                            _patchid = '24302480'
                            ebLogWarn('*** Found patch %s' % ('24302480'))
                    # 12.1 patch OCT
                    if _e.find('24701507') != -1:
                        _libpath = '/u01/app/oracle/product/12.1.0.2/dbhome_1/lib/libvsn12.a.pre24701507'
                        _libpath_new = '/u01/app/oracle/product/12.1.0.2/dbhome_1/lib/libvsn12.a'
                        _cmdrollback = 'cp ' + _libpath + ' ' + _libpath_new
                        if _node.mFileExists(_libpath):
                            ebLogWarn('*** Found patch %s' % ('24701507'))
                            _patchid = '24701507'
                    # 12.1 patch APR
                    if _e.find('23196995') != -1:
                        _libpath = '/u01/app/oracle/product/12.1.0.2/dbhome_1/lib/libvsn12.a.pre23196995'
                        _libpath_new = '/u01/app/oracle/product/12.1.0.2/dbhome_1/lib/libvsn12.a'
                        _cmdrollback = 'cp '+_libpath+ ' ' +_libpath_new
                        if _node.mFileExists(_libpath):
                            ebLogWarn('*** Found patch %s' % ('23196995'))
                            _patchid = '23196995'
                    # 12.1 patch JUL
                    if _e.find('24310686') != -1:
                        _libpath = '/u01/app/oracle/product/12.1.0.2/dbhome_1/lib/libvsn12.a.pre24310686'
                        _libpath_new = '/u01/app/oracle/product/12.1.0.2/dbhome_1/lib/libvsn12.a'
                        _cmdrollback = 'cp '+_libpath+ ' ' +_libpath_new
                        if _node.mFileExists(_libpath):
                            ebLogWarn('*** Found patch %s' % ('24310686'))
                            _patchid = '24310686'
            if _patchid is not None and _libpath is not None:
                _cmd = "export ORACLE_SID=%s%s ; export ORACLE_HOME=%s ; PATH=$PATH:$ORACLE_HOME/bin ; export PATH ;" % (_dbname, _index, _dbpath)
                _cmd += "cd $ORACLE_HOME ; OPatch/opatch rollback -silent -id "+_patchid
                _node.mExecuteCmdLog(_cmd)
                _node.mExecuteCmdLog(_cmdrollback)
            _node.mDisconnect()


    #
    #  Restore just the files backup of the
    #

    def mExecuteRestoreBkupFiles(self):
        ebLogInfo("Restoring ... backup files ")

        if self.__nid_backup_files:
            _node = exaBoxNode(get_gcontext())
            domu_list = [ _domu for _ , _domu in self.mReturnDom0DomUPair()]
            for domu in domu_list:
                _node.mSetUser('oracle')
                _node.mConnect(aHost=domu)

                for k in list(self.__nid_backup_files.keys()):
                    cmd = "cp -f {0} {1}".format(self.__nid_backup_files[k], k)
                    fin, out, err = _node.mExecuteCmd(cmd)
                    _dbinfo  = out.read()
                    _errinfo = err.read()
                    ebLogInfo("{0} {1}".format(_dbinfo, _errinfo))

                _node.mDisconnect()

    def mCheckCrsUp(self, aDomU):
        """
        Checks the CRS status Up or Down. Returns True if up else, False
        """

        _ret = True
        _node = aDomU
        _disconnect = False
        if isinstance(_node, str):
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=aDomU)
            _disconnect = True
        _domU = _node.mGetHostname()
        ebLogInfo("Wait for crs to be up")
        _crs_cmd_pfx, _, _ = self.mGetOracleBaseDirectories(_domU)
        # If we can't find the path to the binaries because oratab has not yet
        # been populated, we use the FS mount path, which should be the same
        if not _crs_cmd_pfx:
            _crs_cmd_pfx = self.mGetClusters().mGetCluster().mGetCluHome()
        _crs_cmd_pfx += '/bin/crsctl '
        _cmd = _crs_cmd_pfx + 'check crs'
        _in, _out, _err = _node.mExecuteCmd(_cmd)
        _output = _out.readlines()
        # expcting the following output
        # CRS-4638: Oracle High Availability Services is online
        # CRS-4537: Cluster Ready Services is online
        # CRS-4529: Cluster Synchronization Services is online
        # CRS-4533: Event Manager is online

        if _output:
            _crs_up = True
            for _line in _output:
                _line = _line.strip()
                if "is online" not in _line:
                    ebLogInfo('*** crs check cluster output: ' +_line)
                    _crs_up = False
                    break
            if _crs_up == True:
                _ret = _ret and True
            else:
                _ret = _ret and False
        else:
            _errors = _err.readlines()
            ebLogError("CRS check output:\n" + "\n".join(_errors))
            _ret = False
        if _disconnect:
            _node.mDisconnect()
        return _ret

    def IsHeteroConfig(self) -> Tuple[bool, Set[str]]:
        _list = []        
        for _dom0, _ in self.mReturnDom0DomUPair():
            _model = self.mGetNodeModel(_dom0)
            _list.append(_model)
        rCode = len(set(_list)) != 1
        rSet = set(_list)
        return rCode, rSet

    def mGetNetworkSlaves(self, aDomU, aNetworkType):
        _domU = aDomU
        _id = None
        _host = None
        _slave = None

        _domU_mac = self.__machines.mGetMachineConfig(_domU)
        _domU_net_list = _domU_mac.mGetMacNetworks()
        for _net_id in _domU_net_list:
            _net_conf = self.__networks.mGetNetworkConfig(_net_id)
            _type = _net_conf.mGetNetType()
            if aNetworkType == _type:
                _id = _net_conf.mGetNetId()
                _host = _net_conf.mGetNetHostName()
                _slave = _net_conf.mGetNetSlave()
        return _id, _host, _slave

    def mHandlerUpdateNtpDns(self):
        """
        Handler to update NTP and DNS information for a cluster
        """
        _oeda_path  = self.mGetOedaPath()
        _oedacli_bin = _oeda_path + '/oedacli'
        _savexmlpath = _oeda_path + '/exacloud.conf'
        _oedacli_mgr = OedacliCmdMgr( _oedacli_bin, _savexmlpath)

        _uuid = self.mGetUUID()
        _patchconfig = self.mGetConfigPath()
        _updatedxml = _oeda_path + '/patched_ntp_dns_'  + _uuid + '.xml'

        # Here, _patchconfig is the path of original xml provided by ecra
        # _updatedxml is the path which will be the patched xml and it is sent
        # back in response to ecra as base64 encoded and compressed content
        self.mExecuteLocal("/bin/cp {} {}".format(_patchconfig, _updatedxml))

        _dom0s, _, _cells, _ = self.mReturnAllClusterHosts()
        _hosts = _dom0s + _cells
        _ntp_list_hosts = self.mCheckConfigOption('ntp_list_hosts')
        _dns_list_hosts = self.mCheckConfigOption('dns_list_hosts')
        for _host in _hosts:
            _oedacli_mgr.mUpdateDnsNtpServers(_host, _updatedxml, _updatedxml, _dns_list_hosts, _ntp_list_hosts)
        self.mSetPatchConfig(_updatedxml)
        return 0

    def mPatchNetworkSlaves(self, aDomU, aClientSlave, aBackSlave):
        _domU = aDomU
        _clientSlave = aClientSlave
        _backupSlave = aBackSlave

        _oeda_path  = self.mGetOedaPath()
        _oedacli_bin = _oeda_path + '/oedacli'
        _savexmlpath = _oeda_path + '/exacloud.conf'
        _oedacli_mgr = OedacliCmdMgr( _oedacli_bin, _savexmlpath)

        _uuid = self.mGetUUID()
        _patchconfig = self.mGetPatchConfig()
        _updatedxml = _oeda_path + '/exacloud.conf/patched_network_slaves_'  + _uuid + '.xml'

        self.mExecuteLocal("/bin/cp {} {}".format(_patchconfig, _updatedxml))

        _id, _host, _ = self.mGetNetworkSlaves(_domU, "client")
        _oedacli_mgr.mUpdateNetworkSlaves(_clientSlave, _id, _host, "client", _patchconfig, _updatedxml)
        _id, _host, _ = self.mGetNetworkSlaves(_domU, "backup")
        _oedacli_mgr.mUpdateNetworkSlaves(_backupSlave, _id, _host, "backup", _updatedxml, _updatedxml)
        self.mSetPatchConfig(_updatedxml)

    def mPatchDRNetworkSlaves(self, aDomU, aDRSlaves, aBridge=None):

        _domU = aDomU
        _dr_slaves = aDRSlaves
        _oeda_path  = self.mGetOedaPath()
        _oedacli_bin = _oeda_path + '/oedacli'
        _savexmlpath = _oeda_path + '/exacloud.conf'
        _oedacli_mgr = OedacliCmdMgr( _oedacli_bin, _savexmlpath)

        _uuid = self.mGetUUID()
        _patchconfig = self.mGetPatchConfig()
        _updatedxml = _oeda_path + '/exacloud.conf/patched_update_dr_slaves_'  + _uuid + '.xml'

        self.mExecuteLocal("/bin/cp {} {}".format(_patchconfig, _updatedxml))

        _id, _host, _ = self.mGetNetworkSlaves(_domU, "other")
        _oedacli_mgr.mUpdateNetworkSlaves(_dr_slaves, _id, _host, "other", _patchconfig, _updatedxml, aBridge=aBridge)
        self.mSetPatchConfig(_updatedxml)

    def mUpdatePassProperty(self, aFlag):
        _flag = aFlag
        _oeda_properties_path = os.path.join(self.__oeda_path,'properties','es.properties')
        if _flag:
            _cmd_str = f"/bin/sed 's/^SKIPPAASPROPERTIES=false/SKIPPAASPROPERTIES=true/' -i {_oeda_properties_path}"
        else:
            _cmd_str = f"/bin/sed 's/^SKIPPAASPROPERTIES=true/SKIPPAASPROPERTIES=false/' -i {_oeda_properties_path}"
        self.mExecuteLocal(_cmd_str, aStdOut=DEVNULL, aStdErr=DEVNULL, aCurrDir=self.__oeda_path)

    def mOEDASkipPassProperty(self, aOptions, aUseXMLPatching=True):
        _use_xml_patching = aUseXMLPatching
        _isHetero, _modelSet = self.IsHeteroConfig()
        _ociexacc = self.mIsOciEXACC()
        _X11:Set[str] = {'X11'}
        _client_slaves, _backup_slaves, _dr_slaves = [],[],[]

        if _ociexacc and _use_xml_patching:
            ebLogInfo("UPDATING XML WITH CLIENT/BACKUP SLAVES...")
            for _dom0, _domU in self.mReturnDom0DomUPair():
                _net_info = self.mGetNetworkSetupInformation(aNetworkType="all", aDom0=_dom0)
                if _net_info and 'client' in _net_info.keys() and 'bond_slaves' in _net_info['client'].keys():
                    _client_slaves = _net_info['client']['bond_slaves'].split()
                if _net_info and 'backup' in _net_info.keys() and 'bond_slaves' in _net_info['backup'].keys():
                    _backup_slaves = _net_info['backup']['bond_slaves'].split()
                self.mPatchNetworkSlaves(_domU, _client_slaves, _backup_slaves)
                if _net_info and 'dr' in _net_info.keys() and 'bond_slaves' in _net_info['dr'].keys():
                    _dr_slaves = _net_info['dr']['bond_slaves'].split()
                    _bridge = _net_info['dr']['bridge']
                    self.mPatchDRNetworkSlaves(_domU, _dr_slaves, aBridge=_bridge)
            _patchconfig = self.mGetPatchConfig()
            self.mUpdateInMemoryXmlConfig(_patchconfig, aOptions)
            ebLogInfo('ebCluCtrl: Saved patched Cluster Config: ' + _patchconfig)
            self.mCopyFile(_patchconfig, self.__remoteconfig)

            ebLogInfo("SKIP PAASPROPERTIES enabled in es.properties for ExaCC")
            self.mUpdatePassProperty(True)

            if self.mCheckConfigOption('reset_net_mapping','True'):
                self.mResetDom0NetworkMapping()
        elif _isHetero and not _ociexacc and _X11 in _modelSet:
            ebLogInfo("SKIP PAASPROPERTIES enabled in es.properties for X11")
            self.mUpdatePassProperty(True)
        else:
            if self.mCheckDom0NetworkType():
                if self.mCheckConfigOption('reset_net_mapping','True'):
                    self.mResetDom0NetworkMapping()

    #BUG 34747667 - Temporary w/a fix added to exacloud to add default gateway point to bondeth0
    #We will disable the exacloud code, once the actual fix will made in OEDA, 
    def mConfigureDefaultGateway(self, aDom0, aDomU, aOptions):
        _dom0 = aDom0
        _domU = aDomU

        _enable_gw = self.mCheckConfigOption('enable_hetero_default_gw')
        if _enable_gw.lower() == "false":
            ebLogInfo('*** enable_hetero_default_gw flag is False. Skipping default gateway configuration.')
            return

        try:
            _nodeU = exaBoxNode(get_gcontext())
            _nodeU.mSetUser("root")
            _nodeU.mConnect(aHost = _domU)

            _cmd_str = "/bin/grep '^GATEWAY' /etc/sysconfig/network-scripts/ifcfg-bondeth0"
            _i, _o, _e = _nodeU.mExecuteCmd(_cmd_str)
            _output = _o.readlines()
            if not _output or len(_output) == 0:
                _nodeU.mDisconnect()
                ebLogError(f'*** DomU:{_domU}:: Gateway not configured in ifcfg-bondeth0')
                raise ExacloudRuntimeError(0x0109, 0xA, 'Gateway not configured in ifcfg-bondeth0')

            _default_gw = _output[0].strip()
            ebLogInfo(f"{_default_gw} for bondeth0 on domU:{_domU}")

            _cmd_str = "/bin/grep '^GATEWAY' /etc/sysconfig/network"
            _i, _o, _e = _nodeU.mExecuteCmd(_cmd_str)
            _output = _o.readlines()
            if not _output or len(_output) == 0:
                _cmd_str = f'echo -e "{_default_gw}" >> /etc/sysconfig/network'
                _nodeU.mExecuteCmd(_cmd_str)
            else:
                _cmd_str = f"/bin/sed '/^GATEWAY/d' -i /etc/sysconfig/network"
                _nodeU.mExecuteCmd(_cmd_str)

                _cmd_str = f'echo -e "{_default_gw}" >> /etc/sysconfig/network'
                _nodeU.mExecuteCmd(_cmd_str)

            _cmd_str = "/bin/grep '^GATEWAYDEV' /etc/sysconfig/network"
            _i, _o, _e = _nodeU.mExecuteCmd(_cmd_str)
            _output = _o.readlines()
            if not _output or len(_output) == 0:
                _cmd_str = f'echo -e "GATEWAYDEV=bondeth0" >> /etc/sysconfig/network'
                _nodeU.mExecuteCmd(_cmd_str)
            else:
                _cmd_str = f"/bin/sed '/^GATEWAYDEV/d' -i /etc/sysconfig/network"
                _nodeU.mExecuteCmd(_cmd_str)

                _cmd_str = f'echo -e "GATEWAYDEV=bondeth0" >> /etc/sysconfig/network'
                _nodeU.mExecuteCmd(_cmd_str)

            #RESTART DOMU TO REFLECT THE DEFAULT GATEWAY
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)

            _vmhandle = ebVgLifeCycle()
            _vmhandle.mSetOVMCtrl(aCtx=get_gcontext(), aNode=_node)
            _cmd = 'shutdown'
            _rc = _vmhandle.mDispatchEvent(_cmd, aOptions, aVMId=_domU, aCluCtrlObj=self)
            if _rc not in [0, 0x0411, 0x0454]:
                _node.mDisconnect()
                ebLogError('*** FATAL :: vmcmd: %s and vmid: %s - Could not be shut down' % (_cmd, _domU))
                raise ExacloudRuntimeError(0x0403, 0xA, 'VM was not able to shutdown')

            if self.mIsOciEXACC() and self.mIsKVM() and luksCharchannelExistsInDom0(aDom0, aDomU):
                mSetLuksPassphraseOnDom0Exacc(self, aDom0, aDomU)
            _rc = self.mRestartVM(_domU, aVMHandle=_vmhandle)
            if _rc != 0:
                _node.mDisconnect()
                ebLogError('*** FATAL :: vmcmd: start and vmid: %s - Could not be started' % (_domU))
                raise ExacloudRuntimeError(0x0411, 0xA, 'VM was not able to restart')

        finally:
            if _nodeU:
                _nodeU.mDisconnect()
            if _node:
                _node.mDisconnect()

    def mCheck2TMemoryRequirements(self, aDom0, aModelSubType="STANDARD"):
        _dom0 = aDom0
        _model_subType: str = aModelSubType
        _is_supported = False

        _physical_memory_attr = self.mCheckConfigOption('physical_memory_size')
        if _physical_memory_attr is not None:
            _exadata_model = self.mGetNodeModel(_dom0)
            _model_attr = _physical_memory_attr.get(_exadata_model, {})
            _physical_memory_size = int(_model_attr.get(_model_subType, "1536"))
            ebLogInfo(f"Physical Memory size {_physical_memory_size}GB from exabox.conf")
        else:
            _physical_memory_size = 1536

        # 2TB Memory minimum Requirements
        # 1. only X9M model supports 2TB Memory
        # 2. Minimum Exadata version supported for 2TB Memory: June 22.1.12.0.0.230524/23.1.3.0.0.230522 releases
        # 3. Dom0's should have 2048 GB of physical memory
        if self.mCompareExadataModel(_exadata_model, 'X9') < 0:
            _is_supported = False
            ebLogInfo(f'*** OCI {_exadata_model} environment detected, not supported for 2TB memory')
            return _is_supported

        _image    = self.mGetImageVersion(_dom0)
        _imagever = ".".join([x.zfill(3) for x in _image.split(".")])

        _hv = getHVInstance(_dom0)
        _physical_memory, _type = _hv.mGetPhysicalMemory()
        _dom0_physical_memory = _physical_memory

        #Minimum Exadata version supported for 2T Memory: June 22.1.12.0.0.230524/23.1.3.0.0.230522
        if _imagever >= "023.001.003.000.000.230522":
            _is_supported = True
            ebLogInfo(f"Dom0 {_dom0} exadata image version {_image} supported for {_physical_memory}{_type} memory")
        elif _imagever >= "022.001.012.000.000.230524":
            _is_supported = True
            ebLogInfo(f"Dom0 {_dom0} exadata image version {_image} supported for {_physical_memory}{_type} memory")
        else:
            _is_supported = False
            ebLogInfo(f"Dom0 {_dom0} exadata image version  {_image} not supported for {_physical_memory}{_type} memory")
            return _is_supported

        if _type.upper() == "TB":
            _dom0_physical_memory = _dom0_physical_memory * 1024

        if _dom0_physical_memory == _physical_memory_size:
            _is_supported = True
            ebLogInfo(f"{_physical_memory}{_type} memory supported on DOM0:{_dom0}")
        else:
            _is_supported = False
            ebLogWarn(f"{_physical_memory}{_type} memory not supported on DOM0:{_dom0} Current Memory Size:{_dom0_physical_memory} Expected Memory Size:{_physical_memory_size}")

        return _is_supported

    def mAdd2TMemorySupport(self, aOptions, aModel, aModelSubType="STANDARD"):
        _exadata_model = aModel
        _model_subType: str = aModelSubType

        if aOptions is not None:
            aOptions.jsonconf = umaskSensitiveData(aOptions.jsonconf)
            _jconf = aOptions.jsonconf
        else:
            _jconf = None

        _oeda_path  = self.mGetOedaPath()
        _oedacli_bin = _oeda_path + '/oedacli'
        _savexmlpath = _oeda_path + '/exacloud.conf'

        _oedacli_mgr = OedacliCmdMgr( _oedacli_bin, _savexmlpath)

        _uuid = self.mGetUUID()
        _patchconfig = self.mGetPatchConfig()
        _updatedxml = _oeda_path + '/exacloud.conf/patched_2tb_memory_support_'  + _uuid + '.xml'

        self.mExecuteLocal("/bin/cp {} {}".format(_patchconfig, _updatedxml))

        _host_type = "KVMHOST"
        _clusterID = self.__clusters.mGetCluster().mGetCluId()

        _physical_memory_attr = self.mCheckConfigOption('physical_memory_size')
        if _physical_memory_attr is not None:
            _model_attr = _physical_memory_attr.get(_exadata_model, {})
            _physical_memory_size = _model_attr.get(_model_subType, "1536")
            ebLogInfo(f"Physical Memory size {_physical_memory_size}GB from exabox.conf")
        else:
            _physical_memory_size = "1536"

        _virtual_memory_size = self.mCheckConfigOption('virtual_memory_size')
        if _virtual_memory_size:
            ebLogInfo(f"Virtual Memory size {_virtual_memory_size} from exabox.conf")
        elif _jconf and 'vm' in list(_jconf.keys()) and 'gb_memory' in list(_jconf['vm'].keys()):
            _virtual_memory_size = str(_jconf['vm']['gb_memory'])
        elif _jconf and 'reshaped_node_subset' in list(_jconf.keys()) and \
            'added_computes' in list(_jconf['reshaped_node_subset'].keys()) and \
            len(_jconf['reshaped_node_subset']['added_computes']) > 0:
            _added_compute_conf0 = _jconf['reshaped_node_subset']['added_computes'][0]
            if 'virtual_compute_info' in list(_added_compute_conf0.keys()) and \
                'vm' in list(_added_compute_conf0['virtual_compute_info'].keys()):
                _acc0_vm = _added_compute_conf0['virtual_compute_info']['vm']
                if 'gb_memory' in list(_added_compute_conf0['virtual_compute_info']['vm'].keys()):
                    _virtual_memory_size  = str(_acc0_vm['gb_memory'])
                else:
                    _virtual_memory_size = "1390"
            else:
                _virtual_memory_size = "1390"
        else:
            _virtual_memory_size = "1390"

        _oedacli_mgr.mUpdatePhysicalMemory(_physical_memory_size, _clusterID, _host_type, _patchconfig, _updatedxml)
        _oedacli_mgr.mUpdateVirtualMemory(_virtual_memory_size, _clusterID, _updatedxml, _updatedxml)
        self.mSetPatchConfig(_updatedxml)
        self.mUpdateInMemoryXmlConfig(_updatedxml, aOptions)
        ebLogInfo('ebCluCtrl: Saved patched Cluster Config: ' + _updatedxml)
        self.mCopyFile(_updatedxml, self.__remoteconfig)

    def mGetPCISlot(self, aEthx, aDom0=None, aInterfaces=[]):
        _ethx = aEthx
        _dom0 = aDom0
        _interfaces = aInterfaces
        _pci_slot = ""

        try:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)

            if _ethx in _interfaces:
                _cmd_str = f"/bin/grep PCI_SLOT_NAME /sys/class/net/{_ethx}/device/uevent"
                _i, _o, _e = _node.mExecuteCmd(_cmd_str)
                _output = _o.readlines()
                if _output:
                    _pci_slot_name = _output[0].strip().split('=')[1]
                    ebLogInfo(f"PCI SLOT NAME for {_ethx}:{_pci_slot_name}")

                _cmd_str = f"""/sbin/lspci -s {_pci_slot_name} -vvv | grep "Physical Slot" """
                _i, _o, _e = _node.mExecuteCmd(_cmd_str)
                _output = _o.readlines()
                if _output:
                    _pci_slot = _output[0].strip().split(':')[1].strip()
                    ebLogInfo(f"Physical PCI Slot for {_ethx}:{_pci_slot}")
        except:
            _err_msg = "Error in fetching Physical Interfaces. Exiting"
            ebLogError(_err_msg)
            raise ExacloudRuntimeError(aErrorMsg=_err_msg)
        finally:
            _node.mDisconnect()

        return _pci_slot

    def mGetPhysicalInterfaceList(self, aDom0=None):
        _dom0 = aDom0
        _interfaces = []
        try:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            _cmd_str = """/bin/find /sys/class/net/ -type l -not -lname "*virtual*" -printf "%f," """
            _i, _o, _e = _node.mExecuteCmd(_cmd_str)
            _output = _o.readlines()
            if _output:
                _interfaces = _output[0].strip().split(',')
                ebLogTrace(f"Physical Interfaces: {_interfaces}")
        except:
            _err_msg = "Error in fetching Physical Interfaces. Exiting"
            ebLogError(_err_msg)
            raise ExacloudRuntimeError(aErrorMsg=_err_msg)
        finally:
            _node.mDisconnect()
        return _interfaces

    def mGetElasticCellList(self, aOptions):
        """
        Returns the new added cells list
        """
        added_cell_hostname_list = []
        if 'reshaped_node_subset' in list(aOptions.jsonconf.keys()):
            _reshape_config = aOptions.jsonconf['reshaped_node_subset']

            for _cell in _reshape_config['added_cells']:
                if _cell['cell_hostname']:
                    added_cell_hostname_list.append(_cell['cell_hostname'])

        return added_cell_hostname_list

    def mGetElasticComputeList(self, aOptions):
        """
        Returns the new added computes list
        """
        added_compute_hostname_list = []
        if 'reshaped_node_subset' in list(aOptions.jsonconf.keys()):
            _reshape_config = aOptions.jsonconf['reshaped_node_subset']

            for _compute in _reshape_config['added_computes']:
                if _compute['compute_node_hostname']:
                    added_compute_hostname_list.append(_compute['compute_node_hostname'])

        return added_compute_hostname_list

    def Is100GbsSpeedSupported(self, aDom0, aNetworkType):
        """  
        Checks if 100Gbps speed is configured, supported by the hardware model,
        and the NICs are in the correct PCI slot for the specified network type,
        using the 'exacc_high_speed_physical_network' configuration structure.

        :param str aDom0: The hostname of the Dom0 node to check.
        :param str network_type: The network to check ('client' or 'backup').
        :return: bool: True if 100Gbps is supported and enabled for the network, False otherwise.
        :raises ValueError: If network_type is not 'client' or 'backup'.
        :raises ExacloudRuntimeError: If prerequisite checks fail (e.g., interface not found).
        """
        _dom0 = aDom0
        _network_type = aNetworkType
        _is_supported = False
        
        if _network_type not in ['client', 'backup']:
            raise ValueError(f"Invalid network_type specified: {_network_type}. Must be 'client' or 'backup'.")
        
        ebLogInfo(f"*** Checking 100Gbps support for '{_network_type}' network on {_dom0}")
        # Configuration based on network_type
        config_flag = None
        default_interfaces = []
        required_slot = None
        min_model = 'X10'
        interfaces_key_in_config = None

        if _network_type == 'client':
            config_flag = 'enable_100gbs_client_network'
            default_interfaces = ["eth1", "eth2"] 
            required_slot = "1"
            interfaces_key_in_config = "client_interfaces"
        elif _network_type == 'backup':
            config_flag = 'enable_100gbs_backup_network'
            default_interfaces = ["eth5", "eth6"]
            required_slot = "2"
            interfaces_key_in_config = "backup_interfaces"
     
        # Check enablement flag
        _enable_100gbs_support = self.mCheckConfigOption(config_flag)
        if not _enable_100gbs_support or str(_enable_100gbs_support).lower() == "false":
            ebLogInfo(f'*** {config_flag} flag is False. Skipping 100Gbs backup network support')
            return False
        
        # Check Model Compatibility
        _exadata_model = self.mGetNodeModel(_dom0)
        if mCompareModel(_exadata_model, 'X10') < 0:
            ebLogInfo(f'*** 100Gbs speed for {_network_type} network supported only on {min_model} env & above. Found model: {_exadata_model}')
            return False
        ebLogInfo(f'*** 100Gbs speed for {_network_type} network is supported on {_exadata_model}')

        # Get the list of interfaces for the network type from 'exacc_high_speed_physical_network'
        _network_interfaces_list = []
        _physical_network_config = self.mCheckConfigOption("exacc_high_speed_physical_network")
        
        if _exadata_model in _physical_network_config:
            _model_specific_config = _physical_network_config[_exadata_model]
            # Get the interface list using the key determined by network_type
            _network_interfaces_list = _model_specific_config.get(interfaces_key_in_config, default_interfaces)
            _optimum_speed = int(_model_specific_config.get("speed", "100000"))
            ebLogInfo(f"{_network_type.capitalize()} Interface list: {_network_interfaces_list} (Speed: {_optimum_speed}) from 'exacc_high_speed_physical_network' for model {_exadata_model}.") 
        else:
            _network_interfaces_list = default_interfaces
            ebLogInfo(f"Could not read valid config from 'exacc_high_speed_physical_network' or model '{_exadata_model}' not found. Using default {_network_type} interfaces: {default_interfaces}")  

        if not _network_interfaces_list:
            _err_msg = f"{_dom0}: No interfaces defined or found for {_network_type} network."
            ebLogError(_err_msg)
            raise ExacloudRuntimeError(aErrorMsg=_err_msg)
        try:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            _physical_interface_list = self.mGetPhysicalInterfaceList(_dom0)
            if not _physical_interface_list:
                _err_msg = f"{_dom0}: Could not retrieve physical interface list."
                ebLogError(_err_msg)
                raise ExacloudRuntimeError(aErrorMsg=_err_msg)
            
            # Check each required interface  
            all_interfaces_ok = True 
            for _ethx in _network_interfaces_list:  
                ebLogTrace(f"Checking {_network_type} interface {_ethx}...")
                if _ethx not in _physical_interface_list:
                    _err_msg = f"{_dom0}: Required {_network_type} interface '{_ethx}' not present in physical interfaces list: {_physical_interface_list}"  
                    ebLogError(_err_msg)
                    all_interfaces_ok = False
                    continue

                # Check PCI Slot - this is the primary hardware check
                _pci_slot = self.mGetPCISlot(_ethx, aDom0=_dom0, aInterfaces=_physical_interface_list)
                ebLogInfo(f"Interface {_ethx} found in PCI slot: {_pci_slot}")

                if _pci_slot == required_slot:
                    ebLogInfo(f"*** Interface {_ethx} is in required slot {required_slot} for {_network_type} 100Gbs speed.")
                else:
                    _err_msg = f"{_dom0}: Required {_network_type} interface '{_ethx}' is in slot {_pci_slot}, but needs to be in slot {required_slot} for 100Gbps support."  
                    ebLogError(_err_msg)
                    all_interfaces_ok = False
                    continue
                
                # Logic to update the speed 
                _status = self.__ethConfig.mValidateInterface(_node, _ethx, _optimum_speed)
                if not _status:
                    _current_speed = self.__ethConfig.mGetCurrentSpeed(_node, _ethx)
                    if _current_speed != _optimum_speed:
                        _rc = self.__ethConfig.mSetCustomSpeed(_node, _ethx, _current_speed, _optimum_speed, _exadata_model)
                        if _rc != 0:
                            _err_msg = f"{_dom0}:Failed to update speed {_optimum_speed} on interface:{_ethx}"
                            ebLogError(_err_msg)

            _is_supported = all_interfaces_ok # Should be True if loop finished

        except Exception as e:
            _err_msg = f"{_dom0}: Unexpected error checking 100Gbs support for {_network_type} network: {e}"
            ebLogError(_err_msg)
            _is_supported = False
        finally:
            if _node:
                _node.mDisconnect()

        if _is_supported:
            ebLogInfo(f"*** 100Gbs Speed IS supported for {_network_type} network on {_dom0}")
        else: 
            ebLogInfo(f"*** 100Gbs Speed IS NOT supported for {_network_type} network on {_dom0}")

        return _is_supported

    def mGetOLVersion(self, aDom0=None):
        _dom0 = aDom0
        _OLVersion = ""

        if not _dom0:
            _dpairs = self.mReturnDom0DomUPair()
            _dom0 = _dpairs[0][0]

        try:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            _cmd_str = "/bin/uname -r"
            _i, _o, _e = _node.mExecuteCmd(_cmd_str)
            _output = _o.readlines()
            if _output:
                _uekStr = _output[0].strip().split('.')[-2]
                _OLVersion = "OL" + _uekStr[2]

            ebLogInfo(f"OL Version:{_OLVersion}")
        except:
            _err_msg = "Error in fetching OL Version. Exiting"
            ebLogError(_err_msg)
            raise ExacloudRuntimeError(aErrorMsg=_err_msg)
        finally:
            _node.mDisconnect()

        return _OLVersion

    def mGetGriddisksByType(self, aGridDisks, aDiskType=None):
        _grid_disks = aGridDisks
        _diskType = aDiskType
        _list_cluster_griddisks = []

        for _grid_disk in _grid_disks:
              if _diskType  in _grid_disk:
                  _list_cluster_griddisks.append(_grid_disk)
        return _list_cluster_griddisks

    def mCheckGridDiskSize(self, aGridDisks):
        _grid_disks = aGridDisks
        _grid_disk_sizes = [ _grid_disk.split()[1] for _grid_disk in _grid_disks]
        return len(set(_grid_disk_sizes)) == 1

    def mValidateGridDiskSize(self, aCell, aGridDisks=None):
        _cell = aCell
        _list_all_griddisk = aGridDisks
        _error_list = []

        if self.IsZdlraProv():
            _catalog_griddisks = self.mGetGriddisksByType(_list_all_griddisk, aDiskType="CATALOG")
            if _catalog_griddisks:
                if self.mCheckGridDiskSize(_catalog_griddisks):
                    ebLogInfo(f"Grid Disks sizes are same for all CATALOG diskGroups")
                else:
                    _err = "Grid Disks sizes are not same for all CATALOG diskGroups"
                    _error_list.append(_err)
                    ebLogError(f"Grid Disks sizes are not same for all CATALOG diskGroups")

            _delta_griddisks = self.mGetGriddisksByType(_list_all_griddisk, aDiskType="DELTA")
            if _delta_griddisks:
                if self.mCheckGridDiskSize(_delta_griddisks):
                    ebLogInfo(f"Grid Disks sizes are same for all DELTA diskGroups")
                else:
                    _err = "Grid Disks sizes are not same for all DELTA diskGroups"
                    _error_list.append(_err)
                    ebLogError(f"Grid Disks sizes are not same for all DELTA diskGroups")
        else:
            _data_griddisks = self.mGetGriddisksByType(_list_all_griddisk, aDiskType="DATA")
            if _data_griddisks:
                if self.mCheckGridDiskSize(_data_griddisks):
                    ebLogInfo(f"Grid Disks sizes are same for all DATA diskGroups")
                else:
                    _err = "Grid Disks sizes are not same for all DATA diskGroups"
                    _error_list.append(_err)
                    ebLogError(f"Grid Disks sizes are not same for all DATA diskGroups")

            _reco_griddisks = self.mGetGriddisksByType(_list_all_griddisk, aDiskType="RECO")
            if _reco_griddisks:
                if self.mCheckGridDiskSize(_reco_griddisks):
                    ebLogInfo(f"Grid Disks sizes are same for all RECO diskGroups")
                else:
                    _err = "Grid Disks sizes are not same for all RECO diskGroups"
                    _error_list.append(_err)
                    ebLogError(f"Grid Disks sizes are not same for all RECO diskGroups")

            _sprc_griddisks = self.mGetGriddisksByType(_list_all_griddisk, aDiskType="SPR")
            if _sprc_griddisks:
                if self.mCheckGridDiskSize(_sprc_griddisks):
                    ebLogInfo(f"Grid Disks sizes are same for all SPARC diskGroups")
                else:
                    _err = "Grid Disks sizes are not same for all SPARC diskGroups"
                    _error_list.append(_err)
                    ebLogError(f"Grid Disks sizes are not same for all SPARC diskGroups")

        if len(_error_list) == 0:
            ebLogInfo(f"Validation for Griddisk Size successful on cell:{_cell}")
        else:
            _err = f"Griddisks Size not same on cell:{_cell}. Cannot proceed further."
            ebLogError(_err)
            return ebError(0x0418)

    def mValidateGridDiskCount(self, aCell, aNode, aGridDisks=None, aDiskGroupSuffix=None, aGridDiskCount=None, aDgNameList=None):
        _cell = aCell
        _node = aNode
        _list_all_griddisk = aGridDisks
        _diskgroup_suffix = aDiskGroupSuffix
        _griddisk_count = aGridDiskCount
        _dgName_list = aDgNameList
        _list_cluster_griddisks = []

        # Iterate over all griddisks of the cell on which aNode is connected to
        for _grid_disk in _list_all_griddisk:
            # Get first part of griddisk name, it contains the Diskgroup suffix
            # E.g. 'RECOC5' from 'RECOC5_CD_10_scaqae14celadm01'
            # E.g. 'RECOC03' from 'RECOC03_CD_11_sea201602exdcl01'
            _grid_disk, _griddisk_size = _grid_disk.split()
            _grid_name_suffix = _grid_disk.split('_')[0]

            # Check if griddisk name first part contains the DiskGroup suffix
            # E.g. 'C03' in 'RECOC03' -> True
            # E.g. 'C5' in 'RECOC03' -> False
            if self.IsZdlraProv():
                if "CATALOG"  in _grid_name_suffix or "DELTA" in _grid_name_suffix:
                    ebLogTrace(f"Grid Disk entry detected {_grid_disk} in "
                                  f"{_node.mGetHostname()}, suffix {_diskgroup_suffix}")
                    _list_cluster_griddisks.append(_grid_disk)
            else:
                if _diskgroup_suffix  in _grid_name_suffix and _grid_name_suffix[:4] in _dgName_list:
                    ebLogTrace(f"Grid Disk entry detected {_grid_disk} in "
                                   f"{_node.mGetHostname()}, suffix {_diskgroup_suffix}")
                    _list_cluster_griddisks.append(_grid_disk)

        ebLogTrace(f"Cluster Griddisk Count:{len(_list_cluster_griddisks)}")
        if _griddisk_count == len(_list_cluster_griddisks):
            ebLogInfo(f"Validation for Griddisk Count successful on cell:{_cell}")
        else:
            _err = f"Missing griddisks detected on cell:{_cell}. Cannot proceed further."
            ebLogError(_err)
            return ebError(0x0418)

    def mValidateGridDisks(self, aCell=None):
        _cell = aCell

        _enable_validation = str(self.mCheckConfigOption("enable_griddisk_validation")).upper()
        if _enable_validation == "FALSE":
            ebLogInfo('*** enable_griddisk_validation flag is False. Skipping Griddisk validation.')
            return 0

        if not _cell:
            _cells = list(self.mReturnCellNodes().keys())
            _cell = _cells[0]

        try:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_cell)

            _dgName_list = []

            # Fetch the DiskGroup suffix
            # e.g. suffix is C03 from RECOC03_CD_11_sea201602exdcl01
            # e.g. suffix is C1 from RECOC1_CD_00_sea201602exdcl01
            if self.IsZdlraProv():
                _diskgroup_suffix = "CATALOG|DELTA"
            else:
                _diskgroup_suffix = self.mGetStorage().mClusterDiskGroupSuffix()

            _cluster = self.mGetClusters().mGetCluster()
            _cluster_groups = _cluster.mGetCluDiskGroups()
            for _dgid in _cluster_groups:
                _dgName = self.mGetStorage().mGetDiskGroupConfig(_dgid).mGetDgName()[:4]
                if _dgName != 'DBFS':
                    _dgName_list.append(_dgName)
            ebLogTrace(f"List of Diskgroup Names:{_dgName_list}")

            #Total cell disks will be local diskcount from XML * len(number of cells)
            _cell_list = self.mReturnCellNodes()
            _mac_cfg   = self.mGetMachines().mGetMachineConfig(_cell)
            _ldisk_cnt = _mac_cfg.mGetLocaldisksCount()
            _disknb = _ldisk_cnt * len(_cell_list)
            _griddisk_count = len(_dgName_list) * _ldisk_cnt
            ebLogTrace(f"LocalDisks Count:{_ldisk_cnt} Total CellDisks:{_disknb} Griddisk Count:{_griddisk_count}")

            # Fetch list of all griddisks from the cell of aNode
            try:
                _list_all_griddisk = self.mGetStorage().mListCellDG(_node, _diskgroup_suffix)
            except Exception as e:
                #Bug 37266604 : check if any cell services down and 
                #attempt to start it once and then retry list griddisks
                ebLogTrace('*** Check and start cell services once and listDGs again')
                if self.mCheckCellsServicesUp(aCellList=[_cell]):
                    ebLogInfo("*** Cell Services running on cell %s " % _cell)
                    _list_all_griddisk = self.mGetStorage().mListCellDG(_node, _diskgroup_suffix)
                else:
                    ebLogError("*** Cell services is not running on cell %s " % _cell)
                    raise ExacloudRuntimeError(0x0743, 0xA, 'Cell services is not running on cell', Cluctrl=self) 

            self.mValidateGridDiskCount(aCell=_cell, aNode=_node, aGridDisks=_list_all_griddisk, 
                            aDiskGroupSuffix=_diskgroup_suffix, aGridDiskCount=_griddisk_count, aDgNameList=_dgName_list)
            self.mValidateGridDiskSize(aCell=_cell, aGridDisks=_list_all_griddisk)

        finally:
            _node.mDisconnect()
        return 0

    def mEnableTfactlService(self, aDomUList):
        """This method checks the state of tfactl service on DOMUs. And accordingly enables and starts the tfactl service.
        Also, a dictionary is created to store the previous state of tfactl service on each DOMU.

        Args:
            aDomUList (list): List of DOMUs.

        Returns:
            dict: A dictionary is created to store the previous state of tfactl service on each DOMU.
            0 - it is enabled and running.
            1 - it is not running.
            2 - it is not enabled.
        """
        _service_started_enabled = {}
        # tfactl service enablement loop on the DOMUs
        # We need to ensure the tfactl service is up and running on all the DOMUs before proceeding
        # to collect the logs - otherwise log collection will fail.
        for _domU in aDomUList:
            _service_started_enabled[_domU] = 0
            with connect_to_host(_domU, get_gcontext()) as _node:
                # Get tfactl status
                _i, _o, _e = _node.mExecuteCmd(f"/usr/bin/tfactl status")
                _tfactl_error = _e.read().strip()
                # If the tfactl service is not running, try starting it
                if "(TFA) is not running" in _tfactl_error:
                    _service_started_enabled[_domU] = 1
                    _i, _o, _e = _node.mExecuteCmd(f"/usr/bin/tfactl start")
                    _tfactl_start_error = _o.read().strip()
                    # If the tfactl service is disabled, try enabling it
                    if "autostart is disabled" in _tfactl_start_error:
                        _service_started_enabled[_domU] = 2
                        _node.mExecuteCmd(f"/usr/bin/tfactl enable")
                        if _node.mGetCmdExitStatus() != 0:
                            ebLogError(f"The tfactl service could not be enabled on DOMU {_domU}. Not collecting tfactl logs.")
                            return
                        # Now the tfactl service is enabled - start it
                        _node.mExecuteCmd(f"/usr/bin/tfactl start")
                        if _node.mGetCmdExitStatus() != 0:
                            ebLogError(f"The tfactl service could not be started on DOMU {_domU}. Not collecting tfactl logs.")
                            return
                        ebLogInfo(f"The tfactl service is now enabled and started on DOMU {_domU}. Ready for collecting tfactl logs.")
                    else:
                        ebLogInfo(f"The tfactl service is now started on DOMU {_domU}. Ready for collecting tfactl logs.")
                elif "No such file or directory" in _tfactl_error:
                    ebLogInfo(f"The tfactl service is not installed on DOMU {_domU}. Not collecting tfactl logs.")
                    return
                else:
                    ebLogInfo(f"The tfactl service is already running on DOMU {_domU}.")
        return _service_started_enabled

    def mRestoreTfactlService(self, aServiceEnabledDict):
        """This method restores the tfactl service state on the DOMUs to the state before tfactl log collection
        started.

        Args:
            aServiceEnabledDict (dict): aServiceEnabledDict is a dict which determines what was the previous state of tfactl service on the DOMUs.
        """
        for _domU in aServiceEnabledDict:
            # if the value is 0, it was in the state of enabled and running
            if aServiceEnabledDict[_domU] == 0:
                continue
            elif aServiceEnabledDict[_domU] == 1:
                # If the value is 1, it was in the state of enabled and stopped
                with connect_to_host(_domU, get_gcontext()) as _node:
                    _node.mExecuteCmd(f"/usr/bin/tfactl stop")
            elif aServiceEnabledDict[_domU] == 2:
                # it was in the state of disabled and stopped
                with connect_to_host(_domU, get_gcontext()) as _node:
                    _node.mExecuteCmd(f"/usr/bin/tfactl stop")
                    _node.mExecuteCmd(f"/usr/bin/tfactl disable")

    def mFetchCrsAsmLogs(self, aDomUList):
        """Method to fetch tfactl logs for components like crs and asm

        Args:
            aDomUList (list): List of DOMUs

        Returns:
            str: Path to the collected tfactl logs
        """
        try:
            _err_string_tfactl = None
            _out_string_tfactl = None
            if self.mCheckConfigOption('tfactl_log_collection') is not None and self.mCheckConfigOption('tfactl_log_collection') == "False":
                ebLogInfo("tfactl log collection is disabled. Not collecting tfactl logs.")
                return
            # _service_started_enabled - dict for all DOMUs to retain the service state.
            # 0 - it is enabled and running.
            # 1 - it is not running.
            # 2 - it is not enabled.
            _service_started_enabled = self.mEnableTfactlService(aDomUList)
            if not _service_started_enabled:
                ebLogError("Error occurred while trying to enable and start the tfactl service.")
                return
            # We came out of the DOMUs loop - we now expect tfactl to be started and running on all DOMUs
            if self.mCheckConfigOption('tfactl_log_collection_period_hrs') is not None:
                _tfactl_log_collection_period_hrs = int(self.mCheckConfigOption('tfactl_log_collection_period_hrs'))
            else:
                # default log collection period is 4 hours
                _tfactl_log_collection_period_hrs = 4
            if self.mCheckConfigOption('tfactl_component_list') is not None:
                _tfactl_component_list = self.mCheckConfigOption('tfactl_component_list')
            else:
                # default log collection component list has install,crs,asm,os components
                if self.mIsXS():
                    _tfactl_component_list = "install,crs,os"
                else:
                    _tfactl_component_list = "install,crs,asm,os"

            _tfactl_component_list = _tfactl_component_list.strip().split(",")
            _tfactl_component_list_string = "-" + " -".join(_tfactl_component_list)
            _cmd_log_collection = f"/usr/bin/tfactl diagcollect {_tfactl_component_list_string} -node all -last {_tfactl_log_collection_period_hrs}h -silent -monitor"
            # Tfactl log collection can only happen with root user
            with connect_to_host(aDomUList[0], get_gcontext(), username="root") as _node:
                ebLogInfo(f"Executing {_cmd_log_collection} on DOMU {aDomUList[0]} with a timeout of 120 minutes.")
                _i, _o, _e = _node.mExecuteCmd(f"{_cmd_log_collection}", aTimeout=7200)
                _err_string_tfactl = _e.read()
                _out_string_tfactl = _o.read()
            _matched_lines = []
            if not _out_string_tfactl:
                _err_msg = f"None output received while executing command {_cmd_log_collection}. Error from command: {_err_string_tfactl}."
                ebLogError(_err_msg)
                return
            _matched_lines = [_line for _line in _out_string_tfactl.split('\n') if "Detailed Logging at" in _line]
            if not _matched_lines:
                _err_msg = f"Unexpected output received while executing command {_cmd_log_collection}. Output from command: {_out_string_tfactl}. Error from command: {_err_string_tfactl}."
                ebLogError(_err_msg)
                return
            _log_output_dir = os.path.dirname(_matched_lines[0].split(":")[1].strip())
            _tar_file_name = _log_output_dir + ".tar.gz"
            # Compress the remote log files collected to a tar file for easy handling of log files
            # And remove the original folder created for tfactl log collection
            with connect_to_host(aDomUList[0], get_gcontext(), username="root") as _node:
                _node.mExecuteCmdLog(f"/usr/bin/tar -czvf {_tar_file_name} {_log_output_dir} --remove-files")
            _oeda_path = self.mGetOedaPath()
            _dest_dir = os.path.join(_oeda_path, "log/tfactl_logs")
            if not os.path.exists(_dest_dir):
                os.makedirs(_dest_dir)
            with connect_to_host(aDomUList[0], get_gcontext()) as _node:
                _download_status = _node.mDownloadRemoteFile(_tar_file_name, _dest_dir)
                if _download_status is False:
                    ebLogError(f"The download of tfactl logs was not successful.")
                # Remove tfactl logs from DOMU after downloading
                _node.mExecuteCmd(f"/usr/bin/rm -rf {_tar_file_name}")
            # Set the tfactl service back to original state in DOMUs
            self.mRestoreTfactlService(_service_started_enabled)
            return _dest_dir
        except Exception as ex:
            ebLogError(f"Error occurred while collecting tfactl logs: {ex}. Error type: {type(ex)}. Error string while running tfactl cmd: {_err_string_tfactl}. Output string while running tfactl cmd: {_out_string_tfactl}.")

    def mCheckCrsIsUp(self, aDomU, aDomUList, aUser='root', aTimeOut=0, aNodeRecovery=False):

        if self.mIsExaScale():
            return

        _domU = aDomU
        _domu_list = aDomUList

        _crs_cmd_pfx, _, _ = self.mGetOracleBaseDirectories(aDomU = _domU)
        with connect_to_host(_domU, get_gcontext(), username=aUser) as _node:
            _crs_cmd_pfx += '/bin/crsctl '

            if aTimeOut != 0:
                _timeout = 60 * int(aTimeOut)
            elif self.mCheckConfigOption('crs_timeout'):
                _timeout = self.mCheckConfigOption('crs_timeout')
                if self.__ociexacc:
                    _timeout = int(_timeout) * 60 * 2
                else:
                    _timeout = int(_timeout) * 60
            else:
                _timeout = 60*60

            _initial_time = time.time()
            _rc = 1
            while _rc != 0:

                if self.mIsXS():
                   _cmd = _crs_cmd_pfx + 'query css votedisk | grep "Located 1 voting disk"'
                else:
                    _cmd = _crs_cmd_pfx + 'query css votedisk | grep "Located 5 voting disk"'
                    if len(self.mReturnDom0DomUPair()) == 1 or len(self.mGetOrigDom0sDomUs()) == 1 or aNodeRecovery:
                        ebLogWarn("Single node detected, checking at least 3 voting disk online")
                        _cmd = _crs_cmd_pfx + 'query css votedisk | grep "Located [345] voting disk"'

                _node.mExecuteCmd(_cmd)
                _rc = _node.mGetCmdExitStatus()

                _elapsed_time = time.time() - _initial_time
                if _timeout < _elapsed_time:
                    self.mFetchCrsAsmLogs(_domu_list)
                    raise ExacloudRuntimeError(0x0114, 0xA, 'Timeout while waiting for voting files to be up. Aborting')

                ebLogTrace("Waiting for voting files to be up & running: {0}".format(_elapsed_time))
                time.sleep(3)

            ebLogInfo("Voting files are up & running")

        _initial_time = time.time()
        for _domu in _domu_list:
            with connect_to_host(_domu, get_gcontext(), username=aUser) as _node:
                _rc = 1
                _restarted = False
                while _rc != 0:
                    _elapsed_time = time.time() - _initial_time
                    # If for _timeout/2 seconds, cluster is not up and running, then restart the cluster.
                    if _elapsed_time > _timeout/2 and _restarted == False:
                        ebLogInfo(f"Attempting cluster services restart on {_domu}")
                        _node.mExecuteCmdLog(_crs_cmd_pfx + 'stop crs -f')
                        if _node.mGetCmdExitStatus() != 0:
                            ebLogError(f"CRS failed to stop on {_domu}")
                        _node.mExecuteCmdLog(_crs_cmd_pfx + 'start crs')
                        _restarted = True

                    if _timeout < _elapsed_time:
                        _node.mDisconnect()
                        self.mFetchCrsAsmLogs(_domu_list)
                        ebLogError(f"Timeout while waiting for crsctl to be online in {_domu}")
                        raise ExacloudRuntimeError(0x0114, 0xA, 'Timeout while waiting for crsctl to be online. Aborting')

                    ebLogTrace("Waiting for crs service to be up & running: {0}".format(_elapsed_time))
                    time.sleep(3)
                    
                    _node.mExecuteCmd(_crs_cmd_pfx + 'check cluster | grep -c online | grep -w 3')
                    _rc = _node.mGetCmdExitStatus()

            ebLogInfo(f"CRS is up and running on {_domu}")


    def mCheckAsmIsUp(self, aDomU, aDomUList, aUser='root', aTimeOut=0, aCheckACFS=True, aGridHome=None):
        """
        Checks if ASM, and optionally ACFS, is up in aDomUList,
        using as source node aDomU

        :param aDomU: source domU where to connect and check ASM and ACFS
        :param aDomUList: list of domUs where to check ASM/ACFS
        :param aUser: an optional user to use when connecting to aDomU through
            ssh
        :param aTimeout: an integere specifying how many minutes to wait
        :param aCheckACFS: a boolean to specify if ACFS must be checkeds
        :param aGridHome: an optional domu path to use as GI home
        """

        if self.mIsExaScale() or self.mIsXS():
            return

        _domU = aDomU
        _domu_list = aDomUList
        _node = exaBoxNode(get_gcontext())
        _node.mSetUser(aUser)
        _node.mConnect(aHost = _domU)

        if aGridHome:
            ebLogInfo(f'*** mCheckAsmIsUp: Using gridhome: {aGridHome}')
            _asm_cmd_pfx = aGridHome
        else:
            _cmd = "cat /etc/oratab | grep '^+ASM.*' | cut -f 2 -d ':'"
            ebLogInfo("Wait for ASM to be up")
            _asm_cmd_pfx = _node.mSingleLineOutput(_cmd)

        # Store GI home
        _gi_home = _asm_cmd_pfx
        _asm_cmd_pfx += '/bin/srvctl '

        if aTimeOut != 0:
            _timeout = 60 * int(aTimeOut)
        elif self.mCheckConfigOption('crs_timeout'):
            _timeout = self.mCheckConfigOption('crs_timeout')
            _timeout = int(_timeout) * 60
        else:
            _timeout = 60*60

        ebLogInfo(f"Check ASM/ACFS max timeout: {_timeout} seconds")

        _initial_time = time.time()
        _rc = 1

        # Check First ASM
        _services = [('asm','ASM is running on')]

        if aCheckACFS and not self.IsZdlraProv() and not self.mIsAdbs():
            _services.append(('filesystem','is mounted on nodes'))

        _flag_first_time_dont_stop = True
        while _services:
            # Get current service info (asm or filesystem from head of list)
            _service, _line_grep = _services[0]

            # Log the status for observability purposes
            _out_status = node_exec_cmd(_node, f"{_asm_cmd_pfx} status {_service}")
            ebLogTrace(f"Status service {_service}:\n{_out_status}")

            _cmd = _asm_cmd_pfx + f"status {_service} | grep '{_line_grep}'"
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            _rc = _node.mGetCmdExitStatus()

            _elapsed_time = time.time() - _initial_time

            # Timeout reached?
            if _timeout < _elapsed_time:
                _node.mDisconnect()
                _err = f'Timeout while waiting for {_service} to be running. Aborting'
                ebLogError(_err)
                self.mFetchCrsAsmLogs(_domu_list)
                raise ExacloudRuntimeError(0x0114, 0xA, _err)

            _out = _o.readlines()
            if not _rc and _out and len(_out):
                _out = _out[0].strip()
                _output_list_tmp=_out.split(' ')
                _output_list=_output_list_tmp[-1].split(',')
                _res = "asm -proxy" if _service == "filesystem" else "asm"
                _domU_restarted = False

                for _domu in _domu_list:
                    if _domu.split('.')[0] not in _output_list:
                        ebLogWarn(f"Detected domU {_domu} to have {_service} down")
                        _domU_restarted = True
                        if _elapsed_time > int(_timeout)/2 :
                            ebLogInfo(f"Attempting {_service} services restart on {_domu}")

                            # Special case: Bug 36558103
                            # Don't stop asm -proxy the first time we want to try
                            # to start it
                            if _service == "filesystem" and _flag_first_time_dont_stop:
                                ebLogWarn(f"Skipping stop of {_service} the first time")
                                _flag_first_time_dont_stop = False
                            else:
                                _node.mExecuteCmdLog(_asm_cmd_pfx + f"stop {_res} -n {_domu} -force")

                            # Wait a bit between stop/start of the service
                            time.sleep(5)
                            _node.mExecuteCmdLog(_asm_cmd_pfx + f"start {_res} -n {_domu}")

                            if _service == "filesystem":
                                # Special case: Bug 36558103
                                # If ACFS is down, in addition to asm -proxy (advm)
                                # we need to also start the local ACFS resource
                                # Get the name of the resource
                                _out_acfs_res = node_exec_cmd_check(
                                    _node,
                                    f"{_gi_home}/bin/crsctl stat res -t | /bin/grep -i acfsvol01.acfs")
                                ebLogTrace(f"Got ACFS Resource {_out_acfs_res} in {_domu}")
                                _acfs_res = _out_acfs_res.stdout.strip()

                                _out_start_acfs = node_exec_cmd(
                                    _node,
                                    f"{_gi_home}/bin/crsctl start res {_acfs_res} -n {_domu} -unsupported")
                                ebLogTrace(f"{_domu} ACFS Start output: {_out_start_acfs}")


                    else:
                        ebLogWarn(f"Detected domU {_domu} to have {_service} up")

                if _domU_restarted is False:
                    ebLogInfo(f'Service {_service} is up on all nodes')
                    _services.pop(0) # Remove started service
                    _initial_time = time.time()
                    continue
                else:
                    _rc = 1

            else:
                _rc = 1
                ebLogTrace("ASM srvctl status didn't produce output, retrying")
                continue

            ebLogInfo(f"Waiting for {_service} to be up & running: {_elapsed_time}")
            time.sleep(20)
        ebLogInfo("ASM and ACFS are up & running")
        _node.mDisconnect()

    def mCheckDBInstanceIsUp(self, aOptions, aRaiseError=True):

        _dbName = self.__dbname
        _domu_list = [ _domu for _ , _domu in self.mReturnElasticAllDom0DomUPair()]

        self.mCheckCrsIsUp(_domu_list[0], _domu_list, aUser='oracle')
        self.mCheckAsmIsUp(_domu_list[0], _domu_list, aUser='oracle')

        _node = exaBoxNode(get_gcontext())
        _node.mSetUser('oracle')
        _node.mConnect(aHost = _domu_list[0])

        if _dbName is None or _dbName == 'undefined':
            _dbName = _node.mSingleLineOutput("tac /etc/oratab | grep -v ASM")
            _dbName = _dbName.split(":")[0].replace("#", "")

        ebLogInfo("Wait for DB to be up")
        _cmd = "cat /etc/oratab | grep '^{0}.*' | cut -f 2 -d ':'".format(_dbName)
        _db_cmd_pfx = _node.mSingleLineOutput(_cmd)
        _db_cmd_pfx = 'export ORACLE_HOME={0}; {0}'.format(_db_cmd_pfx)
        _db_cmd_pfx += '/bin/srvctl '
        if self.mCheckConfigOption('db_timeout') is not None:
            _timeout = int(self.mCheckConfigOption('db_timeout'))
        else:
            if self.__ociexacc:
                _timeout = 60 * 60
            else:
                _timeout = 30 * 60

        _initial_time = time.time()
        _rc = 1
        while _rc != 0:
            _node.mExecuteCmd(_db_cmd_pfx + 'status database -d {0} | grep -c "is running" | grep -w {1}'.format(_dbName, len(_domu_list)))
            _rc = _node.mGetCmdExitStatus()
            _elapsed_time = time.time() - _initial_time
            if _timeout < _elapsed_time:
                _node.mDisconnect()
                if aRaiseError:
                    raise ExacloudRuntimeError(0x0114, 0xA, 'Timeout while waiting for db instance to be online. Aborting')
                else:
                    ebLogInfo(f"DB Instance are not up on {_domu_list[0]}")
                    return

            ebLogVerbose("Wait for DB to be up: {0}".format(_elapsed_time))
            time.sleep(3)

        _node.mDisconnect()
        ebLogInfo("DB Instance is up")

    def mGetClusterSuffix(self):
        _cluster = self.mGetClusters().mGetCluster()
        _cludgroups = _cluster.mGetCluDiskGroups()
        _suffix = ''
        for _dgid in _cludgroups:
            if self.__storage.mGetDiskGroupConfig(_dgid).mGetDgName()[:4] in ['DATA','RECO']:
                _suffix = self.__storage.mGetDiskGroupConfig(_dgid).mGetDgName()[-2:]
                # If cluster number is >= 10, suffix is 3-char
                if 'C' not in _suffix:
                    _suffix = self.__storage.mGetDiskGroupConfig(_dgid).mGetDgName()[-3:]
                break

        ebLogInfo('*** Using Grid Disk cluster suffix: %s' % (_suffix))
        return _suffix

    def mDropPmemlogs(self, aOptions, aCellList=None)-> int:
        """
        Method used to drop pmemlogs on ADB-S
        It uses the flag from exabox.conf: 'drop_pmemlog_adbs' to know if
        we should drop the pmemlogs or not

        :returns int:
            0 : ADB-S and dropped pmemlog with success
            1 : ADB-S and drop of pmemlog is disabled
            2 : Non ADB-S or MVM
        """
        _jconf = aOptions.jsonconf
        _adbs = None
        if _jconf and 'adb_s' in _jconf :
            _adbs = _jconf.get("adb_s")

        if not _adbs or _adbs == "False" or self.mGetSharedEnv():
            ebLogInfo('*** Skipping dropping of pmemlogs ')
            return 2

        # Check exabox.conf property
        _drop_conf_property = "drop_pmemlog_adbs"
        _drop_pmem_adbs =  get_gcontext().mGetConfigOptions().get(
                                    _drop_conf_property, "False")

        if _drop_pmem_adbs.lower() == "false":
            ebLogInfo(f"ADB-S drop pmemlog is disabled with exabox.conf "
                f"property: '{_drop_conf_property}'")
            return 1
        else:
            ebLogInfo(f"ADB-S drop pmemlog is enabled with exabox.conf "
                f"property: '{_drop_conf_property}'")

        def _drop_pmemlogs(_cell_name):
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_cell_name)

            ebLogInfo(f"Dropping pmemlog on cell {_cell_name}")
            _cmdstr = 'cellcli -e drop pmemlog all'
            _node.mExecuteCmdLog(_cmdstr)

            _node.mDisconnect()

        _plist = ProcessManager()

        if aCellList:
            _cell_list = aCellList
        else:
            _cell_list = self.mReturnCellNodes().keys()

        for _cell_name in sorted(_cell_list):
            _p = ProcessStructure(_drop_pmemlogs, [_cell_name,])
            _p.mSetMaxExecutionTime(30*60) # 30 minutes timeout
            _p.mSetJoinTimeout(10)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()

        return 0

    def mExecuteApplySecurityFixes(self, _oeda_path, _version, aOptions=None):
        #
        #   Apply security fix
        #

        # Steps required to apply security fixes to the oracle home
        # of the starter db nid

        ebLogVerbose("mExecuteApplySecurityFixes: oeda path = %s" % _oeda_path)

        _domu_list = [ _domu for _ , _domu in self.mReturnDom0DomUPair()]

        # edit existing dbhome to target the NID Oracle Home
        # to be updated [to get the oracle home ]

        _cmd_oratab_dbhome = '''cat /etc/oratab | grep -e "{0}" '''.format(self.__dbname)

        #
        # execute the command only in 1 domU
        #

        _domu = _domu_list.pop(0)
        with connect_to_host(_domu, get_gcontext()) as _node:
            ebLogInfo("*** Getting dbname: {0} from /etc/oratab".format(self.__dbname))
            fin, out, err = _node.mExecuteCmd(_cmd_oratab_dbhome)
            _dbinfo  = out.read()
            _errinfo = err.read()

            ebLogInfo(" {0} {1}".format(_dbinfo, _errinfo))

            _cmd_oratab_content = 'cat /etc/oratab'
            fin, out, err = _node.mExecuteCmd(_cmd_oratab_content)
            _oratabinfo  = out.read()

        if not _dbinfo:
            ebLogError("*** Fatal Error *** : Database: {0} not found in oratab - please review errors log and try again".format(self.__dbname))
            ebLogInfo("*** oratab file content:\n{0}".format(_oratabinfo))
            raise ExacloudRuntimeError(0x0114, 0xA, 'Database not found in oratab while applying security fixes')

        _dbinfo = _dbinfo.split(":")
        _dbnid_home = None

        if len(_dbinfo) == 3:
            _dbnid_home = _dbinfo[1]
        else:
          ebLogError("*** Fatal Error *** : Database {0} info in oratab is not splitted by three colons as expected".format(self.__dbname))
          ebLogInfo("*** oratab file content:\n{0}".format(_oratabinfo))
          raise ExacloudRuntimeError(0x0114, 0xA, 'Formating error in oratab while applying security fixes')

        # Updating the oeda xml
        _dbhcfg = [ _dbhome for _dbhome in self.__dbhomes.mGetDBHomeConfigs()]

        for dbhome in _dbhcfg:
            dbname = dbhome.mGetDBHomeName()
            ebLogInfo("dbhome : {0}".format(dbname))
            _dbver = dbhome.mGetDBHomeVersion().replace(".","")

            # Map the correct version to match the database
            # The begins with 12100 will be changed as 121
            # The others without 12 like 18100 will be changed as 18
            _miniVersion = _version
            if _miniVersion.startswith("12"):
                _miniVersion = _miniVersion[:3]
            else:
                _miniVersion = _miniVersion[:2]

            if _dbver.startswith(_miniVersion):
                dbhome.mSetDBHomeLocation(_dbnid_home)
            else:
                ebLogInfo(" ***  Removing dbhome %s from XML" % (dbname))
                self.__dbhomes.mRemoveDBHomeConfig(dbhome.mGetDBHomeConfig_ptr(), dbhome)

                for databaseId in copy.deepcopy(self.__databases.mGetDBconfigs()):
                    database = self.__databases.mGetDBconfigs()[databaseId]

                    if database.mGetDBHome() in self.__database_mapping.keys():
                        _oldDbHome = self.__database_mapping[database.mGetDBHome()]
                        if _oldDbHome == dbhome.mGetDBHomeId():
                            ebLogInfo(" *** Removing database: {0}".format(databaseId))
                            self.__databases.mRemoveDatabaseConfig(databaseId)

        # save the current configuration
        self.mSaveXMLClusterConfiguration()
        ebLogInfo('ebCluCtrl: Saved patched Cluster Config: ' + self.__patchconfig)
        self.mApplyCommandsOedacli(aSuffix="SecurityFixes", aOptions=aOptions)

        # Backing up files sqlnet.ora because it is modified by starter dbnid
        # temporary fix, this needs to be solved by Manoj in DBCS layer

        domu_list = [ _domu for _ , _domu in self.mReturnDom0DomUPair()]
        _sqlnetora = "{0}/network/admin/sqlnet.ora".format(_dbnid_home)
        _cmd_sqlnet = "cp -f {0} {0}.orig".format(_sqlnetora)
        self.__nid_backup_files[_sqlnetora] = "{0}.orig".format(_sqlnetora)

        ebLogInfo('Backing up sqlnet ora for each node')

        for domu in domu_list:
            with connect_to_host(domu, get_gcontext(), username='oracle') as _node:
                fin, out, err = _node.mExecuteCmd(_cmd_sqlnet)
                _dbinfo  = out.read()
                _errinfo = err.read()
                ebLogInfo("{0} {1}".format(_dbinfo, _errinfo))
        #
        # [*] using __patchconfig to execute oeda
        #

        # Regenerate the Step table as Patchconfig may have different Step mappings
        self.__remoteconfig = self.__patchconfig
        self.__oeda_step = self.mBuildOedaStepsTable(_oeda_path)

        #Execute the Security Fix on time
        _rc = 1
        _step_asf_retries = int(self.mCheckConfigOption('oedasecurityfixes_retries'))

        _timeout_step_asf = 60
        if self.mCheckConfigOption('oedasecurityfixes_timeout') is not None:
            _timeout_step_asf = int(self.mCheckConfigOption('oedasecurityfixes_timeout'))

        while _step_asf_retries > 0:
            try:
                lCmd = "/bin/bash install.sh -cf {0} -s {1} {2}".format(self.__remoteconfig, \
                                                                        self.mFetchOedaStep(OSTP_APPLY_FIX), \
                                                                        self.mGetOEDAExtraArgs())

                ebLogInfo('Running: ' + lCmd)
                _out = self.mExecuteCmdLog2(lCmd, aTimeOut=_timeout_step_asf*60, aCurrDir=_oeda_path)   #Timeout, if there is still running, raise an ExacloudRuntimeError
                _rc  = self.mParseOEDALog(_out)
                break

            except ExacloudRuntimeError as ere:

                ebLogInfo("Timeout while execution OEDA Step Security Fixes")

                if self.mCheckConfigOption('disable_oedasecurityfixes_retry', 'True'):
                    _step_asf_retries = 0
                else:
                    _step_asf_retries -= 1

                if _step_asf_retries > 0:

                    #Apply changes to retry execution
                    if not self.__shared_env:
                        ebLogInfo("Execute Reboot of Dom0s")

                        #In case of single vm, would be necessary to restart the Dom0s
                        _processes = ProcessManager()
                        for _dom0 , _ in self.mReturnDom0DomUPair():

                            _p = ProcessStructure(self.mRebootNode, [_dom0], _dom0)
                            _p.mSetMaxExecutionTime(60*60) # 60 min timeout
                            _p.mSetJoinTimeout(30)
                            _p.mSetLogTimeoutFx(ebLogWarn)
                            _processes.mStartAppend(_p)

                        _processes.mJoinProcess()

                        #Wait the vms to come up
                        ebLogInfo("Wait Vms to be up")

                        _elapsedVm = 0
                        _initialVmTime = time.time()
                        _vmTimeout = 15 * 60 #10 minutes

                        _node = exaBoxNode(get_gcontext())
                        _vmUp = False
                        while not _vmUp:
                            _vmUp = _node.mCheckPortSSH(_domu_list[0], aTimeout=30)
                            time.sleep(5)

                            _elapsedVm = time.time() - _initialVmTime
                            ebLogInfo("Wait Vm {0} to be up: {1}".format(_domu_list[0], _elapsedVm))

                            if _vmTimeout < _elapsedVm:
                                ebLogError('Timeout while wait for VM on Apply Security Fixes')
                                raise ExacloudRuntimeError(0x0114, 0xA, 'Timeout while wait for VM on Apply Security Fixes', aStackTrace=False)


                        #Wait for crs status onlune
                        ebLogInfo("Wait for crs to be up")
                        with connect_to_host(_domu_list[0], get_gcontext(), username='oracle') as _node:
                            _cmd = "cat /etc/oratab | grep '^+ASM.*' | cut -f 2 -d ':'"
                            _crs_cmd_pfx = _node.mSingleLineOutput(_cmd)
                            _crs_cmd_pfx += '/bin/crsctl '

                            _timeout = 60*60
                            _initial_time = time.time()
                            _rc = 1

                            while _rc != 0:
                                _node.mExecuteCmd(_crs_cmd_pfx + 'check cluster -all | grep -c online | grep -w {0}'.format(3*len(domu_list)))
                                _rc = _node.mGetCmdExitStatus()

                                _elapsed_time = time.time() - _initial_time
                                if _timeout < _elapsed_time:
                                    raise ExacloudRuntimeError(0x0114, 0xA, 'Timeout while wait for crsctl online')

                                ebLogInfo("Wait for crs to be up: {0}".format(_elapsed_time))
                                time.sleep(3)

                            ebLogInfo("Wait for DB to be up")
                            time.sleep(2*60) #Wait other two minutes to resilence

        if not _rc:
            ebLogError('*** Fatal Error *** : Aborting current job - please review errors log and try again.')
            raise ExacloudRuntimeError(0x0114, 0xA, 'OEDA error while applying security fixes')

        self.mCheckDBInstanceIsUp(aOptions)
        return 0

    #
    # Create cloud_properties payload
    # For DBAAS consumption inVM
    #
    def mPrepareCloudPropertiesPayload(self):
        if self.__dbaas_api_payload:
            ebLogInfo("Preparing Dbaasapi Cloud Properties Payload")

            if self.__vmClusterType and self.__vmClusterType == "developer":
                if 'vm' not in self.__dbaas_api_payload["params"]:
                    self.__dbaas_api_payload["params"]["vm"] = {}
                self.__dbaas_api_payload["params"]["vm"]["type"] = "TmV3RGV2"

            if self.isATP() and (self.__exabm or self.__ociexacc):
                _dom0, _ = self.mReturnDom0DomUPair()[0]
                self.__dbaas_api_payload["params"]["atp"]["backup_listener"] = "LISTENER_BKUP"
                self.__dbaas_api_payload["params"]["atp"]["backup_listener_port"] = ebAtpUtils.mGetExaboxATPOption("sec_listener_port")

            # For ADB envs, add the desired exa_map to be used to
            # when runing the adb_setup at the end of create service
            if self.isATP():
                _atp_config = self.mCheckConfigOption('atp')
                if _atp_config:
                    if 'examap_override' in _atp_config and _atp_config['examap_override'] == 'True':
                        self.__dbaas_api_payload["params"]["atp"]["examap_override"] = _atp_config['examap_override']
                        self.__dbaas_api_payload["params"]["atp"]["examap_name"] = _atp_config['examap_name']
                _atp_cprops_config = self.mCheckSubConfigOption('atp','cprops')
                if _atp_cprops_config:
                    for cprops in _atp_cprops_config: 
                        self.__dbaas_api_payload["params"]["atp"][cprops] = _atp_cprops_config[cprops]

            if self.mIsOciEXACC():
                _admin_services = self.mGetOciExaCCServicesSetup()
                if _admin_services:
                    if not self.isATP():
                        for _, _domu in self.mReturnDom0DomUPair():
                            _nodeU = exaBoxNode(get_gcontext())
                            _nodeU.mConnect(aHost=_domu)
                            # NATTED MODE, the local link local dom0 is the first IP of the subnet
                            _,_o,_ = _nodeU.mExecuteCmd("/usr/sbin/ip route show dev eth0 scope link | /usr/bin/cut -d '/' -f 1 | /usr/bin/awk -F. '{printf \"%d.%d.%d.%d\", $1,$2,$3,$4+1}'")
                            _fileserver_ip = _o.read()
                            _fwdproxy_ip   = _fileserver_ip
                            _nodeU.mDisconnect()
                    else:
                        # ATP MODE, direct access to CPS services
                        _fileserver_ip = _admin_services['fileserver']['ip']
                        _fwdproxy_ip   = _admin_services['forwardproxy']['ip']

                        # Bug35009969: inject access_data_par entry into atp payload from cps config file: exacc.json
                        # host-updater is being deprecated and below config file will only exist in old CPS envs
                        _exacc_json_config_path = "/opt/oci/config_bundle/exacc.json"
                        if _exacc_json_config_path and os.path.exists(_exacc_json_config_path):
                            with open(_exacc_json_config_path) as _exacc_json_file_fd:
                                _exacc_json = json.load(_exacc_json_file_fd)
                                if "hostUpdate" in _exacc_json:
                                    self.__dbaas_api_payload["params"]["atp"]["access_data_par"] = _exacc_json["hostUpdate"] 
                        else: 
                            # Bug32614987: inject access_data_par entry into atp payload:
                            #TODO: Check if this code is relevant anymore.
                            _host_updater_path = "/etc/cerberus/yaarp-clients/config/host-updater/external.conf"
                            if _host_updater_path and os.path.exists(_host_updater_path):
                                with open(_host_updater_path) as _host_updater_hocon_file:
                                    lines = _host_updater_hocon_file.readlines()
                                    for line in lines:
                                        kv = line.strip().split(': ')
                                        if kv[0] == "access_data_par":
                                            self.__dbaas_api_payload["params"]["atp"]["access_data_par"] = kv[1].strip("\"")
                                            break

                    if 'common' not in self.__dbaas_api_payload["params"]:
                        self.__dbaas_api_payload["params"]["common"] = {}
                    # Enh34971477:inject parameter for new DNS that will be used at DRCC
                    _config_bundle_path = '/opt/oci/config_bundle/exacc.json'
                    with open(_config_bundle_path, "r") as _config_bundle_file:
                        _config_bundle_dict = dict(json.load(_config_bundle_file))
                        if _config_bundle_dict.get('dnsInternalSuffix'):
                                self.__dbaas_api_payload["params"]["common"]["drcc_dns_suffix"] = _config_bundle_dict['dnsInternalSuffix']
                    if 'fileserver' in _admin_services:
                        if self.mIsOciEXACC() and self.mIsFedramp():
                            self.__dbaas_api_payload["params"]["common"]["nat_fileserver"] = 'patchserver' 
                        else:
                            self.__dbaas_api_payload["params"]["common"]["nat_fileserver"] = _fileserver_ip
                        self.__dbaas_api_payload["params"]["common"]["nat_fileserver_port"] =\
                            _admin_services['fileserver']['port']
                        self.__dbaas_api_payload["params"]["common"]["nat_proxy_port"] = 3080
                        self.__dbaas_api_payload["params"]["localmetadata"] = "False"
                    if 'forwardproxy' in _admin_services:
                        try:
                            # OSS section must exist to set forwardproxy
                            # to a value such as "169.254.200.1:3080 in NAT mode, 10.X.X.X:3080 for ATP"
                            self.__dbaas_api_payload["params"]["diag"]["oss"]["oss_proxy"] =\
                                "{}:{}".format(_fwdproxy_ip,_admin_services['forwardproxy']['port'])
                        except:
                            ebLogWarn("Unable to set Forward Proxy into dbaas payload")
        else:
            _error_str = '*** Error: dbaasapi_payload does not exist'
            ebLogError(_error_str)
            raise ExacloudRuntimeError(0x0116, 0xA, _error_str)

    #
    # Create cloud_properties payload file in DomUs
    # For DBAAS consumption
    #
    def mPushCloudPropertiesPayload(self):
        if self.__dbaas_api_payload:
            aDestFile = '/tmp/dbaas.dbaasapi.cloud_properties.json'
            ebLogInfo("Copying Dbaasapi Cloud Properties Payload into domUs")

            ##################################################################
            # Bug32533418: Important Note:
            ##################################################################
            # For Security reasons,
            # the payload pushed into VM (DomU) will be sanitized
            # the diag section will be completely removed
            # since it contains passwords and its not required for ocde init
            #
            # Since this is Phase 1,
            # it is only required to have the platform information
            # when calling ocde init.
            #
            # File with cleartext passwords shouldn't live for that long.
            # and password setup in wallets will be done in mExecuteDbaaSApi()
            ##################################################################
            sanitized_payload = copy.deepcopy(self.__dbaas_api_payload)
            del sanitized_payload["params"]["diag"]

            # Create and push a file
            with NamedTemporaryFile(delete=False) as tfile:
                tfile.write(six.ensure_binary(json.dumps(sanitized_payload)))
                tfile.close()
                for _, _domu in self.mReturnDom0DomUPair():
                    _nodeU = exaBoxNode(get_gcontext())
                    _nodeU.mConnect(aHost=_domu)
                    _nodeU.mCopyFile(tfile.name, aDestFile)
                    _nodeU.mDisconnect()
                os.unlink(tfile.name)
                assert(not os.path.exists(tfile.name))

    #Same as mPushCloudPropertiesPayload method but to be called for each domu separately.
    #this gives the flexibility of copying different payload to each DomU.
    def mPushCloudPropertiesPayloadEachDomU(self, aDomU):
        if self.__dbaas_api_payload:
            aDestFile = '/tmp/dbaas.dbaasapi.cloud_properties.json'
            ebLogInfo(f"Copying Dbaasapi Cloud Properties Payload into {aDomU}")
            sanitized_payload = copy.deepcopy(self.__dbaas_api_payload)
            del sanitized_payload["params"]["diag"]

            # Create and push a file
            with NamedTemporaryFile(delete=False) as tfile:
                tfile.write(six.ensure_binary(json.dumps(sanitized_payload)))
                tfile.close()
                with connect_to_host(aDomU, get_gcontext()) as _nodeU:
                    _nodeU.mCopyFile(tfile.name, aDestFile)
                os.unlink(tfile.name)
                assert(not os.path.exists(tfile.name))

    #Reupload CloudProperties json if the Guest VM has a different NAT CIDR
    def mModifyAndUploadCloudPropertiesExaCC(self):
        if self.mIsOciEXACC():
            #Capture the Fileserver IP set in initial config
            _SetFileserver_ip = self.__dbaas_api_payload["params"]["common"]["nat_fileserver"]
            if self.__dbaas_api_payload["params"]["common"]["nat_fileserver"] and self.__dbaas_api_payload["params"]["common"]["nat_fileserver"] != 'patchserver':
                for _, _domu in self.mReturnDom0DomUPair():
                    with connect_to_host(_domu, get_gcontext()) as _nodeU:
                        # NATTED MODE, the local link local dom0 is the first IP of the subnet
                        _,_o,_ = _nodeU.mExecuteCmd("/usr/sbin/ip route show dev eth0 scope link | /usr/bin/cut -d '/' -f 1 | /usr/bin/awk -F. '{printf \"%d.%d.%d.%d\", $1,$2,$3,$4+1}'")
                        _fileserver_ip = _o.read()
                    if _SetFileserver_ip != _fileserver_ip:
                        self.__dbaas_api_payload["params"]["common"]["nat_fileserver"] = _fileserver_ip
                        self.mPushCloudPropertiesPayloadEachDomU(_domu)
    #
    # Remove cloud_properties payload file from DomUs
    #
    def mRemoveCloudPropertiesPayload(self):
        if self.__dbaas_api_payload:
            aDestFile = '/tmp/dbaas.dbaasapi.cloud_properties.json'
            ebLogInfo("Removing Dbaasapi Cloud Properties Payload from domUs")
            for _, _domu in self.mReturnDom0DomUPair():
                _nodeU = exaBoxNode(get_gcontext())
                _nodeU.mConnect(aHost=_domu)
                _cmd = "/usr/bin/rm {0}".format(aDestFile)
                _nodeU.mExecuteCmdLog(_cmd)
                _rc = _nodeU.mGetCmdExitStatus()
                fileExists = _nodeU.mFileExists(aDestFile)
                _nodeU.mDisconnect()
                if _rc and fileExists:
                    _error_str = '*** Error: Cloud Properties payload could not be deleted'
                    ebLogError(_error_str)
                    raise ExacloudRuntimeError(0x0116, 0xA, _error_str)

    def mEnsureFedRampCpropsIni(self, aDom0DomUPair, aOptions):
        """
        A simple helper method to ensure all the domUs from aDom0DomUPair
        have field 'common_fedramp' set to 'enabled' in exacc
        cluster with fedramp enabled

        It is up to the caller to ensure the cluster is indeed exacc with
        fedramp enabled. We'll verify in here just the 'common' section

        :param aDom0DomUPair: a dom0/domU pair

        :returns int:
            0 - success
            1 - No fedramp detected in the payload

        :raises ExacloudRuntimeError: unhandled errors

        """

        # Ensure payload has 'fedramp' enabled in the 'common' section
        if (aOptions.jsonconf and str(aOptions.jsonconf.get(
            "dbaas_api", {}).get("params", {}).get(
                "common", {}).get("fedramp")).lower() == "enabled"):
            ebLogInfo("Fedramp enabled detected in payload, we'll ensure "
                "cprops.ini reflects this in the domUs")
        else:
            ebLogInfo("Fedramp enabled not detected in payload, we'll not "
                f"update cprops.ini in the domUs")
            return 1

        _cprops_file = "/var/opt/oracle/cprops/cprops.ini"
        _key_val = {"common_fedramp": "enabled"}

        for _dom0, _domU in aDom0DomUPair:

            with connect_to_host(_domU, get_gcontext()) as _node:

                # Skip if cprops.ini file does not exist
                if not _node.mFileExists(_cprops_file):
                    ebLogWarn(f"{_cprops_file} not found in {_domU}.")
                    continue

                else:
                    _cprops_content = node_exec_cmd_check(_node,
                        f"/bin/cat {_cprops_file}")
                    ebLogTrace(f"{_domU} cprops.ini contents before fedramp "
                        f"check: {_cprops_content}")

                # Patch file using key-value pairs
                node_update_key_val_file(_node, _cprops_file, _key_val, sep='=')
                ebLogInfo("Successfully patched fedramp cprops.ini file for "
                    f"{_domU}")

                _cprops_content = node_exec_cmd_check(_node,
                    f"/bin/cat {_cprops_file}")
                ebLogTrace(f"{_domU} cprops.ini contents after fedramp "
                    f"field setup: {_cprops_content}")

        return 0

    #
    # ByPass dbaas_api parameters in case of found
    #

    def mExecuteDbaaSApi(self, aWhen='dbaasapi.createservice'):
        if self.__dbaas_api_payload:

            if self.__vmClusterType and self.__vmClusterType == "developer":
                if 'vm' not in self.__dbaas_api_payload["params"]:
                    self.__dbaas_api_payload["params"]["vm"] = {}
                self.__dbaas_api_payload["params"]["vm"]["type"] = "TmV3RGV2"

            if self.isATP() and (self.__exabm or self.__ociexacc):
                _dom0, _ = self.mReturnDom0DomUPair()[0]
                self.__dbaas_api_payload["params"]["atp"]["backup_listener"] = "LISTENER_BKUP"
                self.__dbaas_api_payload["params"]["atp"]["backup_listener_port"] = ebAtpUtils.mGetExaboxATPOption("sec_listener_port")

            # For ADB envs, add  the desired exa_map to be used to
            # when runing the adb_setup at the end of create service
            if self.isATP():
                _atp_config = self.mCheckConfigOption('atp')
                if _atp_config:
                    if 'examap_override' in _atp_config and _atp_config['examap_override'] == 'True':
                        self.__dbaas_api_payload["params"]["atp"]["examap_override"] = _atp_config['examap_override']
                        self.__dbaas_api_payload["params"]["atp"]["examap_name"] = _atp_config['examap_name']
                _atp_cprops_config = self.mCheckSubConfigOption('atp','cprops')
                if _atp_cprops_config:
                    for cprops in _atp_cprops_config: 
                        self.__dbaas_api_payload["params"]["atp"][cprops] = _atp_cprops_config[cprops]

            ebLogInfo("Copying DBaaS API Payload into domUs")

            for _, _domu in self.mReturnDom0DomUPair():
                _nodeU = exaBoxNode(get_gcontext())
                _nodeU.mConnect(aHost=_domu)
                if self.mIsOciEXACC():
                    _admin_services = self.mGetOciExaCCServicesSetup()
                    if _admin_services:
                        if not self.isATP():
                            # NATTED MODE, the local link local dom0 is the first IP of the subnet
                            _,_o,_ = _nodeU.mExecuteCmd("/usr/sbin/ip route show dev eth0 scope link | /usr/bin/cut -d '/' -f 1 | /usr/bin/awk -F. '{printf \"%d.%d.%d.%d\", $1,$2,$3,$4+1}'")
                            _fileserver_ip = _o.read()
                            _fwdproxy_ip   = _fileserver_ip
                        else:
                            # ATP MODE, direct access to CPS services
                            _fileserver_ip = _admin_services['fileserver']['ip']
                            _fwdproxy_ip   = _admin_services['forwardproxy']['ip']

                            # Bug35009969: inject access_data_par entry into atp payload from cps config file:  exacc.json
                            # host-updater is being deprecated and below config file will only exist in old CPS envs
                            _exacc_json_config_path = "/opt/oci/config_bundle/exacc.json"
                            if _exacc_json_config_path and os.path.exists(_exacc_json_config_path):
                                with open(_exacc_json_config_path) as _exacc_json_file_fd:
                                    _exacc_json = json.load(_exacc_json_file_fd)
                                    if "hostUpdate" in _exacc_json:
                                        self.__dbaas_api_payload["params"]["atp"]["access_data_par"] = _exacc_json["hostUpdate"] 
                            else: 
                                # Bug32614987: inject access_data_par entry into atp payload:
                                #TODO: Check if this code is relevant anymore.
                                _host_updater_path = "/etc/cerberus/yaarp-clients/config/host-updater/external.conf"
                                if _host_updater_path and os.path.exists(_host_updater_path):
                                    with open(_host_updater_path) as _host_updater_hocon_file:
                                        lines = _host_updater_hocon_file.readlines()
                                        for line in lines:
                                            kv = line.strip().split(': ')
                                            if kv[0] == "access_data_par":
                                                self.__dbaas_api_payload["params"]["atp"]["access_data_par"] = kv[1].strip("\"")
                                                break

                        if 'common' not in self.__dbaas_api_payload["params"]:
                            self.__dbaas_api_payload["params"]["common"] = {}
                        _config_bundle_path = '/opt/oci/config_bundle/exacc.json'
                        with open(_config_bundle_path, "r") as _config_bundle_file:
                            _config_bundle_dict = dict(json.load(_config_bundle_file))
                            if _config_bundle_dict.get('dnsInternalSuffix'):
                                self.__dbaas_api_payload["params"]["common"]["drcc_dns_suffix"] = _config_bundle_dict['dnsInternalSuffix']
                        if 'fileserver' in _admin_services:
                            if self.mIsOciEXACC() and self.mIsFedramp():
                                self.__dbaas_api_payload["params"]["common"]["nat_fileserver"] = 'patchserver'
                            else:
                                self.__dbaas_api_payload["params"]["common"]["nat_fileserver"] = _fileserver_ip
                            self.__dbaas_api_payload["params"]["common"]["nat_fileserver_port"] =\
                                _admin_services['fileserver']['port']
                            self.__dbaas_api_payload["params"]["common"]["nat_proxy_port"] = 3080
                            self.__dbaas_api_payload["params"]["localmetadata"] = "False"
                        if 'forwardproxy' in _admin_services:
                            try:
                                # OSS section must exist to set forwardproxy
                                # to a value such as "169.254.200.1:3080 in NAT mode, 10.X.X.X:3080 for ATP"
                                self.__dbaas_api_payload["params"]["diag"]["oss"]["oss_proxy"] =\
                                    "{}:{}".format(_fwdproxy_ip,_admin_services['forwardproxy']['port'])
                            except:
                                ebLogWarn("Unable to set Forward Proxy into dbaas payload")
                elif self.mIsOciEXACC():
                    ebLogWarn('*** XXX/MR: FIX ME (CC) ROCE/KVM - mExecuteDbaaSApi::mGetOciExaCCNatSetup not tested')


                # Defer the json dump for the rare case VM number is not the same on dom0s
                #resulting on different NATIP range (169.254.200.X)

                aDestFile = "/tmp/dbcs.{0}{1}".format(aWhen, str(self.__uuid))
                # Create and push a file
                with NamedTemporaryFile(delete=False) as tfile:
                    tfile.write(six.ensure_binary(json.dumps(self.__dbaas_api_payload)))
                    tfile.close()
                    _nodeU.mCopyFile(tfile.name, aDestFile)
                    os.unlink(tfile.name)
                    assert(not os.path.exists(tfile.name))

                _nodeU.mDisconnect()

            _rc, _cmd = self.mRunScript(aType='*', aWhen=aWhen)

            # Note: need integration script from gusanche
            if _rc:
                ebLogError('*** Error ('+str(_rc)+') caught during scripts execution for cmd: '+_cmd)
            else:
                if self.isATP():
                    ebLogInfo('Running tfadiag.setup')
                    self.mRunScript(aType='*', aWhen='tfadiag.setup')
                    # Since in ATP, this file contains diag server passwords.
                    ebLogInfo("Cleaning DBaaS API Payload in /tmp/dbcs.{0}{1}".format(aWhen, str(self.__uuid)))
                    _cmd = "/usr/bin/python3 -m json.tool --sort-keys /tmp/dbcs.{1}{0} | /usr/bin/sed  '/passwd/d' > /tmp/dbcs.{0} ; /usr/bin/rm /tmp/dbcs.{1}{0} ; /usr/bin/mv /tmp/dbcs.{0} /tmp/dbcs.{1}{0}".format(str(self.__uuid), aWhen)
                    for _, _domu in self.mReturnDom0DomUPair():
                        _nodeU = exaBoxNode(get_gcontext())
                        _nodeU.mConnect(aHost=_domu)
                        _nodeU.mExecuteCmdLog(_cmd)
                        _nodeU.mDisconnect()
                    ebLogInfo("Done cleaning DBaaS API Payload")

        else:
            ebLogInfo("skipping dbaas api configuration")

    #
    #  Patches the current domU for NID support
    #

    def mExecuteInstallPostGINID(self):

        ebLogVerbose("mExecuteInstallPostGINID: Patches the current domU for NID support.")

        _oeda_path = self.__oeda_path

        ebLogInfo('*** Setting up NID requirements')
        _step_time = time.time()

        # REQ 1 FOR NID (Create ASM Diskgroups)
        self.mAcquireRemoteLock()

        lCmd = "/bin/bash install.sh -cf {0} -s {1} {2}".format(self.__remoteconfig, \
                                                                self.mFetchOedaStep(OSTP_CREATE_ASM), \
                                                                self.mGetOEDAExtraArgs())
        ebLogInfo('Running: ' + lCmd)
        _out = self.mExecuteCmdLog2(lCmd, aCurrDir=_oeda_path)
        _rc = self.mParseOEDALog(_out)

        if not _rc:
            ebLogError('*** Fatal Error *** : Aborting current job - please review errors log and try again.')
            ebLogError('*** Exacloud Operation Failed : Create ASM Diskgroups failed during Create Service')
            raise ExacloudRuntimeError(0x0115, 0xA, 'Exacloud Operation Failed : Create ASM Diskgroups failed during Create Service')

        self.mReleaseRemoteLock()

        self.mLogStepElapsedTime(_step_time, 'Create ASM Diskgroups')

        ebLogInfo("*** DBCS nid requirements ")
        _step_time = time.time()

        _rc, _cmd = self.mRunScript(aType='*', aWhen='post.gi_rpm', aStatusAbort=True)
        if _rc:
            ebLogError('*** Error ('+str(_rc)+') catched during scripts execution for cmd: '+_cmd)
            raise ExacloudRuntimeError(0x0116, 0xA, 'OCDE Step: RPM configuration error')

        if self.isATP():
            _rc, _cmd = self.mRunScript(aType='*', aWhen='post.gi_nid_atp')
        else:
            _rc, _cmd = self.mRunScript(aType='*', aWhen='post.gi_nid')
        if _rc:
            ebLogError('*** Error ('+str(_rc)+') caught during scripts execution for cmd: '+_cmd)
            raise ExacloudRuntimeError(0x0116, 0xA, 'OCDE Step: NID configuration error')

        self.mLogStepElapsedTime(_step_time, 'OCDE NID configuration script')

    #
    # Pach the DBNID bits in the given domU it just copy
    #

    def mUpdateDBNIDBits(self, aOptions):
        ebLogVerbose("mUpdateDBNIDBits: Patch the DBNID bits in the domU")

        # shutting down the vm
        ebLogInfo("Copy DBNID Bits")
        if self.__dyndep_version is None or self.__dyndep_version == '0.0':
            _dyndep_version, _ = self.mDyndepFilesList()
            self.__dyndep_version = _dyndep_version


        # OCDE bits
        required_bits = self.mDynDepNonImageList(['ocde_bits'])

        _bits_dict = []

        if required_bits:
            # [
            #    { "source": <path> , "target" : <path> }
            # ]
            for req in required_bits:
                _bits_dict_entry = dict()
                _bits_dict_entry["info"] = req
                ebLogInfo("{0}".format(req))
                _bits_dict_entry["target"] = '/u02/opt/dbaas_images'
                if "local" in req:
                    _bits_dict_entry["source"] = req["local"]

                # mapping to an specific file name
                if 'map' in req and req['map']:
                    _bits_dict_entry["target"] = '/u02/opt/dbaas_images/' + req["map"].split('/')[-1]

                _bits_dict.append(_bits_dict_entry)

        else:
            # add return code to Error.py
            return 0x511

        # connecting to each domU to copy files

        def _copy_bits(_domu, _rc_status):

            _rc_status[_domu] = 0

            try:
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_domu)

                _node.mExecuteCmdLog('mkdir -p /u02/opt/dbaas_images')
                _node.mExecuteCmdLog('mkdir -p /u02/opt/dbaas_images/nid')

                for _depfile in _bits_dict:
                    if "source" in _depfile and "target" in _depfile:
                        ebLogInfo("updating {0} @ {1}".format(_depfile, _domu))
                        _filename = os.path.basename(_depfile["source"])
                        target_file = os.path.join(_depfile["target"], _filename)
                        ebLogInfo("dest file : {0}".format(target_file))
                        ebLogInfo("source : {0}".format(_depfile["source"]))

                        if _node.mFileExists(target_file):
                            ebLogInfo(" Old file found in the filesystem {0}".format(target_file))
                            _node.mExecuteCmdLog("rm -f {0}".format(target_file))

                        _node.mCopyFile(_depfile["source"], target_file)

                    else:
                        ebLogWarn("*** Not enough information to copy files to domU ***")
                _node.mDisconnect()

            except Exception as copyError:
                ebLogError("Error {0}".format(copyError))
                _rc_status[_domu] = 0x511
                _node.mDisconnect()

        #Create the multiprocess structure
        _plist = ProcessManager()
        _rc_status = _plist.mGetManager().dict()


        for _dom0, _domu in self.mReturnDom0DomUPair():
            _p = ProcessStructure(_copy_bits, (_domu, _rc_status))
            _p.mSetMaxExecutionTime(30*60) # 30 minutes
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)
        _plist.mJoinProcess()


        # validate the return codes
        _rc_all = 0
        for _, _domu in self.mReturnDom0DomUPair():
            ebLogInfo("*** Updating db nid bits in {0} rc {1} ***".format(_domu, _rc_status[_domu]))
            if _rc_status[_domu]:
                _rc_all = ebError(_rc_status[_domu])
                break

        return _rc_all

    def mExecuteExachk(self):

        _step_time = time.time()
        _options = self.mGetArgsOptions()
        _hcObj = ebCluHealthCheck(self, _options)
        _options.healthcheck = 'exachk'
        _hcObj.mDoHealthCheck(_options)

        self.mLogStepElapsedTime(_step_time, 'Exachk run: completed')


    """
    This function executes the cluster check operations as defined by the file named specified by aCheckClusterProfileName.
    aCheckClusterProfileName is a file name of json type location under $EXACLOUD_ROOT/hcprofile directory.
    """
    def mExecuteProfileClusterCheck(self, aCheckClusterProfileName=None):

        if aCheckClusterProfileName is None:
            ebLogWarn("Empty profile name for checkcluster operation. Skipping cluster check operations.")
            return

        _absolute_profile_path = os.path.join(self.__base_path, f"hcprofile/{aCheckClusterProfileName}")
        if not os.path.exists(_absolute_profile_path):
            ebLogWarn(f"Cluster check profile doesn't exist: {_absolute_profile_path}")
            return

        _profile_json_configuration = {}
        with open(_absolute_profile_path) as _file_fd:
            _profile_json_configuration = json.load(_file_fd)
        _step_time = time.time()
        _options = copy.deepcopy(self.mGetArgsOptions())
        _options.jsonconf = _profile_json_configuration
        _options.cmd = "checkcluster"
        _hcObj = ebCluHealthCheck(self, _options)
        _hcObj.mDoHealthCheck(_options)

        self.mLogStepElapsedTime(_step_time, f"Profile based cluster check run for {aCheckClusterProfileName} completed")


    def mGetWalletInfo(self):

        _oracle_home = ''
        _dbagent_wallet_loc = ''
        _domu_list = [ _domu for _ , _domu in self.mReturnDom0DomUPair()]
        _domu = _domu_list[0]
        _node = exaBoxNode(get_gcontext())
        _node.mSetUser("opc")
        _node.mConnect(aHost=_domu)

        _, _o, _ = _node.mExecuteCmd("cat /var/opt/oracle/creg/grid/grid.ini | grep oracle_home")
        _out = _o.readlines()
        if _out and len(_out):
            _out = _out[0].strip()
            if '=' in _out:
                _oracle_home = _out.split('=')[1]

        _, _o, _ = _node.mExecuteCmd("cat /var/opt/oracle/creg/grid/grid.ini | grep dbagent_wallet_loc")
        _out = _o.readlines()
        if _out and len(_out):
            _out = _out[0].strip()
            if '=' in _out:
                _dbagent_wallet_loc = _out.split('=')[1]

        _node.mDisconnect()
        return _oracle_home, _dbagent_wallet_loc


    def mAddWalletEntry(self, aKeyStore, aWalletLoc, aWalletKey, aPasswd):

        _key_store = aKeyStore
        _wallet_loc = aWalletLoc
        _passwd = aPasswd
        _wallet_key = str(uuid.uuid1()) + '.' + aWalletKey

        _domu_list = [ _domu for _ , _domu in self.mReturnDom0DomUPair()]
        _domu = _domu_list[0]
        _node = exaBoxNode(get_gcontext())

        try:

            _node.mSetUser("opc")
            _node.mConnect(aHost=_domu)

            _cmd = f'{_key_store} -wrl {_wallet_loc} -createEntry "{_wallet_key}" "{_passwd}"'
            _node.mExecuteCmdLog(_cmd)

            _ret = _node.mGetCmdExitStatus()
            ebLogInfo("*** mAddWalletEntry: mExecuteCmdsAuthInteractive return {0} ***".format(_ret))

        finally:
            _node.mDisconnect()

        return _wallet_key


    def mGetWalletViewEntry(self, aWalletKey):

        _wallet_key = aWalletKey
        _passwd = ''

        _oracle_home, _wallet_loc = self.mGetWalletInfo()
        if _wallet_loc == '':
            _passwd = _wallet_key
            return _passwd

        _key_store = _oracle_home + '/bin/mkstore'

        _domu_list = [ _domu for _ , _domu in self.mReturnDom0DomUPair()]
        _domu = _domu_list[0]
        _node = exaBoxNode(get_gcontext())
        _node.mSetUser("opc")
        _node.mConnect(aHost=_domu)

        _cmd  = '{0} -wrl {1} -viewEntry {2} | grep {2}'.format(_key_store, _wallet_loc, _wallet_key)
        _, _o, _e = _node.mExecuteCmd(_cmd)
        _out = _o.readlines()
        if _out and len(_out):
            _out = _out[0].strip()
            if '=' in _out:
                _passwd = _out.split('=')[1].strip()

        # If dbagent_wallet did not contain the password,
        # use db_wallet to fetch the password.
        if _passwd == '':

            _wallet_loc = ''
            _, _o, _e = _node.mExecuteCmd("/usr/bin/grep 'wallet_loc=' /var/opt/oracle/creg/*.ini")
            _out = _o.readlines()
            if _out and len(_out):
                _out = _out[0].strip()
                _wallet_loc = _out.split('=')[1].strip()

            _wallet_key = 'passwd'
            _cmd  = '{0} -wrl {1} -viewEntry {2} | grep {2}'.format(_key_store, _wallet_loc, _wallet_key)

            _, _o, _e = _node.mExecuteCmd(_cmd)
            _out = _o.readlines()
            if _out and len(_out):
                _out = _out[0].strip()
                if '=' in _out:
                    _passwd = _out.split('=')[1].strip()

        _node.mDisconnect()
        return _passwd


    def mRemoveRemoteCFG(self):
        _str_dbNameCfg_remoteloc = '/tmp/' + self.__dbname_cfg.split('/')[-1]
        for _, _domu in self.mReturnDom0DomUPair():
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domu)
            _node.mExecuteCmd('rm -f {0}'.format(_str_dbNameCfg_remoteloc))
            _node.mDisconnect()


    def mExecuteInstallStarterDBNID(self, aCmd, aOptions, _oeda_path, step_list, db_step_list):
        ebLogVerbose("mExecuteInstallStarterDBNID: aCmd = %s, oeda path = %s" % (aCmd, _oeda_path))
        _jconf = aOptions.jsonconf
        if 'dbVersion' in list(_jconf.keys()):
            _version = _jconf['dbVersion'][0:3]
        elif 'version' in list(_jconf['dbParams'].keys()):
            _version = _jconf['dbParams']['version'][0:3]

        self.__create_nid_starterdb = True

        for step in step_list:

            if step == OSTP_INSTALL_EXCHK:
                #
                # run exachk after started DB creation
                #
                self.mExecuteExachk()

            if step == OSTP_PREDB_INSTALL:

                # Check elastic support for Starter DB
                _step_time = time.time()
                self.mUpdateStatusOEDA(True, OSTP_PREDB_INSTALL, step_list, 'Check Elastic Support for Starter DB')
                _elastic_required,_nodelist = self.mCheckNodeListParam()
                if _elastic_required is True:
                    self.mCheckElasticSupportStarterDB(_nodelist)
                    self.mLogStepElapsedTime(_step_time, 'PREDB INSTALL : Elastic Support required for Starter DB')

                # Set PGA/SGA on OEDA properties
                #
                _step_time = time.time()

                # For starter DBNID the database password must be changed es.properties
                # after database creation because it is required to login into RMAN once
                # the db is already provisioned to apply security fixes
                #

                errno = self.mUpdateOEDAProperties(aOptions)
                if errno:
                    return errno

                self.mLogStepElapsedTime(_step_time, 'PREDB INSTALL : Updating OEDA environment')
                #
                # Reset SSH Cluster Keys
                #
                _step_time = time.time()
                self.mUpdateStatusOEDA(True, OSTP_PREDB_INSTALL, step_list, 'Reset SSH Cluster knowhost entries')
                self.mResetClusterSSHKeys(aOptions)
                self.mLogStepElapsedTime(_step_time, 'PREDB INSTALL : Reset SSH Cluster knowhost entries')
                #
                # Generate switch keys
                #
                _step_time = time.time()
                self.mUpdateStatusOEDA(True, OSTP_PREDB_INSTALL, step_list, 'Generate Switch Keys')
                self.mHandlerGenerateSwitchesKeys()
                self.mLogStepElapsedTime(_step_time, 'PREDB INSTALL : Generate Switch Keys')
                #
                # Copy additional files
                #
                _step_time = time.time()
                self.mUpdateStatusOEDA(True, OSTP_PREDB_INSTALL, step_list, 'Copy additional files')
                # ER 27503421
                self.mCopyVmexacsRpm()
                # Bug32540420: For ADBD, we use rpmupd logic to update dbaastools_exa rpm
                # during create service.
                # We shouldnt update any rpm after CreateService has finished (for ADB-D)
                if not self.isATP():
                    # Bug 27216120
                    self.mUpdateRpm('dbaastools_exa_main.rpm')
                # ER 25900538 and 27555477
                if self.__exabm or self.__ociexacc:
                    self.mCopyCreateVIP()
                    self.mCopyOneoffZipToDomus()
                self.mLogStepElapsedTime(_step_time, 'PREDB INSTALL : Copy additional files')

                _step_time = time.time()
                self.mVerifyDBBitsOnDomU()
                self.mLogStepElapsedTime(_step_time, 'PREDB INSTALL : DB Bits verification')

            if step == OSTP_DBNID_INSTALL:
                _step_time = time.time()
                self.mUpdateStatusOEDA(True, OSTP_DBNID_INSTALL, step_list, 'Flush ebtables in DOM0')
                self.mSetupEbtablesOnDom0(aMode=False)
                self.mLogStepElapsedTime(_step_time, 'Flush ebtables in DOM0')
                #
                # DBNID installation follows the same path as the additional db
                #
                ebLogInfo('Verifying GI/Clusterware version')
                _mrecreategridrequired = self.mCheckGridVersion()
                if _mrecreategridrequired:
                    ebLogInfo("*** Recreate service necessary. GI/Clusterware not at the required version ***")
                    raise ExacloudRuntimeError(0x0725, 0x0A, "GI/Clusterware not at the required version.", Cluctrl = self)

                _step_time = time.time()

                #Workaround starter DB When only 19.0.0.0 images are avaialable
                for _, _domu in self.mReturnDom0DomUPair():
                    _node = exaBoxNode(get_gcontext())
                    _node.mConnect(aHost=_domu)
                    ebLogInfo("*** Creating db directory /u01/app/oracle/product/12.1.0.2/dbhome_1 in host:{0}***".format(_domu))
                    _cmd = "mkdir -p /u01/app/oracle/product/12.1.0.2/dbhome_1"
                    _node.mExecuteCmdLog(_cmd)
                    _node.mDisconnect()
                self.mLogStepElapsedTime(_step_time, 'DBNID INSTALL : Completed')

                # Copy OCDE files in ODEA staging dir
                #
                _step_time = time.time()
                self.mUpdateStatusOEDA(True, OSTP_DBNID_INSTALL, step_list, 'Copy OCDE Log file')
                self.mCopyOCDELogFile()
                self.mLogStepElapsedTime(_step_time, 'Copy OCDE Log file')

            if step == OSTP_APPLY_FIX_NID:
                _step_time = time.time()

                if step == OSTP_APPLY_FIX_NID and self.mCheckConfigOption('skip_afs_step','True'):
                    ebLogInfo("*** skipping apply security fixes ***")
                    continue

                #Run Cluster Integrity Tests Before Apply Security Fixes
                if self.__clu_verify:
                    _pchecks = ebCluPreChecks(self)
                    _pchecks.mCheckClusterIntegrity(False)

                self.mCopyMemInfoFile(_oeda_path)
                self.mUpdateStatusOEDA(True, step, step_list, 'APPLY SECURITY FIXES')
                errno = self.mExecuteApplySecurityFixes(_oeda_path, _version, aOptions)
                if errno:
                    # Remove cfg file from domus
                    self.mRemoveRemoteCFG()
                    raise ExacloudRuntimeError(0x0118, 0x0A, "OEDA : Oeda Error while applying security fixes")

                # Bug 31879396 Wait for ASM to be truly up
                _domu_list = [ _domu for _ , _domu in self.mReturnDom0DomUPair()]
                self.mCheckAsmIsUp(_domu_list[0], _domu_list)

                # restoring remote files
                self.mExecuteRestoreBkupFiles()
                self.mLogStepElapsedTime(_step_time, 'APPLY SECURITY FIXES : Completed')

            if step == OSTP_POSTDB_INSTALL:
                #
                # ER 28621699: Configure ATP wallets
                #
                if self.isATP():
                    # Run the setup script of ATP wallets in DOMUs
                    _step_time = time.time()
                    self.mUpdateStatusOEDA(True, OSTP_POSTDB_INSTALL, step_list, 'Install ATP Wallets')
                    _domUs = list(map(operator.itemgetter(1),self.mReturnDom0DomUPair()))
                    _dbname = self.__dbname
                    ebAtpUtils.mInstallAtpWallets(_domUs,_dbname)
                    self.mLogStepElapsedTime(_step_time, 'Install ATP Wallets')
                #
                # Add customer SSH Key in DOMUs
                #
                _step_time = time.time()
                self.mUpdateStatusOEDA(True, OSTP_POSTDB_INSTALL, step_list, 'Patch VM SSH Keys')
                self.mPatchVMSSHKey(aOptions)
                self.mLogStepElapsedTime(_step_time, 'Patch VM SSH Keys')

                #
                # POST DB NID SSH Key patching (default root/opc/oracle)
                #
                _step_time = time.time()
                self.mUpdateStatusOEDA(True,OSTP_POSTDB_INSTALL,step_list,'Post DB NID create SSH Key patching')
                self.mPostDBSSHKeyPatching(aOptions, 'addkey', ['root','oracle','opc'])
                self.mLogStepElapsedTime(_step_time, 'Post DB NID create SSH Key patching')

                #
                #  POST DB NID SETUP
                #  Setting db default values. this is because some parameters were overwritten by
                #    apply security fixes
                #
                _rc, _cmd = self.mRunCmdScript("posdb_nid")
                if _rc:
                    # Remove cfg file from domus
                    self.mRemoveRemoteCFG()
                    ebLogError('*** Error cannot apply dbnid fixes to starter db')
                    raise ExacloudRuntimeError(0x0118, 0x0A, "DBAAS : Cannnot apply DBNID fixes to Starter DB")

                # During integration with ECRA, this should be changed.
                if not self.mCheckConfigOption('client_v6_support', 'False') and self.__exabm == True:
                    ebLogInfo(type(self.__config.mGetConfigElement('networks')))
                    _networks = self.__config.mGetConfigElement('networks')
                    ebLogInfo('*** Configure v6 Client Network and DB listeners')
                    _node = _jconf["customer_network"]["nodes"]
                    for _v6 in _node:
                        self.__networks.mSetNetworkConfigV6(_jconf["clustername"], _v6["v6client"])
                        self.__machines.mAppendMachinesConfigV6(_jconf["clustername"], _v6["v6client"])
                        self.__clusters.mAppendClusterVipV6(_jconf["clustername"], _v6["v6vip"], _v6["v6client"]["domu_oracle_name"])
                    self.__clusters.mAppendClusterScanV6(_jconf["clustername"], _jconf["customer_network"]["v6scan"])

                    self.mSaveXMLClusterConfiguration()
                    self.mSetConfigPath(self.__patchconfig)
                    self.mParseXMLConfig(aOptions)
                    ebLogInfo('*** Configure v6 Client Network and DB listeners')
                    ebLogInfo('*** Path ' + str(self.__patchconfig))
                    self.mConfigV6Client()
                    self.mConfigScanV6IPsPostDBInstall()

            if step == OSTP_DG_CONFIG:
                #
                # DG SETUP
                #
                if self.mCheckDBConfigOption(aOptions,"dg_config","yes"):
                    self.mUpdateStatusOEDA(True, OSTP_POSTDB_INSTALL, step_list, 'Configuring Dataguard for Database')
                    _rc = self.mConfigureDataguard(aOptions)
                    self.mLogStepElapsedTime(_step_time, 'Configuring Dataguard')
                    if not _rc:
                        # Remove cfg file from domus
                        self.mRemoveRemoteCFG()
                        ebLogError('*** Error In configuring Dataguard, Please check logs')
                        return ebError(0x0502)

            if step == OSTP_END_INSTALL:
                #
                # Alter cloud_user's password in the cells.
                #
                if self.mCheckConfigOption('exacli_use_db_pwd', 'True'):

                    _step_time = time.time()
                    self.mUpdateStatusOEDA(True, OSTP_POSTDB_INSTALL, step_list, 'Alter user-password')
                    try:
                        self.mUpdateCloudUser(aOptions)
                    except:
                        ebLogError("Error while updating cloud user password")
                    self.mLogStepElapsedTime(_step_time, 'Alter user-password')

                #
                # Create ADBS cloud_user
                #
                self.mCreateAdbsUser(aOptions)

                # Enable Storage Cell remote password change for exacli users
                self.mEnableRemotePwdChange(aOptions)

                #
                # perform CRS reboot
                #
                self.mExecuteCRSReboot()

                # Remove cfg file from domus
                self.mRemoveRemoteCFG()

                ebLogInfo('*** Exacloud Operation Successful : Create Starter DBNID completed')
                self.mUpdateStatusOEDA(True, OSTP_END_INSTALL, step_list, 'Create Starter DBNID Completed')

        return 0

    def mSetWalletEntry(self, aOptions):
        _dict = {}
        _jconf = {}
        if aOptions is not None:
            aOptions.jsonconf = umaskSensitiveData(aOptions.jsonconf)
            _jconf = aOptions.jsonconf
        
        _oracle_home, _dbagent_wallet_loc = self.mGetWalletInfo()
        _key_store = _oracle_home + '/bin/mkstore'
        if 'dbParams' in list(_jconf.keys()):
            _dict = _jconf.get("dbParams", {})
        elif 'vm' in list(_jconf.keys()) and "adminPassword" in list(_jconf["vm"].keys()):
            _dict = _jconf.get("vm", {})

        for _k in list(_dict.keys()):
            if not self.IsZdlraProv() and ("passwd" in _k or "adminPassword" in _k) and _dbagent_wallet_loc and _dict[_k]:
                _wallet_key = self.mAddWalletEntry(_key_store, _dbagent_wallet_loc, _k, _dict[_k])
                _password = self.mGetWalletViewEntry(_wallet_key)
                if _password == _dict[_k]:
                    ebLogInfo('*** Add Wallet Entry success')
                else:
                    ebLogError('*** Add Wallet Entry Failed')

    def mParseVMKey(self, aOptions):
        _jconf = {}
        if aOptions is not None:
            aOptions.jsonconf = umaskSensitiveData(aOptions.jsonconf)
            _jconf = aOptions.jsonconf

        #
        # Handle SSH Key
        #
        _ssh_public_key = ""
        if 'tools_ssh' in list(_jconf.keys()):
            self.__tools_key_public  = _jconf['tools_ssh']['ssh_public_key'].rstrip()
            self.__tools_key_private = _jconf['tools_ssh']['ssh_private_key']
        # Search for ssh public key in payload under 'vm' section
        # This should come in base64 encoding
        elif 'vm' in list(_jconf.keys()):
            _ssh_public_key = _jconf.get("vm", {}).get("sshkey", False)
            if _ssh_public_key:
                self.__tools_key_public = _ssh_public_key.rstrip()
                self.__tools_key_private = None

                # Verify sshkey is not empty and is base64 encoded
                if not self.__tools_key_public or not check_string_base64(self.__tools_key_public):
                    _err = ("sshkey given on payload is empty or invalid (base64 encoding)")
                    ebLogError(_err)
                    raise ExacloudRuntimeError(0x0823, 0xA, _err)
                ebLogTrace("Exacloud using sshkey on payload under 'vm' field")
                self.__tools_key_public = base64.b64decode(
                        self.__tools_key_public).decode()

        # Check if present in legacy 'sshkey' section if we didn't find it
        # already under vm section
        if 'sshkey' in list(_jconf.keys()) and not _ssh_public_key:
            if _jconf['sshkey'] is not None:
                self.__tools_key_public = _jconf['sshkey'].rstrip()
                self.__tools_key_private = None
            else:
                ebLogError ("*** Error *** SSH Key not provided or incorrect")
                raise ExacloudRuntimeError(0x0406, 0xA, "SSH Key not provided or incorrect", aStackTrace=False)

        if aOptions.debug and self.__tools_key_public:
            ebLogInfo('*** Tools keys found')

    def mUpdateExacliPwd(self, aOptions):
        if self.mIsOciEXACC():
            #Bug 35504780: SET EXACLI CLOUD USER PASSWORD IN STARTERDB FOR WAVE4
            ebLogInfo("Setting cloud_user password...")
            step_list = [OSTP_END_INSTALL]
            step = OSTP_END_INSTALL

            #
            # Add customer SSH Key in DOMUs
            #
            _step_time = time.time()
            self.mParseVMKey(aOptions)
            self.mUpdateStatusOEDA(True, step, step_list, 'Patch VM SSH Keys')
            self.mPatchVMSSHKey(aOptions)
            self.mLogStepElapsedTime(_step_time, 'Patch VM SSH Keys')

            #
            # POST DB NID SSH Key patching (default root/opc/oracle)
            #
            _step_time = time.time()
            self.mUpdateStatusOEDA(True, step, step_list,'Post DB NID create SSH Key patching')
            self.mPostDBSSHKeyPatching(aOptions, 'addkey', ['root','oracle','opc'])
            self.mLogStepElapsedTime(_step_time, 'Post DB NID create SSH Key patching')

            #
            # Alter cloud_user's password in the cells.
            #
            if self.mCheckConfigOption('exacli_use_db_pwd', 'True'):

                _step_time = time.time()
                self.mUpdateStatusOEDA(True, step, step_list, 'Alter user-password')
                try:
                    self.mSetWalletEntry(aOptions)
                    self.mUpdateCloudUser(aOptions)
                except Exception as e:
                    ebLogError("Error while updating cloud user password")
                    ebLogError(e)
                self.mLogStepElapsedTime(_step_time, 'Alter user-password')

            #
            # Create ADBS cloud_user
            #
            self.mCreateAdbsUser(aOptions)

            # Enable Storage Cell remote password change for exacli users
            self.mEnableRemotePwdChange(aOptions)
        else:
            ebLogInfo("Updating cloud_user password is supported only for starterDB in exacc env.")

    def mUpdateCloudUser(self, aOptions, aCellList=None, aPasswd=None):

        _options = aOptions

        # Capture clustername and form cloud_user_<clustername>
        _domU = self.mReturnDom0DomUPair()[0][1]
        _node = exaBoxNode(get_gcontext())
        _node.mConnect(aHost=_domU)

        _path, _, _ = self.mGetOracleBaseDirectories(aDomU = _domU)
        _, _o, _ = _node.mExecuteCmd(_path + '/bin/olsnodes -c')
        _node.mDisconnect()
        _newusr = "cloud_user_" + _o.read().strip()

        _options.user_operation = "list_user"

        _usrconfig = ebCluResManager(self, _options)

        # Find the list of users (in the cells)
        _rc = _usrconfig.mUserConfig(_options)
        _data = _usrconfig.mGetData()

        _usrlist = _data["Users"]

        ebLogVerbose("mUpdateCloudUser: List of users: %s" % _data["Users"])

        if "cloud_user" in _usrlist:
            _user = "cloud_user"
        elif _newusr in _usrlist:
            _user = _newusr
        else:
            ebLogInfo("No cloud user found to be updated.")
            return

        # If the password is not passed as argument, pick it up from payload
        if aPasswd:
            _passwd = aPasswd

        elif self.IsZdlraProv():
            _passwd = self.mGetZDLRA().mGetWalletViewEntry("passwd")

        # As part of starterDB removal effort, ECRA should send this password
        # under aOptions.jsonconf.vm.adminPassword
        elif _options.jsonconf:
            _passwd = _options.jsonconf.get(
                    "vm", {}).get("adminPassword", "")

            # If password not present in adminPassword, try to fetch it from legacy
            # sshkey section in the payload
            if not _passwd:
                ebLogTrace("SSH Key not provided under adminPassword, fetching from legacy "
                        "dbParams section")
                _passwd = _options.jsonconf.get(
                        "dbParams", {}).get("passwd", "")
                _passwd = b64encode(_passwd.encode("utf-8")).decode("utf-8")

            # Verify password is not empty and is base64 encoded
            if not _passwd or not check_string_base64(_passwd):
                _err = ("cloud_user / exacli password provided on payload "
                    f"by ECRA is invalid/not base64 encoded: '{_passwd}'")
                ebLogError(_err)
                raise ExacloudRuntimeError(0x0823, 0xA, _err)

        else:
            _err = "No password provided to be used for cloud_user/exacli"
            ebLogError(_err)
            raise ExacloudRuntimeError(0x0823, 0xA, _err)

        # Ensure b64 password
        if not check_string_base64(_passwd):
            _passwd = b64encode(_passwd.encode("utf-8")).decode("utf-8")

        # DB Admin password accepts the following special chars ->  # !#^*()[]{}:+-_<>,
        # cloud_user (cell user) password accepts the following special chars -> !@#$%^&*()-_
        # For cloud_user, invalid chars are - '[', ']', '{', '}', ':', '+', '<', '>', ','
        # Thus, it is possible that setting of cloud_user password to DB ADmin password fails.
        # Appropriate warning is reported in cluresmgr.py in that case.

        _options.user_operation = "alter_user"
        if _options.jsonconf is None:
            _options.jsonconf = {}
        _options.jsonconf["user"] = _user

        # This password will come in base64 encoding on the payload,
        # we just need to convert it to bytes object
        _options.jsonconf["password"] = _passwd.encode()

        _usrconfig = ebCluResManager(self, _options)

        # Execute 'alter_user' cmd on the list of cells provided (incase sent)
        if aCellList:
            _usrconfig.mSetCells(aCellList)
        # Alter cloud_user's password (in the cells)
        _rc = _usrconfig.mUserConfig(_options)

        # Cloud user will be stored in DomU
        _domUs = list(map(operator.itemgetter(1),self.mReturnDom0DomUPair()))
        ebExaCCSecrets(_domUs).mPushExacliPasswdToDomUs(b64decode(_passwd.encode()).decode())

    #
    # On dev and QA racks, delete cloud_user_<clustername> and cloud_role
    #
    def mDeleteCloudUser(self, aOptions, deleteAll):

        _options = aOptions

        _clusterName = self.__clusters.mGetCluster().mGetCluName()

        _newusr = "cloud_user_" + _clusterName
        _options.user_operation = "list_user"
        _usrconfig = ebCluResManager(self, _options)

        # Find the list of users (in the cells)
        _rc = _usrconfig.mUserConfig(_options)
        _data = _usrconfig.mGetData()

        _usrlist = _data["Users"]

        ebLogVerbose("mDeleteCloudUser: List of users: %s" % _data["Users"])

        # Delete both cloud_user and cloud_user_<clustername>
        _options.user_operation = "delete_user"
        _userdeleted = None
        if ((deleteAll == True) and ("cloud_user" in _usrlist)):
            ebLogInfo('*** mDeleteCloudUser: Deleting cloud_user on cells')
            _options.jsonconf["user"] = "cloud_user"
            _rc = _usrconfig.mUserConfig(_options)
            _userdeleted = True

        # we will like to delete the following cloud_user_<cluster> in all situations
        if _newusr in _usrlist:
            ebLogInfo('*** mDeleteCloudUser: Deleting %s on cells' %(_newusr))
            _options.jsonconf["user"] = _newusr
            _rc = _usrconfig.mUserConfig(_options)
            _userdeleted = True
        if _userdeleted is None:
            ebLogInfo("No cloud user found to be deleted.")

        # Delete cloud_role
        if deleteAll == True:
            ebLogInfo('*** mDeleteCloudUser: Deleting cloud_role on cells')
            _options.user_operation = "delete_role"
            _options.jsonconf["role"] = "cloud_role"
            _options.jsonconf["force"] = "True"
            _rc = _usrconfig.mUserConfig(_options)

        # Called in create service path; Errors, if any, are non-fatal to CS
        # Hence return without checking.

    def mExecuteInstallStarterDB(self, aCmd, aOptions, _oeda_path, step_list, db_step_list):
        ebLogVerbose("mExecuteInstallStarterDB: aCmd = %s, oeda path = %s" % (aCmd, _oeda_path))

        #
        # DB_INSTALL MAIN LOOP
        #
        for step in step_list:

            if step == OSTP_PREDB_INSTALL:
                #
                # Set PGA/SGA on OEDA properties
                #
                _step_time = time.time()
                self.mUpdateOEDAProperties(aOptions)
                self.mLogStepElapsedTime(_step_time, 'PREDB INSTALL : Updating OEDA environment')
                #
                # Reset SSH Cluster Keys
                #
                _step_time = time.time()
                self.mUpdateStatusOEDA(True, step, step_list, 'Reset SSH Cluster knowhost entries')
                self.mResetClusterSSHKeys(aOptions)
                self.mLogStepElapsedTime(_step_time, 'PREDB INSTALL : Reset SSH Cluster knowhost entries')
                #
                # Check for MULTI-VM the DB name existing on the cell side
                #
                if self.__shared_env and not self.mCheckConfigOption('skip_dbname_check','True'):
                    self.mCheckDBNameOnCells()
                #
                # Update XML configuration (obsolete step ?
                #
                _step_time = time.time()
                self.mUpdateStatusOEDA(True, step, step_list, 'Copy/Update Cluster Configuration in DOM0')
                self.mCopyFileToClusterConfiguration(self.__configPath, 'db_install_cluster.xml')
                self.mLogStepElapsedTime(_step_time, 'PREDB INSTALL : Copy/Update Cluster Configuration in DOM0')
                #
                # PRE-DB Scripts
                #
                _step_time = time.time()
                self.mUpdateStatusOEDA(True, step, step_list, 'Running External PREDB Scripts')
                _rc = self.mRunScript(aType='*',aWhen='pre.db_install')
                self.mLogStepElapsedTime(_step_time, 'PREDB INSTALL : Running External Scripts')

                #
                # PRE-DB ExaCM
                #

                if self.__exacm is True and self.__amos is False :
                    self.mExaCMDBPreChecks( aOptions.jsonconf )
                else:
                    ebLogInfo('[INFO] Skipping NFS ExaCM PreChecks')

                #
                # Read/Update GI and DB images and BPL
                #
                _step_time = time.time()
                self.mUpdateStatusOEDA(True, step, step_list, 'Read/Update GI and DB images and BPL')
                _rc, _upg, _vdct = self.__images_version = self.mCheckImagesVersion()
                self.mLogStepElapsedTime(_step_time, 'PREDB INSTALL : Read/Update GI and DB images and BPL')
                if _rc: return _rc

            if step in db_step_list:
                #
                # Update Current OEDA Step
                #
                self.mUpdateStatusOEDA(True,step,step_list)
                #
                # Apply security patches
                #
                if step == OSTP_APPLY_FIX and self.mCheckConfigOption('skip_afs_step','True'):
                    continue
                if step == OSTP_CREATE_PDB and not self.mIsCdbPdb():
                    continue
                lCmd = "/bin/bash install.sh -cf {0} -s {1} {2}".format(self.__remoteconfig, \
                                                                self.mFetchOedaStep(str(step)), \
                                                                self.mGetOEDAExtraArgs())
                ebLogInfo('Running: ' + lCmd)
                _out = self.mExecuteCmdLog2(lCmd, aCurrDir=_oeda_path)
                _rc  = self.mParseOEDALog(_out)
                self.mUpdateStatusOEDA(_rc,step,step_list)
                if not _rc:
                    ebLogError('*** Fatal Error *** : Aborting current job - please review errors log and try again.')
                    raise ExacloudRuntimeError(0x0118, 0xA, 'OEDA error during installation of starter DB')

            if step == OSTP_POSTDB_INSTALL:
                #
                # HACK: xxx/MR: Temporaty workaround adding missing entry in /etc/oratab - to be removed when fixed in 12.2 DB klone image
                #
                self.mPatchOratab122()
                #
                # Add customer SSH Key in DOMUs
                #
                _step_time = time.time()
                self.mUpdateStatusOEDA(True, step, step_list, 'Patch VM SSH Keys')
                self.mPatchVMSSHKey(aOptions)
                self.mLogStepElapsedTime(_step_time, 'Patch VM SSH Keys')

                _step_time = time.time()
                self.mUpdateStatusOEDA(True, step, step_list, 'Flush ebtables in DOM0')
                self.mSetupEbtablesOnDom0(aMode=False)
                self.mLogStepElapsedTime(_step_time, 'Flush ebtables in DOM0')
                #
                # Run postDB scripts
                #
                _step_time = time.time()
                self.mUpdateStatusOEDA(True, step, step_list, 'Running External POSTDB Scripts')
                _rc, _cmd = self.mRunScript(aType='*',aWhen='post.db_install',aStatusAbort=True)
                self.mLogStepElapsedTime(_step_time, 'Running External POSTDB Scripts (RC:%s)' % (str(_rc)))
                if _rc:
                    self.mCopyOCDELogFile()
                    ebLogError('*** Error ('+str(_rc)+') caught during scripts execution for cmd: '+_cmd)
                    return ebError(0x0501)                                 # ERROR_501 : POST_DB ERROR OCDE EXECUTION FAILED
                #
                # Copy OCDE files in ODEA staging dir
                #
                _step_time = time.time()
                self.mUpdateStatusOEDA(True, step, step_list, 'Copy OCDE Log file')
                self.mCopyOCDELogFile()
                self.mLogStepElapsedTime(_step_time, 'Copy OCDE Log file')

            if step == OSTP_DG_CONFIG:
                #
                # Configure dataguard if dataguard is enabled
                #
                if self.mCheckDBConfigOption(aOptions, "dg_config", "yes"):
                    self.mUpdateStatusOEDA(True, step, step_list, 'Configuring Dataguard for Database')
                    _rc = self.mConfigureDataguard(aOptions)
                    self.mLogStepElapsedTime(_step_time, 'Configuring Dataguard')
                    if not _rc:
                        ebLogError('*** Error In configuring Dataguard, Please check logs')
                        return ebError(0x0502)

            #
            # DB_INSTALL IS COMPLETED !!!
            #
            if step == OSTP_END_INSTALL:

                #
                # Alter cloud_user's password in the cells.
                #
                if self.mCheckConfigOption('exacli_use_db_pwd', 'True'):

                    _step_time = time.time()
                    self.mUpdateStatusOEDA(True, step, step_list, 'Alter user-password')
                    try:
                        self.mUpdateCloudUser(aOptions)
                    except:
                        ebLogError("Error while updating cloud user password")
                    self.mLogStepElapsedTime(_step_time, 'Alter user-password')

                #
                # Create ADBS cloud_user
                #
                self.mCreateAdbsUser(aOptions)

                # Enable Storage Cell remote password change for exacli users
                self.mEnableRemotePwdChange(aOptions)

                #
                # DB install completed.
                #
                ebLogInfo('*** Exacloud Operation Successful : Create Starter DB completed')
                self.mUpdateStatusOEDA(True, step, step_list, 'Create Starter DB Completed')

        return 0

    def mCheckDBConfigOption(self, aOptions, aKey , aValue=None):
        ebLogVerbose("mCheckDBConfigOption: aKey = %s, aValue = %s" % (aKey, aValue))

        _jconf = None
        if aOptions.jsonconf:
            _jconf = aOptions.jsonconf

            if "dbParams" not in _jconf:
                ebLogError('Could not find dbParams in jsonconf')
                return None

            if aKey not in _jconf['dbParams']:
                ebLogError('Could not find {0} in dbParams'.format(aKey))
                return None

            if aValue is not None:
                if _jconf['dbParams'][aKey] == aValue:
                    return True
                else:
                    return False
            else:
                return _jconf['dbParams'][aKey]
        else:
            ebLogError('Could not find jsonconf in aOptions')
            return None

    def mConfigureDataguard(self, aOptions):
        _dbname = self.__dbname
        ebLogVerbose("mConfigureDataguard: Database name = %s" % _dbname)

        _dom0U = self.mReturnDom0DomUPair()
        _elastic_op,_nodelist = self.mCheckNodeListParam()
        if _elastic_op is True:
            _dom0U = self.mCheckNodeList(_nodelist)
        for _, _domu in _dom0U:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domu)
            _cmd_str = "/var/opt/oracle/ocde/assistants/dg/dgcc -dbname " + _dbname
            ebLogInfo("Running cmd on DomU(" + _domu + ")" + _cmd_str)
            _node.mExecuteCmdLog(_cmd_str)
            _rc = _node.mGetCmdExitStatus()
            _node.mDisconnect()
            self.mCopyDataguardLogfiles()
            if _rc:
                return 0
        return 1

    def mExecuteCRSReboot(self, aRequestedOperations=["start", "stop"]) -> None:
        """Restart CRS services in the cluster.

        :returns: nothing.
        :raises ExacloudRuntimeError: if an error occurred.
        """
        if self.mCheckConfigOption("bypass_crs_reboot", "True"):
            ebLogInfo("Skipping CRS reboot")
            return

        domus = [domu for _, domu in self.mReturnDom0DomUPair()]

        try:
            # We just connect to one DomU and trigger CRS restart in all
            # cluster from there.
            with connect_to_host(domus[0], self.mGetCtx()) as node:
                # get GI home
                grep = node_cmd_abs_path_check(node, "grep")
                cmd = f"{grep} oracle_home /var/opt/oracle/creg/grid/grid.ini"
                ret = node_exec_cmd_check(node, cmd)
                gi_home = ret.stdout.splitlines()[0].split('=')[1].strip()

                crsctl = os.path.join(gi_home, "bin/crsctl")

                # Restart CRS in all cluster.  If the following crsctl commands
                # fail we log stdout/stderr (this is important for diag).
                ebLogInfo("Restarting CRS services in the cluster")
                if "stop" in aRequestedOperations:
                    node_exec_cmd_check(
                        node, f"{crsctl} stop cluster -all",
                        log_stdout_on_error=True)
                if "start" in aRequestedOperations:
                    node_exec_cmd_check(
                        node, f"{crsctl} start cluster -all",
                        log_stdout_on_error=True)

            self.mCheckCrsIsUp(domus[0], domus)
            self.mCheckAsmIsUp(domus[0], domus)
        except Exception as exp:
            msg = f"Failed to restart CRS services in the cluster: {exp}"
            ebLogError(msg)
            raise ExacloudRuntimeError(aErrorMsg=msg) from exp

    def mExecuteDeleteStarterDB(self, aCmd, aOptions, _oeda_path, step_list, db_step_list):
        ebLogVerbose("mExecuteDeleteStarterDB: aCmd = %s, oeda path = %s" % (aCmd, _oeda_path))

        #
        # DB DELETE MAIN LOOP
        #
        for step in step_list:

            if step == OSTP_PREDB_DELETE:
                #
                # Update XML configuration (obsolete step ?
                #
                _step_time = time.time()
                self.mUpdateStatusOEDA(True, step, step_list, 'Copy/Update Cluster Configuration in DOM0')
                self.mCopyFileToClusterConfiguration(self.__configPath, 'db_delete_cluster.xml')
                self.mLogStepElapsedTime(_step_time, 'PREDB DELETE : Copy/Update Cluster Configuration in DOM0')
                #
                # Run External Scripts
                #
                _step_time = time.time()
                self.mUpdateStatusOEDA(True, step, step_list, 'Running External PREDB Scripts')
                self.mRunScript(aType='*',aWhen='pre.db_delete')
                self.mLogStepElapsedTime(_step_time, 'PREDB DELETE : Running External Scripts')

            if step == OSTP_INSTALL_DB:
                #
                # Special Cases
                #
                self.mBug23715436()

            if step in db_step_list:
                #
                # Update Current OEDA Step
                #
                if self.IsZdlraProv():
                    errno = self.mUpdateOEDAProperties(aOptions)
                    if errno:
                        return errno

                self.mUpdateStatusOEDA(True,step,step_list)
                if self.__debug:
                    ebLogInfo('*** STEP: %s %s %s' % (str(step), str(OSTP_CREATE_PDB), str(self.mIsCdbPdb())))
                if step == OSTP_CREATE_PDB and not self.mIsCdbPdb():
                    continue
                if step == OSTP_APPLY_FIX and self.mCheckConfigOption('skip_afs_step','True'):
                    continue
                lCmd = "/bin/bash install.sh -cf {0} -u {1} {2}".format(self.__remoteconfig, \
                                                                self.mFetchOedaStep(str(step)), \
                                                                self.mGetOEDAExtraArgs())
                ebLogInfo('Running: ' + lCmd)
                _out = self.mExecuteCmdLog2(lCmd, aCurrDir=_oeda_path)
                _rc = self.mParseOEDALog(_out)
                self.mUpdateStatusOEDA(_rc,step,step_list)
                # Note: Errors during undo are not critical
                if not _rc:
                    ebLogWarn('*** Warning *** : Last step did not complete successfully - Continue with next step in undo mode')

            if step == OSTP_POSTDB_DELETE:
                _step_time = time.time()
                self.mUpdateStatusOEDA(True, step, step_list, 'Running External POSTDB Scripts')
                self.mRunScript(aType='*',aWhen='post.db_delete')
                _rc = self.mRemoveGridListener()
                if not _rc:
                    ebLogWarn('*** Warning *** Error in removing grid listener file- ignoring in undo mode')

                self.mRollbackEncryption(aOptions.jsonconf)
                self.mATPUnlockListeners() #ATP will be detected from inside the function
                self.mLogStepElapsedTime(_step_time, 'POSTDB DELETE: Running External Scripts')

            if step == OSTP_END_INSTALL:

                ebLogInfo('*** Exacloud Operation Successful : Delete Starter DB completed')
                self.mUpdateStatusOEDA(True, step, step_list, 'Delete Starter DB Completed')

        if self.IsZdlraProv():
            self.__ZDLRA.mUpdateHugePages(aOptions)

        return 0

    def mRemoveGridListener(self):
        # removes the listener.ora file from grid_home, it will be regenerated by crs.
        ebLogVerbose("mRemoveGridListener: removes the listener.ora file from grid_home.")

        _rc = 0
        for _, _domu in self.mReturnDom0DomUPair():
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domu)
            _cmd_str = "cat /etc/oratab | grep '^+.*' "
            _, _out, _ = _node.mExecuteCmd(_cmd_str)
            _out = _out.readlines()
            if (_out is not None and len(_out) != 0):
                _grid_home = _out[0].split(":")[1].strip()
            else :
                _node.mDisconnect()
                return 0
            _listener_file_loc = _grid_home+"/network/admin/listener.ora"
            _listener_backup_loc = _listener_file_loc + ".delstarterbak." + time.strftime("%Y%m%d-%H%M%S")
            _srvctl_loc =  _grid_home + "/bin/srvctl"
            _cmd_str = "mv  " + _listener_file_loc +" " +_listener_backup_loc + " ; " + _srvctl_loc + " stop listener ;" +  _srvctl_loc + " start listener ;"
            _node.mExecuteCmdLog(_cmd_str)
            _status = _node.mGetCmdExitStatus()
            _node.mDisconnect()
            if self.__debug:
                ebLogInfo('*** status for cmd : ' + _cmd_str + ' : %s' % (str(_rc)))
            if _status :
                _rc = 1
        return not _rc

    def mRegisterVgComponents(self):
        #1. Register VM Operations
        _vm_obj = ebVgLifeCycle()
        self.__CompRegistry.mRegisterComponent("vm_operations", _vm_obj)

        #2. Register CPU Operations

    def mExecuteStep(self, aCmd, aOptions=None, aOedaPath=None):

        ebLogVerbose("mExecuteStep: aCmd = %s " % aCmd)

        # TODO: Check if we are running in Agent mode. This is should be asserted.
        if not aOptions:
            aOptions = self.mGetArgsOptions()

        _coptions = get_gcontext().mGetConfigOptions()

        # Check if OEDA is installed/available
        if not self.__oeda_path:
            if aOedaPath:
                _oeda_path = aOedaPath
            else:
                _oeda_path = self.__ctx.mGetOEDAPath()
            if not self.__node.mFileExists(_oeda_path+'/install.sh'):
                ebLogError('Could not find OEDA one command installer in: '+_oeda_path)
                # TODO: Raise an exception
                return ebError(0x0100)                                              # ERROR_100 : OEDA INSTALL NOT FOUND
            self.__oeda_path = _oeda_path
        else:
            _oeda_path = self.__oeda_path

        if aCmd == 'validate_elastic_shapes':
            _pchecks = ebCluPreChecks(self)
            _rc = _pchecks.mFetchHardwareAlerts(aOptions, aStep="ELASTIC_SHAPES_VALIDATION")
            return _rc
        if aCmd == 'xsvault':
            return self.mGetCommandHandler().mHandlerXsVaultOperation()
        if aCmd == 'xsput':
            return self.mGetCommandHandler().mHandlerXsPut()
        if aCmd == 'xsget':
            return self.mGetCommandHandler().mHandlerXsGet()
        if aCmd == 'infra_vm_states':
            return self.mHandlerVmsStatesOperation()

    def mExecuteOEDAStep(self, aCmd, aOptions=None, aOedaPath=None):

        if ebCluCmdCheckOptions(aCmd, ['instant_commands']):
            _rc = self.mExecFunc(aCmd, aOptions)
            return _rc

        global gOedaSTable

        ebLogVerbose("mExecuteOEDAStep: aCmd = %s " % aCmd)

        # TODO: Check if we are running in Agent mode. This is should be asserted.
        if not aOptions:
            aOptions = self.mGetArgsOptions()

        _coptions = get_gcontext().mGetConfigOptions()

        # Check if OEDA is installed/available
        if not self.__oeda_path:
            if aOedaPath:
                _oeda_path = aOedaPath
            else:
                _oeda_path = self.__ctx.mGetOEDAPath()
            if not self.__node.mFileExists(_oeda_path+'/install.sh'):
                ebLogError('Could not find OEDA one command installer in: '+_oeda_path)
                # TODO: Raise an exception
                return ebError(0x0100)                                              # ERROR_100 : OEDA INSTALL NOT FOUND
            self.__oeda_path = _oeda_path
        else:
            _oeda_path = self.__oeda_path

        _prop_file = f"{self.__oeda_path}/properties/es.properties"
        if not self.__node.mFileExists(_prop_file):
            ebLogError('Oeda file es.properties not present in {}'.format(_prop_file))
            return ebError(0x0100)

        #
        # Check version of OEDA
        #
        _cmd_str = self.__oeda_path+'/install.sh -h | grep Version'
        _cmd_str1 = '/bin/bash install.sh -h'
        _cmd_str2 = '/bin/grep Version'

        _retryCount = 5
        _sleepTime = 30 #seconds
        _success = False
        _oeda_version = None
        _oeda_long_version = None

        while _retryCount > 0 and not _success:
            _, _cmd_out1, _ = self.mExecuteCmd(_cmd_str1, aCurrDir=self.__oeda_path)
            _, _out, _ = self.mExecuteCmd(_cmd_str2, aCurrDir=self.__oeda_path, aStdIn=_cmd_out1)
            if self.__cmd_status == 0 and _out:
                _out = _out.readlines()
                if _out:
                    _oeda_long_version = _out[0]
                    _success = True
                    break
            _retryCount = _retryCount - 1
            time.sleep(_sleepTime)
        if not _success :
            ebLogError('*** Unable to fetch the oeda version using installer command %s : %s' % (self.__node.mGetHostname(), _cmd_str))
            return ebError(0x0101)                                        # ERROR_101 : COULD NOT EXECUTE OEDA INSTALLER

        try:
            if '.' in _oeda_long_version:
                _oeda_version = '.'.join(_oeda_long_version.split('.')[0:2])[-6:]
            else:
                _oeda_version = _oeda_long_version.split(':')[1].strip()  # " Version: YYMMDD" format
        except:
            ebLogError('*** Could not fetch OEDA one command version')
            return ebError(0x0102)                                             # ERROR_102 : COULD NOT FETCH OEDA VERSION

        try:
            _c_major = int(_coptions['oeda_version'].split('.')[0])
            _c_minor = int(_coptions['oeda_version'].split('.')[1])
            if '.' in _oeda_version:
                _i_major = int(_oeda_version.split('.')[0])
                _i_minor = int(_oeda_version.split('.')[1])
            else:
                _dtime   = datetime.datetime.strptime(_oeda_version,'%y%m%d')
                _i_major = int(_dtime.strftime('%y')) # Extract YY
                _i_minor = int(_dtime.strftime('%j')) # Convert MMDD to DOY
            if _i_major < _c_major or (_i_major == _c_major and _i_minor < _c_minor):
                ebLogWarn('*** Invalid OEDA version installed. Found '+_oeda_version+' but expecting at least '+_coptions['oeda_version']+' or higher.')
                #return ebError(0x0103)                                       # ERROR_103 : INVALID OEDA VERSION DETECTED
        except:
            ebLogError('*** Error while fetching OEDA version config parameter check exabox.conf')
            return ebError(0x0104)                                   # ERROR_104 : MISSING OEDA VERSION IN EXACLOUD CONF

        if not self.mCheckConfigOption('disable_xml_check','True') and not self.__skip_xml_checks:
            ebLogInfo('*** OEDA XML Version: %s' % (self.__header.mGetRawOEDAVersion()))
            _xml_version = self.__header.mGetHeaderOEDAVersion()
            if _oeda_version != _xml_version and self.__verbose:
                ebLogInfo('*** OEDA version in Cluster XML: ' + _xml_version + ' OEDA version installed: ' + _oeda_version)
                #return ebError(0x0105)

        get_gcontext().mSetOEDAVersion(_oeda_long_version.split(' ')[-1])
        get_gcontext().mSetOEDAHostname(self.__node.mGetHostname())
        ebLogInfo('*** OEDA Version installed: '+_oeda_version)
        self.mSetOedaVersYYMMDD(_oeda_version)

        if aCmd == 'version':
            return 0

        if not self.__remoteconfig:
            ebLogError('::mExecuteOEDAStep XML Cluster configuration not available')
            return ebError(0x0200)

        # Check if OEDA translation table is available
        if self.__oeda_stable is None and not self.mIsClusterLessXML():
            if self.mBuildOedaStepsTable(_oeda_path) == -1:
                return ebError(0x0300)

        _rc = self.mExecFunc(aCmd, aOptions)
        # TODO:Many handlers inside mExecFunc do not return anything. 
        # Untill we fix all those handlers, returning 0 as default.
        if _rc is None:
            return 0
        return _rc


    def mExecFunc(self, aCmd, aOptions):

        # IMPORTANT NOTE:
        #
        # endpoints to be added in exacloud SHOULD have it's main code included
        # in clucommandhandler.py, so PLEASE use it when:
        # CREATING a new endpoint, PLEASE put it there.
        # MODIFYING an existing endpoint, PLEASE try to move/refactor in there.
        handler = self.mGetCommandHandler()

        # NOTE: For clarity, please add new endpoints keeping the 
        # alphabetical order.
        mFuncDict ={
                    'add_vm_extra_size'             : handler.mHandlerAddVmExtraSize,
                    'admin_switch_connect'          : handler.mHandlerAdminSwitchConnectEndpoint,
                    'asmsnmp_dbsnmp_passwd'         : handler.mHandlerSnmpPasswords,
                    'adbs_insert_key'               : handler.mHandlerADBSInsertKey,
                    'addebtrule'                    : self.mHandlerAddEbtablesRuleOnDom0,
                    'add_atpbackup_routes'          : handler.mHandlerATPBackupRoutes,
                    'arp_operation'                 : self.mHandlerARPOper,
                    'block_operations'              : handler.mHandlerUpdateBlockState,
                    'bonding_operation'             : self.mHandlerBondingOper,
                    'cavium_collect_diag'           : handler.mHandlerCaviumColletDiag,
                    'cavium_reset'                  : handler.mHandlerCaviumReset,
                    'create_diag'                   : handler.mHandlerGenIncidentFile,
                    'checkcluster'                  : handler.mHandlerCheckCluster,
                    'collect_log'                   : handler.mHandlerCollectLog,
                    'chk_vm_timedrift'              : self.mHandlerCheckTimeDrift,
                    'checkconn'                     : handler.mHandlerCheckConnection,
                    'checkconfig'                   : handler.mHandlerCheckConfig,
                    'coredump'                      : self.mHandlerCoreDump,
                    'cell_ibinfo'                   : handler.mHandlerGetCellIBInfo,
                    'cluster_details'               : handler.mHandlerClusterDetails,
                    'collect_vmcore_logs'           : handler.mHandlerGetVMCoreLogs,
                    'cleanup_exawatcher_log'        : cleanupExaWatcherLogs,
                    'cpu_info'                      : self.mHandlerClusterCPUInfo,
                    'clusterclujson'                : self.mHandlerSaveClusterDomUList,
                    'createservice'                 : self.mHandlerCreateService,
                    'cells_reset'                   : self.mHandlerCellsReset,
                    'deleteservice'                 : self.mHandlerDeleteService,
                    'db_install'                    : self.mHandlerInstallDB,
                    'db_delete'                     : self.mHandlerDeleteDB,
                    'dom0_atpiptables_change'       : handler.mHandlerATPIPTables,
                    'dom0_set_nat_iptables'         : self.mHandlerSetupNATIptablesOnDom0,
                    'dg_fresh_setup'                : self.mHandlerSetupFreshDG,
                    'dg_repeat_setup'               : self.mHandlerSetupDGRepeat,
                    'delete_infra_node_exacc'       : self.mHandlerDeleteInfraNodeSSHAndDNS,
                    'deletekeys'                    : self.mHandlerDeleteKeys,
                    'deletekeysondisk'              : self.mHandlerDeleteOndiskKeys,
                    'desecurepwd'                   : self.mHandlerDesecureDOMUPwd,
                    'dom0_push_certificate'         : self.mHandlerDom0CopyCert,
                    'dom0_push_gpg_keys'            : self.mHandlerCopyCSSHubKeys,
                    'dom0_storage_info'             : self.mHandlerGetEXAVMStorage,
                    'dataguard'                     : self.mHandlerDataguardOper,
                    'disableebt'                    : self.mHandlerDisableEbtablesOnDom0,
                    'delebtrule'                    : self.mHandlerDeleteEbtablesRuleOnDom0,
                    'diskgroup'                     : self.mHandlerDiskgroupOper,
                    'env_info'                      : handler.mHandlerGetEnvInfo,
                    'elastic_info'                  : handler.mHandlerElasticInfo,
                    'enableebt'                     : self.mHandlerEnableEbtablesOnDom0,
                    'enablecos'                     : self.mHandlerEnableCOS,
                    'exacc_infra_patch_list'        : self.mHandlerListEXACCPatchPayloads,
                    'exacc_patch_list_metadata'     : handler.mHandlerEXACCInfraPatchPayloadList,
                    'exacompute_patch_nodes'        : handler.mHandlerExaComputePatch,
                    'exascale_auto_file_encryption' : handler.mHandlerXsEnableAutoFileEncryption,
                    'exportkeys'                    : self.mHandlerExportKeys,
                    'em_cluster_details'            : self.mHandlerEMClusterDetails,
                    'em_db_details'                 : self.mHandlerEMDBDetails,
                    'elastic_cell_update'           : self.mHandlerElasticCellUpdate,
                    'elastic_cell_info'             : self.mHandlerElasticCellInfo,
                    'end_install'                   : self.mHandlerRemoveDomUsKeys,
                    'reset_cells_vlan'              : self.mHandlerResetVlan,
                    'exakms_migrate'                : self.mHandlerExaKmsMigrate,
                    'exassh'                        : self.mHandlerRefreshExassh,
                    'fetchkeys'                     : handler.mHandlerFetchKeys,
                    'force_oedadelete'              : self.mHandlerOEDADelete,
                    'fetch_exawatcher_logs'         : self.mHandlerExawatcherLogs,
                    'get_cluster_details_exacc'     : self.mHandlerGetClusterDetailsExacc,
                    'get_css_misscount'             : self.mHandlerSetCSSMisscount,
                    'get_exawatcher_log'            : self.mHandlerGetExawatcherLogs,
                    'generate_switches_keys'        : self.mHandlerGenerateSwitchesKeys,
                    'get_dom0_existing_guests_size' : self.mHandlerGetDom0ExistingGuestsSize,
                    'hardware_alerts'               : self.mHandlerGetHWAlerts,
                    'host_state'                    : self.mHandlerHostState,
                    'healthcheckpostprov'           : handler.mHandlerHealthCheckPostProv,
                    'imagebase_post_validation'     : self.mHandlerImageBasePostValidation,
                    'importkeys'                    : self.mHandlerImportKeys,
                    'iorm'                          : self.mHandlerIORMOper,
                    'info'                          : handler.mHandlerInfo,
                    'injectkeys'                    : self.mHandlerInjectExistingKeys,
                    'imageinfo'                     : self.mHandlerGetImageInfo,
                    'jumbo_operation'               : handler.mHandlerJumboOper,
                    'lockdown'                      : self.mHandlerSetupLockdown,
                    'listkeys'                      : self.mHandlerListOEDASSHKeys,
                    'lock_dbmusers'                 : self.mHandlerLockDBMUsers,
                    'lock_cellusers'                : self.mHandlerLockCellUsers,
                    'list_exawatcher_log'           : self.mHandlerListExawatcherLogs,
                    'luks'                          : self.mHandlerLuksOperation,
                    'monitor_bonding'               : self.mHandlerMonitorBonding,
                    'monitor_cluster'               : self.mHandlerMonitorCluster,
                    'mount_volume'                  : handler.mHandlerMountVolume,
                    'network_update'                : self.mHandlerNetworkUpdate,
                    'ociexacc_migration'            : self.mHandlerEXACCMigration,
                    'ociexacc_cpssetup'             : self.mHandlerCPSSetup,
                    'open_access_control'           : self.mHandlerEnableAccessControlIlom,
                    'opctl_cmd'                     : handler.mHandlerOpctlCmd,
                    'op_cleanup'                    : self.mHandlerOperationCleanup,         
                    'oedaaddkey'                    : handler.mHandlerAddOEDAKey,
                    'oedaaddkey_host'               : handler.mHandlerAddOEDAKeyByHost, 
                    'postvm'                        : self.mHandlerMonitorCluster,
                    'partition'                     : self.mHandlerPartitionOperation,
                    'prepare_compute'               : handler.mHandlerPrepareCompute,
                    'preprov_backup'                : self.mHandlerPreProvBackup,
                    'prechecks'                     : self.mHandlerPreChecks,
                    'patch_cluster_xml'             : self.mHandlerPatchClusterInfo, 
                    'reset_locks'                   : self.mHandlerResetRemoteLocks,
                    'resize_exavmimages'            : self.mHandlerResizeExaVMImages,
                    'rack_info'                     : self.mHandlerClusterXmlInfo,
                    'run_compliance_tool'           : handler.mHandlerRunCompTool,
                    'resetenv'                      : handler.mHandlerResetEnv,
                    'restart_remoteec'              : self.mHandlerRestartRemoteEc,
                    'rollback_bonding_migration'    : self.mHandlerRollbackBondingMigration,
                    'rotatekeys'                    : self.mHandlerRotateKeys,
                    'resize'                        : self.mHandlerResizeVMCpuCount,
                    'reset_network_mapping'         : self.mHandlerResetDom0NetworkMapping,
                    'reset_all_dom0'                : self.mHandlerResetDom0ToFreshImage,
                    'run_script'                    : self.mHandlerRunScript,
                    'sparseclone'                   : self.mHandlerCloneSparse,
                    'scheduler_operations'          : self.mHandlerScheduleOperations,
                    'secure_vms'                    : self.mHandlerRemoveDomUsKeys,
                    'set_css_misscount'             : self.mHandlerGetCSSMisscount,
                    'setupebt'                      : self.mHandlerSetupEbtablesOnDom0,
                    'spine_switch_connect'          : self.mHandlerRoceSpineSwitchesConnect,
                    'status'                        : self.mHandlerGetStatus,
                    'stop_start_host_via_ilom'      : self.mHandlerStopStartHostViaIlom,
                    'switches_reset'                : self.mHandlerSwitchesReset,
                    'storage_resize'                : self.mHandlerResizeStorage,
                    'sim_install'                   : self.mHandlerSimInstall,
                    'securepwd'                     : self.mHandlerSecureDOMUPwd,
                    'userconfig'                    : self.mHandlerConfigureUser, 
                    'unlockall'                     : self.mHandlerUnlockAll,
                    'unlock_using_ilom'             : self.mHandlerUnlockDeviceUsingIlom,
                    'unlock_dbmusers'               : self.mHandlerUnLockDBMUsers,
                    'unlock_cellusers'              : self.mHandlerUnLockCellUsers,
                    'unmount_volume'                : handler.mHandlerUnmountVolume,
                    'unsetebt'                      : self.mHandlerUnSetupEbtablesOnDom0,
                    'update_ntp_dns'                : self.mHandlerUpdateNtpDns,
                    'vm_tmpkey_op'                  : handler.mHandlerVmTmpKeyOp,
                    'vm_cmd'                        : self.mHandlerVmCmd,
                    'validate_cell'                 : handler.mHandlerValidateCell,
                    'validate_exaversion'           : self.mHandlerValidateExaversion,
                    'validate_volumes'              : handler.mHandlerValidateVolumes,
                    'vmconsole'                     : self.mHandlerVMConsole,
                    'vmgi_reshape'                  : self.mHandlerVMGIReshape,
                    'vmbackup-oss'                  : self.mHandlerVMBackupOSS,
                    'vmbackup'                      : self.mHandlerVMBackup,
                    'vmgi_preprov'                  : self.mHandlerVMGIPreProv,
                    'vmgi_reconfig'                 : self.mHandlerVMGIReconfig,
                    'vmgi_rollback'                 : self.mHandlerVMGIRollback,
                    'vmgi_infradelete'              : self.mHandlerVMGIInfraDelete,
                    'vmgi_install'                  : self.mHandlerVMGIInstall,
                    'vmgi_delete'                   : self.mHandlerVMGIDelete,
                    'vmgi_deconfig'                 : self.mHandlerVMGIDeconfig,
                    'vm_install'                    : self.mHandlerVMInstall,
                    'vm_delete'                     : self.mHandlerVMDelete,
                    'node_details'                  : self.mHandlerNodeDetails,
                    'dom0_details'                  : self.mHandlerDom0Details,
                    'node_images'                   : self.mHandlerNodeImages,
                    'reshape_precheck'              : self.mHandlerReshapePrecheck,
                    'validate_compute'              : handler.mHandlerPostCompute,
                    'set_disk_scrubbing_window'     : self.mHandlerSetScrubbingWindow,
                    'configure_host_access_control' : handler.mHandlerConfigureHostAcess,
                    'configure_vm_console'          : handler.mHandleConfigureVMConsole,
                    'configdns'                     : handler.mHandlerConfigDns,
                    'network_reconfig'              : handler.mHandlerNetworkReconfig,
                    'revert_network_reconfig'       : handler.mHandlerRevertNetworkReconfig,
                    'node_subset_precheck'          : self.mHandlerNodeSubsetPrecheck,
                    'modify_network_bonding'        : handler.mHandlerNetworkBondingModification,
                    'validate_network_bonding'      : handler.mHandlerNetworkBondingValidation,
                    'cell_storage_celldisks'        : handler.mHandlerCelldiskStorage,
                    'reclaim_mountpoint_space'      : handler.mHandlerReclaimMountpointSpace,
                    'check_roce_configured_dom0'    : handler.mHandlerCheckRoceConfiguredDom0,
                    'configure_dom0_roce'           : handler.mHandlerConfigureDom0Roce,
                    'xsvault'                       : handler.mHandlerXsVaultOperation,
                    'xsconfig'                      : self.mHandlerXsConfigOperation,
                    'xs_vol_attach'                 : self.mHandlerVolAttach,
                    'xs_vol_detach'                 : self.mHandlerVolDetach,
                    'xs_vol_resize'                 : self.mHandlerVolResize,
                    'faultinjection'                : self.mHandlerFaultInjection,
                    'exascale_cell_update'          : handler.mHandlerXsUpdateExascale,
                    'secure_cell_erase'             : self.mHandlerSecureCellErase,
                    'exascale_acfs_cmd'             : handler.mHandlerXsAcfsOperations,
                    'xsget'                         : handler.mHandlerXsGet,
                    'xsput'                         : handler.mHandlerXsPut,
                    'get_cluster_details_consistency': self.mHandlerGetClusterDetailsforConsistency,
                    'exascale_nodes_conf'           : handler.mHandlerXsUpdateNodesConf
                   }


        if aCmd in mFuncDict.keys():
            ebLogInfo(f'Running the cmd - {aCmd}')
            return mFuncDict[aCmd]()

        elif ebCluCmdCheckOptions(aCmd, ['dbaas_tool']):
            return self.mHandlerDBaaSTool()

        elif ebCluCmdCheckOptions(aCmd, ['create_oeda_ssh_keys']):
            return self.mCreateOEDASSHKeys(aOptions)

        elif ebCluCmdCheckOptions(aCmd, ['validate_keys']):
            return self.mValidateKeys(aOptions)

        elif ebCluCmdCheckOptions(aCmd, ['dbaas_api_operation']):
            ebLogInfo('Performing DBaaS API operation')
            _dbaasobj = ebCluDbaas(self, aOptions)
            return _dbaasobj.mClusterDbaas(aOptions, aCmd)

        elif ebCluCmdCheckOptions(aCmd, ['patch']):
            return self.mHandlerPatching()

        elif ebCluCmdCheckOptions(aCmd, ['set_shared_env']):
            self.mSetSharedEnvironment(True)

        elif ebCluCmdCheckOptions(aCmd, ['set_non_shared_env']):
            self.mSetSharedEnvironment(False)

        else:
            ebLogInfo("Cmd not supported")
           
    def mHandlerRoceSpineSwitchesConnect(self):
        try:
            self.mReturnandConnectRoceSpineSwitches()
            return 0
        except:
            return 1

    def mReturnandConnectRoceSpineSwitches(self):
        '''
         This method sets up key based access between
         Exacloud and Roce Spine switch and return the Spine
         Switch list to be consumed by Infra Patching and other
         Exacloud operations.

         Spine switch patching will be performed only in
         case of PatchSwitchType set to "rocespine" or "spine"
         or "all"
        '''
        _configured_spine_switch_list = []
        if not self.mIsOciEXACC():
            ebLogError(f"This feature is only supported for EXACC Currently. Exiting!")
            return
        _sshsetup = ebCluSshSetup(self)
        _, _configured_spine_switch_list = _sshsetup.mSetCiscoSwitchSSHPasswordless(aGenerateSpineSwitchKeys=True, aGenerateAdminSwitchKeys=False)
        return list(set(_configured_spine_switch_list))
                
    def mHandlerReshapePrecheck(self):
        aOptions = self.mGetArgsOptions()

        _dom0s, _domUs, _cells, _ = self.mReturnAllClusterHosts()
        cluSshTest = ebCluServerSshConnectionCheck(self)
        _rc = cluSshTest.mServerSshConnectionCheck(aOptions, _domUs, _dom0s, _cells)
        if _rc != 0:
            return _rc
            
        cluReshapePrecheck = ebCluReshapePrecheck(self)
        _rc = cluReshapePrecheck.mRunReshapePrecheck(aOptions)
        return _rc
    
    def mHandlerStopStartHostViaIlom(self):
        """
        Entry point for host power operations via ILOM: start, stop, and lowpowermode.

        Payload sample:
        {
            "operation": "start | stop | lowpowermode",
            "parallel_process": true/false,  # optional for start/stop
            "host_ilom_pair": {
                "iad103716exdd013.iad103716exd.adminiad1.oraclevcn.com" : "iad103716exdd013lo.iad103716exd.adminiad1.oraclevcn.com"
                "iad103712exdcl07.iad103712exd.adminiad1.oraclevcn.com": "iad103712exdcl07lo.iad103712exd.adminiad1.oraclevcn.com"
            },
            # Optional for lowpowermode:
            "lowpoweroperation": "schedule | schedule_add | schedule_remove | schedule_clear | get_all",
            "lowPowerModeSchedule": [  # optional unless schedule intent
                {
                    "startTimestamp": "2025-01-03T18:00:00-07:00",
                    "durationMinutes": 720,
                    "frequency": "daily"
                }
            ],
            "lowPowerModeUntil": "..." | "" | "NEVER"
        }
        """
        aOptions = self.mGetArgsOptions()
        _reqobj = self.mGetRequestObj()
        _stopStartViaIlom = ebCluStartStopHostFromIlom(self)

        try:
            _json = aOptions.jsonconf
            if not isinstance(_json, dict):
                raise ExacloudRuntimeError(0x0208, 0xA, "Missing or invalid jsonconf")

            _operation = _json.get("operation")
            if not _operation:
                raise ExacloudRuntimeError(0x0208, 0xA, "Missing 'operation' in payload")

            if _operation == "lowpowermode":
                _data_d = _stopStartViaIlom.mHandleLowPowerMode(aOptions)
            elif _operation in ("start", "stop"):
                _data_d = _stopStartViaIlom.mStopStartHostViaIlom(aOptions)
            else:
                raise ExacloudRuntimeError(0x0208, 0xA, f"Unsupported operation: {_operation}")

            if _reqobj is not None:
                _reqobj.mSetData(json.dumps(_data_d, sort_keys=True))
                _db = ebGetDefaultDB()
                _db.mUpdateRequest(_reqobj)
            else:
                ebLogTrace(json.dumps(_data_d, sort_keys=True, indent=4))
            ebLogInfo("Power operation has been completed successfully")
        except Exception as e:
            ebLogError(f"Power operation failed: {str(e)}")
            raise ExacloudRuntimeError(0x0208, 0xA, f"Power operation failed: {str(e)}")
 
    def mHandlerNodeSubsetPrecheck(self):
        aOptions = self.mGetArgsOptions()

        _, _domUs, _, _ = self.mReturnAllClusterHosts()
        cluSshTest = ebCluServerSshConnectionCheck(self)
        _rc = cluSshTest.mServerSshConnectionCheck(aOptions, _domUs)

        if _rc != 0:
            return _rc
        
        cluNodeSubsetPrecheck = ebCluNodeSubsetPrecheck(self)
        _rc = cluNodeSubsetPrecheck.mRunNodeSubsetPrecheck(aOptions)
        
        return _rc

    def mHandlerDom0Details(self):
        aOptions = self.mGetArgsOptions()
        _data_d = {}
        _data_d["dom0s"] = []

        def _mGetRebootTime(aDom0, _reboot_time):
            _dom0 = aDom0
            _reboot_time[_dom0] = self.mGetLastRebootTime(_dom0)


        _plist = ProcessManager()
        _reboot_time = _plist.mGetManager().dict()

        _xml_dom0List = [_dom0 for _dom0, _ in self.mReturnDom0DomUPair()]
        _dom0_list = []
        if aOptions is not None and aOptions.jsonconf is not None and 'dom0s' in list(aOptions.jsonconf.keys()):
            _jc_dom0list = aOptions.jsonconf.get('dom0s')
            _dom0_list = copy.deepcopy(_jc_dom0list)
            for _dom0 in _jc_dom0list:
                #lets ensure this list is valid
                if _dom0 not in _xml_dom0List:
                    ebLogWarn('node {} not found in xml. removing it'.format(_dom0))
                    _dom0_list.remove(_dom0)

        #If dom0 is not provided, then lets return back data for all dom0s!
        if not _dom0_list:
            _dom0_list = _xml_dom0List

        # Parallelize execution on dom0s
        #_dpairs = self.mReturnDom0DomUPair()
        for _dom0 in _dom0_list:
            _p = ProcessStructure(_mGetRebootTime, [_dom0, _reboot_time], _dom0)
            _p.mSetMaxExecutionTime(30*60) # 30 minutes
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()

        for _dom0 in _dom0_list:
            if _dom0 not in _reboot_time:
                ebLogError('***Unable to fetch lastreboot time for {}'.format(_dom0))
                continue
            _dom0_d= {}
            ebLogInfo('***lastreboot time for {0}: {1}'.format(_dom0, _reboot_time[_dom0]))
            _dom0_d['hostname'] = _dom0
            _dom0_d['lastreboot'] = _reboot_time[_dom0]
            _data_d["dom0s"].append(_dom0_d)

        _reqobj = self.mGetRequestObj()
        if _reqobj is not None:
            _reqobj.mSetData(json.dumps(_data_d, sort_keys=True))
            _db = ebGetDefaultDB()
            _db.mUpdateRequest(_reqobj)
        else:
            if self.__debug:
                ebLogInfo(json.dumps(_data_d, sort_keys=True, indent=4))

        ebLogInfo("Dom0 details complete")

    def mHandlerNodeDetails(self):
        _data_d = {}
        _data_d["dom0s"] = []
        _data_d["cells"] = []

        _dom0s, _domUs, _cells, _switches = self.mReturnAllClusterHosts()

        for _cell in _cells:
            _imagever = self.mGetImageVersion(_cell)
            _cell_d= {}
            _cell_d['hostname'] = _cell
            _cell_d['imageversion']= _imagever
            _data_d["cells"].append(_cell_d)

        for _dom0 in _dom0s:
            _imagever = self.mGetImageVersion(_dom0)
            _lastreboottime = self.mGetLastRebootTime(_dom0)
            _dom0_d= {}
            _dom0_d['hostname'] = _dom0
            _dom0_d['imageversion']= _imagever
            _dom0_d['lastreboot'] = _lastreboottime
            _data_d["dom0s"].append(_dom0_d)

        if not self.mIsKVM():
            _data_d["ibswitches"] = []
            for _switch in _switches:
                _frmwrver = self.mGetSwitchFirmwareVersion(_switch)
                _switch_d= {}
                _switch_d['hostname'] = _switch
                _switch_d['firmwareversion']= _frmwrver
                _data_d["ibswitches"].append(_switch_d)

        _reqobj = self.mGetRequestObj()
        if _reqobj is not None:
            _reqobj.mSetData(json.dumps(_data_d, sort_keys=True))
            _db = ebGetDefaultDB()
            _db.mUpdateRequest(_reqobj)
        else:
            if self.__debug:
                ebLogInfo(json.dumps(_data_d, sort_keys=True, indent=4))

        ebLogInfo("Node details complete")

    def mHandlerNodeImages(self):
        _targetType = None
        _dom0s = None
        _cells = None
        _data_d = {}
        _data_d["dom0s"] = []
        _data_d["cells"] = []
        aOptions = self.mGetArgsOptions()
        _options = aOptions
        # Parse the json and read the attributes
        _inputjson = _options.jsonconf
        if _inputjson:
            if 'targetType' in _inputjson.keys() and _inputjson['targetType'].strip():
                _targetType = _inputjson['targetType'].strip()
                ebLogInfo(f"Fetching image versions for targetType {_targetType}")
            if 'rackName' in _inputjson.keys() and _inputjson['rackName'].strip():
                _rackName = _inputjson['rackName'].strip()
                ebLogInfo(f"Fetching image versions for rackName {_rackName}")

        if _targetType and _targetType == "dom0":
            _dom0s, _, _, _ = self.mReturnAllClusterHosts()
        elif _targetType and _targetType == "cell":
             _, _, _cells, _ = self.mReturnAllClusterHosts()
        else:
            _dom0s, _, _cells, _ = self.mReturnAllClusterHosts()

        if _cells:
            for _cell in _cells:
                _imagever = self.mGetImageVersion(_cell)
                _cell_d= {}
                _cell_d['hostname'] = _cell
                _cell_d['imageversion']= _imagever
                _data_d["cells"].append(_cell_d)
                ebLogInfo(f"Fetched image versions for cells")

        if _dom0s:
            for _dom0 in _dom0s:
                _imagever = self.mGetImageVersion(_dom0)
                _lastreboottime = self.mGetLastRebootTime(_dom0)
                _dom0_d= {}
                _dom0_d['hostname'] = _dom0
                _dom0_d['imageversion']= _imagever
                _dom0_d['lastreboot'] = _lastreboottime
                _data_d["dom0s"].append(_dom0_d)
                ebLogInfo(f"Fetched image versions for dom0s")

        _reqobj = self.mGetRequestObj()
        if _reqobj is not None:
            _reqobj.mSetData(json.dumps(_data_d, sort_keys=True))
            _db = ebGetDefaultDB()
            _db.mUpdateRequest(_reqobj)
        else:
            if self.__debug:
                ebLogInfo(json.dumps(_data_d, sort_keys=True, indent=4))

        ebLogInfo("Image Version Fetched")

    def mHandlerSetScrubbingWindow(self):
        aOptions = self.mGetArgsOptions()

        _pchecks = ebCluPreChecks(self)
        return _pchecks.mSetDiskScrubbingWindow(aOptions)

    def mHandlerVMInstall(self):
        aOptions = self.mGetArgsOptions()
        return self.mGetCluCtrlDeprecateObj().mMgmtVMGI(True,aOptions,aMode=VM_MODE)

    def mHandlerVMDelete(self):
        aOptions = self.mGetArgsOptions()
        return self.mGetCluCtrlDeprecateObj().mMgmtVMGI(False,aOptions,aMode=VM_MODE)

    def mHandlerVolAttach(self):
        aOptions = self.mGetArgsOptions()                       
        if self.isBaseDB() or self.isExacomputeVM() or self.isDBonVolumes():                                  
            _exascale = ebCluExaScale(self)                     
            return _exascale.mVolAttach(aOptions)                   
        else:
            raise ExacloudRuntimeError(0x0734, 0xA, 'endpoint not supported in non ExadbXS env')

    def mHandlerVolDetach(self):
        aOptions = self.mGetArgsOptions()                      
        if self.isBaseDB() or self.isExacomputeVM() or self.isDBonVolumes():                                  
            _exascale = ebCluExaScale(self)                     
            return _exascale.mVolAttach(aOptions, aUndo=True)
        else:
            raise ExacloudRuntimeError(0x0734, 0xA, 'endpoint not supported in non ExadbXS env')

    def mHandlerVolResize(self):
        aOptions = self.mGetArgsOptions()
        if self.isBaseDB() or self.isExacomputeVM() or self.isDBonVolumes():
            _exascale = ebCluExaScale(self)
            return _exascale.mVolResize(aOptions)
        else:
            raise ExacloudRuntimeError(0x0734, 0xA, 'endpoint not supported in non ExadbXS env')

    def mHandlerMonitorBonding(self):
        aOptions = self.mGetArgsOptions()
        
        # DEPRECATED: "bonding_operation" above must be used instead
        if clubonding.is_bonding_supported(self):
            if aOptions.bondingoperation:
                if not aOptions.bondingjson:
                    raise ExacloudRuntimeError(0x0786,0xA,"No bonding json file passed. It is required for bonding operation -bondingjc <json file path>")

                if not  os.path.exists(aOptions.bondingjson):
                    raise ExacloudRuntimeError(0x0786,0xA,"Bonding json configuration file is not available at:{0}".format(aOptions.bondingjson))

                with open(aOptions.bondingjson) as bondingjson_file:
                    ecraPayloadDict = json.load(bondingjson_file)

                if aOptions.bondingoperation == 'enable':
                    clubonding.configure_bonding_if_enabled(
                        self, payload=ecraPayloadDict,
                        configure_bridge=True, configure_monitor=False)
                elif aOptions.bondingoperation == 'disable':
                    clubonding.cleanup_bonding_if_enabled(
                        self, payload=ecraPayloadDict,
                        cleanup_bridge=True, cleanup_monitor=False)
                else:
                    ebLogError("*** Inavlid option used for the monitorbonging operation ***")
            else:
                ebLogInfo('aOptions.monitorbonding absent')
        else:
            _bonding_property = self.mCheckConfigOption('activate_oci_bonding')
            _error_msg = "Bonding configuration only supported in ExaBM and ATP environments and as long as activate_oci_bonding property is set in exabox.conf"
            _env_msg = "ATP={0},ExaBM={1} activate_oci_bonding={2}".format(self.isATP(),self.mIsExabm(),_bonding_property if _bonding_property else "None")
            raise ExacloudRuntimeError(0x0786,0xA,"\n".join([_error_msg,_env_msg]))

    def mHandlerPatching(self):
        _infrapatchobj = None
        aOptions = self.mGetArgsOptions()
        aCmd = self.mGetCmd()

        if ebCluCmdCheckOptions(aCmd, ['patchmgr_logs']):
            # Path where we save patchmgr logs.
            _log_path   = self.__oeda_path+'/log/patchmgr_logs'

        if ebCluCmdCheckOptions(aCmd, ['custom_patch_log']):
            # Path where we save custom operation logs.
            _log_path = self.__oeda_path+'/log/patch_logs'

        if aOptions.singleworkerpatching:
            _single_worker_patching = aOptions.singleworkerpatching
            if _single_worker_patching and _single_worker_patching == 'enabled':
                ebLogInfo(f"Invoking single worker patching request through ebCluInfraSingeRequestPatch")
                _infrapatchobj = ebCluInfraSingeRequestPatch(self, aOptions)
                return _infrapatchobj.mInvokePatchExecutor(aCmd,_log_path)
        else:
            ebLogInfo(f"Invoking master and child worker patching request through ebCluInfraPatch")
            _infrapatchobj = ebCluInfraPatch(self, aOptions)
            return _infrapatchobj.mInvokePatchExecutor(aCmd,_log_path)
            
    def mHandlerDBaaSTool(self):
        """
            DBAAS TOOLS call
        """
        aOptions = self.mGetArgsOptions()
        aCmd = self.mGetCmd()

        dbaas_params = {
                "db_patch"   : ["dbname", "op", "patchid"],
                "db_backup"  : ["dbname", "action"],
                "dbaas_patch": ["op", "patchid"]    # dbaas patch does not required dbname
                }

        if aOptions.jsonconf:
            _jconf = aOptions.jsonconf

            # validating required parameters for patching
            for _p in dbaas_params[aCmd]:
                if _p not in _jconf:
                    ebLogError("'{0}' field not provided".format(_p))
                    return ebError(0x0602)

            _dbaasjson = json.dumps(_jconf, indent=4, sort_keys=True)
            # remap boolean parameters


            self.__dbpatch = b64encode(_dbaasjson)
            _dbprt, _dbpout = self.mRunCmdScript(aCmd)
            if _dbprt:
                return ebError(0x0503)

        else:
            ebLogError('The input DBCS json should have this fields {0}'.format(list(dbaas_params[aCmd].keys())))
            return ebError(0x0602)


    def mHandlerGetStatus(self):
        #
        # This is an example of Cluster wide commands handler.
        #
        aOptions = self.mGetArgsOptions()
        # GI Status for all resources
        _cmd = '/u01/app/12.1.0.2/grid/bin/crsctl status res -t'

        def _status_fn(aNodeHandle,aOptions):
            aNodeHandle.mExecuteCmdLog(_cmd)

        self.mExecuteOnClusterDomU(_status_fn,aOptions)

    def mHandlerVMGIInstall(self):
        aOptions = self.mGetArgsOptions()
        # Update operations entry in dom0.
        self.mDom0UpdateCurrentOpLog(aOptions, "Create Service", "started")
        self.mGetCluCtrlDeprecateObj().mMgmtVMGI(True,aOptions)
        self.mDom0UpdateCurrentOpLog(aOptions, "Create Service", "ended")

    def mHandlerVMGIDelete(self):
        aOptions = self.mGetArgsOptions()
        # Update operations entry in dom0.
        self.mDom0UpdateCurrentOpLog(aOptions, "Delete Service", "started")
        self.mDeleteVnumaMarker()
        self.mGetCluCtrlDeprecateObj().mMgmtVMGI(False, aOptions)
        self.mDom0UpdateCurrentOpLog(aOptions, "Delete Service", "ended")

        if self.mCheckConfigOption('reconfig_backup', 'True'):
            _backupTool = self.__factoryPreprovReconfig.mCreateBackupTool()
            _backupTool.mDeleteAll()

        
    def mHandlerCoreDump(self):
        aOptions = self.mGetArgsOptions()
        ebLogInfo("In DomU Core Dump routine")
        coreDumpUtil = ebCoredumpUtil(doms=self.mReturnDom0DomUPair(), payload=aOptions.jsonconf)
        coreDumpUtil.mRunCoredumpUtil()

    def mHandlerGetDom0ExistingGuestsSize(self):
        ebLogInfo("Get computed size for existing guests.")
        aOptions = self.mGetArgsOptions()
        _dom0_list = []
        _payload_and_xml = False
        _dpairs = self.mReturnDom0DomUPair()
        _dom0_list_xml = [ _dom0 for _dom0 , _ in _dpairs]
        _dom0_list_payload = None
        if aOptions.jsonconf:
            if "dom0s" in aOptions.jsonconf:
                _dom0_list_payload = aOptions.jsonconf["dom0s"]
            if "payload_and_xml" in aOptions.jsonconf:
                _payload_and_xml = "true" in aOptions.jsonconf["payload_and_xml"]
            if _payload_and_xml and _dom0_list_payload:
                _dom0_list += list(set(_dom0_list_payload + _dom0_list_xml))
            elif not _payload_and_xml and _dom0_list_payload:
                _dom0_list = _dom0_list_payload
            else:
                _dom0_list = _dom0_list_xml
        else:
            _dom0_list = _dom0_list_xml
        # For each DOM0, run du -sb  /EXAVMIMAGES/GuestImages if /EXAVMIMAGES/GuestImages exists
        # And calculate round(value_obtained/1073741824.0, 3) -> Round it off to 3 decimal places.
        # Here: 1073741824.0 = 1024 * 1024 * 1024 (We are converting bytes to GB and rounding it off to 3 decimal places)
        _dom0_disk_utilization_dict = {}
        _guest_images_folder = "/EXAVMIMAGES/GuestImages"
        for _dom0 in _dom0_list:
            with connect_to_host(_dom0, self.mGetCtx()) as _node:
                if _node.mFileExists(_guest_images_folder):
                    _du_cmd = node_cmd_abs_path_check(_node, "du")
                    _du_cmd_run = f"{_du_cmd} -sb {_guest_images_folder}"
                    _, _stdout, _stderr = _node.mExecuteCmd(_du_cmd_run)
                    _stdout_string = _stdout.read().strip()
                    _stderr_string = _stderr.read().strip()
                    _rc = _node.mGetCmdExitStatus()
                    if _rc != 0 or not _stdout_string or _stdout_string == "":
                        _msg = f"The command {_du_cmd_run} failed on DOM0 {_dom0} with stderr: {_stderr_string} and stdout: {_stdout_string}."
                        ebLogError(_msg)
                        raise ExacloudRuntimeError(0x0403, 0xA, _msg)
                    ebLogInfo(f"Command {_du_cmd_run} ran successfully on dom0 {_dom0}. stdout: {_stdout_string}.")
                    _disk_utilization_bytes = int(_stdout_string.split()[0])
                    # Here: 1073741824.0 = 1024 * 1024 * 1024 (We are converting bytes to GB and rounding it off to 3 decimal places)
                    _disk_utilization_giga_bytes = round(_disk_utilization_bytes/1073741824.0, 3)
                    _dom0_disk_utilization_dict[_dom0] = f"{_disk_utilization_giga_bytes} GB"
                    ebLogTrace(f"Space occupied by {_guest_images_folder} on {_dom0} is {_disk_utilization_giga_bytes} GB.")
                else:
                    ebLogWarn(f"The path {_guest_images_folder} does not exist on dom0 {_dom0}. Setting disk utilized to 0.0")
                    _dom0_disk_utilization_dict[_dom0] = "0.0 GB"
        _reqobj = self.mGetRequestObj()
        if _reqobj is not None:
            _reqobj.mSetData(json.dumps(_dom0_disk_utilization_dict, sort_keys=True))
            _db = ebGetDefaultDB()
            _db.mUpdateRequest(_reqobj)
        else:
            #Console output
            ebLogInfo(json.dumps(_dom0_disk_utilization_dict, sort_keys=True))

    def mHandlerHostState(self):
        aOptions = self.mGetArgsOptions()
        if not aOptions.hostname:
            ebLogError('Missing target hostname parameter')
            return(0x0119)
        _opHostname = aOptions.hostname

        ebLogInfo('*** Checking if host %s is pingable' %(_opHostname))
        if not self.mPingHost(_opHostname):
            ebLogError('*** %s host does not respond to ping' % (_opHostname))
            return (0x0403)
        ebLogInfo('*** Host %s is pingable' %(_opHostname))

        _node = exaBoxNode(get_gcontext())
        ebLogInfo('*** Attempting connection to: %s' % (_opHostname))
        try:
            _node.mConnect(aHost=_opHostname)
            _node.mExecuteCmdLog('hostname -f')
        except:
            ebLogInfo('*** SSH to %s failed' % (_opHostname))
            return (0x0411)
        _node.mDisconnect()

    def mHandlerCPSSetup(self):
        if self.mIsKVM():  # RoCE
            if self.__ociexacc:
                if self.mIsRoCEQinQ():
                    ebLogInfo('RoCE must have been configured during CPS reimage. Verifying setup is correct.')
                    self.mCheckCPSQinQSetup()
                else:
                    _exacc_roce_cps = ExaCCRoCE_CPS(self.__debug, True)
                    _exacc_roce_cps.mSetupCPSRoCE()
        else:  # Infiniband
            self.mCheckSwitchesRegistration()
            _allGuids = self.mGetAllGUID()
            _pkeys = self.mCheckPkeysConfig(_allGuids, False)
            if self.__ociexacc and _pkeys is not None:
                _exacc_ib_cps = ExaCCIB_CPS(_allGuids, _pkeys, self.__debug, True)
                _dom0s, _, _cells, _ = self.mReturnAllClusterHosts()
                _exacc_ib_cps.mSetupIBSwitches(_dom0s, _cells)
                _exacc_ib_cps.mSetupCPSIB()

    def mHandlerGetExawatcherLogs(self):
        aOptions = self.mGetArgsOptions()
        _exawobj = exaBoxExaWatcher(self)
        _exawobj.mCollectExaWatcherLogs(aOptions)

    def mHandlerListExawatcherLogs(self):
        aOptions = self.mGetArgsOptions()
        _exawobj = exaBoxExaWatcher(self)
        _exawobj.mListExaWatcherLogs(aOptions)

    def mHandlerLuksOperation(self):
        aOptions = self.mGetArgsOptions()

        _resp = executeLuksOperation(self)

        _reqobj = self.mGetRequestObj()
        if _reqobj is not None:
            _reqobj.mSetData(json.dumps(_resp, sort_keys=True))
            _db = ebGetDefaultDB()
            _db.mUpdateRequest(_reqobj)

        ebLogInfo(f"JSON Response:\n{_resp}")

    def mHandlerEXACCMigration(self):
        # Perform configuration actions at existing Gen1 ExaCC infrastructure
        # to make it compatible with Gen2 model
        if self.mIsOciEXACC():
            ebLogInfo('About to configure Gen1 cluster for migration to Gen2')
            migrateExaCCGen1ClusterToGen2(self)
        else:
            ebLogInfo('No ExaCC-OCI env detected. Nothing to do.')

    def mHandlerOEDADelete(self):

        _stepList = [OSTP_CREATE_GDISK, OSTP_CREATE_CELL, OSTP_SETUP_CELL, OSTP_CREATE_USER, OSTP_CREATE_VM]

        # Execute OEDA steps 6 - 2
        for _step in _stepList:
            _oedaStep = self.mFetchOedaStep(str(_step))
            ebLogInfo(f'Running OEDA UNDO step {_oedaStep}...')
            _lCmd = f'/bin/bash install.sh -cf {self.__remoteconfig} -u {_oedaStep} {self.mGetOEDAExtraArgs()}'

            if _step == OSTP_CREATE_GDISK:
                _lCmd += ' -override'

            _out = self.mExecuteCmdLog2(_lCmd, aCurrDir = self.__oeda_path)
            _rc = self.mParseOEDALog(_out)

            if not _rc:
                ebLogError(f'There was an error in step {_oedaStep}. Command is set to continue.')


    def mHandlerARPOper(self):
        aOptions = self.mGetArgsOptions() 
        if aOptions.arpoperation:
            if aOptions.arpoperation == 'enable':
                ebLogVerbose('aOptions.arpoperation enable')
                self.mConfigureArp()
            elif aOptions.arpoperation == 'disable':
                ebLogVerbose('aOptions.arpoperation disable')
                self.mConfigureArp(enable=False)
            else:
                ebLogVerbose('aOptions.arpoperation = ' + aOptions.arpoperation)
        else:
            ebLogInfo('aOptions.arpoperation absent')


    def mHandlerNetworkUpdate(self):
        # update network in exaBoxCluCtrl, the XML and the cluster
        clunetupdate.handle_network_update(self)

        # update request to return patched XML
        req = self.mGetRequestObj()

        if req:
            req.mSetXml(self.mGetPatchConfig())
            ebGetDefaultDB().mUpdateRequest(req)

    def mHandlerRollbackBondingMigration(self):
        """
        Handle rollback of bonding migration
        """
        clunetupdate.handle_rollback_bonding_migration(self)
        # update request to return patched XML
        req = self.mGetRequestObj()

        if req:
            req.mSetXml(self.mGetPatchConfig())
            ebGetDefaultDB().mUpdateRequest(req)

    def mHandlerBondingOper(self):
        clubonding.handle_bonding_operation(self)

    def mHandlerPreChecks(self):

        if self.mIsExaScale():
            _exascale = ebCluExaScale(self)
            _exascale.mRunExaDbXsChecks()
        else:
            _ib_target = not self.mIsKVM()
            _pchecks = ebCluPreChecks(self)
            _pchecks.mRunAllPreChecks(aVerboseMode=True,aIbTarget=_ib_target)

    def mHandlerVMGIPreProv(self, aOptions=None):
        if not aOptions: 
            aOptions = self.mGetArgsOptions()
        self.__preprovisioning = True

        if ('steplist' in aOptions and aOptions.steplist == "ESTP_PREVM_CHECKS") or ('steplist' not in aOptions):
            self.mDom0UpdateCurrentOpLog(aOptions, "Create Service", "started")

        if 'steplist' in aOptions and aOptions.steplist:
            stepdriver = csDriver(self, aOptions)
            _rc = stepdriver.handleRequest()

        if ('steplist' in aOptions and aOptions.steplist == "ESTP_POSTGI_NID") or ('steplist' not in aOptions):
            self.mSetupEbtablesOnDom0(aMode=False)
            self.mVerifyClusterwareBmCloud()
            self.mDom0UpdateCurrentOpLog(aOptions, "Create Service", "ended")

            #Add flag of UseDNS on sshd
            _util = self.__factoryPreprovReconfig.mCreatePreprovUtil()
            _util.mSetPreprovEnv()
            self.mAddUseDnsFlag()

    def mHandlerPreProvBackup(self):
        _backupTool = self.__factoryPreprovReconfig.mCreateBackupTool()
        _backupTool.mBackupAll()

    def mHandlerVMGIInfraDelete(self, aOptions=None):

        _infradeleteEndpoint = ebCluInfraDelete(self)
        _infradeleteEndpoint.mExecute()

        return 0


    def mHandlerVMGIReconfig(self, aOptions=None):

        if not aOptions: 
            aOptions = self.mGetArgsOptions()

        # Enable TFA blackout before reconfig

        _doTFA = True
        for _, _domU in self.mReturnDom0DomUPair():
            if not self.mPingHost(_domU):
                _doTFA = False
                break

        if _doTFA and self.isATP():
            self.mEnableTFABlackout(True, "Reconfig blackout", aOptions)

        if self.isATP():
            # Execute reconfig
            _reconfig = self.__factoryPreprovReconfig.mCreateReconfig()
            _reconfig.mExecuteReconfig()

            if self.__debug:
                ebLogInfo(json.dumps(_reconfig.mGetStepRecord(), indent=4))

            #Remove flag of UseDNS on sshd
            self.mRemoveUseDnsFlag()
        else:
            mVMReconfig(self, aOptions)

        if self.isATP():
            # Disable TFA blackout after reconfig
            self.mEnableTFABlackout(False, "Reconfig blackout", aOptions)

            # Configure bonding.
            #
            # Configure bridge only if static monitoring brigde is not supported.
            conf_bridge = \
                not clubonding.is_static_monitoring_bridge_supported(
                    self, payload=aOptions.jsonconf)
            clubonding.configure_bonding_if_enabled(
                self, payload=aOptions.jsonconf,
                configure_bridge=conf_bridge, configure_monitor=True)

    def mHandlerVMGIRollback(self):

        if self.mCheckConfigOption('reconfig_backup', 'True'):
            _reconfig = self.__factoryPreprovReconfig.mCreateReconfig()
            _reconfig.mExecuteRollback()
        else:
            ebLogInfo("Reconfig backup is disabled")

    def mHandlerRunScript(self, aOptions=None):
        if not aOptions: 
            aOptions = self.mGetArgsOptions()
        if aOptions.scriptname:
            self.__scriptname = aOptions.scriptname
        #self.mRunScript(aType='*',aWhen='post.db_install')
        # use the -s / --script (note there is no plural) to specify the aWhen
        _rc, _cmd = self.mRunScript(aType='*',aWhen=aOptions.scriptname,aStatusAbort=True)
        if _rc:
            ebLogError('*** Error ('+str(_rc)+') caught during scripts execution for cmd: '+_cmd)
    
    def mHandlerExawatcherLogs(self, aOptions=None):
        if not aOptions: 
            aOptions = self.mGetArgsOptions()
        ebLogInfo('***fetch_exawatcher_logs')
        exawatcher = exaBoxExaWatcher(self)
        exawatcher.mCollectExaWatcherLogs(aOptions)

    def mHandlerSimInstall(self):
        aOptions = self.mGetArgsOptions()
        print ('*** SIM_INSTALL ***')

    def mHandlerSwitchesReset(self):
        if self.mIsKVM():
            ebLogWarn('*** Access to switches not a valid code flow on KVM systems')
            return 0
        _pchecks = ebCluPreChecks(self)
        _pchecks.mResetSwitches()

    def mHandlerImageBasePostValidation(self):

        _allResults = {}
        _bomSpec = {}

        if os.path.exists("image_base_bom.json"):
            with open("image_base_bom.json") as _f:
                _bomSpec = json.load(_f)

        if "image_base_bom" in self.mGetOptions().jsonconf:
            _bomSpec = self.mGetOptions().jsonconf["image_base_bom"]

        if not _bomSpec:
            raise ExacloudRuntimeError(0x0825, 0xA, "Missing Bom Spec")

        for _, _domU in self.mReturnDom0DomUPair():
            with connect_to_host(_domU, self.mGetCtx()) as _node:

                _result = {}

                # Validate RPMs
                for _rpm in _bomSpec["artifacts"]["rpms"].keys():

                    if _rpm == "ahf_setup":
                        continue

                    _rpmName = _bomSpec["artifacts"]["rpms"][_rpm]["name"]

                    _cmd = "/bin/rpm -q --qf '%{VERSION}.%{RELEASE}' " + _rpmName
                    _, _o, _ = _node.mExecuteCmd(_cmd)
                    _version = _o. read().strip()

                    if not _version or _version < _bomSpec["artifacts"]["rpms"][_rpm]["version"]:

                        _msg = f"RPM {_rpmName} difference in version. Installed: '{_version}'"
                        _msg = f"{_msg}, In Bom: '{_bomSpec['artifacts']['rpms'][_rpm]['version']}'"

                        _result["RPMS"] = {
                            "Status": "Failure",
                            "Reason": _msg
                        }

                if "RPMS" not in _result:
                    _result["RPMS"] = {"Status": "Success"}

                # Validate DBCS Agent
                _cmd = '/usr/sbin/service dbcsagent status | grep "Active"'
                _, _o, _ = _node.mExecuteCmd(_cmd)
                _status = _o.read()

                if "running" in _status:
                    _result["DBCS_AGENT"] = {"Status": "Success"}
                else:
                    _msg = f"DBCS agent is not running: {_status}"
                    _result["DBCS_AGENT"] = {
                        "Status": "Failure",
                        "Reason": _msg
                    }

                # Validate DBCS Admin
                _cmd = '/usr/sbin/service dbcsadmin status | grep "Active"'
                _, _o, _ = _node.mExecuteCmd(_cmd)
                _status = _o.read()

                if "running" in _status:
                    _result["DBCS_ADMIN"] = {"Status": "Success"}
                else:
                    _msg = f"DBCS agent is not running: {_status}"
                    _result["DBCS_ADMIN"] = {
                        "Status": "Failure",
                        "Reason": _msg
                    }

                # Validate AHF
                _cmd = "/bin/cat /opt/oracle.ahf/install.properties"
                _, _o, _ = _node.mExecuteCmd(_cmd)
                _status = _o.read()

                if "gold" in _status:
                    _result["AHF"] = {
                        "Status": "Failure",
                        "Reason": f"AHF still has 'gold' hostname: {_status}"
                    }

                else:
                    _result["AHF"] = {"Status": "Success"}

                # Validate Grid Version
                _cmd = "/u01/app/23.0.0.0/gridhome_1/bin/oraversion -compositeVersion"
                _, _o, _ = _node.mExecuteCmd(_cmd)
                _status = _o.read().strip()

                if _status in _bomSpec["artifacts"]["gi"]:
                    _result["GRID"] = {"Status": "Success"}
                else:
                    _msg = f'Grid difference in version. Installed: {_status}'
                    _msg = f'{_msg}, In Bom: {_bomSpec["artifacts"]["gi"]}'

                    _result["GRID"] = {
                        "Status": "Failure",
                        "Reason": _msg
                    }

            _allResults[_domU] = _result

        ebLogInfo(f"Validation Status: {json.dumps(_allResults, indent=4, sort_keys=True)}")

        if "Failure" in str(_result):
                raise ExacloudRuntimeError(0x0825, 0xA, "Failure while validate")

        return 0

    def mHandlerEMClusterDetails(self, aOptions=None):
        
        if not aOptions: 
            aOptions = self.mGetArgsOptions()
        _pchecks = ebCluPreChecks(self)
        return _pchecks.mEMClusterDetails(aOptions)

    def mHandlerEMDBDetails(self, aOptions=None):
        
        if not aOptions: 
            aOptions = self.mGetArgsOptions()
        _pchecks = ebCluPreChecks(self)
        return _pchecks.mEMDBDetails(aOptions)

    def mHandlerDom0CopyCert(self, aOptions=None):
        if not aOptions:
            aOptions = self.mGetArgsOptions()
        if self.mIsOciEXACC() and self.mIsFedramp(aOptions):
            self.mPushCertificatestoDom0()
            
    def mHandlerCopyCSSHubKeys(self, aOptions=None):
        if self.mIsOciEXACC() and self.mIsFedramp(aOptions):
            # gpg keys path
            gpg_keys_cps_path = '/opt/oci/exacc/certs/gpgkey/oracleamerica_release_key.asc'
            gpg_keys_dom0_path = '/tmp/oracleamerica_release_key.asc'
            # check gpg keys exist in the path
            if not os.path.exists(gpg_keys_cps_path):
                ebLogError(f"CSS Hub's GPG keys do not exist at given path :{gpg_keys_cps_path}")
                raise ExacloudRuntimeError(0x0825, 0xA, "Keys path do not exist")
            # connect to dom0s and copy keys, run install commands
            for _dom0, _ in self.mReturnDom0DomUPair():
                try:
                    _node = exaBoxNode(get_gcontext())
                    _node.mConnect(aHost=_dom0)
                    _node.mCopyFile(gpg_keys_cps_path, gpg_keys_dom0_path)
                    # check file is copied successfully
                    _cmdstr = "rpm --import /tmp/oracleamerica_release_key.asc; rpm -q --queryformat '%{SUMMARY}\n' $(rpm -q gpg-pubkey)"
                    _f, _o, _e = _node.mExecuteCmd(_cmdstr)
                    _out = _o.readlines()
                    for output in _out:
                        ebLogInfo(f'{output.strip()}')
                    _rc = _node.mGetCmdExitStatus()
                    if _rc == 0:
                        ebLogInfo(f"CSS Hub gpg keys successfully installed on dom0:{_dom0}")
                    else:
                        ebLogError(f"CSS Hub gpg keys installed on dom0:{_dom0} failed with error:{_e.readlines()}!")
                except Exception as e:
                    _err_msg = f"Failed to install CSS Hub GPG keys to dom0 {_dom0 }due to error : {e}"
                    ebLogError(_err_msg)
                    raise ExacloudRuntimeError(0x0825, 0xA, aErrorMsg=_err_msg)
                finally:
                    _node.mDisconnect()
        
    def mHandlerDiskgroupOper(self):
        aOptions = self.mGetArgsOptions()
        _cmdOp = aOptions.diskgroupOp
        if _cmdOp in ['create', 'update_add_sparse', 'resize', 'info', 'rebalance', 'drop', 'precheck']:
            ebLogInfo('*** Performing diskgroup %s operation' %(_cmdOp))
            _diskgroupobj = ebCluManageDiskgroup(self, aOptions)
            return _diskgroupobj.mClusterManageDiskGroup(aOptions)
        else:
            ebLogError("Invalid operation for diskgroup command")
            return ebError(0x0602)

    def mHandlerPartitionOperation(self):
        aOptions = self.mGetArgsOptions()
        _cmdOp = aOptions.partitionOp
        if _cmdOp in ['resize', 'info']:
            ebLogInfo('*** Performing local disk partition %s operation' %(_cmdOp))
            _partitionobj = ebCluManageDomUPartition(self)
            return _partitionobj.mClusterManageDomUPartition(_cmdOp, aOptions)
        else:
            ebLogError("Invalid operation for local disk command")
            return ebError(0x0602)

    def mHandlerResizeStorage(self, Options=None):
        if Options:
            aOptions = Options
        else:
            aOptions = self.mGetArgsOptions()

        with self.remote_lock():
            ebLogInfo('*** Performing storage resize operation')
            _clu_utils = ebCluUtils(self)
            _storage = ebCluManageStorage(self, aOptions)
            _stepSpecificDetails = _clu_utils.mStepSpecificDetails("reshapeDetails", 'ONGOING', "ASM reshape in progress", "","ASM")
            _clu_utils.mUpdateTaskProgressStatus([], 0, "ASM Reshape", "In Progress", _stepSpecificDetails)
            _rc = _storage.mClusterStorageResize(aOptions)
            if _rc is not None and _rc != 0:
                _msg = 'Error occurred during storage resize operation. Please check exacloud logs for more details.'
                raise ExacloudRuntimeError(0x0150, 0xA, _msg)

            # Make sure we store final size used
            # in 'data' field from status response
            try:
                _storage_data = {}
                dg_size_dict = {}
                _cluster = self.mGetClusters().mGetCluster()
                cludgroups = _cluster.mGetCluDiskGroups()
                _storage.mFetchAndSaveDGSizes(cludgroups, dg_size_dict)
                _sum = 0
                for k, v in dg_size_dict.items():
                    _sum = _sum + v['totalgb']

                _storage_data['storageGB'] = (_sum / 3) #factor out HIGH REDUNDANCY

            except Exception as e:
                ebLogWarn(f"Error while calculating total StorageGB size. Falling back "
                    f"to 0")
                _storage_data['storageGB'] = 0

            _stepSpecificDetails = _clu_utils.mStepSpecificDetails("reshapeDetails", 'DONE', "ASM reshape is completed", "","ASM")
            _clu_utils.mUpdateTaskProgressStatus([], 100, "ASM Reshape", "DONE", _stepSpecificDetails, _storage_data)

        return _rc
    
    def mHandlerElasticCellUpdate(self):
        ## self.__shared_env will be TRUE for 'elastic_cell_update' only. It shall be false for 'elastic_cell_info'
        aOptions = self.mGetArgsOptions()

        ebLogInfo('*** Performing cell management operation')
        self.mPublishNewNodeKey(aOptions)

        _elasticCellManager = ebCluElasticCellManager(self, aOptions, False)
        _rc = _elasticCellManager.mClusterCellUpdate(aOptions)

        return _rc

    def mHandlerElasticCellInfo(self):
        ## self.__shared_env will be TRUE for 'elastic_cell_update' only. It shall be false for 'elastic_cell_info'                        
        aOptions = self.mGetArgsOptions()
        self.mAcquireRemoteLock()
                     
        ebLogInfo('*** Performing cell management operation')                   
        self.mPublishNewNodeKey(aOptions)                                       
                     
        _elasticCellManager = ebCluElasticCellManager(self, aOptions, True) 
        _rc = _elasticCellManager.mClusterCellInfo(aOptions)                
                     
        self.mReleaseRemoteLock()                                               
        return _rc

    def mHandlerVMBackupOSS(self):
        aOptions = self.mGetArgsOptions()
        aCmd = self.mGetCmd()

        # This endpoint is deprecated, we'll have a flag disabling this flow
        # by default
        if (get_gcontext().mGetConfigOptions().get(
            "vmbackup", {}).get("force_ecra_oss_api", "").lower() != "true"):
            ebLogWarn(f"This VMBackup to OSS handler is deprecated, you "
                f"can enable it by setting to 'True' the flag "
                "['vmbackup']['force_ecra_oss_api'] in exabox.conf")
            return 1

        ebLogWarn(f"Running vmbackup to oss deprecated flow")

        _vmboss = ebCluVmbackupObjectStore(self, self.__config)
        ebLogInfo('*** Performing vmbackup-oss operation')
        _vmboss.mExecute(aOptions, aCmd)

        return 0

    def mHandlerVMBackup(self):
        aOptions = self.mGetArgsOptions()

        ebLogInfo('*** Performing vmbackup operation')
        _vmbackup = ebCluManageVMBackup(self)
        _rc = _vmbackup.mExecuteOperation(aOptions)

        return _rc

    def mHandlerConfigureUser(self):
        aOptions = self.mGetArgsOptions()
        self.mAcquireRemoteLock()

        ebLogInfo('*** Performing usrconfig operation')
        _usrconfig = ebCluResManager(self, aOptions)
        _rc = _usrconfig.mUserConfig(aOptions)

        if aOptions.user_operation == "alter_user":
            _inputjson = aOptions.jsonconf
            if 'password' in list(_inputjson.keys()):
                _passwd = _inputjson['password']
                if not check_string_base64(_passwd):
                    _passwd = b64encode(_passwd.encode("utf-8")).decode("utf-8")
                # Cloud user will be stored in DomU
                _domUs = list(map(operator.itemgetter(1),self.mReturnDom0DomUPair()))
                ebExaCCSecrets(_domUs).mPushExacliPasswdToDomUs(b64decode(_passwd.encode()).decode(), aUser="opc")
        self.mReleaseRemoteLock()
        return _rc

    def mHandlerVMConsole(self):
        aOptions = self.mGetArgsOptions()
        _jsonconf = aOptions.jsonconf
        #Check if action comes from payload
        if _jsonconf and \
            'action' in _jsonconf and \
            _jsonconf['action'] in ["install", "uninstall", "status"]:

            _action = _jsonconf['action']
        elif aOptions.vmconsole_action is not None:
            #action comes from exacloud cli
            _action = aOptions.vmconsole_action
        else:
            raise ExacloudRuntimeError(0x0119, 0xA, "Missing parameter 'action'")

        _vmc_dpy = VMConsoleDeploy(self, aOptions)
        if _action == 'install':
            return _vmc_dpy.mInstall()
        elif _action == 'uninstall':
            return _vmc_dpy.mUninstall()
        elif _action in ['status', 'get_status']:
            if _jsonconf and "output_file" in _jsonconf:
                if os.path.isdir(os.path.dirname(_jsonconf["output_file"])):
                    return _vmc_dpy.mGetStatus(aOutputFile=_jsonconf["output_file"])
                else:
                    raise ExacloudRuntimeError(0x0119, 0xA, "Please select a valid directory for 'output_file'")
            else:
                return _vmc_dpy.mGetStatus()
        elif _action == 'get_status_nocopy':
            if _jsonconf and "output_file" in _jsonconf:
                if os.path.isdir(os.path.dirname(_jsonconf["output_file"])):
                    return _vmc_dpy.mGetStatus(aNoCopy=True, aOutputFile=_jsonconf["output_file"])
                else:
                    raise ExacloudRuntimeError(0x0119, 0xA, "Please select a valid directory for 'output_file'")
            else:
                return _vmc_dpy.mGetStatus(aNoCopy=True)

    def mHandlerVmCmd(self):
        aOptions = self.mGetArgsOptions()

        if aOptions.vmcmd is None or aOptions.vmcmd == 'None':
            ebLogWarn('VM_CMD called without any valid request/command to perform')
            return ebError(0x0400)
        else:
            ebLogInfo('VM_CMD (Dispatch): '+aOptions.vmcmd)

        _vmid = aOptions.vmid
        #
        # If _vmid is not provided vm_cmd is applied to all VMs in the cluster
        #
        if _vmid is None:
            if aOptions.jsonconf and "vmid" in aOptions.jsonconf:
                _vmid = aOptions.jsonconf["vmid"]
            else:
                if aOptions.debug:
                    ebLogInfo('VM_ID not provided. Default to ALL VMs in cluster')
                _vmid = '_all_'
            ebLogInfo('VM_ID: '+_vmid)
        else:
            if aOptions.debug:
                ebLogInfo('VM_ID: '+aOptions.vmid)

        _vmcmd = aOptions.vmcmd

        _vmhandle = self.__CompRegistry.mGetComponent("vm_operations")
        #
        # Misc. VM commands
        #
        if _vmcmd == 'list':
            return vmcontrol.vmList(get_gcontext(), self.__clusters, self.__machines, aOptions, self.mGetRequestObj(), ebGetDefaultDB())

        #
        # VM vcpu management
        #
        if _vmcmd in ['cpustatus', 'resizecpus', 'addcpus', 'removecpus', 'enablebursting']:
            if not self.mIsKVM():# if XEN then apply the shared environment check for the remote locks to work
                self.mCheckSharedEnvironment()
            self.mAcquireRemoteLock()
            if _vmcmd == 'enablebursting':
                _rc = self.mManageVMCpusBursting(_vmcmd, _vmid, aOptions)
            else:
                _rc = self.mManageVMCpusCount(_vmcmd, _vmid, aOptions)
            self.mReleaseRemoteLock()
            return _rc

        if _vmcmd == "prepare_move":
            ebCluExaScale(self).mPrepareVmMoveOEDA(aOptions)

        if _vmcmd == "move":
            try:
                _exascale = ebCluExaScale(self)
                _exascale.mPerformVmMove(aOptions)
            except Exception as ex:
                _res = json.dumps({'failed_step': _exascale.mGetFailedStep()})
                _req = self.mGetRequestObj()
                if _req is not None:
                    _req.mSetData(_res)
                    db = ebGetDefaultDB()
                    db.mUpdateRequest(_req)
                raise ExacloudRuntimeError(0x0811, 0xA, "VM move operation failed!!!") from ex

        #
        # Serial Console
        #
        if _vmcmd in ["create_console_ssh", "console_history", "delete_console_ssh"]:
            _rc = 0
            if _vmid == '_all_' and _vmcmd == "console_history":
                ebLogWarn(f"Unknown VM_ID: {_vmid} for VCMD:{_vmcmd}")
                return ebError(0x0402)

            for _dom0, _domU in self.mReturnDom0DomUPair():
                if _domU == _vmid or _vmid == '_all_':
                    _node = exaBoxNode(get_gcontext())
                    if not _node.mIsConnectable(aHost=_dom0, aTimeout=5):
                        ebLogError(f"The Dom0 {_dom0} is not connectable.")
                        return ebError(0x0421)
                    ebLogInfo(f"{_vmcmd} operation triggered for Dom0 - {_dom0} : DomU {_domU}")

                    _consoleobj = serialConsole(self, aOptions)
                    if _vmcmd == "create_console_ssh":
                        _consoleobj.mCreateSSH(_dom0, _domU)
                    elif _vmcmd == "console_history":
                        _consoleobj.mCaptureConsoleHistory(_dom0, _domU)
                    elif _vmcmd == "delete_console_ssh":
                        _consoleobj.mRemoveSSH(_dom0, _domU)

            return _rc

        #
        # VM sshkey management
        #
        if _vmcmd in ['resetkey', 'rescuekey', 'addkey', 'deletekey', 'listkey']:
            if not _vmid:
                # If _vmid is not provided assume _all_ VMs - the vm list in the json payload has precendence
                _vmid == '_all_'

            if _vmcmd in [ 'addkey','deletekey','listkey']:
                self.mManageVMSSHKeys(_vmcmd, _vmid, aOptions,aMode=False)
            else:
                self.mManageVMSSHKeys(_vmcmd, _vmid, aOptions)

            self.mSyncKeysOverNetworkSend(aOptions, aForceFullSync=True)
            self.mSyncKVDBOverNetworkSend(aOptions)

            return 0

        if _vmcmd == 'sshkey':
            if not _vmid:
                ebLogError('VM Id required for vm_cmd sshkey to be performed')
                return ebError(0x0401)
            _found = False
            _rc    = 0
            _sshkey_str = 'echo "%s" >> ~/.ssh/authorized_keys'
            #
            # SSH Key (option via file) or payload
            #
            _sshkey = aOptions.sshkey
            if _sshkey:
                f=open(_sshkey)
                _sshkey=f.read()
                f.close()
            else:
                _sshkey = self.__tools_key_public
            if _sshkey is None:
                ebLogError('*** VM add SSH key failed (no SSH key provided')
                return ebError(0x4003)

            for _dom0, _domu in self.mReturnDom0DomUPair():
                if _domu == _vmid or _vmid == '_all_':
                    _node = exaBoxNode(get_gcontext())
                    _node.mConnect(aHost=_domu)
                    _node.mExecuteCmdLog('mkdir -p ~/.ssh ; ls ~/.ssh')
                    if _sshkey:
                        _node.mExecuteCmdLog(_sshkey_str % (_sshkey))
                    else:
                        ebLogWarn('SSHKEY not available')
                    _node.mDisconnect()
                    _found = True
            if not _found:
                ebLogWarn('Unknown VM_ID: '+_vmid)
                return ebError(0x0402)
            if _rc:
                ebLogWarn('VM_CMD: '+_vmcmd+' for VM_ID: '+_vmid+' was not successful')
                return ebError(0x0403)
            return 0

        if _vmcmd == 'ping':
            _data_response = {}
            PING = "ping"
            STATUS = "vmdom0status"

            if not _vmid:
                ebLogError('VM Id required for vm_cmd ping to be performed')
                return ebError(0x0401)
            _found = False
            _rc = ebError(0x0420)        # ERROR_420 : VMID IS NOT VALID
            for _dom0, _domu in self.mReturnDom0DomUPair():

                # Check for _vmid or '_all_'
                if _domu == _vmid or _vmid == '_all_':

                    _data_response[_domu] = {}
                    _node = exaBoxNode(get_gcontext())

                    # Make sure the dom0 is connectable to avoid
                    # ECRA timeouts. We are giving a timeout of 5 seconds,
                    # as this has been proven to work consistently
                    # e.g. on the SLA project
                    if not _node.mIsConnectable(aHost=_dom0, aTimeout=5):
                        ebLogError(f"The Dom0 {_dom0} is not connectable to "
                            f"check the virsh/xen status of {_domu}")
                        return ebError(0x0421)
                    _node.mConnect(aHost=_dom0)

                    # Get VM status from the Dom0
                    _data_response[_domu][STATUS] = self.mGetVMStatus(
                            _node, _domu)

                    # Run ping on VM
                    _vmhandle.mSetOVMCtrl(aCtx=get_gcontext(), aNode=_node)
                    _rc = _vmhandle.mDispatchEvent(_vmcmd, aOptions, _domu)
                    _node.mDisconnect()

                    if _rc > 0:
                        if _rc == 0x0420:        # ERROR_420 : VMID IS NOT VALID
                            ebLogWarn(f"Unknown VM_ID: {_vmid}")
                        ebLogWarn(f"VM_CMD: {_vmcmd} for VM_ID: {_vmid} was not successful")
                        _rc = ebError(_rc)
                        _data_response[_domu][PING] = False
                    else:
                        _data_response[_domu][PING] = True

            # Return reqobj to ECRA
            _reqobj = self.mGetRequestObj()
            if _reqobj is not None:
                _reqobj.mSetData(json.dumps(_data_response))
                _db = ebGetDefaultDB()
                _db.mUpdateRequest(_reqobj)
            else:
                #Console output
                ebLogInfo(json.dumps(_data_response, sort_keys=True, indent=4))
            return _rc

        if _vmcmd in ['shutdown', 'start', 'bounce', 'force_shutdown', 'force_restart']:
            if not _vmid:
                ebLogError('VM Id required for vm_cmd shutdown to be performed')
                return ebError(0x0401)

            #Function to hadle the request of the VM
            def mExecuteCmdVmSP(aDom0, aDomU, aCmd, aErrors):

                _rc = 0

                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=aDom0)
                _handler = self.mCheckConfigOption('vm_handler')

                # Log if VM is pingable
                if not self.mPingHost(aDomU):
                    ebLogWarn('*** DomU (%s) is not pingable for command: %s.' % (aDomU, aCmd))
                else:
                    ebLogInfo('*** DomU (%s) is pingable for command: %s.' % (aDomU, aCmd))

                # Check if VM is running from virt layer (xen or kvm)
                _vm = getHVInstance(aDom0)
                _domain_list = _vm.mGetDomains()
                ebLogTrace(f"Domains listed {_domain_list}")
                if aDomU in _domain_list:
                    ebLogWarn(f'*** DomU {aDomU} is running in Dom0 {aDom0}')
                    _is_running = True
                else:
                    ebLogWarn(f'*** DomU {aDomU} is NOT running in Dom0 {aDom0}')
                    _is_running = False

                # Lets use virsh as default for now due to issues in oedacli and vm_maker layers.
                if (_handler is None or _handler == "virsh"):
                    _vmhandle.mSetOVMCtrl(aCtx=get_gcontext(), aNode=_node)
                    if aCmd == 'start' or (aCmd in ['bounce', "force_restart"] and not _is_running):
                        if aCmd in ['bounce', "force_restart"]:
                            ebLogInfo(f"The vm command is bounce and domU {aDomU} is not up and running. So Proceeding with start of domU.")
                        #
                        # Wait for VM network start to be up
                        #
                        if self.mIsOciEXACC() and self.mIsKVM() and luksCharchannelExistsInDom0(aDom0, aDomU):
                            mSetLuksPassphraseOnDom0Exacc(self, aDom0, aDomU)
                        _rc = self.mRestartVM(aDomU, aVMHandle=_vmhandle)
                    else:
                        #
                        # Dispatch shutdown / bounce
                        #
                        if aCmd == 'force_shutdown' or aCmd == "force_restart":

                            # Sending aCluCtrlObj=None to avoid dom0 lock
                            _rc = _vmhandle.mDispatchEvent("destroy", aOptions, aVMId=aDomU, aCluCtrlObj=None)

                            if aCmd == "force_restart" and _rc == 0:
                                _rc = self.mRestartVM(aDomU, aVMHandle=_vmhandle)

                        else:
                            if aCmd == 'bounce':
                                ebLogInfo(f"The vm command is bounce and domU {aDomU} is up and running.")
                            _rc = _vmhandle.mDispatchEvent(aCmd, aOptions, aVMId=aDomU, aCluCtrlObj=self)

                        if _rc in [ 0x0411 ]:
                            _rc = 0
                            ebLogTrace('*** vmcmd: %s and vmid: %s already shutdown' % (aCmd, aDomU))
                else:
                    # xxx/MR: FIXME
                    # xxx/MR: Temporary w/a until help method is provided and/or this pushed in registry/context
                    _vm.setExtraConfig(self.mGetConfigPath(),self.mGetOedaPath())
                    if aCmd == 'start' or (aCmd == 'bounce' and not _is_running):
                        if aCmd in ['bounce', "force_restart"]:
                            ebLogInfo(f"The vm command is bounce and domU {aDomU} is not up and running. So Proceeding with start of domU.")
                        _rc = _vm.startDomU(aDomU)
                    elif aCmd == 'shutdown':
                        _rc = _vm.stopDomU(aDomU)
                        if _rc in [ 0x0411 ]:
                            _rc = 0
                            ebLogTrace('*** vmcmd: %s and vmid: %s already shutdown' % (aCmd, aDomU))
                    elif aCmd == 'force_shutdown':
                        if _is_running:
                            _rc = _vm.mDestroyVM(aDomU)
                        else:
                            ebLogInfo(f'Vm {aDomU} is already shutdown. Do nothing for force_shutdown command.')
                            _rc = 0
                    elif aCmd == 'force_restart':
                        if _is_running:
                            _rc = _vm.mDestroyVM(aDomU)
                        else:
                            ebLogInfo(f'Vm {aDomU} is already shutdown. Continue with start up.')

                        if _rc == 0:
                            _rc = _vm.restartDomU(aDomU)
                    else:
                        _rc = _vm.restartDomU(aDomU)
                _node.mDisconnect()

                # Handle Errors
                if _rc == 0x0420:
                    ebLogWarn('Unknown VM_ID: '+_vmid)
                    aErrors.append({'vm' : aDomU, 'error_code': ebError(0x0420), 'error': 'VMID IS NOT VALID'})
                elif _rc == 0x0410:
                    aErrors.append({'vm' : aDomU, 'error_code': ebError(0x0410), 'error': 'VM ALREADY RUNNING'})
                elif _rc == 0x0411:
                    aErrors.append({'vm' : aDomU, 'error_code': ebError(0x0410), 'error': 'VM EXPECTED TO RUN'})
                elif _rc == 0x0412:
                    aErrors.append({'vm' : aDomU, 'error_code': ebError(0x0412), 'error': 'VMID INVALID OR VM NOT CONFIGURED'})
                elif _rc == 0x0421:
                    aErrors.append({'vm' : aDomU, 'error_code': ebError(0x0421), 'error': 'DOM0 IS NOT CONNECTABLE'})
                elif _rc:
                    aErrors.append({'vm' : aDomU, 'error_code': ebError(0x0403), 'error': 'VM_CMD: '+aCmd+' for VM_ID: '+_vmid+' was not successful'})
                else:
                    aErrors.append({'vm' : aDomU, 'error_code': 0, 'error': 'No Error'})

            #Execute the Subprocesses
            _processes = ProcessManager()
            _rcs = _processes.mGetManager().list()
            # with 50 vm project, the worst time will be 5m * 50vms * 60s = 15000s
            _timeout_vm_shutdown = int(get_gcontext().mGetConfigOptions().get("timeout_vm_cmd_multiprocess_seconds", "15000"))
            if aOptions.jsonconf is not None and aOptions.jsonconf.get('non_rolling_patching'):
                _timeout_vm_shutdown = 300 #5mins

            for _dom0, _domu in self.mReturnDom0DomUPair():
                if _domu in _vmid.split(',') or _vmid == '_all_':
                    _p = ProcessStructure(mExecuteCmdVmSP, [_dom0, _domu, _vmcmd, _rcs], '{0}-{1}'.format(_domu, _vmcmd))
                    _p.mSetMaxExecutionTime(_timeout_vm_shutdown)
                    _p.mSetJoinTimeout(30)
                    _p.mSetLogTimeoutFx(ebLogWarn)
                    _processes.mStartAppend(_p)
            _processes.mJoinProcess()

            if _processes.mGetStatus() == "killed":
                ebLogError('Timeout while executing VM services')
                raise ExacloudRuntimeError(0x0403, 0xA, 'Timeout while executing VM services', aStackTrace=False)

            for _process in _processes.mGetProcessList():
                if _process.exitcode != 0:
                    _msg = "The process {0} exit with code: {1}".format(_process, _process.exitcode)
                    ebLogError(_msg)
                    raise ExacloudRuntimeError(0x0403, 0xA, _msg)

            #Display the errors
            _rcs = list([x for x in _rcs if x['error_code'] != 0])
            _ret = 0
            if _rcs == []:
                ebLogInfo('*** Success on VM Service with cmd: "{0}"'.format(_vmcmd))
            else:
                for _rc in _rcs:
                    ebLogWarn("Error on VM {0}, Error '{1}'".format(_rc['vm'], _rc['error']))
                    _ret = _rc['error_code']
            return _ret

        #
        # VM memory management
        #
        if _vmcmd in ['memset']:
            get_gcontext().mSetRegEntry('ssh_post_fix', 'False')
            return self.mManageVMMemory(_vmcmd, _vmid, aOptions)

        #
        # MVM migration
        #
        if _vmcmd in ['migrate_mvm']:
            # Iterating through mReturnDom0DomUNATPair() since domU NAT name is needed for JSON monitor file
            for _dom0, _domU in self.mReturnDom0DomUNATPair():
                with connect_to_host(_dom0, self.mGetCtx()) as _node:

                    # Update Bondmonitor RPM
                    ebLogInfo(f'BONDING: Updating RPM in: {_dom0}')
                    clubonding.install_bond_monitor_rpm(_node, clubonding.get_bond_monitor_rpm_local_path(), domu=_domU)
                    ebLogInfo(f'BONDING: RPM updated in: {_dom0}')

                    # Removing directory extra_images_dir from Dom0s
                    extra_images_dir = '/EXAVMIMAGES/ExtraImgs'
                    ebLogInfo(f"*** Removing {extra_images_dir} directory in Dom0: {_dom0} ***")
                    _node.mExecuteCmd(f"/bin/rm -rf {extra_images_dir}")
                    ebLogInfo(f"*** Directory {extra_images_dir} removed in Dom0: {_dom0} ***")

                    # Setup Jumbo Frames on bondeth{n} & vmbondeth{n} interfaces only
                    ebLogInfo("*** Setting up Jumbo Frames")
                    _interfaces: List[str] = clubonding.get_node_nics(_node)
                    _bridge_prefix: str = clubonding.BRIDGE_INTERFACE_FMT.format('')
                    _bridges: Set[str] = { _if_name.split('.')[0] for _if_name in _interfaces
                                           if _if_name.startswith(_bridge_prefix) }
                    for _bridge in _bridges:
                        _mtu = clujumboframes.ebMTUSize.JUMBO
                        ebLogInfo(f"*** Setting up {_bridge} MTU to {_mtu} in {_dom0}")
                        clujumboframes.nodeConfigBridgeMtu(_node, _bridge, _mtu)
                    ebLogInfo("*** Jumbo Frames setup complete")

                    # Copy dom0_iptables_setup.sh script and execute it
                    ebLogInfo(f"*** Copying dom0_iptables_setup.sh to Dom0: {_dom0} ***")
                    _remote_iptables_script = '/opt/exacloud/network/dom0_iptables_setup.sh'
                    _node.mExecuteCmd('/bin/mkdir -p /opt/exacloud/network/')
                    _node.mCopyFile('scripts/network/dom0_iptables_setup.sh', _remote_iptables_script)
                    ebLogInfo(f"*** File copied to {_dom0} ***")
                    ebLogInfo(f"Running 'dom0_iptables_setup' script in {_dom0}")
                    _node.mExecuteCmdLog(f"/bin/sh {_remote_iptables_script}")

            # Set shared environment
            self.mSetSharedEnvironment()

            ebLogInfo("*** MVM migration finished.")

    def mHandlerVmsStatesOperation(self):
        ebLogInfo("*** Performing VMs'   States Fetch Operation")
        aOptions = self.mGetArgsOptions()
        _inputjson = aOptions.jsonconf
        rc = 0
        err = ""
        _data_d = {}
        _reqobj = self.mGetRequestObj()
        def validate_input_args():
            if isinstance(_inputjson, dict):
                if "vmsList" in _inputjson and isinstance(_inputjson["vmsList"], list):
                    # Iterate over shallow copy
                    for vm in _inputjson["vmsList"][:]:
                        if not isinstance(vm, dict):
                            ebLogError("VM element must be a dictionary: {0}".format(vm))
                            ebLogWarn("Skipping last element")
                            # Remove from input copy
                            _inputjson["vmsList"].remove(vm)
                else:
                    _msg = "Input jsonconf field must be a dictionary"
                    _response["success"] = "True" if (rc == 0) else "False"
                    _response["error"] = _msg
                    if _reqobj is not None:
                        _db = ebGetDefaultDB()
                        _reqobj.mSetData(json.dumps(_response, sort_keys = True))
                        _db.mUpdateRequest(_reqobj)
                    raise ExacloudRuntimeError(0x0746, 0xA, _msg)
            else:
                _msg = "vmsList in jsonconf must me a list of dictionaries"
                _response["success"] = "True" if (rc == 0) else "False"
                _response["error"] = _msg
                if _reqobj is not None:
                    _db = ebGetDefaultDB()
                    _reqobj.mSetData(json.dumps(_response, sort_keys = True))
                    _db.mUpdateRequest(_reqobj)
                raise ExacloudRuntimeError(0x0746, 0xA, _msg)

        # Validate the input arguments to ensure the input has list "vmsList".
        # "vmsList" is a list of objects EACH of which will have below structure:
        # {
        #     "vmNatHostname" : "<VM_NAT_HOSTNAME>",
        #     "exaunitId" : <EXAUNIT_ID>
        # }
        validate_input_args()

        # We just read the VM_NAT_HOSTNAME and check for SSH PORT connectivity
        # Use the same input object to and add boolean field "portSSH" to the
        # same VM object indicating success (True) or failure(False)

        for vm in _inputjson["vmsList"][:]:
            vm["portSSH"] = self.mCheckSshd(vm["vmNatHostname"], 2, 5)

        ### Update request object with the response payload
        _response = {}
        _response["success"] = "True"
        _response["error"] = err
        _response["output"] = {}
        _response["output"]["vmsList"] = _inputjson["vmsList"]
        ebLogInfo(json.dumps(_response, sort_keys = True))

        if _reqobj is not None:
            _db = ebGetDefaultDB()
            _reqobj.mSetData(json.dumps(_response, sort_keys = True))
            _db.mUpdateRequest(_reqobj)
        else:
            ebLogWarn("Request object is null")

        return 0

    def mHandlerXsConfigOperation(self):
        ebLogInfo("*** Perform Xs Config Operation")
        aOptions = self.mGetArgsOptions()
        _utils = self.mGetExascaleUtils()
        return _utils.mExecuteXSConfigOp(aOptions)

    def mAddXsPingTargets(self):
        # Add IPs to cprops.ini file of each domU
        ebLogInfo("Attempting to add ping targets to cprops.ini file.")
        _cpropsPath = "/var/opt/oracle/cprops/cprops.ini"

        # Whitelist nodes in payload, check if they contain 'customer_network' (Create Service) or 
        # 'added_computes' (Add Node)
        _whitelist = []
        _jsonConf = self.mGetArgsOptions().jsonconf
        _customerNetwork = _jsonConf.get('customer_network')
        _reshapedNodeSubset = _jsonConf.get('reshaped_node_subset')

        if _customerNetwork:
            _nodes = _customerNetwork.get('nodes', [])
            for _node in _nodes:
                _client = _node['client']
                _hostname = f"{_client['hostname']}.{_client['domainname']}"
                _whitelist.append(_hostname)
        elif _reshapedNodeSubset:
            _nodes = _reshapedNodeSubset.get('added_computes', [])
            for _node in _nodes:
                _hostname = _node.get('compute_node_hostname')
                _whitelist.append(_hostname)

        # Iterate through each domU
        for _, _domU in self.mReturnDom0DomUPair():
            # Skip if domU is not whitelisted
            if not _domU in _whitelist:
                ebLogInfo(f"Skipped {_domU} since it's not in the payload")
                continue
            # Extract net list from XML
            _domU_machine = self.__machines.mGetMachineConfig(_domU)   
            _domU_net_list = _domU_machine.mGetMacNetworks()
            # Extract gateway from net list
            for _net in _domU_net_list:
                _netcnf = self.__networks.mGetNetworkConfig(_net)
                # Skip if net is not of type 'client'
                if not _netcnf.mGetNetType() == "client":
                    continue
                _gw = _netcnf.mGetNetGateWay()
                ebLogInfo(f"Found gw: {_gw} for {_domU}")
                # Connect to host
                with connect_to_host(_domU, get_gcontext()) as _host:
                    # Skip if cprops.ini file does not exist
                    if not _host.mFileExists(_cpropsPath):
                        ebLogWarn(f"{_cpropsPath} not found in {_domU}.")
                        continue
                    # Patch file using key-value pairs
                    ebLogInfo(f"Adding the following IP to cprops.ini: {_gw}")
                    _keyVal = {"ping_targets": _gw}
                    node_update_key_val_file(_host, _cpropsPath, _keyVal, sep='=')
                    ebLogInfo(f"Successfully patched cprops.ini file for {_domU}")

    def mConfigureHostAccess(self, aOptions):

        if not self.mCheckConfigOption('secure_ssh_all', 'False'):
            _dom0List = self.mReadComputes()
            _step_time = time.time()
            self.mSecureDom0SSH(_dom0List)
            self.mLogStepElapsedTime(_step_time, 'Secure DOM SSH')

        if not self.mCheckConfigOption('secure_ssh_all', 'False'):
            _cellList = self.mReadCellMachines()
            self.mAcquireRemoteLock()
            _step_time = time.time()
            self.mSecureCellsSSH(_cellList)
            self.mLogStepElapsedTime(_step_time, 'Secure Cell SSH config')
            self.mReleaseRemoteLock()

        return 0
    
    def mConfigureVMConsole(self, aOptions, aDom0DomUList=None):

        _remoteFileName = "/opt/exacloud/config_info.json"

        if aDom0DomUList:
            _dom0UList = aDom0DomUList
        else:
            _dom0UList = self.mReturnDom0DomUPair()

        if self.mIsOciEXACC():
            _cloud_type = "exacc"
        else:
            _cloud_type = "exacs"
        _data = {"cloud_type": _cloud_type}

        _json_object = json.dumps(_data, indent=4)
        with NamedTemporaryFile(mode='w', delete=True) as _tmp_file:
            _tmp_file.write(_json_object)
            _tmp_file.flush()
            for _dom0, _ in _dom0UList:
                try:
                    _node = exaBoxNode(get_gcontext())
                    _node.mConnect(aHost=_dom0)
                    if not _node.mFileExists(_remoteFileName):
                        _node.mCopyFile(_tmp_file.name, _remoteFileName)
                finally:
                    _node.mDisconnect()
        return 0

    def mSetScanPropertyFalseOeda(self):
        """
        Set the property ADDSCANIPTOHOSTSFILEONEXC to false in es.properties for
        an oeda request id. Customer provided a non resolvable scan name for
        provisioning in the payload which resulted in oeda adding the entry
        in /etc/hosts. The scan name can get resolved to 3 IP addresses
        which caused issue during dns verification. Now, since the entry
        was added to /etc/hosts, the undo and retry also failed. So, we
        need to make sure that non resolvable scan name is not added to /etc/hosts
        and instead an error is thrown.
        """
        ebLogInfo('Setting ADDSCANIPTOHOSTSFILEONEXC to false in oeda es.properties.')
        _oeda_properties_path = os.path.join(self.mGetOedaPath(),'properties','es.properties')
        if not os.path.exists(_oeda_properties_path):
            ebLogWarn(f"OEDA property path {_oeda_properties_path} doesn't exist. Not setting ADDSCANIPTOHOSTSFILEONEXC to false.")
            return
        _err_msg = "ADDSCANIPTOHOSTSFILEONEXC property could not be set to false in {0} due to an error. Output is {1}. Error is {2}."
        _cmd_prop_exists = f"/bin/grep -q ADDSCANIPTOHOSTSFILEONEXC {_oeda_properties_path}"
        _rc, _i, _o, _e = self.mExecuteLocal(_cmd_prop_exists)
        if _rc != 0:
            ebLogWarn(f'ADDSCANIPTOHOSTSFILEONEXC property does not exist in {_oeda_properties_path}. Adding it to file.')
            try:
                with open(_oeda_properties_path, "a") as _file_object:
                    _file_object.write("ADDSCANIPTOHOSTSFILEONEXC=false")
            except Exception as ex:
                ebLogError(f"ADDSCANIPTOHOSTSFILEONEXC property could not be set to false in {_oeda_properties_path} due to an error. Error is {ex}.")
                return
        else:
            _cmd_change_prop = f"/bin/sed 's/^ADDSCANIPTOHOSTSFILEONEXC=true/ADDSCANIPTOHOSTSFILEONEXC=false/' -i {_oeda_properties_path}"
            _rc, _i, _o, _e = self.mExecuteLocal(_cmd_change_prop)
            if _rc != 0:
                ebLogError(_err_msg.format(_oeda_properties_path, _o, _e))
                return
        _cmd_verify_prop = f"/bin/grep -q 'ADDSCANIPTOHOSTSFILEONEXC=false' {_oeda_properties_path}"
        _rc, _i, _o, _e = self.mExecuteLocal(_cmd_verify_prop)
        if _rc != 0:
            ebLogError(_err_msg.format(_oeda_properties_path, _o, _e))
        else:
            ebLogInfo(f'ADDSCANIPTOHOSTSFILEONEXC property is set to false in {_oeda_properties_path}.')

    def mHandlerCreateService(self):
        aOptions = self.mGetArgsOptions()

        self.mDom0UpdateCurrentOpLog(aOptions, "Create Service", "started")
        stepdriver = csDriver(self, aOptions)
        _rc = stepdriver.handleRequest()
        self.mDom0UpdateCurrentOpLog(aOptions, "Create Service", "ended")
        return _rc

    def mGridDeconfig(self):
        _count = 1 
        for _, _domU in self.mReturnDom0DomUPair(): 

            _node = exaBoxNode(get_gcontext()) 
            _node.mConnect(aHost=_domU)

            if _count == 1:
                _cmd = '/u01/app/19.0.0.0/grid/crs/install/rootcrs.sh -deconfig -force' 
                ebLogInfo("%s: *** Running command: %s" % (datetime.datetime.now(), _cmd)) 
                _, _o, _ = _node.mExecuteCmd(_cmd)
            else:
                _cmd = '/u01/app/19.0.0.0/grid/crs/install/rootcrs.sh -deconfig -force -lastnode'
                ebLogInfo("%s: *** Running command: %s" % (datetime.datetime.now(), _cmd)) 
                _, _o, _ = _node.mExecuteCmd(_cmd)

            _out = _o.read() 
            ebLogInfo("%s: *** Grid deconfig output::\n %s" % (datetime.datetime.now(), _out)) 

            _node.mExecuteCmd("chown grid:oinstall /u01/app/19.0.0.0/grid/cv/remenv/exectask") 
            _node.mExecuteCmd("chown grid:oinstall /u01/app/19.0.0.0/grid") 
            _node.mExecuteCmd("chown grid:oinstall /u01/app/19.0.0.0/grid/crs/config/rootconfig.sh")
            _node.mExecuteCmd("chown grid:oinstall /u01/app/19.0.0.0/grid/crs/install/crsconfig_params")
            _node.mExecuteCmd("chown grid:oinstall -R /u01/app/grid/diag/crs/")
            _now = time.strftime("%H:%M:%S") 
            _host = _domU.split('.')[0]  
            _node.mExecuteCmd(f"rm -rf /u01/app/grid/crsdata/@global ")
            _node.mExecuteCmd(f"rm -rf /u01/app/grid/crsdata/{_host}")
            _node.mExecuteCmdLog(f"ls -l /u01/app/grid/crsdata/")
            _node.mExecuteCmd(f"rm -rf /u01/app/19.0.0.0/grid/gpnp/{_host}*")
            _node.mExecuteCmdLog("ls -l /u01/app/19.0.0.0/grid/gpnp/")
            _node.mExecuteCmdLog("rm -f /etc/oracle/ocr.loc*")
            _node.mExecuteCmdLog("rm -f /etc/oracle/olr.loc*")
            _node.mExecuteCmdLog("ls -l /etc/oracle/")
            _node.mExecuteCmd("sed -i '/grid/d' /etc/oratab")
            _node.mExecuteCmd(f"chmod 775 /u01/app/grid/diag/crs/{_host}/crs/*")

            _node.mDisconnect()                                                                                                                                              
            _count = _count + 1                                                                                                                                              

        for _cell_name in list(self.mReturnCellNodes().keys()):
            with connect_to_host(_cell_name, get_gcontext()) as _cell_node:
                _cluster = self.mGetClusters().mGetCluster()
                _cluster_groups = _cluster.mGetCluDiskGroups()
                for _dgid in _cluster_groups:
                    _dgName = self.mGetStorage().mGetDiskGroupConfig(_dgid).mGetDgName()
                    _cmd = f"cellcli -e DROP GRIDDISK ALL prefix={_dgName} force"
                    _cell_node.mExecuteCmdLog(_cmd)

    def mHandlerVMGIDeconfig(self):

        # start the VM if in shutdown state.
        for _dom0, _domU in self.mReturnDom0DomUPair():      

            _n = exaBoxNode(get_gcontext()) 
            if not _n.mIsConnectable(aHost=_domU):
                with connect_to_host(_dom0, get_gcontext()) as _node: 
                    _vmhandle = ebVgLifeCycle() 
                    _vmhandle.mSetOVMCtrl(aCtx=get_gcontext(), aNode=_node)     
                    if self.mIsOciEXACC() and self.mIsKVM() and luksCharchannelExistsInDom0(_dom0, _domU):
                        mSetLuksPassphraseOnDom0Exacc(self, _dom0, _domU)
                    _rc = self.mRestartVM(_domU, aVMHandle=_vmhandle) 
                    if _rc != 0:         
                        raise ExacloudRuntimeError(0x0441, 0xA, 'Failed to perform vm restart')   

        self.mGridDeconfig()  
                                                                                                                                                                             
        for _dom0, _domU in self.mReturnDom0DomUPair(): 

            _node = exaBoxNode(get_gcontext())  
            _node.mConnect(aHost=_dom0)  

            _cmd = "virsh list --all; ls -ld /etc/libvirt/qemu/{0}*.xml".format(_domU)  
            ebLogInfo("%s: *** Running command: %s" % (datetime.datetime.now(), _cmd)) 
            _node.mExecuteCmdLog(_cmd)

            _cmd = "cp -p /etc/libvirt/qemu/{0}.xml /root/{0}.xml".format(_domU)   
            ebLogInfo("%s: *** Running command: %s" % (datetime.datetime.now(), _cmd)) 
            _node.mExecuteCmd(_cmd)

            _cmd = "virsh shutdown {0}".format(_domU) 
            ebLogInfo("%s: *** Running command: %s" % (datetime.datetime.now(), _cmd)) 
            _node.mExecuteCmdLog(_cmd)  
            if _node.mGetCmdExitStatus() != 0:   
                ebLogError(f"Failed to delete the VM {_domU}")  
                raise ExacloudRuntimeError(0x0117, 0xA, 'Timeout Exception')

            _cmd = "virsh list --all; ls -ld /etc/libvirt/qemu/{0}*.xml".format(_domU) 
            ebLogInfo("%s: *** Running command: %s" % (datetime.datetime.now(), _cmd))
            _node.mExecuteCmdLog(_cmd)   

            _cmd = "cp /etc/libvirt/qemu/{0}*.xml /root/{0}.{1}.xml".format(_domU,str(datetime.datetime.now()).replace(" ","T"))  
            ebLogInfo("%s: *** Running command: %s" % (datetime.datetime.now(), _cmd)) 
            _node.mExecuteCmd(_cmd)     

            _node.mDisconnect()

    def mHandlerDeleteService(self):
        aOptions = self.mGetArgsOptions()

        self.mDom0UpdateCurrentOpLog(aOptions, "Delete Service", "started")

        if not self.mIsXS() and not self.__run_all_undo_steps and "ESTP_INSTALL_CLUSTER" in aOptions.steplist:
            self.mDom0UpdateCurrentOpLog(aOptions, "Delete Service", "ended")
            return 0

        aOptions.undo = True
        stepdriver = csDriver(self, aOptions)
        _rc = stepdriver.handleRequest()
        self.mDom0UpdateCurrentOpLog(aOptions, "Delete Service", "ended")
        return _rc

    def mHandlerVMGIReshape(self):
        aOptions = self.mGetArgsOptions()
        self.mDom0UpdateCurrentOpLog(aOptions, "Reshape Operation", "started")

        _step_time = time.time()
        # Bypass standard network checks
        if self.mCheckDom0NetworkType():
            if self.mCheckConfigOption('reset_net_mapping','True'):
                self.mResetDom0NetworkMapping()
        self.mLogStepElapsedTime(_step_time, 'Check Network Discovery Completed')

        self.mPublishNewNodeKey(aOptions)
        _reshape = ebCluReshapeCompute(self, aOptions)
        _reshape.mReshapeVMGI(aOptions)

        self.mDom0UpdateCurrentOpLog(aOptions, "Reshape Operation", "completed")

    def mHandlerEnableCOS(self):
        self.mAcquireRemoteLock()
        self.mModifyService()
        self.mReleaseRemoteLock()

    def mHandlerCheckTimeDrift(self):
        _pchecks = ebCluPreChecks(self)
        return _pchecks.mCheckVMTimeDrift()

    def mHandlerGetHWAlerts(self):
        aOptions = self.mGetArgsOptions()
        _pchecks = ebCluPreChecks(self)
        _pchecks.mFetchHardwareAlerts(aOptions, aStep="ESTP_PREVM_CHECKS")
        return 0

    def mHandlerScheduleOperations(self):
        aOptions = self.mGetArgsOptions()
        _schedobj = ebCluScheduleManager(self)
        return _schedobj.mHandleRequest(aOptions)

    def mHandlerInstallDB(self):
        aOptions = self.mGetArgsOptions()
        aCmd = self.mGetCmd()

        self.mUpdateExacliPwd(aOptions)

        _dpairs = self.mReturnDom0DomUPair()
        _domu_list = [ _domu for _ , _domu in _dpairs]
        if not self.mCheckCrsUp(_domu_list[0]):
            self.mExecuteCRSReboot(aRequestedOperations=["start"])

        """
        X. Install Database Software
        Y. Relink Database with RDS
        Z. Create ASM Diskgroups
        W. Create Databases
        """
        self.mUpdateStatus('db_install')

        ebLogInfo('IN db_install PATH')
        _pchecks = ebCluPreChecks(self)
        _pchecks.mCheckScanName()

        if self.IsZdlraProv() or not self.__enable_nid_starterdb:
            if not self.mCheckConfigOption('zdlra_starterdb_step') == 'True':
                ebLogInfo('StarterDB test for Zdlra is not supported !')
                ebLogInfo('Performing addkey op !')
                self.mPostDBSSHKeyPatching(aOptions, 'addkey', ['root','oracle','opc'])
                return 0

            step_list = [ OSTP_PREDB_INSTALL, OSTP_INSTALL_DB, OSTP_RELINK_DB, OSTP_CREATE_DB,
                      OSTP_CREATE_PDB, OSTP_END_INSTALL]
            db_step_list = [ OSTP_INSTALL_DB, OSTP_RELINK_DB, OSTP_CREATE_DB, OSTP_CREATE_PDB]

            #Run Cluster Integrity Tests
            if self.__clu_verify:
                _pchecks.mCheckClusterIntegrity(False)

            _rc = self.mExecuteInstallStarterDB(aCmd, aOptions, self.__oeda_path, step_list, db_step_list)

            #Run Cluster Integrity Tests
            if self.__clu_verify:
                _pchecks.mCheckClusterIntegrity(True)

        else:
            if not self.mCheckConfigOption("force_starter_db_install") == 'True':
                ebLogInfo("StarterDB flow disabled through config option: 'force_starter_db_install'")
                return 0

            # NID Environment check
            if not self.mCheckNIDEnvironment():
                # checking if recreate is necessary
                ebLogInfo('Verifying GI/Clusterware version')
                _mrecreategridrequired = self.mCheckGridVersion()
                if _mrecreategridrequired is True:
                    ebLogInfo("*** Recreate service necessary. GI/Clusterware not at the required version ***")
                    return ebError(0x0725)

                ebLogInfo("*** Updating the environment to support NID")
                # 2- patching the vms
                ebLogInfo("*** Update bits in the domU image")
                _rt = self.mUpdateDBNIDBits(aOptions)
                if _rt:
                    # add error code in case of NID update
                    return ebError(0x0510)

                self.mExecuteInstallPostGINID()

            step_list = [OSTP_PREDB_INSTALL, OSTP_DBNID_INSTALL, OSTP_POSTDB_INSTALL, OSTP_DG_CONFIG, OSTP_END_INSTALL]
            _enable_exachk = self.mCheckConfigOption('enable_exachk')
            if _enable_exachk:
                if 'post_create_starterdb' in _enable_exachk and _enable_exachk['post_create_starterdb'] == 'True':
                    step_list.insert(step_list.index(OSTP_END_INSTALL), OSTP_INSTALL_EXCHK)

            db_step_list = []

            _rc = self.mExecuteInstallStarterDBNID(aCmd, aOptions, self.__oeda_path, step_list, db_step_list)

            # Only Setup ATP Backup Listener if StarterDBNID is successful
            if _rc == 0:
                if self.__exabm and self.isATP() and self.mCheckClusterNetworkType():
                    for _dom0, _domU in self.mReturnDom0DomUPair():
                        ebLogInfo("*** ATP etc/hosts on %s ***" % _domU)
                        AtpAddScanname2EtcHosts(None, self.__ATP, _domU).mExecute()
                ### Only need to be run on one domU
                    ebLogInfo("*** ATP Listener on %s ***" % _domU)
                    AtpSetupSecondListener(None, self.__ATP, self.mReturnDom0DomUPair(), self.mGetMachines(), self.mGetNetworks(), None, self.mGetClusters, aOptions).mExecute()
                    AtpSetupASMListener(None, self, None).mExecute()
                ### All the Above is for EXABM, below is EXACC-ATP hook (VGE:NEED TO BE EXTERNALIZED)
                if self.isATP() and self.__ociexacc:
                    _node = exaBoxNode(get_gcontext())
                    # First domU
                    _all_domU = list(map(operator.itemgetter(1),self.mReturnDom0DomUPair()))
                    _first_domU = _all_domU[0]
                    _node.mConnect(aHost=_first_domU)
                    _listener_info = ebExaCCAtpListener.sExtractInfoFromDomU(_node)
                    if self.__debug:
                        ebLogDebug("ExaCCAtp Listener Info: {}".format(_listener_info))

                    if not _listener_info:
                        ebLogWarn("Error on obtaining ATP Listener info, skip setup")
                        _node.mDisconnect()
                    else:
                        # VGE: Need to be refactored
                        _root_commands, _grid_commands, _final_grid_commands, _final_root_commands = ebExaCCAtpListener.sGenerateListenerCommands(**_listener_info)
                        if self.__debug:
                            ebLogDebug("ExaCCAtp Commands: root({}), grid({}), final_grid({}), final_root_commands({})".format(_root_commands, _grid_commands, _final_grid_commands, _final_root_commands))

                        for _cmd in _root_commands:
                            _node.mExecuteCmdLog(_cmd)
                        # Reconnect as Grid
                        _node.mDisconnect()
                        _node = exaBoxNode(get_gcontext())
                        _node.mSetUser('grid')
                        _node.mConnect(aHost=_first_domU)
                        for _cmd in _grid_commands:
                            _node.mExecuteCmdLog(_cmd)

                        # Register ASM Backup listener on all DomU,
                        # reuse connected first domU then for loop on other
                        ebExaCCAtpListener.sRegisterListenerOnBKUPOnly(_node,
                                            _listener_info['aListenerPort'])
                        _node.mDisconnect()
                        for _other_domU in _all_domU[1:]:
                            _node = exaBoxNode(get_gcontext())
                            _node.mSetUser('grid')
                            _node.mConnect(aHost=_other_domU)
                            ebExaCCAtpListener.sRegisterListenerOnBKUPOnly(_node,
                                            _listener_info['aListenerPort'])
                            _node.mDisconnect()

                        # Now execute final grid commands on first domU
                        _node = exaBoxNode(get_gcontext())
                        _node.mSetUser('grid')
                        _node.mConnect(aHost=_first_domU)
                        for _cmd in _final_grid_commands:
                            _node.mExecuteCmdLog(_cmd)

                        # Then connect as root to bounce cluster
                        _node.mDisconnect()
                        _node = exaBoxNode(get_gcontext())
                        _node.mConnect(aHost=_first_domU)
                        for _cmd in _final_root_commands:
                            _node.mExecuteCmdLog(_cmd)
                        _node.mDisconnect()
                        # Sleep 2 min as I saw that clusterware take a bit of time here to mount ACFS
                        time.sleep(120)

            ebLogInfo("*** _rc from mExecuteInstallStarterDBNID: %s ***" % str(_rc))

            #Run Cluster Integrity Tests
            if self.__clu_verify:
                _pchecks = ebCluPreChecks(self)
                _pchecks.mCheckClusterIntegrity(True)

        return _rc
    
    def mHandlerDeleteDB(self):
        aOptions = self.mGetArgsOptions()
        aCmd = self.mGetCmd()

        _dpairs = self.mReturnDom0DomUPair()
        _domu_list = [ _domu for _ , _domu in _dpairs]
        if not self.mCheckCrsUp(_domu_list[0]):
            self.mExecuteCRSReboot(aRequestedOperations=["start"])

        if self.IsZdlraProv() and not (self.mCheckConfigOption('zdlra_starterdb_step') == 'True'):
            ebLogInfo('StarterDB test for Zdlra is not supported !')
            return 0

        # Dont run delete db for starterDB unless force_starter_db_install is set to True
        # if flow is addb_delete run flow normally
        elif not self.mCheckConfigOption("force_starter_db_install") == 'True':
            ebLogInfo("StarterDB flow disabled through config option: 'force_starter_db_install'")
            return 0

        # For starter db nid
        #   the delete db is the same as the additional
        if not self.IsZdlraProv() and self.__enable_nid_starterdb:
            _elastic_op,_nodelist = self.mCheckNodeListParam()
            if _elastic_op is True:
                _dom0U = self.mCheckNodeList(_nodelist)
                _db_install = self.mCheckInstallDB(_dom0U)
                if _db_install is False:
                    return ebError(0x0514)

        self.mUpdateStatus('db_delete')
        step_list = [OSTP_PREDB_DELETE, OSTP_CREATE_PDB, OSTP_CREATE_DB, OSTP_RELINK_DB,
                     OSTP_INSTALL_DB, OSTP_END_INSTALL]
        db_step_list = [OSTP_CREATE_PDB, OSTP_CREATE_DB, OSTP_RELINK_DB,OSTP_INSTALL_DB]
        _rc = self.mExecuteDeleteStarterDB(aCmd, aOptions, self.__oeda_path, step_list, db_step_list)

        if self.__factoryPreprovReconfig.mCreatePreprovUtil().mIsPreprovEnv():
            if self.mCheckConfigOption('reconfig_backup', 'True'):
                self.__factoryPreprovReconfig.mCreateBackupTool().mBackupAll()
            else:
                ebLogInfo("Reconfig backup is disabled")

        if _rc == 0 and self.IsZdlraProv() and not self.mCheckConfigOption('zdlra_retain_dbhomes', 'True'):
            ebLogInfo("*** Attempting to remove db homes after starterdb delete operation !")
            _ddp = self.mReturnDom0DomUPair()
            for _dom0, _domU in _ddp:

                _host_node = exaBoxNode(get_gcontext())
                _host_node.mConnect(aHost=_dom0)

                _host_node.mExecuteCmd("/opt/exadata_ovm/vm_maker --list --disk-image --domain {} | grep '/db[^/]*\.img'".format(_domU))
                _db_homes_attached = (_host_node.mGetCmdExitStatus() == 0)
                if _db_homes_attached:
                    _exadiskobj = exaBoxKvmDiskMgr(self)
                    _exadiskobj.mUnmountOedaDbHomes(_dom0,_domU, _host_node)
                    ebLogInfo("*** Removed OEDA DBHomes from domU: {}".format(_domU))
                else:
                    ebLogInfo("*** OEDA DBHomes were already removed from domU: {}".format(_domU))

                _host_node.mDisconnect()

        return _rc


    def mHandlerSetupFreshDG(self):
        aOptions = self.mGetArgsOptions()
        aCmd = self.mGetCmd()

        # 1 This command is used to create DB files that are necessary for setting up dataguard.
        #   Dataguard setup can be:
        #   - Fresh setup (dg_fresh_setup command), for cases when there is _no_ starter DB on the Exaunit.
        #   - Setup (dg_setup command), for cases when starter DB is already present.
        # 2 The flow for fresh setup is:
        #   - Create starter DB
        #   - Delete starter DB, leave out ASM disk groups, security patches etc.
        self.mUpdateStatus('dg_fresh_setup')
        self.__standbydb = "-standbydb"

        # Create starter DB
        if not self.__enable_nid_starterdb:
            step_list = [ OSTP_PREDB_INSTALL, OSTP_INSTALL_DB, OSTP_RELINK_DB, OSTP_CREATE_ASM, OSTP_CREATE_DB,
                      OSTP_CREATE_PDB, OSTP_POSTDB_INSTALL]
            db_step_list = [ OSTP_INSTALL_DB, OSTP_RELINK_DB, OSTP_CREATE_ASM, OSTP_CREATE_DB, OSTP_CREATE_PDB]
            _rc = self.mExecuteInstallStarterDB(aCmd, aOptions, self.__oeda_path, step_list, db_step_list)
            if _rc:
                return _rc
            # Delete instance only
            step_list = [OSTP_CREATE_PDB, OSTP_CREATE_DB]
            db_step_list = [OSTP_CREATE_PDB, OSTP_CREATE_DB]
            _rc = self.mExecuteDeleteStarterDB(aCmd, aOptions, self.__oeda_path, step_list, db_step_list)
            if _rc:
                return _rc
        else:
            # create starter db steps
            step_list = [OSTP_PREDB_INSTALL, OSTP_DBNID_INSTALL, OSTP_POSTDB_INSTALL]
            db_step_list = []

            _rc =  self.mExecuteInstallStarterDBNID(aCmd, aOptions, self.__oeda_path, step_list, db_step_list)
            if _rc:
                return _rc
            step_list = [OSTP_CREATE_DB]
            for _, _domu in self.mReturnDom0DomUPair():
                _node = exaBoxNode(get_gcontext())
                _node.mSetUser('oracle')
                _node.mConnect(aHost=_domu)
                _cmd_str = "cat /etc/oratab | grep '^%s:' " % self.__dbname
                _, _out, _ = _node.mExecuteCmd(_cmd_str)
                _out = _out.readlines()
                if (_out is not None and len(_out) != 0):
                    _oracle_home = _out[0].split(":")[1].strip()
                else :
                    _node.mDisconnect()
                    return 1
                _db_passwd = aOptions.jsonconf['dbParams']['passwd']
                _db_passwd = self.mGetWalletViewEntry(_db_passwd)
                _cmd_str = "export ORACLE_HOME=" + _oracle_home + "; cd $ORACLE_HOME; $ORACLE_HOME/bin/dbca -deleteDatabase -silent -sourceDB " + self.__dbname + " -continueOnNonFatalErrors true -sysDBAPassword " + "\"" + _db_passwd + "\" -sysDBAUserName sys"
                _node.mExecuteCmdLog(_cmd_str)
                _rc = _node.mGetCmdExitStatus()
                _node.mDisconnect()
                if self.__debug:
                    ebLogInfo('*** status for cmd : ' + _cmd_str + ' : %s' % (str(_rc)))
                if _rc:
                    return _rc
                break

        # Run external script to create the marker file. This will be used by
        # DG orchestration.
        _rc, _cmd = self.mRunScript(aType='*',aWhen='dg_fresh_setup',aStatusAbort=True)
        if _rc:
            self.mCopyOCDELogFile()
            ebLogError('*** Error ('+str(_rc)+') caught during scripts execution for cmd: '+_cmd)
            return ebError(0x0502) # ERROR_502 : OCDE EXECUTION FAILED

        # Report success
        ebLogInfo('*** Exacloud Operation Successful : DG fresh setup completed')
        self.mUpdateStatusOEDA(True, OSTP_CREATE_DB, step_list, 'DG fresh setup completed')

    def mHandlerSetupDGRepeat(self):

        self.mUpdateStatus('dg_repeat_setup')
        self.__standbydb = "-standbydb"

        ebLogInfo('*** Exacloud Operation : DG repeat setup... ')
        _rc, _cmd = self.mRunScript(aType='*',aWhen='dg_repeat_setup',aStatusAbort=True)
        if _rc:
            self.mCopyOCDELogFile()
            ebLogError('*** Error ('+str(_rc)+' caught during scripts execution for cmd: '+_cmd)
            return ebError(0x0502) # ERROR_502 : OCDE EXECUTION FAILED

        # Report success
        ebLogInfo('*** Exacloud Operation Successful : DG repeat setup completed')
        self.mUpdateStatus('DG repeat setup completed')
    
    def mHandlerIORMOper(self):
        aOptions = self.mGetArgsOptions()
        ebLogInfo('Performing IORM operation')
        _iormobj = ebCluResManager(self, aOptions)
        return _iormobj.mClusterIorm(aOptions)

    def mHandlerDataguardOper(self):
        aOptions = self.mGetArgsOptions()
        ebLogInfo('*** Performing dataguard operation')
        _dataguard = ebCluDataguardManager(self)
        return _dataguard.mClusterDataguard(aOptions)

    def mHandlerCloneSparse(self):
        aOptions = self.mGetArgsOptions()
        aCmd = self.mGetCmd()
        ebLogInfo('*** Performing %s operation' %(aCmd))
        _sparseobj = ebCluSparseClone(self, aOptions)
        return _sparseobj.mClusterSparseclone(aOptions)
    
    def mHandlerGetImageInfo(self):
        aOptions = self.mGetArgsOptions()
        ebLogInfo('*** Fetching imageinfo details from cluster')
        _data_d = {}
        _err = None
        _rc = -1
        def _mUpdateRequestData(rc, aData, err):
            """
            Updates request object with the response payload
            """
            _reqobj = self.mGetRequestObj()
            _response = {}
            _response["success"] = "True" if (rc == 0) else "False"
            _response["error"] = err
            _response["output"] = aData
            if _reqobj is not None:
                _db = ebGetDefaultDB()
                _reqobj.mSetData(json.dumps(_response, sort_keys = True))
                _db.mUpdateRequest(_reqobj)
            elif aOptions.jsonmode:
                ebLogJson(json.dumps(_response, indent=4, sort_keys = True))

        _dom0s, _domUs, _cells, _ = self.mReturnAllClusterHosts()

        for _dom0 in _dom0s:
            _imagever = self.mGetImageVersion(_dom0)
            _data_d["dom0"]= _imagever
            break

        ebLogVerbose('*** Imageinfo of dom0 is %s' %(_data_d["dom0"]))

        for _cell in _cells:
            _imagever = self.mGetImageVersion(_cell)
            _data_d["cell"]= _imagever
            break

        ebLogVerbose('*** Imageinfo of cell is %s' %(_data_d["cell"]))

        _rc = 0

        _mUpdateRequestData(_rc,_data_d,_err)

    def mHandlerRestartRemoteEc(self, aOptions=None):
        if not aOptions:
            aOptions = self.mGetArgsOptions()

        _err = None
        _rc = -1
        _valid_host = []

        def _mUpdateRequestData(rc, err):
            """
            Updates request object with the response payload
            """
            _reqobj = self.mGetRequestObj()
            _response = {}
            _response["success"] = "True" if (rc == 0) else "False"
            _response["error"] = err
            if _reqobj is not None:
                _db = ebGetDefaultDB()
                _reqobj.mSetData(json.dumps(_response, sort_keys = True))
                _db.mUpdateRequest(_reqobj)
            elif aOptions.jsonmode:
                ebLogJson(json.dumps(_response, indent=4, sort_keys = True))

        if not self.__ociexacc:
            ebLogInfo('*** Doing nothing as it is not OCI EXACC environment')
            _mUpdateRequestData(_rc,_err)
            return _rc
        _valid_host.append('localhost')
        _remote_cps = self.mCheckConfigOption('remote_cps_host')
        if _remote_cps:
            _valid_host.append(_remote_cps)

        # the command to restart the remoteec service
        _cmd_restart_remoteec = '/usr/bin/sudo /usr/bin/systemctl restart remotemgmtagent'
        for _host in  _valid_host:
            _node = exaBoxNode(get_gcontext(), aLocal= False if _host != 'localhost' else True)
            _node.mConnect(aHost=_host)
            _i, _o, _e = _node.mExecuteCmd(_cmd_restart_remoteec)
            _out = _o.readlines()
            _rc = _node.mGetCmdExitStatus()
            if _rc != 0:
                ebLogError('Failed to restart Remoteec: %s' %(_out))
                _mUpdateRequestData(_rc,_out)
                _node.mDisconnect()
                return _rc
            else:
                ebLogInfo('*** Restarted Remoteec Successfully: %s' %(_out))
                _node.mDisconnect()
        _mUpdateRequestData(_rc,_err)
        return _rc

    def mGetGiImageList(self):
        """
        Returns the Giimagelist for multiimagesupport env.
        """
        
        _repo_inventory_data = self.mGetRepoInventory()['grid-klones']
        _exacs_versions = []
        _atp_versions = []
        # images which are no more used in exacc
        _expired_images = self.mCheckConfigOption('exacc_skip_gi_list')

        try:
            for __klone in _repo_inventory_data:
                if ('service' in __klone.keys()) and ('version' in __klone.keys()):
                    if __klone['service'][0] == "EXACS" and __klone['version'] not in _expired_images:
                        _exacs_versions.append(__klone['version'])
                    if __klone['service'][0] == "ATP":
                        _atp_versions.append(__klone['version'])
        except Exception as e:
            _err_msg = f"Failed to create giimagelist payload due to error : {e}"
            ebLogError(_err_msg)
            raise ExacloudRuntimeError(aErrorMsg=_err_msg)
        
        _giimagelist_details = {
            "giimagelist": {
            "EXACS": _exacs_versions,
            "ATP": _atp_versions
            }
        }

        return _giimagelist_details

    def mHandlerListEXACCPatchPayloads(self, aOptions=None):
        """
        List CPS and other infra component's PatchPayloads directories
        """

        if not aOptions: 
            aOptions = self.mGetArgsOptions()

        _data_d = {}
        _errString = None
        _rc = -1
        ebLogInfo('*** Get list of exacc infra patching payloads directories')

        def _mUpdateRequestData(aRetCode, aData, aErr):
            """
            Updates request object with the response payload
            """
            _reqobj = self.mGetRequestObj()
            _response = {}
            if aErr:
                _response["error"] = aErr
            _response["patchlist"] = aData
            if _reqobj is not None:
                _db = ebGetDefaultDB()
                _reqobj.mSetData(json.dumps(_response, sort_keys = True))
                _db.mUpdateRequest(_reqobj)
            elif aOptions.jsonmode:
                ebLogJson(json.dumps(_response, indent=4, sort_keys = True))

        _result = {}
        _rc = 0
        _target_type = None
        if self.__ociexacc:
            if aOptions is not None and aOptions.jsonconf is not None and 'target_type' in aOptions.jsonconf.keys():
                _target_type = aOptions.jsonconf.get('target_type')
                if str(_target_type).upper() == "CPS":
                    _repository_root = self.mCheckConfigOption("repository_root")
                    if _repository_root is None:
                        _repository_root = '/u01/downloads'
                    _repo_cpsos_download_location = "{0}/{1}".format(_repository_root, "cpsos")
                    _rc, _errString, _result = self.mGetFolderList(_target_type, _repo_cpsos_download_location)
                elif _target_type is None or _target_type == "" or str(_target_type).upper() == "GI-IMAGES":
                    _repo_download_location = self.mCheckConfigOption('ociexacc_exadata_patch_download_loc')
                    if _repo_download_location is None:
                        _repo_download_location = '/u01/downloads/exadata'
                    _rc, _errString, _result = self.mGetFolderList("exadata", _repo_download_location)
                else:
                    _rc = -1
                    _errString = "Target type {0} is not valid. Valid target types: cps".format(_target_type)
            else:
                _repo_download_location = self.mCheckConfigOption('ociexacc_exadata_patch_download_loc')
                if _repo_download_location is None:
                    _repo_download_location = '/u01/downloads/exadata'
                _rc, _errString, _result = self.mGetFolderList("exadata", _repo_download_location)

            self.mLoadRepoInventory()
            if not self.mGetRepoInventory():
                raise ExacloudRuntimeError(0x0111, 0xA, 'No image repository configured. Check repository_root value at exabox.conf')
            if _target_type and str(_target_type).upper() == "GI-IMAGES":
                ebLogInfo("*** Getting the giimagelist ***")
                _giimagelist_details = self.mGetGiImageList()
                ebLogInfo("The giimagelist is: %s\n" % (json.dumps(_giimagelist_details, indent=3)))
                _result.update(_giimagelist_details)

        ebLogInfo("*** mHandlerListEXACCPatchPayloads is : %s\n" % (json.dumps(_result, indent=3)))
        _mUpdateRequestData(_rc,_result,_errString)

    # CPS sample result
    # {
    #     "patchlist": {
    #         "20.1.4.0.0_201203": {
    #             "details": [
    #                 "DBPatchFile",
    #                 "DomuYumRepository"
    #             ],
    #             "folder": "/u01/downloads/cpsos/20.1.4.0.0_201203/PatchPayloads/20.1.4.0.0.201203/"
    #         },
    #         "20.1.6.0.0_210113": {
    #             "details": [
    #                 "DBPatchFile",
    #                 "DomuYumRepository"
    #             ],
    #             "folder": "/u01/downloads/cpsos/20.1.6.0.0_210113/PatchPayloads/20.1.6.0.0.210113/"
    #         },
    #         "21.1.0.0.0.210104": {
    #             "details": [
    #                 "DBPatchFile",
    #                 "DomuYumRepository"
    #             ],
    #             "folder": "/u01/downloads/cpsos/21.1.0.0.0.210104/PatchPayloads/21.1.0.0.0.210104/"
    #         }
    #     }
    # }
    #
    # NON-CPS (Infra Components) sample result
    # {
    #     "patchlist": {
    #         "20.1.4.0.0.201203": {
    #             "details": [
    #                 "CellPatchFile",
    #                 "DBPatchFile",
    #                 "Dom0YumRepository",
    #                 "DomuYumRepository",
    #                 "SwitchPatchFile"
    #             ],
    #             "folder": "/u01/downloads/exadata//PatchPayloads/20.1.4.0.0.201203/"
    #         },
    #         "20.1.6.0.0.210113": {
    #             "details": [
    #                 "CellPatchFile",
    #                 "DBPatchFile",
    #                 "Dom0YumRepository",
    #                 "DomuYumRepository",
    #                 "SwitchPatchFile"
    #             ],
    #             "folder": "/u01/downloads/exadata//PatchPayloads/20.1.6.0.0.210113/"
    #         },
    #         "21.1.0.0.0.201229": {
    #             "details": [
    #                 "CellPatchFile",
    #                 "DBPatchFile",
    #                 "Dom0YumRepository",
    #                 "DomuYumRepository",
    #                 "SwitchPatchFile"
    #             ],
    #             "folder": "/u01/downloads/exadata//PatchPayloads/21.1.0.0.0.201229/"
    #         },
    #         "21.1.0.0.0.210204": {
    #             "details": [
    #                 "CellPatchFile",
    #                 "DBPatchFile",
    #                 "Dom0YumRepository",
    #                 "DomuYumRepository",
    #                 "SwitchPatchFile"
    #             ],
    #             "folder": "/u01/downloads/exadata//PatchPayloads/21.1.0.0.0.210204/"
    #         }
    #     }
    # }
    def mGetFolderList(self, aType, aLocation):
        _target_dir_dict = {}
        # Infra components _cmd example:
        #    /bin/ls /u01/downloads/exadata/PatchPayloads/
        _cmd = '/bin/ls {0}/PatchPayloads'.format(aLocation)
        if str(aType).upper() == "CPS":
            # CPS _cmd example:
            #    /bin/ls /u01/downloads/cpsos/
            _cmd = '/bin/ls {0}'.format(aLocation)
        _rc, _, _out, _err = self.mExecuteLocal(_cmd)
        if _rc == 0:
            if _out is None or _out == "":
                dirs = []
            else:
                dirs = str(_out).split()
            for dir in dirs:
                _folder = ""
                if str(aType).upper() == "CPS":
                    # CPS _folder example:
                    #    /u01/downloads/cpsos/20.1.6.0.0_210113/PatchPayloads/20.1.6.0.0.210113/
                    _bundle_patch_version = ".".join(dir.rsplit("_", 1))
                    _folder = "{0}/{1}/PatchPayloads/{2}/".format(aLocation, dir, _bundle_patch_version)
                    _cmd = '/bin/ls {0}'.format(_folder)
                else:
                    # Infra components _folder example:
                    #    /u01/downloads/exadata/PatchPayloads/20.1.6.0.0.210113/
                    _bundle_patch_version = dir
                    _folder = "{0}/PatchPayloads/{1}/".format(aLocation, dir)
                    _cmd = '/bin/ls {0}'.format(_folder)
                _rc, _, _out, _err = self.mExecuteLocal(_cmd)
                if _out is not None and _out != "":
                    _target_dir_dict[_bundle_patch_version] = {}
                    _target_dir_dict[_bundle_patch_version]["details"] = str(_out).split()
                    _target_dir_dict[_bundle_patch_version]["folder"] = _folder
        return _rc, _err, _target_dir_dict

    def mHandlerValidateExaversion(self, aOptions=None):

        if not aOptions: 
            aOptions = self.mGetArgsOptions()

        _data_d = {}
        _errString = None
        _rc = -1
        """              
        Validate Exadata tar bundle existence for a version                         
        """              
        ebLogInfo('*** Validating the Exadata tar bundle existence')            

        def _mUpdateRequestData(rc, aData, err):
            """
            Updates request object with the response payload
            """
            _reqobj = self.mGetRequestObj()
            _response = {}
            _response["success"] = "True" if (rc == 0) else "False"
            _response["error"] = err
            _response["output"] = aData
            if _reqobj is not None:
                _db = ebGetDefaultDB()
                _reqobj.mSetData(json.dumps(_response, sort_keys = True))
                _db.mUpdateRequest(_reqobj)
            elif aOptions.jsonmode:
                ebLogJson(json.dumps(_response, indent=4, sort_keys = True))

        if aOptions is not None and aOptions.jsonconf is not None and \
           'patch_version' in aOptions.jsonconf.keys() and 'target_type' in aOptions.jsonconf.keys():
            _patch_version = aOptions.jsonconf.get('patch_version')
            _target_type = aOptions.jsonconf.get('target_type')
            _target_dir_dict = {}
            for _target in _target_type:
                if _target.lower() == 'all':
                    _target_dir_dict = {'dom0':'Dom0YumRepository', 'domu':'DomuYumRepository', 'cell':'CellPatchFile', 'ibswitch':'SwitchPatchFile'}
                elif _target.lower() == 'dom0':
                    _target_dir_dict['dom0'] = 'Dom0YumRepository'
                elif _target.lower() == 'domu':
                    _target_dir_dict['domu'] = 'DomuYumRepository'
                elif _target.lower() == 'cell':
                    _target_dir_dict['cell'] = 'CellPatchFile'
                elif _target.lower() == 'ibswitch':
                    _target_dir_dict['ibswitch'] = 'SwitchPatchFile'

            _repo_download_location = ''
            if self.__ociexacc:
                _repo_download_location = self.mCheckConfigOption('ociexacc_exadata_patch_download_loc')
            else:
                if self.__debug:
                    ebLogDebug('*** ociexacc parameter set to "False" in '
                                 'exabox.conf.  Retaining the patch path to '
                                 'default exacloud location.')
            _result = []
            _data_d['patch_version'] = _patch_version.strip()
            for key, value in _target_dir_dict.items():
                _res={}
                _res['patch_location'] = value
                _res['target_type'] = key
                _cmd = '/bin/ls ' + _repo_download_location + '/PatchPayloads/' + _patch_version.strip() + '/' + value
                _rc, _, _out, _err = self.mExecuteLocal(_cmd)
                if _rc == 0:
                    if _out is not None or _out != '':
                        _res['patch_exists'] = True
                    else:
                        _res['patch_exists'] = False
                else:
                    ebLogError("Error in execution of cmd: %s error is: %s" %(_cmd, _err))
                    _errString = ' '.join(_err)
                _result.append(_res)
            _data_d['output']= _result
            _rc = 0
            ebLogInfo("*** mHandlerValidateExaversion is : %s" % (json.dumps(_data_d)))
            _mUpdateRequestData(_rc,_data_d,_errString)

    def mHandlerClusterCPUInfo(self, aOptions=None):

        if not aOptions: 
            aOptions = self.mGetArgsOptions()

        ebLogInfo('*** Performing get Cluster CPU info')                        
        # KVM TODO: Replace way to get vcpu (xen vcpu-list) by a cal to abstraction laye; (nelchan)
        if self.mIsKVM():
            _exacpueobj = exaBoxKvmCpuMgr(self)
            _exacpueobj.mClusterCPUInfoKvm(aOptions)
            return

        _data_d = {}
        _err = None
        _rc = -1
        def _mUpdateRequestData(rc, aData, err):
            """
            Updates request object with the response payload
            """
            _reqobj = self.mGetRequestObj()
            _response = {}
            _response["success"] = "True" if (rc == 0) else "False"
            _response["error"] = err
            _response["output"] = aData
            if _reqobj is not None:
                _db = ebGetDefaultDB()
                _reqobj.mSetData(json.dumps(_response, sort_keys = True))
                _db.mUpdateRequest(_reqobj)
            elif aOptions.jsonmode:
                ebLogJson(json.dumps(_response, indent=4, sort_keys = True))

        _cmd = "xm vcpu-list | grep -v Domain-0| grep -v ^Name | awk '{print $1,$2,$7}' |sort |uniq"
        for _dom0, _ in self.mReturnDom0DomUPair():
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            _out = _o.readlines()
            _err = _e.readlines()
            if not _out or not len(_out):
                ebLogError('No Allocation found in %s' % (_dom0))
                _node.mDisconnect()
                continue
            _index=0
            _result = []
            for _oline in _out:
                _res={}
                _oword = _oline.strip().split(" ")
                if len(_oword) < 3:
                        ebLogError('Less number of fields found in %s' % (_oline))
                        _node.mDisconnect()
                        continue
                _res["vm_detail"]= _oword[0].strip()
                _res["vcpus"]= _oword[1].strip()
                _res["pinning_range"]= _oword[2].strip()
                _result.append(_res)
            _data_d[_dom0]= _result
            _node.mDisconnect()
            _rc = 0
            _err = None
        ebLogInfo("mClusterCPUInfo is : %s" % (json.dumps(_data_d)))
        _mUpdateRequestData(_rc,_data_d,_err)

    def mForeignParams(self):
        '''Return all parameters that are not spected by CLUCtrl'''
        aOptions = self.mGetArgsOptions()
        cluparams = set(['hostname', 'patchcluinterface', 'sshkey', 'disablepkey', 'cmd',
                        'vmid', 'configpath', 'jsonconf', 'skip_serase', 'scriptname',
                        'pkeyconf', 'vmcmd', 'debug'])
        reqparams = set(aOptions.__dict__.keys())
        frgparams = reqparams - cluparams
        return frgparams

    def mDict2CMDParams(self, sdict):
        '''Convert a dict in a string formated as command parameters'''
        strparams = ''
        for k, v in list(sdict.items()):
            if isinstance(v, bool):
                strparams += '--{0} '.format(k)
            else:
                strparams += '--{0}={1} '.format(k, v)
        return strparams


    def mHandlerGetCSSMisscount(self):
        return self.mDomUCSSMisscountHandler(True)

    def mHandlerSetCSSMisscount(self):
        aOptions = self.mGetArgsOptions()
        return self.mDomUCSSMisscountHandler(False, aOptions.css_misscount)

    #
    # CSS Misscount handler for both get and set css misscount. aMode=True means get mode
    #
    def mDomUCSSMisscountHandler(self,aMode=True, aMisscount=0):
        _res = {}

        if not aMode:
            if not (aMisscount.isdigit() and int(aMisscount) in range(15,60)):
                ebLogError("*** Invalid Misscount value: should be in range [15,60]")
                return

        # _cmd runs on DomU, taking GI home from grid.ini and updating the set and get CSS Misscount accordingly #
        _creg = '/var/opt/oracle/creg/grid/grid.ini'
        if aMode:
            _cmd = "`sed -n \'s#^oracle_home=\(/.*\)$#\\1" + '/bin/crsctl get css misscount' + "#p' " + _creg + "`" + " | grep -o \"[0-9]*\" | tail -1"
        else:
            _cmd = "`sed -n \'s#^oracle_home=\(/.*\)$#\\1" + '/bin/crsctl set css misscount ' + aMisscount + "#p' " + _creg + "`"
        ebLogInfo("*** _cmd: {0}".format(_cmd))
        for _ , _domU in self.mReturnDom0DomUPair():
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domU)
            _i, _o, _e = _node.mExecuteCmd(_cmd)
            if not _o:
                _node.mDisconnect()
                ebLogError("CSS Miscount operation failed on host {0}".format(_domU))
                return
            _output = _o.readlines()
            for _oline in _output:
                _oline = _oline.strip()
                if _oline:
                    _res[_domU]=_oline
            _node.mDisconnect()
            if not aMode: break # set css misscount execute on one DomU reflects on all nodes of Cluster #
        return json.dumps(_res)

    def mSetupOedaStaging(self, aJob):

        _uuid = self.__uuid
        if aJob:
            _uuid = aJob.mGetUUID()

        _exaunit_id = "0000-0000-0000-0000"
        _req_obj = self.mGetRequestObj()
        if _req_obj:
            _params = _req_obj.mGetParams()
            _exaunit_id = f"exaunit_{str(_params['exaunitid']).zfill(11)}" if _params.get("exaunitid") else "0000-0000-0000-0000"
        self.mSetExaunitID(_exaunit_id)

        self.__oeda_path = self.__ctx.mGetOEDAPath()
        _rpath = f"{self.__oeda_path}/requests/{_exaunit_id}_{_uuid}"
        self.mExecuteCmdLog(f"/bin/mkdir -p {_rpath}")
        self.mExecuteCmdLog(f"/bin/mkdir -p {_rpath}/WorkDir",aLogAsWarn=True)

        ebLogTrace("mSetupOedaStaging: uuid of the job = %s" % _uuid)

        _stagesh_path = self.__oeda_path+'/stage.sh'
        ebLogInfo('_stagesh_path : ' + _stagesh_path)

        # Bug 35485114: Option to repopulate new oeda/stage.sh script
        _force_recreate = get_gcontext().mGetConfigOptions().get(
                        "force_recreate_oeda_stage", "False")

        if (os.path.exists(_stagesh_path) is True and
                _force_recreate.lower() != "true"):

            _cmd  = f"/bin/bash stage.sh {_exaunit_id}_{_uuid}"
            self.mExecuteCmdLog(_cmd, aCurrDir=self.__oeda_path)
            self.__oeda_path = f"{self.__oeda_path}/requests/{_exaunit_id}_{_uuid}"
            self.mSetOEDARequestsPath(self.__oeda_path)
            ebLogInfo(' self.__oeda_path : ' + self.__oeda_path)

            ebLogInfo("Calling mUpdateOEDAPropertiesFromFile: Updating es.properties from properties/OEDAProperties.json.")
            self.mUpdateOEDAPropertiesFromFile('properties/OEDAProperties.json')

            if self.mIsXS() and not self.mIsExaScale():
                ebLogInfo("Calling mUpdateOEDAPropertiesFromFile: Updating es.properties from properties/OEDAProperties_XS.json.")
                self.mUpdateOEDAPropertiesFromFile('properties/OEDAProperties_XS.json')

            return

        _staging_sh = ('(cd requests/"$1" ;'
                    'ln -sf ../../*.sh . ;'
                    'ln -sf ../../config . ;'
                    'ln -sf ../../oedacli . ;'
                    'ln -sf ../../jre* . ;'
                    'ln -sf ../../jdk* . ;'
                    'ln -sf ../../Lib . ;'
                    'mkdir -p log ;'
                    'ln -sf ../../out . ;'
                    'cp -rf ../../properties . ;'
                    'mkdir -p WorkDir ;'
                    'cd WorkDir ;'
                    'if [ "$(ls -A ../../../WorkDir)" ];'
                        'then ln -sf ../../../WorkDir/* .;'
                    'fi ;'
                    'rm -f Oeda*;'  # Delete Oeda['Errors|StackTrace].json from base OEDA, if present
                    'rm -f Diag*;'  # Delete Diag Zips copied from base OEDA, if present
                    'cd ../.. ;'
                    'rm -f latest ;'
                    'ln -sf "$1" latest ;)')
        _rpath = f"{self.__oeda_path}/requests/{_exaunit_id}_{_uuid}"

        with open(_stagesh_path, 'w') as _file_desc:
            _file_desc.write(_staging_sh)

        _cmd = "/bin/chmod u+x stage.sh"
        self.mExecuteCmd(_cmd, aCurrDir=self.__oeda_path)

        _cmd  = f"/bin/bash stage.sh  {_exaunit_id}_{_uuid}"
        self.mExecuteCmdLog(_cmd, aCurrDir=self.__oeda_path)
        self.mSetOEDARequestsPath(_rpath)
        self.__oeda_path = _rpath

        ebLogInfo("Calling mUpdateOEDAPropertiesFromFile: Updating es.properties from properties/OEDAProperties.json.")
        self.mUpdateOEDAPropertiesFromFile('properties/OEDAProperties.json')

        if self.mIsXS() and not self.mIsExaScale():
            ebLogInfo("Calling mUpdateOEDAPropertiesFromFile: Updating es.properties from properties/OEDAProperties_XS.json.")
            self.mUpdateOEDAPropertiesFromFile('properties/OEDAProperties_XS.json')


    #Bug 30340127
    def mSetEnvTypeInConfiguration(self):
        _env_type = "PROD"
        if self.mCheckConfigOption('target_env','discovery') and not self.mEnvTarget():
            _env_type = "DEV"
        elif self.mCheckConfigOption('target_env') in ['dev','qa','dbqa','cqa']:
            _env_type = "DEV"

        get_gcontext().mSetConfigOption('env_type', _env_type)
        ebLogInfo("Environment type is set to: %s" % get_gcontext().mGetConfigOptions()['env_type'])

        _domUMachinesSet = set()
        _dom0_domU_relation = {}

        for _dom0, _domU in self.mReturnDom0DomUPair():
            _domUMachinesSet.add(_domU)
            _dom0_domU_relation[_domU] = _dom0
            _nat_hostname = f"_natHN_{_domU}"
            if get_gcontext().mCheckRegEntry(_nat_hostname):
                _nat_domu_hostname = get_gcontext().mGetRegEntry(_nat_hostname)
                _dom0_domU_relation[_nat_domu_hostname] = _dom0
            else:
                ebLogWarn(f"nat hostname unavailable for {_domU}")

        ebLogTrace(f"Value of domu to dom0 mapping: {_dom0_domU_relation}")
        get_gcontext().mSetRegEntry('domU_set', list(_domUMachinesSet))
        get_gcontext().mSetRegEntry('_dom0_domU_relation', _dom0_domU_relation)


    def mDispatchNonXMLCluster(self, aCmd, aOptions=None, aJob=None):

        ebLogVerbose("mDispatchNonXMLCluster: aCmd = %s" % aCmd)

        _rc = ebError(0x0000)
        #
        # Handle aJob not provided (e.g. direct call w/o Agent)
        #
        class _DummyJob(object):
            def __init__(self):
                self.__uuid    = str(uuid.uuid1())
            def mGetUUID(self):
                return self.__uuid
            def mGetWorker(self):
                return 0
        if not aJob:
            aJob = _DummyJob()
        #
        # Setup oeda staging if needed/possible
        #
        _oeda_path=None

        self.__base_path = self.__ctx.mGetBasePath()
        self.__oeda_path = self.__ctx.mGetOEDAPath()
        self.__uuid = aJob.mGetUUID()
        self.__cmd  = aCmd

        #Register Vg Components (VM,CPU..)
        self.mRegisterVgComponents()

        # Detect the post provisioning endpoints of clucontrol
        if aCmd == 'vm_cmd':
            get_gcontext().mSetRegEntry('ssh_post_fix', 'True')
        else:
            get_gcontext().mSetRegEntry('ssh_post_fix', 'False')

        # Calculate Fedramp
        self.mIsFedramp(aOptions)

        self.mCalculateNoOeda(aCmd)

        if not self.mIsNoOeda():
            self.mSetupOedaStaging(aJob)
            get_gcontext().mSetConfigOption('info_oeda_req_path', self.mGetOedaPath())

        self.__skip_xml_checks = True
        self.__host_list = []

        if aOptions is not None and aOptions.jsonconf is not None and 'ostype' in list(aOptions.jsonconf.keys()):
            _inputjson = aOptions.jsonconf
            if _inputjson:
                if 'quarter-rack-servers' in _inputjson.keys() and _inputjson['quarter-rack-servers']:
                    for _key in _inputjson['quarter-rack-servers']:
                        self.__host_list.append(_key['hostname'] + '.' +  _key['domainname'])
                if 'elastic-servers' in _inputjson.keys() and _inputjson['elastic-servers']:
                    for _key in _inputjson['elastic-servers']:
                        self.__host_list.append(_key['hostname'] + '.' +  _key['domainname'])

            if str(aOptions.jsonconf['ostype']).upper() == 'KVM':
                self.__kvm_enabled = True
                self.__keys_repo = self.mGetBasePath() + '/clusters/keys/'
                get_gcontext().mSetConfigOption('enable_kvm', 'True')
            elif str(aOptions.jsonconf['ostype']).upper() == 'IB':
                ebLogInfo("ostype IB not supported")
        else:
            self.__kvm_enabled = False
            get_gcontext().mSetConfigOption('enable_kvm', 'False')

        if ebCluCmdCheckOptions(aCmd, []):
            try:
                _rc = self.mExecuteStep(aCmd, aOptions, aOedaPath=self.__oeda_path)
            except Exception as oops:
                if type(oops) != type(ExacloudRuntimeError()):
                    ebLogWarn('*** Oops Caught - %s' % type(oops) )
                    ebLogWarn('*** Exception: %s' % (str(oops)))
                #
                # ...
                # Check Exception needs to be raised again
                # ...
                raise
            finally:
                self.mCleanKeysOedaFolder()
        else:
            self.mCleanKeysOedaFolder()

        if self.__debug:
            ebLogDebug('*** _CC:RC: %s' % (_rc))

        return _rc


    def mRefreshExaKmsSingleton(self):

        if not ebCluCmdCheckOptions(self.__cmd, ['refresh_exakms']):
            return
        
        # For adbd oneoff patching we do not need ecdsa support
        if self.mIsAdbs() and self.__cmd in ['oneoff', 'oneoffv2']:
            ebLogTrace(f'Skipping ExaKms refresh for ADBS patching : {self.__cmd}')
            return

        self.mParseXMLConfig(self.__options, aMinimalParse=True)

        # Code change to support ADB support for RSA
        if self.isATP():
            _keytype = self.mCheckConfigOption("exakms_default_keygen_algorithm_adbd")
            if _keytype:
                get_gcontext().mSetRegEntry("exakms_default_keygen_algorithm", _keytype)
                ebLogInfo(f"ADBD Environment detect, fallback ssh keys to: {_keytype}")

        # Change support for ECDSA in FIPs + OL8
        seStatus = self.mGetSELinuxMode("domu")
        _enableFips = self.mGetEnableFipsPayload()

        # Login to domU in patching operation to known Se Linux type
        if ebCluCmdCheckOptions(self.__cmd, ['patch']) or self.__cmd in ['infra_patch_operation']:

            _username = None
            _fetchHost = ""
            _isOL8 = False
            _param = self.__options.jsonconf

            if "AdditionalOptions" in _param:
                for _addParam in _param["AdditionalOptions"]:
                    if "exasplice" in _addParam and _addParam["exasplice"].lower() == "yes":
                        ebLogInfo("Exasplice patching, skip key type detection")
                        return

            # OL8 Patching
            if "TargetVersion" in _param and _param["TargetVersion"].upper() != "LATEST":
                if version_compare(_param["TargetVersion"], "23.1.0") >= 0:
                    _isOL8 = True

                    if "TargetType" in _param:

                        if "dom0" in list(map(lambda x: x.lower(), _param["TargetType"])):
                            if "ComputeNodeList" in _param and _param["ComputeNodeList"]:
                                ebLogTrace("Selecting fetch host from ComputeNodeList")
                                _fetchHost = _param["ComputeNodeList"][0]
                            if "Dom0domUDetails" in _param and _param["Dom0domUDetails"]:
                                ebLogTrace("Selecting fetch host from Dom0domUDetails")
                                _fetchHost = tuple(_param["Dom0domUDetails"].keys())[0]
                            if _fetchHost is None or _fetchHost == "":
                                ebLogTrace("Selecting fetch host from XML")
                                _fetchHost = self.mReturnDom0DomUPair()[0][0]

                        if "cell" in list(map(lambda x: x.lower(), _param["TargetType"])):
                            if "StorageNodeList" in _param and _param["StorageNodeList"]:
                                _fetchHost = _param["StorageNodeList"][0]
                            else:
                                _fetchHost = tuple(self.mReturnCellNodes().keys())[0]

                        if "domu" in list(map(lambda x: x.lower(), _param["TargetType"])):
                            try:
                                ebLogInfo("Selecting fetch host from XML")
                                _fetchHost = self.mReturnDom0DomUPair()[0][1]
                            except:
                                ebLogInfo("Selecting fetch host from Payload")
                                _domUDetails = tuple(_param["Dom0domUDetails"].values())[0]["domuDetails"]
                                _fetchHost = _domUDetails[0]["domuNatHostname"]

                            if self.mIsOciEXACC() and ebCluCmdCheckOptions(self.__cmd, ['opc_enabled_false']):
                                get_gcontext().mSetRegEntry('opc_enabled', 'False')
                                _username = 'root'

            ebLogInfo(f"Fetch host: {_fetchHost}, OL8: {_isOL8}")

            if _fetchHost and _isOL8:

                # Check SE_LINUX
                _linesSeLinux = []
                _fipsStatus = ""
                
                with connect_to_host(_fetchHost, get_gcontext(), username=_username) as _node:

                    _, _o, _ = _node.mExecuteCmd("sestatus")
                    if _node.mGetCmdExitStatus() == 0:
                        _linesSeLinux = _o.readlines()

                    _, _o, _ = _node.mExecuteCmd("/opt/oracle.cellos/host_access_control fips-mode --status")
                    if _node.mGetCmdExitStatus() == 0:
                        _fipsStatus = _o.read()

                sestatus = {}
                for _line in _linesSeLinux:
                    _match = re.match("(.*):\s*(.*)", _line)
                    if _match:
                        sestatus[_match.group(1).strip()] = _match.group(2).strip()

                if "SELinux status" in sestatus and \
                   "Current mode" in sestatus and \
                   sestatus["SELinux status"].lower() == "enabled" and \
                   sestatus["Current mode"].lower() == "enforcing":

                    get_gcontext().mSetRegEntry("exakms_default_keygen_algorithm", "ECDSA")
                    ebLogInfo(f"Patch SE_LINUX + OL8 Environment detect, fallback ssh keys to: ECDSA")

                # Check FIPs
                if "FIPS mode is configured and active" in _fipsStatus or \
                   "FIPS mode is configured but not activated" in _fipsStatus:
                    get_gcontext().mSetRegEntry("exakms_default_keygen_algorithm", "ECDSA")
                    ebLogInfo(f"Patch FIPs + OL8 Environment detect, fallback ssh keys to: ECDSA")

        # Review SE Linux from payload
        else:

            _majorityVersion = self.mGetMajorityHostVersion(ExaKmsHostType.DOMU)
            if _majorityVersion == "OL8":

                if _enableFips:
                    get_gcontext().mSetRegEntry("exakms_default_keygen_algorithm", "ECDSA")
                    ebLogInfo(f"FIPs + OL8 Environment detect, fallback ssh keys to: ECDSA")

                if seStatus == "enforcing":
                    get_gcontext().mSetRegEntry("exakms_default_keygen_algorithm", "ECDSA")
                    ebLogInfo(f"SE_LINUX ENFORCING + OL8 Environment detect, fallback ssh keys to: ECDSA")


        get_gcontext().mSetExaKmsSingleton(ExaKmsSingleton())
        _exakms = get_gcontext().mGetExaKms()
        ebLogInfo(f"ExaKms Default Key Algorithm: {_exakms.mGetDefaultKeyAlgorithm()}")

    def mCleanUpExaKmsSingleton(self):

        if get_gcontext().mCheckRegEntry("exakms_default_keygen_algorithm"):
            get_gcontext().mDelRegEntry("exakms_default_keygen_algorithm")

        get_gcontext().mSetExaKmsSingleton(ExaKmsSingleton())


    def mDispatchCluster(self, aCmd, aOptions=None, aJob=None):

        ebLogVerbose("mDispatchCluster: aCmd = %s" % aCmd)

        if aOptions and aOptions.jsonconf:
            _aparams_str = aOptions.jsonconf
            _mask_params = maskSensitiveData((_aparams_str), use_mask=False)
            ebLogTrace(f"mDispatchCluster: ECRA payload:  {json.dumps(_mask_params, indent=4)}")

        if self.mCheckConfigOption('conn_pool_enabled', 'True') and \
           ebCluCmdCheckOptions(aCmd, ['conn_pool_enabled']):

            ebLogInfo("CONN_POOL_ENABLED")
            _connkey = f"{threading.get_ident()}-{os.getpid()}"
            _connectionPool = exaBoxNodePool(_connkey)
            get_gcontext().mSetRegEntry(f'SSH-POOL-{_connkey}', _connectionPool)

        _rc = ebError(0x0000)
        #
        # Handle aJob not provided (e.g. direct call w/o Agent)
        #
        class _DummyJob(object):
            def __init__(self):
                self.__uuid    = str(uuid.uuid1())
            def mGetUUID(self):
                return self.__uuid
            def mGetWorker(self):
                return 0
        if not aJob:
            aJob = _DummyJob()
        #
        # Setup oeda staging if needed/possible
        #
        _oeda_path=None
        _exceptionRaised = False

        self.__base_path = self.__ctx.mGetBasePath()
        self.__oeda_path = self.__ctx.mGetOEDAPath()
        self.__uuid = aJob.mGetUUID()
        self.__cmd  = aCmd
        _db = ebGetDefaultDB()

        #Register Vg Components (VM,CPU..)
        self.mRegisterVgComponents()

        # Detect the post provisioning endpoints of clucontrol
        if aCmd == 'vm_cmd':
            get_gcontext().mSetRegEntry('ssh_post_fix', 'True')
        else:
            get_gcontext().mSetRegEntry('ssh_post_fix', 'False')

        # Register aOptions
        get_gcontext().mSetRegEntry("aOptions", aOptions)

        self.mSetExaScale(False)
        if aOptions is not None and aOptions.jsonconf is not None and \
           "storageType" in list(aOptions.jsonconf.keys()) and \
           str(aOptions.jsonconf['storageType'].upper()) == "EXASCALE":
            ebLogInfo("*** EXASCALE ENABLE")
            self.mSetExaScale(True)
            self.mSetStorageType("EXASCALE")
        elif aOptions is not None and hasattr(aOptions, "storageType") and \
           str(aOptions.storageType).upper() == "EXASCALE":
            ebLogInfo("*** EXASCALE ENABLE")
            self.mSetExaScale(True)
            self.mSetStorageType("EXASCALE")
        elif aOptions is not None and aOptions.jsonconf is not None and \
           "storageType" in list(aOptions.jsonconf.keys()) and \
           str(aOptions.jsonconf['storageType'].upper()) == "XS":
            ebLogInfo("*** XS(EXASCALE) ENABLE")
            self.mSetXS(True)
            self.mSetStorageType("XS")
        elif aOptions is not None and aOptions.jsonconf is not None and \
           "rack" in list(aOptions.jsonconf.keys()) and "storageType" in list(aOptions.jsonconf["rack"].keys()) and \
           str(aOptions.jsonconf['rack']['storageType'].upper()) == "XS":
            ebLogInfo("*** XS(EXASCALE) ENABLE")
            self.mSetXS(True)
            self.mSetStorageType("XS")
        else:
            self.mSetStorageType("ASM")

        self.mIsFedramp(aOptions)

        if aCmd == 'vm_tmpkey_op' and 'badNodes' in list(aOptions.jsonconf.keys()):
            self.mParseXMLConfig(self.__options)
            handler = CommandHandler(self)
            return handler.mHandlerVmTmpKeyOp()

        if aCmd in ['elastic_info','vmgi_reshape'] and 'reshaped_node_subset' in list(aOptions.jsonconf.keys()):
            _reshape_config = aOptions.jsonconf['reshaped_node_subset']
            if 'node_recovery_flow' in _reshape_config.keys() and _reshape_config['node_recovery_flow'] == True:
                self.mDeleteNodeRecoveryCheck(aCmd, aOptions)
                if self.__skip_dom0_validation:
                    self.mParseXMLConfig(self.__options)
                    self.mSaveXMLClusterConfiguration()
                    self.mSetConfigPath(self.__patchconfig)
                    ebLogInfo('ebCluCtrl: Saved patched Cluster Config: ' + self.__patchconfig)

                    _reshape = ebCluReshapeCompute(self, aOptions)
                    _reshape_conf = _reshape.mGetReshapeConf()
                    _reshape_domu_list = [x['domU']['hostname'] for x in _reshape_conf['nodes']]
                    for _deleteNodeDomU in _reshape_domu_list:
                        _reshape.mRemoveNodeFromCRS(_deleteNodeDomU)

                    _oeda_path = self.mGetOedaPath()
                    self.mExecuteCmd('/bin/mkdir -p '+self.__oeda_path+'/exacloud.conf')
                    _patchconfig = self.mGetPatchConfig()
                    _deletenodexml = _oeda_path + '/exacloud.conf/deletenode_' + self.__uuid + '.xml'
                    self.mExecuteLocal("/bin/cp {} {}".format(_patchconfig, _deletenodexml))
                    self.mSetPatchConfig(_deletenodexml)
                    self.mPatchXMLForNodeSubset(aOptions)
                    self.mSaveXMLClusterConfiguration()
                    ebLogInfo('ebCluCtrl: Saved patched Cluster Config: ' + self.__patchconfig)
                    return 0

        if self.mCheckConfigOption('remove_root_access', 'True'):
            if ebCluCmdCheckOptions(aCmd, ['opc_enabled_false']):
                get_gcontext().mSetRegEntry('opc_enabled', 'False')
                ebLogTrace("Setting opc_enabled to False")
            else:
                get_gcontext().mSetRegEntry('opc_enabled', 'True')
                ebLogTrace("Setting opc_enabled to True")
        else:
            get_gcontext().mSetRegEntry('opc_enabled', 'False')
            ebLogTrace("Setting opc_enabled to False")

        if ebCluCmdCheckOptions(aCmd, ['patch']):
            try:
                # Refresh default key type
                self.mRefreshExaKmsSingleton()

            except SSHException as e:
                if aCmd == "exacompute_patch_nodes":
                    raise e
                else:
                    ebLogError(f"*** Exception in dispatch cluster: {e}")
                    ebLogError(f"{traceback.format_exc()}")
                    _rc = PATCHING_NODE_SSH_CHECK_FAILED
                    _code, _error_msg, _ = ebPatchFormatBuildError(_rc)
                    _db.mUpdateChildRequestError(self.__uuid, _code, _error_msg)
                    return _rc

            except Exception as e:
                raise e

        else:
            # Refresh default key type
            self.mRefreshExaKmsSingleton()

        # Detect XML Patching
        self.mCalculateSkipXmlPatching()

        def _default_handler(aCmd, aOptions):

            if aCmd == 'restart_remoteec':
                return self.mHandlerRestartRemoteEc(aOptions)

            if aCmd == 'exacompute_patch_nodes':
                ebLogInfo("No xml required for exacompute_patch_nodes")
                handler = CommandHandler(self)
                return handler.mHandlerExaComputePatch()

            if aCmd in [
                    "db_install", "db_delete"]:
                #enable exacli for exacc env
                self.mUpdateExacliPwd(aOptions)

                ebLogWarn("Starter DB and regular DB flows are now deprecated "
                    "in Exacloud")
                if not self.mCheckConfigOption("force_starter_db_install") == 'True':
                    ebLogInfo("StarterDB flow is not forced through config "
                        "option: 'force_starter_db_install'. This is a no-op")
                    return 0


            # Check shared environemt only for install/delete operation - review other ops if applicable
            if (ebCluCmdCheckOptions(aCmd, ['check_shared_env']) and not self.__skip_dom0_validation) or \
               self.mIsExaScale():
                self.mCheckSharedEnvironment()

            # Calculate OEDA
            self.mCalculateNoOeda(aCmd)

            if not self.mIsNoOeda():
                self.mSetupOedaStaging(aJob)
                get_gcontext().mSetConfigOption('info_oeda_req_path', self.mGetOedaPath())

            if not self.mIsNoOeda():
                self.mRestoreOEDASSHKeys(aOptions)

            if aCmd == "selinuxpolicy_update":
                return self.mProcessSELinuxUpdate(aOptions)

            if aCmd == "get_custom_policies":
                return self.mGetGeneratedSELinuxPolicies(aOptions)

            # 
            # Mainly wrapper around mPatchClusterConfig
            try:

                # mCreateCluster will patch XML with GI/DB info
                _rc = self.mCreateCluster(aOptions)
                if _rc: return ebError(_rc)

                # Update DB request fields
                self.mUpdateRequestClusterName(self.mGetClusterName())

            except ExacloudRuntimeError as ecre:
                # Clean temp config files with sensitive data
                if self.__dbname_cfg:
                    self.mExecuteLocal('/bin/rm -f '+self.__dbname_cfg)
                _rc = ecre.mGetErrorCode()
                ebLogError('*** Exception caught: %s - %s - %s' % (hex(ecre.mGetErrorCode()),hex(ecre.mGetErrorType()),ecre.mGetErrorMsg()))
                raise

            return self.mExecuteOEDAStep(aCmd,aOptions,aOedaPath=_oeda_path)

        #
        # white_list Operations are allowed when another is running on the same cluster
        # Check first if we are already running a request/operation for this cluster
        #

        if aCmd == 'version' or (aCmd == 'checkcluster' and aOptions.healthcheck == 'conf') or aCmd == 'host_state':
            self.__skip_xml_checks = True

        if not self.__skip_xml_checks:

            # parse the xml config
            self.mParseXMLConfig(aOptions, aMinimalParse=True)
            #create Table with clustername if record replay used
            recordReplayOption = self.mCheckConfigOption('record_replay')
            ebRecordReplay.mInitRecordReplay(recordReplayOption, str(self.__uuid),self.mGetClusterName(),
                self.mCheckConfigOption('repository_root'),get_gcontext().mGetBasePath())

            #
            # set ui_oedaxml flag if it is KVM
            #
            if not self.mGetUiOedaXml():
                ui_oedaxml = False
                for _, _domU in self.mReturnDom0DomUPair():
                    _domUConfig  = self.__machines.mGetMachineConfig(_domU)
                    version = _domUConfig.mGetVersionNum()
                    if _domUConfig. mGetGuestCores() or _domUConfig.mGetGuestMemory() or _domUConfig.mGetGuestLocalDiskSize() or _domUConfig.mGetMacOsType() == "LinuxKVMGuest" or version >= 2:
                        ui_oedaxml = True
                        break
                if ui_oedaxml:
                    self.mSetUiOedaXml(True)
                    ebLogInfo("*** ui_oeda_xml is set to TRUE")

            self.mGenerateExacloudXML(aOptions)
            self.mParseXMLConfig(aOptions)

            self._dom0U_list = copy.deepcopy(self.mReturnDom0DomUPair(aIsClusterLessXML=self.mIsClusterLessXML()))

            #This function makes use of cluster information so cluster configuration is required.
            self.mSetEnvTypeInConfiguration()
        _key = self.__key

        self.mUpdateRequestClusterName(self.mGetClusterName())

        _cmd_wl1_ok = False
        _pt = False

        '''
        TODO for disk, memory, elastic op:
        _elastic_op = ["vmgi_reshape", "elastic_cell_update", "elastic_cell_info"]
        _disk_resize = ["add_vm_extra_size"]

        if aCmd in _elastic_op:
                _cmd_wl2_ok = True

        if aCmd in _disk_resize:
                _cmd_wl2_ok = True

        memory resize usecase via 'memset' vm_cmd command
        '''

        if aCmd == 'vm_cmd':
            if aOptions is not None and aOptions.vmcmd is not None and aOptions.vmcmd != 'None':
                if aOptions.vmcmd in ['ping']:
                    _cmd_wl1_ok = True

        if aCmd == 'vmbackup-oss':
            if aOptions is not None and aOptions.jsonconf is not None and 'operation' in aOptions.jsonconf.keys():
                if aOptions.jsonconf['operation']=='list':
                    _cmd_wl_ok = True

        _pt = True
        if _db.mCheckRegEntry(_key) and not ebCluCmdCheckOptions(aCmd, ['white_list']):
            if _cmd_wl1_ok:
                ebLogInfo('*** Letting %s operation through !' %(aOptions.vmcmd))

            elif (
                aOptions is not None and aOptions.jsonconf is not None and \
                "concurrent_operation" in aOptions.jsonconf and str(aOptions.jsonconf["concurrent_operation"]).upper() == "TRUE"
            ):
                ebLogInfo('*** Letting %s operation through by Payload parameter!')

            else:

                _entry = _db.mGetRegEntryByKey(_key)
                _cleanDB = False

                if _entry: 
                    _wkPort = _entry[3] # Worker Port

                    if not _db.mGetWorker(_wkPort):

                        ebLogInfo(f"Cleanup stale Reg Entry: {_entry}")
                        _db.mDelRegEntry(_key)
                        _cleanDB = True

                if not _cleanDB:
                    _pt = _db.mCommandPassThrough(aCmd, aClustername=_key)
                    if _pt:
                        ebLogInfo('*** Letting %s operation through !' %(aCmd))

        if _pt is False:
            ebLogError('*** Operation on the same cluster in progress aborting request')
            ebLogError('*** key: %s' % (_key))
            return ebError(0x099)                                  # ERROR_099 : ALREADY OP IN PROGRESS ON SAME CLUSTER

        #
        # Log Key Check only for non whitelisted command/request type
        #
        if _key and not ebCluCmdCheckOptions(aCmd, ['white_list']):
            ebLogInfo('*** KEY_CHECK: (%s/%s)' %(_db.mCheckRegEntry(_key), _key))

        _white_list = False
        if ebCluCmdCheckOptions(aCmd, ['white_list']) or _cmd_wl1_ok:
                _white_list = True
        else:
            _db.mSetRegEntry(_key,'True', str(self.__uuid), str(aJob.mGetWorker()))

        #
        # Execute/Dispatch request cmd
        #
        if ebCluCmdCheckOptions(aCmd, []):
            try:
                # Restore root access
                if ebCluCmdCheckOptions(aCmd, ["cmd_restore_root"]):
                    self.mRestoreRootAccess(aOptions)

                _rc = _default_handler(aCmd, aOptions)
            except Exception as oops:

                _exceptionRaised = True

                if type(oops) != type(ExacloudRuntimeError()):
                    ebLogWarn('*** Oops Caught - %s' % type(oops) )
                    ebLogWarn('*** Exception: %s' % (str(oops)))
                #
                # Release any acquired remote lock cluster
                #
                if self.remote_lock.get_request_state() == 1:
                    ebLogWarn('*** REMOTE LOCK ACTIVE. RELEASE REQUIRED.')
                    self.remote_lock.clear()
                #
                # Cleanup / Rollback
                #
                # Clean temp config files with sensitive data
                if self.__dbname_cfg:
                    self.mExecuteLocal('/bin/rm -f '+self.__dbname_cfg)
                #
                # ...
                # Check Exception needs to be raised again
                # ...
                raise
            finally:
                if self.mCheckConfigOption('conn_pool_enabled', 'True') and \
                   ebCluCmdCheckOptions(aCmd, ['conn_pool_enabled']):
                    # Delete connection pool
                    _connkey = f"{threading.get_ident()}-{os.getpid()}"
                    _connectionPool = get_gcontext().mGetRegEntry(f'SSH-POOL-{_connkey}')
                    _connectionPool.mCloseConnections()
                    get_gcontext().mDelRegEntry(f'SSH-POOL-{_connkey}')

                #don't delete keys if a wl and a non wl operation are concurrent.
                if (ebCluCmdCheckOptions(aCmd, ['white_list']) or _cmd_wl1_ok) and _db.mCheckRegEntry(_key):
                    pass

                elif not ebCluCmdCheckOptions(aCmd, ['keys_cmd']):
                    self.mHandlerDeleteOndiskKeys()

                if ebCluCmdCheckOptions(aCmd, ['cleanup_keys']):
                    self.mCleanupKeys()
                    pass

                # Sync up keys.db to the Passive node, if the host information
                # is available
                self.mSyncKeysOverNetworkSend(aOptions)
                self.mSyncKVDBOverNetworkSend(aOptions)

                # Remove temporal root access
                if ebCluCmdCheckOptions(aCmd, ["cmd_restore_root"]):
                    try:
                        self.mUnRestoreRootAccess(aOptions)
                    except Exception as e:
                        if type(e) != type(ExacloudRuntimeError()):
                            ebLogWarn('*** Caught Exception on mUnRestoreRootAccess - {}'.format(type(e)))
                            ebLogWarn('*** Exception: {}'.format(str(e)))

                # Delete multi switch entry
                if get_gcontext().mCheckRegEntry("ADDED_SWITCHES"):
                    get_gcontext().mDelRegEntry("ADDED_SWITCHES")

                # Delete multi switch entry
                if get_gcontext().mCheckRegEntry("aOptions"):
                    get_gcontext().mDelRegEntry("aOptions")

                # ExaKms cleanup
                self.mCleanUpExaKmsSingleton()

                #
                # Remove lock on current cluster
                #
                if _white_list == False:
                    _db.mDelRegEntry(_key)
                self.mUIOedaCliXmlCleanUp()
                self.mCleanKeysOedaFolder()
                # Export the Recorded Table into file
                if self.mCheckConfigOption('record_replay', 'RECORD') and self.mGetClusterName():
                    ebRecordReplay.mExportRecordedTable()

        else:
            _db.mDelRegEntry(_key)
            self.mCleanKeysOedaFolder()

        if self.__debug:
            ebLogDebug('*** _CC:RC: %s' % (_rc))

        return _rc

    def mDeleteNodeRecoveryCheck(self, aCmd, aOptions):
        _reshape_config = aOptions.jsonconf['reshaped_node_subset']
        if aCmd in ['elastic_info','vmgi_reshape'] and 'node_recovery_flow' in _reshape_config.keys() and _reshape_config['node_recovery_flow'] == True:
            if 'reshaped_node_subset' in list(aOptions.jsonconf.keys()) and \
              'removed_computes' in list(aOptions.jsonconf['reshaped_node_subset'].keys()) and \
              len(aOptions.jsonconf['reshaped_node_subset']['removed_computes']) > 0:
                _removeComputeConf = aOptions.jsonconf['reshaped_node_subset']['removed_computes']
                for _reshape_conf  in _removeComputeConf:
                    _deleteNodeDom0 = _reshape_conf['compute_node_hostname']
                    _deleteNodeDomU = _reshape_conf['compute_node_virtual_hostname']
                    _node = exaBoxNode(get_gcontext())
                    if not _node.mIsConnectable(aHost = _deleteNodeDom0, aTimeout=5):
                        self.__skip_dom0_validation = True
                        self.__delete_node_name.append([_deleteNodeDom0, _deleteNodeDomU])
                        ebLogInfo("*** Host %s of the node %s to be removed is not reachable. Marking the request as success"%(_deleteNodeDom0,_deleteNodeDomU))
    
    def mExecuteOnNode(self, aHost, aCmd):
        ebLogVerbose("mExecuteOnNode: aHost = %s, aCmd = %s" % (aHost, aCmd))
        _node = exaBoxNode(get_gcontext())
        _node.mConnect(aHost=aHost)
        _node.mExecuteCmdLog(aCmd)
        _node.mDisconnect()

    def mExecuteOnNodeCheck(self, aHost, aCmd, aUser=None):
        ebLogVerbose("mExecuteOnNodeCheck: aHost = %s, aCmd = %s" % (aHost, aCmd))

        node = exaBoxNode(get_gcontext())
        node.mSetUser(aUser)
        node.mConnect(aHost=aHost)
        struser = aUser if aUser is not None else 'root'
        ebLogInfo('*** Runing {0} as user: {1} on host {2}'.format(aCmd, struser, aHost))
        _, _ro, _re = node.mExecuteCmd(aCmd)
        sro = _ro.read().strip()
        sre = _re.read().strip()
        rc = node.mGetCmdExitStatus()
        node.mDisconnect()
        return sro, sre, rc

    def mExecuteCmd(self, aCmd, aOptions=None, aCurrDir=None, aStdIn=PIPE, aStdOut=PIPE, aStdErr=PIPE, aTimeout=None):

        ebLogVerbose("mExecuteCmd: aCmd = %s " % aCmd)

        _current_dir = aCurrDir
        _stdin = aStdIn
        _std_out = aStdOut
        _stderr = aStdErr
        _timeout = aTimeout
        fin, fout, ferr = self.__node.mExecuteCmd(aCmd, aCurrDir=_current_dir, aStdIn=_stdin, aStdOut=_std_out, aStdErr=_stderr, aTimeout=_timeout)
        self.__cmd_status = self.__node.mGetCmdExitStatus()
        return fin, fout, ferr

    def mExecuteCmdLogAsync(self, aCmd, aOptions=None):

        ebLogVerbose("mExecuteCmdLogAsync: aCmd = %s" % aCmd)

        self.__tmp_buffer = ''
        self.__tmp_status = 0

        def _read_cb(aData):
            ebLogInfo(aData,aNoNL=True)
            self.__tmp_buffer = self.__tmp_buffer + aData

        def _error_cb(aData):
            ebLogError(aData,aNoNL=True)
            self.__tmp_buffer = self.__tmp_buffer + aData

        def _status_cb(aStatus):
            ebLogInfo('*** Async Cmd status returned: '+str(aStatus))
            self.__tmp_status = aStatus

        _callbacks = [_read_cb, None, _error_cb, _status_cb ]
        self.__node.mExecuteCmdAsync(aCmd, _callbacks)

        return [self.__tmp_status, self.__tmp_buffer]

    def mExecuteCmdLog(self, aCmd, aOptions=None, aSkipLog=False, aCurrDir=None, aStdIn=PIPE, aStdOut=PIPE, aStdErr=PIPE,aLogAsWarn=False):

        ebLogVerbose("mExecuteCmdLog: aCmd = %s, aSkipLog = %s" % (aCmd, aSkipLog))

        _current_dir = aCurrDir
        _stdin = aStdIn
        _std_out = aStdOut
        _stderr = aStdErr
        fin, fout, ferr = self.mExecuteCmd(aCmd, aCurrDir=_current_dir, aStdIn=_stdin, aStdOut=_std_out, aStdErr=_stderr)
        out = fout.readlines()
        if out and not aSkipLog:
            for e in out:
                ebLogInfo(e[:-1].strip())
        err = ferr.readlines()
        if err and not aSkipLog:
            for e in err:
                if aLogAsWarn:
                    ebLogWarn(e[:-1].strip())
                else:
                    ebLogError(e[:-1].strip())
        return [out, err]

    @ebRecordReplay.mRecordReplayWrapper
    def mExecuteCmdLog2(self, aCmd, aOptions=None, aSkipLog=False, aTimeOut=None, aCurrDir=None, aStdIn=PIPE, aStdOut=PIPE, aStdErr=PIPE):

        #
        # No default timeout (some operation can take a very long time (aka secure cell erase 3 passes)
        #
        _timeout = aTimeOut
        _current_dir = aCurrDir
        _stdin = aStdIn
        _std_out = aStdOut
        _stderr = aStdErr
        #
        # Redirect stderr to stdout
        #
        if type(aCmd) == list:
            _args = aCmd
        else:
            _args = shlex.split(aCmd)

        _proc = subprocess.Popen(_args, stdin=_stdin, stdout=_std_out, stderr=_stderr, cwd=_current_dir)
        _fd_out = wrapStrBytesFunctions(_proc.stdout)

        _out = []
        _err = []

        _poll_o = select.poll()
        _poll_o.register(_proc.stdout, select.POLLIN)
        _s_time = time.time()

        while 1:
            _poll_rc = _poll_o.poll(0)
            if _poll_rc:
                _ol = _fd_out.readline()
                if _ol is not None and _ol != '':
                    if not aSkipLog:
                        ebLogInfo(_ol.strip())
                    _out += [_ol]
                else:
                    break
            _e_time = time.time() - _s_time
            if _timeout is not None and _e_time >= _timeout:
                ebLogInfo("Terminate actual Command SubProcess: {0}".format(aCmd))
                _out = subprocess.check_output(['/bin/ps', '-ax']).decode('utf8')
                _matching_process = list(filter(lambda x: '{}'.format(aCmd) in x, _out.split('\n')))
                _pids = list(map(lambda x: x.split()[0], _matching_process))
                ebLogInfo("List of process to be killed: {0}".format(_pids))
                for _pid in _pids:
                    _process = psutil.Process(int(_pid))
                    for _proc in _process.children(recursive=True):
                        ebLogInfo(f"Killing child process {_proc.pid} of parent {_pid}.")
                        _proc.kill()
                    ebLogInfo(f"Killing parent process {_pid}.")
                    _process.kill()
                raise ExacloudRuntimeError(0x0117, 0xA, 'Timeout Exception')
            # TODO: Add expo backoff timeout
            time.sleep(1)

        return [_out,_err]

    def mCopyFile(self, aLocalFile, aRemoteFile):

        self.__node.mCopyFile(aLocalFile, aRemoteFile)

    @ebRecordReplay.mRecordReplayWrapper
    def mExecuteLocal(self, aCmd, aCurrDir=None, aStdIn=PIPE, aStdOut=PIPE, aStdErr=PIPE, aTimeOut=None,
                      aLogOutError=False):

        _opt = get_gcontext().mGetArgsOptions()
        if isinstance(_opt, dict) and 'mock_cmds' in _opt:
            _i, _o, _e = self.mExecuteCmd(aCmd,  aCurrDir, aStdIn, aStdOut, aStdErr)
            _rc = self.__node.mGetCmdExitStatus()
            return _rc, _i, _o.read(), _e.read()

        _args = shlex.split(aCmd)
        _current_dir = aCurrDir
        _stdin = aStdIn
        _std_out = aStdOut
        _stderr = aStdErr
        try:
            _proc = subprocess.Popen(_args, stdin=_stdin, stdout=_std_out, stderr=_stderr, cwd=_current_dir)
            _std_out, _std_err = wrapStrBytesFunctions(_proc).communicate(timeout=aTimeOut)
            _rc = _proc.returncode
        except Exception as e:
            _rc = 2
            _std_out = ""
            _std_err = str(e)

        ebLogTrace(f"*** mExecuteLocal: Executed command: {aCmd}. Return code: {_rc}.")
        if aLogOutError:
            ebLogTrace(f"*** mExecuteLocal: Output: \n{_std_out} Error: \n{_std_err}")
        return _rc, None, _std_out, _std_err

    #
    # Check/validate the NFS patching and/or backup mount point in starter db
    #  provisioning before it create the db
    #
    def mExaCMDBPreChecks (self, jsonconf = None):

        _jconf = jsonconf
        _step_time = time.time()
        if _jconf and 'dbParams' in list(_jconf.keys()) :
            _use_nfs = False

            if _jconf['dbParams']['bkup_nfs'] == 'yes' :

                #Validate NFS-Backup mount point.
                if 'bkup_nfs_loc' in list(_jconf['dbParams'].keys()) :
                    _use_nfs = True

                    ebLogInfo ("[INFO] Validating NFS-Backup mount point. ")

                    for _, _domU in self.mReturnDom0DomUPair():
                        self.mTestNodeNFSMP(host=_domU, path='/mnt/dbaas_backup',
                                            device=_jconf['dbParams']['bkup_nfs_loc'],
                                            rw=True, tFile='nfs.test')

                else :
                    ebLogError ("*** Error *** Missing value: bkup_nfs_loc/nfsRemoteBakcup")
                    raise ExacloudRuntimeError(0x0506, 0xA, 'NFS mount point validation failure.')

            #Validate NFS-Patching mount point.
            if 'patch_nfs_loc' in list(_jconf['dbParams'].keys()) :
                _use_nfs = True

                ebLogInfo ("[INFO] Validating NFS-Patching mount point")

                for _, _domU in self.mReturnDom0DomUPair():

                    self.mTestNodeNFSMP(host=_domU, path='/mnt/dbaas_patch',
                                        device=_jconf['dbParams']['patch_nfs_loc'],
                                        rw=False, tFile='exa_map' )

            if _use_nfs :
                self.mLogStepElapsedTime(_step_time, 'PREDB INSTALL : NFS Mount Point Validation')

        else :
            ebLogInfo ("[WARING] Can't make ExaCM DB Pre Check without the DB parameters.")


    #
    # Validate/test a NFS mount point in one node
    #
    # host   = host to connect and test NFS
    # path   = local directory were to mount
    # device = device to mount in the given/default directory (path)
    # rw     = if True equals read & write, else just read
    # tFile  = test file to read/write
    #
    # Return if succes, raise ExacloudRuntimeError 506 if fail
    #
    def mTestNodeNFSMP (self, host=None, path='/mnt/test', device=None, rw=False, tFile='nfs.test'):
        if device is None or host is None :
            ebLogError ("*** Error *** Can't validate NFS without specify the host and/or the device.")
            raise ExacloudRuntimeError(0x0506, 0xA, 'NFS mount point validation failure.')

        # If is Busy Or Already mounted
        _busy = False
        _bstr = 'is busy or already mounted'
        _rw   = ' ro '
        _cmd2 = ' cat > /dev/null '
        if rw :
            _rw   = ' rw '
            _cmd2 = ' echo "test" > '

        _node = exaBoxNode(get_gcontext())
        _node.mConnect(aHost=host)

        #Unmount
        _cmd = "umount "+path
        ebLogInfo ("*** Executing \""+_cmd+"\" in domU \""+host+"\"." )
        _node.mExecuteCmdLog(_cmd)
        #Create directory and mount point
        _cmd = "mkdir "+path+" ; mount -t nfs "+device+" "+path+" -o"+_rw+" ;"
        ebLogInfo ("*** Executing \""+_cmd+"\" in domU \""+host+"\"." )

        _, _o, _e = _node.mExecuteCmd (_cmd)
        _rc0 = _o.channel.recv_exit_status()
        err = _e.readlines()
        if err :
            for e in err :
                ebLogInfo ("*** Error returner: %d : %s"%(_rc0, e[:-1] ) )
                if _bstr in e :
                    _busy = True
        #Test read/write
        _cmd = _cmd2 + path +"/"+ tFile
        ebLogInfo ("*** Executing \""+_cmd+"\" in domU \""+host+"\"." )
        _node.mExecuteCmdLog(_cmd)
        _rc1 = _node.mGetCmdExitStatus()
        #unmount
        _cmd = "umount "+path
        ebLogInfo ("*** Executing \""+_cmd+"\" in domU \""+host+"\"." )
        _node.mExecuteCmdLog(_cmd)

        _node.mDisconnect()

        if _rc0 == 32 and _busy :

            ebLogInfo ("** Warning ** Mount point \""+path+"\" is busy or already mounted.")

        elif _rc0 != 0 or _rc1 != 0 :
            ebLogError ("*** Error *** While validating NFS mount point with the device: \""+device+"\".")
            raise ExacloudRuntimeError(0x0506, 0xA, 'NFS mount point validation failure.')

        return

    #
    # Rollback the Encryption of the DB
    #
    # fix the bug 24960842 - where the encryption is not rolledback for the
    # recreate_stater_db with 12.2
    #
    # jsonconf = json configuration, need the dbparams sub-json
    #
    # Return if success, warnning if fail
    #
    def mRollbackEncryption (self, jsonconf = None) :
        _jconf = jsonconf

        if _jconf and 'dbParams' in list(_jconf.keys()) and '122' in _jconf['dbParams']['version'] :

            ebLogInfo("*** Rollingback the encryption")


            for _, _domU in self.mReturnDom0DomUPair():
                _node = exaBoxNode(get_gcontext())
                _node.mSetUser('oracle')
                _node.mConnect(aHost=_domU)

                # ORACLE_HOME should be like:
                #_OH = "/u01/app/oracle/product/12.2.0.1/dbhome_1"
                _OH = None
                # Searching the oracle home for the starter db
                _cmd = 'cat /etc/oratab | grep dbhome_1 | sed \'s/.*\:\(.*\)\:.*/\\1/\''
                ebLogInfo ("*** Executing: '"+_cmd+"' in domU: '"+_domU+"'." )

                _, _o, _e = _node.mExecuteCmd (_cmd)
                out = _o.readlines()
                if out :
                    for o in out :
                        ebLogInfo("Output : '"+o.rstrip()+"'.")
                        if 'dbhome_1' in o :
                            _OH = o.rstrip()
                            break

                if _OH is None :
                    ebLogInfo("*** Warning - Can't found the ORCLE_HOME in /etc/oratab, unsig default.")
                    _OH = "/u01/app/oracle/product/12.2.0.1/dbhome_1"

                ebLogInfo("*** Info - Using '"+_OH+"' as ORACLE_HOME.")

                _cmd = "ls "+_OH+"/lib/libvsn12.a* "
                ebLogInfo ("*** Executing: '"+_cmd+"' in domU: '"+_domU+"'." )
                _node.mExecuteCmdLog(_cmd)
                _rc = _node.mGetCmdExitStatus()

                _rc1 = _rc2 = None

                if _rc == 0 :
                   _exp_oh = 'export ORACLE_HOME='+_OH
                   _cmd = _exp_oh+' ; cd '+_OH+'/lib ; cp libvsn12.a.default libvsn12.a " '
                   ebLogInfo("*** Executing \""+_cmd+"\" in domU \""+_domU+"\".")
                   _node.mExecuteCmdLog(_cmd)
                   _rc1 = _node.mGetCmdExitStatus()
                   if self.__debug :
                       ebLogInfo("** [Debug] Retuned code (_rc1) was '"+str(_rc1)+"'.")

                   _cmd = _exp_oh+' ; cd '+_OH+'/rdbms/lib ; make -f ins_rdbms.mk cloud_off ioracle"'
                   ebLogInfo("*** Executing \""+_cmd+"\" in domU \""+_domU+"\".")
                   _node.mExecuteCmdLog(_cmd)
                   _rc2 = _node.mGetCmdExitStatus()
                   if self.__debug :
                       ebLogInfo("** [Debug] Returned code (_rc2) was '"+str(_rc2)+"'.")


                _node.mDisconnect()

                if _rc != 0 :
                    ebLogInfo("*** Warning ***  can't found file \""+_OH+"/lib/libvsn12.a\".")

                elif (_rc1 is not None and _rc1 != 0) or (_rc2 is not None and _rc2 != 0) :
                    ebLogInfo("*** Warning *** can't rollback encryption in host: "+_domU)

                if  (_rc1 is not None and _rc1 == 0) and (_rc2 is not None and _rc2 == 0) :
                    ebLogInfo("*** Info - The encryption rollback in host '"+_domU+"' has ended successfully.")


        else :
            ebLogInfo("*** Skipping Rollback Encryption for 12.2")

        return
    #
    # Disable TFA
    #
    def mDisableTFA(self):
        for _, _domu in self.mReturnDom0DomUPair():
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domu)
            _node.mExecuteCmdLog('/etc/init.d/init.tfa stop')
            _node.mExecuteCmdLog('/etc/init.d/init.tfa disable')
            ebLogInfo("INFO: Disabled TFA on "+_domu)
            _node.mDisconnect()

    #
    # Enables/Disables TFA blackout
    #
    def mEnableTFABlackout(self, aEnable, aReason, aOptions=None):
        _action = 'enable' if aEnable else 'disable'

        for _, _domu in self.mReturnDom0DomUPair():
            _node = None

            try:
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_domu)

                ebLogInfo("Getting TFA_HOME location from {0} for blackout {1} operation ({2})".format(_domu, _action, aReason))
                _cmd = "grep TFA_HOME= /etc/init.d/init.tfa | cut -d= -f2"
                _i, _o, _e = _node.mExecuteCmd(_cmd)

                if not _o:
                    ebLogError("Error, TFA_HOME location not found in {0} for blackout {1} operation ({2})".format(_domu, _action, aReason))
                    continue

                _tfa_home = _o.read().strip()
                _cmd = os.path.join(_tfa_home, "bin/tfactl blackout ");
                _blk_enable = 'add -targettype host -timeout none -reason "' + aReason + '"'
                _blk_disable = 'remove -targettype host'
                _blk_op = _blk_enable if aEnable else _blk_disable
                _cmd += _blk_op
                ebLogInfo("Executing TFA blackout {0} operation ({1}) on {2}: {3}".format(_action, aReason, _domu, _cmd))
                _i, _o, _e = _node.mExecuteCmd(_cmd)
                _output = _o.read().strip() if _o else ""
                _error = _e.read().strip() if _e else ""
                ebLogInfo("Result of TFA blackout {0} operation ({1}) on {2}: output=[{3}], error=[{4}]".format(_action, aReason, _domu, _output, _error))
            except Exception as e:
                ebLogError("Error during TFA blackout {0} operation ({1}) on {2}: {3}".format(_action, aReason, _domu, traceback.format_exc()))
            finally:
                if _node:
                    _node.mDisconnect()

    def mApplyHostAccessControlWA(self):

        # WA for bug 32224053
        if not self.__shared_env and not self.mCheckConfigOption("disable_wa_hostaccesscontrol", "True"):

            _rebootCells = []

            for _cell_name in self.mReturnCellNodes():
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_cell_name)
                try:
                    _host_access_control = '/opt/oracle.cellos/host_access_control'
                    _, _o, _e = _node.mExecuteCmd(_host_access_control + ' pam-auth --status')
                    _output = ""

                    if _o:
                        _output += _o.read()

                    if _e:
                        _output += _e.read()

                    if "host_access_control is already running" in _output:
                        ebLogInfo("Error on host access control: {0}".format(_output))

                        _node.mExecuteCmdLog("cat /var/run/host_access_control.pid")
                        if _node.mGetCmdExitStatus() == 0:

                            ebLogInfo("Found PID file on cell: {0}".format(_cell_name))
                            _rebootCells.append(_cell_name)

                except Exception as e:

                    ebLogInfo("Exception found: {0}".format(e))
                    _rebootCells.append(_cell_name)

                finally:
                    _node.mDisconnect()

                # Then Reboot the modified cells in parallel as well
                for _cellToReboot in _rebootCells:
                    ebLogInfo("Rebooting cell {0}.".format(_cellToReboot))
                    self.mRebootNode(_cellToReboot)



    def mHardenOCISecurity(self):
        """
        Security hardening for OCI STIG compliance
        """
        if not self.__exabm:
            return

        self.mApplyHostAccessControlWA()

        def _run_cmd(aCmd,aNode):
            _node = aNode
            _maxRetries = 3
            _retries = 0
            _cmd_status = False
            
            while _retries < _maxRetries:
                _node.mExecuteCmdLog(aCmd)
                if _node.mGetCmdExitStatus():
                    ebLogWarn(f'Command : {aCmd} failed with exit status:{_node.mGetCmdExitStatus()}, retrying ...')
                    _retries += 1
                    ebLogTrace("Retrying in 10 seconds...")
                    time.sleep(10)
                else:
                    _cmd_status = True
                    break
            if not _cmd_status:
                ebLogError(f'All attemps to run cmd : {aCmd} failed')
                _error_str = f'mExecuteCmd for OCI security hardening failed: {_node.mGetHostname()} for {aCmd}'
                raise ExacloudRuntimeError(0x0760, 0xA, _error_str)
        
        def mHardenOCISecurityForCell(aCell):
            _cell_name = aCell
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_cell_name)
            try:
                # Account lock after 3 times consecutive failures
                _host_access_control = '/opt/oracle.cellos/host_access_control'
                _run_cmd(_host_access_control + ' pam-auth --deny 3', _node)
                _run_cmd(_host_access_control + ' pam-auth --status', _node)

                # Shell and SSH timeout after 900 sec idle
                _run_cmd(_host_access_control + ' idle-timeout --shell 900 ' +
                                                              '--client 900', _node)
                _run_cmd(_host_access_control + ' idle-timeout --status', _node)

                # Set login warning banner
                _banner = '/opt/oracle.cellos/login_banner.txt'
                _banner_txt = 'Warning: This system is restricted to ' \
                              'authorized users for business purposes only.'
                _run_cmd('echo "%s" > %s' % (_banner_txt, _banner), _node)
                _run_cmd(_host_access_control + ' banner --file ' + _banner, _node)
                _run_cmd(_host_access_control + ' banner --status', _node)

                # TODO: Disable RC4 in SSH Cipher (When EM issues resolved)
                #_run_cmd(_host_access_control + ' sshciphers -b -d -p arcfour')
                #_run_cmd(_host_access_control + ' sshciphers --status')
            except Exception:
                raise
            finally:
                _node.mDisconnect()
        
        def mHardenOCISecurityForNode(aDom0):
            _dom0 = aDom0
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            try:
                # Install additional RPMs (screen : CCE-26940-7)
                _security_rpms = self.mDynDepNonImageList(['security_rpms'])[0]
                if _security_rpms:
                    for _rpm in _security_rpms:
                        _, _o, _ = _node.mExecuteCmd('rpm -q %s' % _rpm['package'])
                        _out = _o.read().strip()
                        ebLogInfo('Checking DOM0: %s : RPM: %s' % (_dom0, _out))
                        if _out != _rpm['version']:
                            try:
                                if int(_rpm['version'].split('.')[1]) <= int(_out.split('.')[1]):
                                    ebLogWarn('*** CHECK/UPDATE RPMs VERIONS: {0} - {1}'.format(_rpm['version'],_out))
                                    continue
                            except:
                                ebLogError('*** __ERROR__ CHECK/UPDATE RPMs VERIONS: {0} - {1}'.format(_rpm['version'], _out))
                                continue
                            ebLogInfo('Install %s to %s' % (_rpm['package'], _dom0))
                            _localfile = _rpm['local']
                            _remotefile = '/tmp/%s' % os.path.basename(_localfile)
                            _node.mCopyFile(_localfile, _remotefile)
                            try:
                                _run_cmd('rpm -Uvh --force %s' % _remotefile, _node)
                            except:
                                ebLogError('*** ERROR WHILE INSTALLING SECURITY RPM: {}'.format(_remotefile))
                else:
                    ebLogWarn('*** DYNDEP read failed, skipping RPM install')
            except Exception:
                raise
            finally:
                _node.mDisconnect()
        
        #
        # Parallel Execution on all cells of the cluster
        #
        _plist = ProcessManager()


        # Cell
        for _cell_name in self.mReturnCellNodes():
            _p = ProcessStructure(mHardenOCISecurityForCell, [_cell_name], _cell_name)
            _p.mSetMaxExecutionTime(30*60) #30 minutes timeout
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()

        _plist = ProcessManager()
        # Dom0
        for _dom0, _ in self.mReturnDom0DomUPair():
            _p = ProcessStructure(mHardenOCISecurityForNode, [_dom0], _dom0)
            _p.mSetMaxExecutionTime(30*60) #30 minutes timeout
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()

    def mDisableQoSM(self):

        if self.mCheckConfigOption('disable_QoSM', "True"):
            for _, _domU in self.mReturnDom0DomUPair():
                try:
                    _node = exaBoxNode(get_gcontext())
                    _node.mSetUser('grid')
                    _node.mConnect(aHost=_domU)

                    ebLogInfo("*** Disable QoSM in ATP host {0}".format(_domU))

                    _cmd = "srvctl disable qosmserver"
                    _node.mExecuteCmdLog(_cmd)

                    _cmd = "srvctl stop qosmserver"
                    _node.mExecuteCmdLog(_cmd)

                    _node.mDisconnect()

                except Exception as e:
                    ebLogError(str(e))
                    raise ExacloudRuntimeError(0x0760, 0xA, str(e))
        else:
            ebLogInfo("*** Disable QoSM not Configured")

    def mRemoveRspFiles(self):

        _cmd = "find /tmp -maxdepth 1 -mindepth 1 -type f -name 'Grid-*.rsp' -delete"
        for _, _domU in self.mReturnDom0DomUPair():
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domU)
            ebLogInfo("*** Removing rsp temporary files on %s" % _domU)
            _node.mExecuteCmdLog(_cmd)
            _node.mDisconnect()

    def mAtpConfig(self):

        #DomU
        for _, _domU in self.mReturnDom0DomUPair():
            try:
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_domU)

                #Host Updater Workaround
                if not self.mCheckConfigOption('disable_hostupdater_workaround', 'True'):
                    _, _o, _e = _node.mExecuteCmd("curl http://169.254.169.254/opc/v1/instance/canonicalRegionName")

                    if _node.mGetCmdExitStatus() == 0:
                        _out = _o.read().strip()
                        if _out and _out.find("404") == -1:
                            _node.mExecuteCmd("echo '{0}' > /etc/region".format(_out))
                    else:
                        ebLogWarn("Curl command of canonicalRegionName not works correctly")
                        ebLogWarn(_o.read().strip())
                        ebLogWarn(_e.read().strip())

                #Check the user configuration
                _remapUtil = ebMigrateUsersUtil(self)
                _valid, _msg = _remapUtil.mValidateUsersRange(_node)
                if not _valid:
                    if self.mCheckConfigOption('guid_min_exception', 'True'):
                        raise ExacloudRuntimeError(0x0760, 0xA, '*** {0}'.format(_msg))
                    else:
                        ebLogWarn('*** Warning: {0}'.format(_msg))

                #Save the current file of sshd_config
                _cmd = "test ! -f /etc/ssh/sshd_config.bkbyHostUpdater && cp /etc/ssh/sshd_config /etc/ssh/sshd_config.bkbyHostUpdater"
                _node.mExecuteCmd(_cmd)

                #Add the AuthorizedKeysFile to the sshd_config
                _cmd = "sed -i 's/.*AuthorizedKeysFile.*/AuthorizedKeysFile \/etc\/ssh-keys\/%u/g' /etc/ssh/sshd_config"
                ebLogInfo("*** Add AuthorizedKeysFile to sshd_config")
                _node.mExecuteCmd(_cmd)

                #Restart sshd
                _cmd = "service sshd restart"
                _node.mExecuteCmd(_cmd)

                #Add sudoers
                _cmd = "! (cat /etc/sudoers | grep --regex '^%access-sudoers') && echo '%access-sudoers ALL = (ALL) NOPASSWD:ALL' >> /etc/sudoers"
                ebLogInfo("*** Add sudoers users")
                _node.mExecuteCmd(_cmd)

                #create the folder /etc/ssh-keys/
                _cmd = "mkdir -p /etc/ssh-keys"
                _node.mExecuteCmd(_cmd)

                #Add root keys
                _cmd = 'ln -sf /root/.ssh/authorized_keys /etc/ssh-keys/root'
                ebLogInfo("*** Create the symbolic links of the users to the folder /etc/ssh-keys")
                _node.mExecuteCmd(_cmd)

                #Add the another users keys
                _cmd = 'ls /home | awk \'{printf "ln -sf /home/%s/.ssh/authorized_keys /etc/ssh-keys/%s;", $1, $1}\' | /bin/sh'
                _node.mExecuteCmd(_cmd)
                _node.mDisconnect()

            except Exception as e:
                _node.mDisconnect()
                raise ExacloudRuntimeError(0x0760, 0xA, str(e))

    def mDisablePasswordExpiration(self):

        for _, _domU in self.mReturnDom0DomUPair():
            try:
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_domU)

                #Update all the users to no expire the password
                _node.mExecuteCmd("cat /etc/passwd | awk -F: '{print \"chage -M -1 -E -1 \" $1}' | sh")
                _node.mExecuteCmd("/opt/oracle.cellos/host_access_control password-policy --PASS_MAX_DAYS=-1 --PASS_MIN_DAYS=-1 --PASS_WARN_AGE=-1")

                _node.mDisconnect()

            except Exception as e:
                ebLogError("Could not disable password expiration on {0}:{1}".format(_domU, str(e)))
                raise ExacloudRuntimeError(0x0760, 0xA, str(e))

    def mGetIlomPass(self):

        _lastpwd = ""

        _secretsPath = "/u01/oci/secrets.py"
        if os.path.exists(_secretsPath):

            ebLogInfo(f"Fetching ilom info using the script: {_secretsPath}")
            _rc, _, _o, _ = self.mExecuteLocal(
                f"{_secretsPath} --action get --secret_name=exadata-computeilom-root")

            if _rc == 0:
                _lastpwd = _o.strip()

        if not _lastpwd:
            _lastpwd = str(self.mCheckConfigOption("ilom_pwd_b64"))
            _lastpwd = base64.b64decode(_lastpwd).decode('utf8')

        return _lastpwd


    def mCheckCaviumInstanceDomUs(self):
        try:
           _rc = self.mCCIDomUs()
           if _rc:
               ebLogInfo('CheckCaviumInstanceDomUs on all DomUs completed successfully')
           elif self.mCheckConfigOption('checkcaviumfatal', 'True'):
               _error = 'CheckCaviumInstanceDomUs failure aborting request'
               ebLogError(_error)
               raise ExacloudRuntimeError(0x0741, 0xA, _error)
           else:
               ebLogError('CheckCaviumInstanceDomUs failed - soft error requested')
        except:
           _error = 'CheckCaviumInstanceDomUs fatal exception aborting request'
           raise ExacloudRuntimeError(0x0741, 0xA, _error)

    def mCCIDomUs(self):
        def _mCheckCavInst(aDomU, aStatus):
            _domU = aDomU
            _rc_status = aStatus
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domU)
            _t, _d, _b = 4, 3, 2
            _rc = False
            _out = None
            _to = 50
            _header = 'Authorization: Bearer Oracle'
            for _r in range(_t):
                _t0 = time.time()
                _in, _out, _err = _node.mExecuteCmd(f'curl -m {_to} -H "{_header}" http://169.254.169.254/opc/v2/instance')
                _et = int((time.time() - _t0) % 60)
                if _et >= _to:
                    ebLogInfo(f"Cavium check connection timeout - retries: {_r}")
                else:
                    ebLogInfo(f"Cavium check elapsed time: {_et} sec")
                _rc = _node.mGetCmdExitStatus()
                if not _rc:
                    break
                time.sleep(_d)
                _d *= _b
            _node.mDisconnect()
            if _out and not _rc:
                _json = ''.join( _out.readlines())
                if _json[0] == '{':
                    _kv = json.loads(_json)
                    ebLogInfo(f"Cavium instance check on {_domU} successful - {_kv['id']}")
                else:
                    ebLogError(f'Invalid Cavium data for {_domU} : {_json}')
                    _rc_status.append("False")
            else:
                ebLogError(f"Cavium instance check on {_domU} failed _rc: {_rc}\n{_out} ")
                _rc_status.append("False")
            _rc_status.append("True")
        _plist = ProcessManager()
        _rc_status = _plist.mGetManager().list()
        for _, _domU in self.mReturnDom0DomUPair():
            _p = ProcessStructure(_mCheckCavInst, [_domU, _rc_status])
            _p.mSetMaxExecutionTime(30*60) # 30 minutes
            _p.mSetJoinTimeout(60)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)
        _plist.mJoinProcess()
        for _rc in _rc_status:
            if _rc == "False":
                return False
        return True

    def mHandlerUnlockDeviceUsingIlom(self):

        ebLogInfo("Running mUnlockDeviceUsingIlom")
        _vmpwd   = umask("nnqdBcnEAGwiQG8SJEFOTQ==")

        _cmds = []

        # First element of the list appended is the stream Exacloud must wait
        # to receive before sending the second element in the list (cmd)
        _cmds.append(['->', 'start -script /SP/Console'])
        _cmds.append(['\(', 'root'])
        _cmds.append(['Password:', _vmpwd])
        _cmds.append(['#', "sed --follow-symlinks -i 's/.*PermitRootLogin\ without-password.*/PermitRootLogin\ yes/g' /etc/ssh/sshd_config;"])
        _cmds.append(['#', "sed --follow-symlinks -i 's/.*PasswordAuthentication\ no.*/PasswordAuthentication\ yes/g' /etc/ssh/sshd_config;"])
        _cmds.append(['#', "sed --follow-symlinks -i 's@^auth       required     pam_tally2.so deny=@#auth       required     pam_tally2.so deny=@g'  /etc/pam.d/sshd;"])
        _cmds.append(['#', "service sshd restart;"])
        _cmds.append(['#', "pam_tally2 --user=root --reset;"])
        _cmds.append(['#', '/opt/oracle.cellos/host_access_control access --open;'])
        _cmds.append(['#', 'exit;'])

        self.mIlomsCommandStream(_cmds)

        # Configure ips
        _configure = self.mCheckConfigOption("secure_host_access_control", "True")
        aOptions = self.mGetArgsOptions()
        if _configure and aOptions.jsonconf:
            self.mSecureDom0SSH()
            self.mSecureCellsSSH()

    def mHandlerEnableAccessControlIlom(self):

        ebLogInfo("Running mHandlerEnableAccessControlIlom")
        _vmpwd   = umask("nnqdBcnEAGwiQG8SJEFOTQ==")

        _cmds = []

        # First element of the list appended is the stream Exacloud must wait
        # to receive before sending the second element in the list (cmd)
        _cmds.append(['->', 'start -script /SP/Console'])
        _cmds.append(['\(', 'root'])
        _cmds.append(['Password:', _vmpwd])
        _cmds.append(['#', '/opt/oracle.cellos/host_access_control access --open'])
        _cmds.append(['#', 'exit'])

        self.mIlomsCommandStream(_cmds)

        # Configure ips
        _configure = self.mCheckConfigOption("secure_host_access_control", "True")
        aOptions = self.mGetArgsOptions()
        if _configure and aOptions.jsonconf:
            self.mSecureDom0SSH()
            self.mSecureCellsSSH()

    def mIlomsCommandStream(self, aCmds):

        ebLogInfo("Running mIlomsCommandStream")

        _lastpwd = self.mGetIlomPass()

        _ilomList = set([x for x in self.__iloms.mGetIlomsList() if x is not None])

        for _ilom in _ilomList:

            _maxTries = 3
            _tries    = 0

            _netId    = self.__iloms.mGetIlomConfig(_ilom).mGetIlomNetworkId()
            _netIlom  = self.__networks.mGetNetworkConfig(_netId)
            _ilomName = "{0}.{1}".format(_netIlom.mGetNetHostName(), _netIlom.mGetNetDomainName())

            while _tries < _maxTries:

                if _tries != 0:
                    _lastpwd = getpass.getpass("Password for {0}: ".format(_ilomName))

                try:
                    _node = exaBoxNode(get_gcontext())
                    _node.mSetUser("root")
                    _node.mSetPassword(_lastpwd)

                    ebLogInfo("Try authentication: {0}".format(_ilomName))
                    _node.mConnectAuthInteractive(aHost=_ilomName)

                    _node.mExecuteCmdsAuthInteractive(aCmds)
                    ebLogInfo("Read from socket: [{0}]".format(_node.mGetConsoleRawOutput()))

                    _node.mDisconnect()
                    break

                except Exception as e:
                    ebLogInfo(e)
                    _tries += 1

    def mSecureSSHCiphers(self):

        #Remove Arcfour on non Oracle Linux Release 7 hosts
        _hosts = self.mGetHostsByTypeAndOLVersion(ExaKmsHostType.DOMU, "OL6")
        _switches = self.mReturnSwitches(True)

        for _host in _hosts:
            if self.mIsKVM() and _host in _switches:
                continue

                ebLogInfo("Remove 'arcfour' cipher on {0} with OL6")
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_host)

                _node.mExecuteCmd('sed -i "s/,arcfour//g" /etc/ssh/sshd_config')
                _node.mExecuteCmd('sed -i "s/arcfour,//g" /etc/ssh/sshd_config')
                _node.mExecuteCmd("service sshd restart")

                _node.mDisconnect()

    def mRestartDnsmasq(self):
        """ Reset the dnsmasq service on a cps env"""
        ebDNSConfig.mRestartDnsmasq(self.mCheckConfigOption("remote_cps_host"))

    def mVerifyClusterwareBmCloud(self):

        _skipFirst = True
        for _, _domu in self.mReturnDom0DomUPair():

            if _skipFirst:
                _skipFirst = False
            else:
                ebLogInfo("Verify clusterware bm_cloud on {0}".format(_domu))

                _node = exaBoxNode(get_gcontext())
                _node.mSetUser("grid")
                _node.mConnect(aHost=_domu)

                #Find grid location
                _cmd = "cat /etc/oratab | grep '^+.*' "
                _, _out, _ = _node.mExecuteCmd(_cmd)
                _out = _out.readlines()
                _grid_home = _out[0].split(":")[1].strip()

                #Apply Verified
                _shortname = _domu.split(".")[0]
                _cmd = "{0}/bin/cluvfy stage -post crsinst -n {1}".format(_grid_home, _shortname)
                _node.mExecuteCmdLog(_cmd)

                _node.mDisconnect()

    def mUpdateHugePagesSysctlConf(self, aDomU, aCurrMem, aNewMem, aMinHugepageMem = ''):

        _memsizeMB = int(aNewMem)
        _currvmem = aCurrMem
        _domU = aDomU
        _hugepagesize = None
        _param = "vm.nr_hugepages"
        new_paramval = 0

        with connect_to_host(aDomU, get_gcontext()) as _node:

            _cmd = "/bin/grep Hugepagesize /proc/meminfo | /usr/bin/awk '{print$2/1024}'"
            _in, _out, _err = _node.mExecuteCmd(_cmd)
            if _out:
                _out = _out.readlines()
                _hugepagesize = _out[0].strip()
                ebLogInfo("Hugepagesize from meminfo: {0}".format(_hugepagesize))

            if not _hugepagesize:
                #Default of 2MB Hugepagesize
                _hugepagesize = "2"

            ebLogInfo("System mem is : {0}".format(_currvmem))
            _hugepagesize = int(_hugepagesize)
            
            _factor = _memsizeMB / float(_currvmem)
            _, curr_paramval = self.mGetSysCtlConfigValue(_node, _param, True)
            if curr_paramval is None:
                ebLogError(f"Failed to get the value of the sysctl parameter: {_param}")
                return -1

            ebLogInfo("*** Current value of %s is %s" % (_param, curr_paramval))
            new_paramval = int(int(curr_paramval) * _factor)

            if (_hugepagesize*new_paramval > _memsizeMB*0.6):
                ebLogWarn("Hugepage memory {0} greater than 60% of new system memory of {1}".format(_hugepagesize*new_paramval, _memsizeMB))
                new_paramval = int((_memsizeMB*0.6)/_hugepagesize)
                ebLogInfo("*** hugepage value after setting it to 60% of the memory : {0}".format(new_paramval)) 

            if new_paramval <= 0:
                ebLogInfo("*** Hugepages are disable, current value: '{0}'".format(curr_paramval))
                return 0

            if aMinHugepageMem != '':
                _min_total_hugepagesize = int(aMinHugepageMem.split(" ")[0].strip()) * 1024
                if _min_total_hugepagesize > _hugepagesize * int(new_paramval):
                    #check if the current hugepage is more then the min required huge page and the value is not more then 60% of total memory then use the existing hugepage value
                    if ((_hugepagesize * int(curr_paramval)) >= _min_total_hugepagesize and (_hugepagesize * int(curr_paramval)) <= _memsizeMB*0.6):
                        ebLogInfo("*** Not changing hugepages as the current value is proper")
                        return 0
                    #set it to min required hugepage size if the above conditions fails and the reshape is downsize request
                    elif (int(new_paramval) < int(curr_paramval) and _min_total_hugepagesize <= _memsizeMB*0.6):
                        ebLogInfo("*** Setting the hugepages to the minmum required hugepages")
                        new_paramval = int(_min_total_hugepagesize/_hugepagesize)
                    else:
                        _detail_error = 'New Hugepage count is %s which is less than the minimum required value %d for the VM. Not proceeding with the change' % (new_paramval, int(_min_total_hugepagesize/_hugepagesize))
                        ebLogError('*** ' + _detail_error)
                        return -1

            ebLogInfo("*** New hugepage value is : {0}".format(new_paramval))    
            return 0 if self.mSetSysCtlConfigValue(_node, "vm.nr_hugepages", new_paramval, aRaiseException=False, aInstantApply=False, aValidate=False) else -1


    def mRegenerateInitramfs(self, aDomU):
        ebLogInfo("*** regenerate the initramfs file to reflect the system configuration change")
        with connect_to_host(aDomU, get_gcontext()) as _node:
            ebLogTrace("Checking if initramfs exists on {}".format(aDomU))
            _,_uname, _ = node_exec_cmd_check(_node, "/bin/uname -r")
            _file = "/boot/initramfs-%s.img"%(_uname.strip())
            if _node.mFileExists(_file):
                ebLogInfo("*** The Initramfs file exists. Path is " + _file)
                # create a backup
                _backup_path = "/var/initramfs-%s.img"%(_uname.strip())
                if _node.mFileExists(_backup_path):
                    node_exec_cmd_check(_node,"/bin/rm -rf {0}".format(_backup_path))
                node_exec_cmd_check(_node,f'/usr/bin/cp {_file}  {_backup_path}')
            _dracut_path = node_cmd_abs_path_check(_node, "dracut", sbin=True)
            _omit_drivers = self.mCheckConfigOption('omit_drivers_initramfs')
            if _omit_drivers:
                _dracut_cmd = f"{_dracut_path} --omit-drivers '{_omit_drivers}' -f"
            else:
                _dracut_cmd = f"{_dracut_path} -f"
            node_exec_cmd_check(_node, _dracut_cmd)
            _initramfs_backup_delete_cmd = "find /boot/ -name 'initramfs*backup.img' -delete"
            try:
                node_exec_cmd_check(_node, _initramfs_backup_delete_cmd)
            except Exception as ex:
                ebLogError(f"Error on executing command: {_initramfs_backup_delete_cmd}. Error is: {ex}")

    # Function needs to be called for none exacscale env to set the hugepages
    def mUpdateHugepagesForCluster(self, aPercent):
        ebLogInfo("*** Update the Huges pages to %s Percent of total VM memory" % (aPercent))
        _hugepage = None
        for _dom0, _domu in self.mReturnDom0DomUPair():
            _hv = getHVInstance(_dom0)
            _currmaxvmem = _hv.mGetVMMemory(_domu, 'MAX_MEM')
            with connect_to_host(_domu, get_gcontext()) as _node:
                _hugepagesize = None
                _cmd = "/bin/grep Hugepagesize /proc/meminfo | awk '{print$2/1024}'"
                _in, _out, _err = _node.mExecuteCmd(_cmd)
                if _out:
                    _out = _out.readlines()
                    _hugepagesize = _out[0].strip()
                    ebLogInfo("Hugepagesize from meminfo: {0}".format(_hugepagesize))
                if not _hugepagesize:
                    #Default of 2MB Hugepagesize
                    _hugepagesize = "2"
                _hugepagesize = int(_hugepagesize)
                ebLogInfo("System mem is : {0}".format(_currmaxvmem))
                _hugepage = int((_currmaxvmem * int(aPercent))/(_hugepagesize * 100))

                self.mSetSysCtlConfigValue(_node, "vm.nr_hugepages", _hugepage, aRaiseException=False)


    def mRemoveFqdnOnDomU(self):

        for _, _domu in self.mReturnDom0DomUPair():

            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domu)
            _hostname = _domu.split(".")[0]

            ebLogInfo("Update Hostname from {0} to {1}".format(_domu, _hostname))

            _node.mExecuteCmd("hostname | grep '\.'")

            if _node.mGetCmdExitStatus() == 0:

                if self.mIsHostOL8(_domu) or self.mIsHostOL7(_domu):
                    _node.mExecuteCmd("hostnamectl set-hostname {0}".format(_hostname))
                else:
                    _node.mExecuteCmd("hostname {0}".format(_hostname))

            _node.mExecuteCmd("cat /etc/sysconfig/network | grep 'HOSTNAME' | grep '\.'")
            if _node.mGetCmdExitStatus() == 0:
                _node.mExecuteCmd("sed -i '/HOSTNAME/d' /etc/sysconfig/network")
                _node.mExecuteCmd("echo 'HOSTNAME={0}' >> /etc/sysconfig/network".format(_hostname))

            _node.mDisconnect()

    def mRestoreMissingMountPoints(self):

        for _dom0, _ in self.mReturnDom0DomUPair():

            _node = exaBoxNode(get_gcontext())

            try:
                _node.mConnect(aHost=_dom0)

                _, _o, _ = _node.mExecuteCmd("/bin/cat /etc/fstab | /bin/grep LABEL | /bin/awk '{print $2}'")
                _expectedMountPoints = _o.readlines()

                _, _o, _ = _node.mExecuteCmd("/bin/mount")
                _currentMountPoints = _o.read()

                for _expectedMP in _expectedMountPoints:

                    if "/home" not in _expectedMP and \
                       "/boot" not in _expectedMP and \
                       "/var" not in _expectedMP:
                        continue

                    if "on {_expectedMP}" in _currentMountPoints:
                        ebLogInfo(f"Mount point '{_expectedMP}' already mounted")
                    else:
                        ebLogInfo(f"Mounting '{_expectedMP}'")
                        _node.mExecuteCmdLog(f"/bin/mount {_expectedMP}")

            finally:
                _node.mDisconnect()


    def mChangeMinFreeKb(self):

        for _, _domu in self.mReturnDom0DomUPair():

            if self.__shared_env:
                ebLogInfo("Skip Change Min Free KB since enviroment is multi vm")
                break

            else:

                _image    = self.mGetImageVersion(_domu)
                _imagever = ".".join([x.zfill(3) for x in _image.split(".")[0:3]])

                if _imagever < "018.001.008":
                    ebLogInfo("Skip Change Min Free KB on {0} since image version is lower than 18.1.8+".format(_domu))

                else:
                    _node = exaBoxNode(get_gcontext())
                    _node.mConnect(aHost=_domu)

                    _node.mExecuteCmd('numactl --hardware | grep "node 1"')
                    if _node.mGetCmdExitStatus() != 0:
                        ebLogInfo("Skip Change Min Free KB since NUMA is not active")

                    else:
                        self.mSetSysCtlConfigValue(_node, "vm.min_free_kbytes", "1048576", aRaiseException=False)
                        ebLogInfo("Changed Min Free KB from 524288 to 1048576")

                    _node.mDisconnect()


    def mAddOracleFolderTmpConf(self):
        """ Exclude several Oracle tmp folders from systemd's cleanup service """

        ebLogInfo(f"disable_systemd_tmpfile : '{self.mCheckConfigOption('disable_systemd_tmpfile')}'")

        if not self.mCheckConfigOption('disable_systemd_tmpfile','True'):
            return

        for _, _domu in self.mReturnDom0DomUPair():

            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domu)

            _awk_cmd = node_cmd_abs_path_check(_node, "awk")
            _cp_cmd = node_cmd_abs_path_check(_node, "cp")
            _systemctl_cmd = node_cmd_abs_path_check(_node, "systemctl")

            _folders =  ["/tmp/.oracle", "/var/tmp/.oracle", "/usr/tmp/.oracle"]
            for _tmp_f in _folders:

                ebLogInfo(f"Add 'x {_tmp_f}' to '/usr/lib/tmpfiles.d/tmp.conf' in VM {_domu}")

                _futurename = f'/tmp/tmpfiletmpconfig.{time.time()}'
                _cmd = """
                        %s 'BEGIN { flag=1; } {
                            if ($0 ~ " %s") { print "x %s"; flag=0; }
                            else { print $0; }
                        } END { if (flag == 1){ print "x %s"; } }
                        ' /usr/lib/tmpfiles.d/tmp.conf > %s
                       """ % (_awk_cmd, _tmp_f, _tmp_f, _tmp_f, _futurename)
                _node.mExecuteCmd(_cmd)

                _cmd = f'{_cp_cmd} {_futurename} /usr/lib/tmpfiles.d/tmp.conf'
                _node.mExecuteCmd(_cmd)

            ebLogInfo("*** Restarting systemd-tmpfiles-clean.timer service ***")
            _cmd = f"{_systemctl_cmd} restart systemd-tmpfiles-clean.timer"
            _node.mExecuteCmd(_cmd)

            _node.mDisconnect()

    def mRemoveExacloudVMKeys(self):

        if not self.mIsKVM() or self.__ociexacc:
            _keys_dir1 = self.__cluster_path + "/keys/"
        _keys_dir2 = self.mGetBasePath() + "/oeda/requests/*/WorkDir/"

        for _, _domU in self.mReturnDom0DomUPair():
            _host = _domU.split('.')[0]

            # Remove the VM keys (for root, opc, grid, oracle) from
            # Exacloud - cluster - keys dir and oeda - WorkDir
            if not self.mIsKVM() or self.__ociexacc:
                _file_list = glob.glob('{0}/id_rsa.{1}*'.format(_keys_dir1, _host))
                _file_str = ' '.join([str(_file) for _file in _file_list])
                _cmd = "/bin/rm -rf {0}".format(_file_str)
                self.mExecuteLocal(_cmd)
            _file_list = glob.glob('{0}/id_rsa.{1}*'.format(_keys_dir2, _host))
            _file_str = ' '.join([str(_file) for _file in _file_list])
            _cmd = "/bin/rm -rf {0}".format(_file_str)
            self.mExecuteLocal(_cmd)

    def mReplaceInFile(self, aNode, aFile, aRegex, aNewRegex, aPattern=None):

        #Check if new regex already on the file
        aNode.mExecuteCmd("/bin/cat {0} | /bin/grep -x '{1}'".format(aFile, aNewRegex))
        if aNode.mGetCmdExitStatus() != 0:

            aNode.mExecuteCmd("/bin/cat {0} | /bin/grep '{1}'".format(aFile, aRegex))

            #If there is already the line, remove it
            if aNode.mGetCmdExitStatus() == 0:
                aNode.mExecuteCmd("/bin/sed -i --follow-symlinks '/{0}/d' {1}".format(aRegex, aFile))

            if aPattern is None:
                #Append new line with the new regex
                aNode.mExecuteCmd("/bin/echo '{0}' >> {1}".format(aNewRegex, aFile))
            else:
                #Append new line after the pattern
                aNode.mExecuteCmd(
                    "/bin/sed -i --follow-symlinks '/{0}/a {1}' {2}".format(aPattern,aNewRegex, aFile))

    def mConfigureSyslogIlomHost(self):

        _lastpwd = self.mGetIlomPass()

        def mConfigureSyslog(_ilomHost):
            if _ilomHost['type'] not in ["dom0", "cell"]:
                return

            ebLogInfo("Configure Syslog Ilom Host: {0}".format(_ilomHost))

            #Dom0/Cell changes
            try:
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_ilomHost['host'])

                #Step 1, Configure rsyslog.conf
                _hostIp = ""
                if _ilomHost['type'] == "dom0":
                    _master = self.mGetCurrentMasterInterface(aDom0=_ilomHost['host'])
                    _hostIp = _node.mSingleLineOutput("ip a s {0} | grep 'inet ' | tr '/' ' ' | awk '{print $2}'".format(_master))
                else:
                    _hostIp = _node.mSingleLineOutput("ip a s eth0 | grep 'inet ' | tr '/' ' ' | awk '{print $2}'")

                self.mReplaceInFile(_node, "/etc/rsyslog.conf", '$UDPServerRun', '$UDPServerRun 514', '# Provides UDP syslog reception')
                self.mReplaceInFile(_node, "/etc/rsyslog.conf", '$UDPServerAddress', '$UDPServerAddress {0}'.format(_hostIp), '# Provides UDP syslog reception')
                self.mReplaceInFile(_node, "/etc/rsyslog.conf", '$ModLoad imudp', '$ModLoad imudp', '# Provides UDP syslog reception')
                self.mReplaceInFile(_node, "/etc/rsyslog.conf", '*.info;.*\/var\/log\/messages', '*.info;mail.none;authpriv.none;cron.none  /var/log/messages', "# Don'\\''t log private authentication messages!")
                _node.mExecuteCmdLog("service rsyslog restart")

                #Step 2, Set the syslog client IP in ILOM
                _node.mExecuteCmdLog('ipmitool sunoem cli "set /SP/clients/syslog/1/ address={0}"'.format(_hostIp))
                if _node.mGetCmdExitStatus() != 0:

                    _ilomNode = exaBoxNode(get_gcontext())
                    _ilomNode.mSetPassword(_lastpwd)
                    _ilomNode.mConnectAuthInteractive(aHost=_ilomHost['ilom'])

                    _cmds = []
                    _cmds.append(['->', 'set /SP/clients/syslog/1/ address={0}'.format(_hostIp)])
                    _cmds.append(['->', 'show /SP/clients/syslog/1'])
                    _cmds.append(['->', 'exit'])

                    _ilomNode.mExecuteCmdsAuthInteractive(_cmds)
                    _ilomNode.mDisconnect()

                else:
                    _node.mExecuteCmdLog('ipmitool sunoem cli "show /SP/clients/syslog/1"')

                _node.mDisconnect()

            except Exception as e:
                ebLogWarn("Could not configure Syslog on Ilom: {0}".format(e))

        _plist = ProcessManager()

        for _ilomHost in self.mReturnIlomHostInfo():
            _p = ProcessStructure(mConfigureSyslog, [_ilomHost])
            _p.mSetMaxExecutionTime(120) 
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()

    def mConfigureSyslogIBSwitches(self):
        if self.mIsKVM():
            ebLogWarn('*** KVM target detected - bypass Syslog IB switches setup')
            return
        # Get syslog format from exabox.conf
        # ib_syslog_format is same with SYSLOGFORMAT of OEDA properties
        __coptions = get_gcontext().mGetConfigOptions()
        _syslog_format = __coptions['ib_syslog_format']
        if _syslog_format is None or _syslog_format == '':
            ebLogError(
                '*** Could not find syslog format from exabox.conf')
            return

        # Change syslog format of switches
        _switches = self.mReturnSwitches(True)
        for _switch in _switches:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_switch)
            ebLogInfo(
                '*** Change syslog format of IB switch {0}'.format(_switch))
            try:
                self.mReplaceInFile(_node, "/etc/rsyslog.conf",
                        '$ActionFileDefaultTemplate',
                        '$ActionFileDefaultTemplate ExadataTmpl',
                        '# Use default timestamp format')
                self.mReplaceInFile(_node, "/etc/rsyslog.conf",
                        '$template ExadataTmpl',
                        '$template ExadataTmpl, ' + '"' + _syslog_format + '"',
                        '# Use default timestamp format')
                _node.mExecuteCmdLog("service rsyslog restart")
            except Exception as e:
                ebLogWarn(
                    '*** Could not change syslog format of IB switch {0}: {1}'
                    .format(_switch, e))
            _node.mDisconnect()

    def mAppendOedaKeysLog(self, aOptions):

        _time    = time.ctime()
        _logFile = "/opt/exacloud/keys/oedakeys.log"

        if not self.mIsKVM():
            _dom0s, _domUs, _cells, _switches = self.mReturnAllClusterHosts()
        else:
            _dom0s, _domUs, _cells, _ = self.mReturnAllClusterHosts()
            _switches = []
        _hosts = _dom0s + _cells + _switches

        for _host in _hosts:

            ebLogInfo("Run mAppendOedaKeysLog on {0}, file: {1}".format(_host, _logFile))

            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_host)

            #Prepare the folder since /opt/exacloud does not exist on cel
            _path = _logFile.split("/")[1:-1]
            for i in range(0, len(_path)):
                _folder = "/" + "/".join(_path[0:i+1])
                _node.mExecuteCmd("mkdir -p {0}".format(_folder))

            #Put the log content
            _node.mExecuteCmd("echo '*** Start Keys operation ***' >> {0}".format(_logFile))
            _node.mExecuteCmd("echo 'Time: {0}' >> {1}".format(_time, _logFile))

            if aOptions is not None and aOptions.clusterctrl is not None:
                _node.mExecuteCmd("echo 'Cluster Cmd: {0}' >> {1}".format(aOptions.clusterctrl, _logFile))

            _rc, _, _o, _ = self.mExecuteLocal("/bin/hostname")
            _host = _o.strip()
            _node.mExecuteCmd("echo 'Exacloud Hostname: {0}' >> {1}".format(_host, _logFile))

            _username = getpass.getuser()
            _node.mExecuteCmd("echo 'Exacloud User: {0}' >> {1}".format(_username, _logFile))

            _node.mExecuteCmd("echo 'Exacloud Path: {0}' >> {1}".format(os.path.abspath(sys.path[0]), _logFile))

            _node.mDisconnect()

    def mCopyExacmPatchKeyScript(self):
        _file_list = [ ("exacm_keys_patch_file.py", "u+x"), ("README.txt", None) ]
        _local_dir = "scripts/domukeys"
        _domu_dir = "/opt/exacloud/domukeys"
        _config_dir_cmd = "mkdir -p {0}".format(_domu_dir)
        _chmod_cmd = None
        _dpairs = self.mReturnDom0DomUPair()
        for _, _domU in _dpairs:
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domU)
            _node.mExecuteCmd(_config_dir_cmd)
            for _script_name, _chmode in _file_list:
                _source_script = "{0}/{1}".format(_local_dir, _script_name)
                _target_script = "{0}/{1}".format(_domu_dir, _script_name)
                _node.mCopyFile(_source_script, _target_script)
                if _chmode is not None:
                    _chmod_cmd = "chmod {0} {1}".format(_chmode, _target_script)
                    _node.mExecuteCmd(_chmod_cmd)
                _msg = "*** Saving script from '{0}' on '{1}:{2}' ".format(_source_script, _domU, _target_script)
                ebLogInfo(_msg)
            _node.mDisconnect()
        return

    def mGetAllDomUPrivateIPs(self):
        #Returns a list of Private IP addresses associated with the cluster
        _dPairs = self.mReturnDom0DomUPair()
        _IPList = []
        for _, _domU in _dPairs:
            _domU_mac = self.__machines.mGetMachineConfig(_domU)
            if _domU_mac is None:
                ebLogError('*** failed to retrieve machine config for domu: %s' %(_domU))
            _domU_net_list = _domU_mac.mGetMacNetworks()
            for _net in _domU_net_list:
                _netcnf = self.__networks.mGetNetworkConfig(_net)
                if _netcnf.mGetNetType() in [ 'private' ]:
                    _IPList.append(_netcnf.mGetNetIpAddr())
        return _IPList

    def mATPUnlockListeners(self, aNode=None):
        def _unlock_domU(aDomU, aOL8=True):
            _,_o,_ = aDomU.mExecuteCmd('/var/opt/oracle/ocde/rops atp_enabled')
            # During DeleteDB ATP flag is not sent by ECRA, detect from VM
            if not (_o and '1' in _o.read().strip()): # if not ATP
                return False
            if aOL8:
                aDomU.mExecuteCmd("nft flush chain ip filter INPUT")
                if self.__ociexacc:
                    ebExaCCAtpFiltering.sSetDomURules(aDomU)
                aDomU.mExecuteCmd("nft list ruleset > /etc/sysconfig/nftables.conf")
                return True
            else:
                aDomU.mExecuteCmd('iptables -F INPUT')
                ebLogInfo('ATP: Unlocking listeners: *** Saving iptables rules...')
                if self.__ociexacc:
                    #setup ExaCC-OCI ATP network lockdown
                    ebExaCCAtpFiltering.sSetDomURules(aDomU)
                aDomU.mExecuteCmdLog("sh -c '/sbin/iptables-save > /etc/sysconfig/iptables'")
                return True

        # Bug 36814084
        # By default we will skip the execution of this method. All the iprules
        # to set/unset inside the ADBD domUs will be done by ADBD code
        # To force it's execution set the flag 'skip_atpseclistener' to 'False'
        # in exabox.conf
        _SKIP_ATPSECLISTENER = "skip_atpseclistener"
        if str(get_gcontext().mGetConfigOptions().get(
                _SKIP_ATPSECLISTENER, "true")).lower() == "false":
            ebLogWarn("Executing the mATPUnlockListeners flow due to "
                F"exabox.conf flag: '{_SKIP_ATPSECLISTENER}'")

        else:
            ebLogWarn("Skipping mATPUnlockListeners due to exabox.conf flag: "
                f"'{_SKIP_ATPSECLISTENER}'")
            return 2

        # if active ssh connection is provided, use it, elsewise run on all domUs
        if aNode:
            _unlock_domU(aNode)
        else:
            for _,_domU in self.mReturnDom0DomUPair():
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_domU)
                _ol8 = self.mIsHostOL8(_domU)
                _rc = _unlock_domU(_node, _ol8)
                _node.mDisconnect()
                if not _rc:
                    return

    def mEnableDom0Service(self, aServiceName, aSingleDom0Node=None, aSingleDom0Name=''):
        """
            Enable a Service to be persistant across dom0 Reboot
            :param str aServiceName: name of the service
                                     example: 'iptables'
            :param exaboxNode aSingleDom0Node: OPTIONAL, reuse connected Node
            :param str aSingleDom0Name:        OPTIONAL, name of connected dom0

        """
        # Function for Single Dom0
        def _single_dom0_enable_service(aNode, aDom0, aServiceName):
            if self.mIsKVM():
                _state = aNode.mSingleLineOutput('systemctl is-enabled {}'.format(aServiceName)).strip()
                ebLogInfo("Service {} is {} on dom0 {}".format(
                          aServiceName, _state, aDom0))
                if _state == "disabled":
                    aNode.mExecuteCmd('systemctl enable {}'.format(
                                      aServiceName))
                    _state = aNode.mSingleLineOutput('systemctl is-enabled {}'.format(aServiceName)).strip()
                    ebLogInfo("Service {} is now {} on dom0 {}".format(
                          aServiceName, _state, aDom0))
            else:
                ebLogInfo("Check Config {} on {}".format(aServiceName, aDom0))

                _, _o, _ = aNode.mExecuteCmd("chkconfig --list {} 2>&1".format(aServiceName))
                _output = _o.read().strip()

                ebLogInfo(_output)

                if re.search("2\:on\s{1,}3\:on\s{1,}4\:on\s{1,}5\:on\s{1,}", _output) is None:
                    ebLogInfo("Up Service of {} on {}".format(aServiceName, aDom0))
                    aNode.mExecuteCmd("chkconfig {} on".format(aServiceName))
                    aNode.mExecuteCmdLog("chkconfig --list {} 2>&1".format(aServiceName))

        #Optimization to reuse connection
        if aSingleDom0Node:
            _single_dom0_enable_service(aSingleDom0Node, aSingleDom0Name, aServiceName)
        else:
            for _dom0, _ in self.mReturnDom0DomUPair():
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_dom0)
                _single_dom0_enable_service(_node, _dom0, aServiceName)
                _node.mDisconnect()
        
    def mStartDom0Service(self, aServiceName, aSingleDom0Node=None, aSingleDom0Name=''):
        """
            Start a service across dom0 if the service is not active.
            :param str aServiceName: name of the service
                                     example: 'nftables'
            :param exaboxNode aSingleDom0Node: OPTIONAL, reuse connected Node
            :param str aSingleDom0Name:        OPTIONAL, name of connected dom0

        """
        # Function for Single Dom0
        def _single_dom0_start_service(aNode, aDom0, aServiceName):
            _state = aNode.mSingleLineOutput('systemctl is-active {}'.format(aServiceName)).strip()
            ebLogInfo("Service {} is {} on dom0 {}".format(
                        aServiceName, _state, aDom0))
            if _state != "active":
                aNode.mExecuteCmd('systemctl start {}'.format(
                                    aServiceName))
                _state = aNode.mSingleLineOutput('systemctl is-active {}'.format(aServiceName)).strip()
                ebLogInfo("Service {} is now {} on dom0 {}".format(
                        aServiceName, _state, aDom0))
        #Optimization to reuse connection
        if aSingleDom0Node:
            _single_dom0_start_service(aSingleDom0Node, aSingleDom0Name, aServiceName)
        else:
            for _dom0, _ in self.mReturnDom0DomUPair():
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_dom0)
                _single_dom0_start_service(_node, _dom0, aServiceName)
                _node.mDisconnect()


    def mATPSecureListeners(self):

        # Bug 36814084
        # By default we will skip the execution of this method. All the iprules
        # to set/unset inside the ADBD domUs will be done by ADBD code
        # To force it's execution set the flag 'skip_atpseclistener' to 'False'
        # in exabox.conf
        _SKIP_ATPSECLISTENER = "skip_atpseclistener"
        if str(get_gcontext().mGetConfigOptions().get(
                _SKIP_ATPSECLISTENER, "true")).lower() == "false":
            ebLogWarn("Executing the mATPSecureListeners flow due to "
                F"exabox.conf flag: '{_SKIP_ATPSECLISTENER}'")

        else:
            ebLogWarn("Skipping mATPSecureListeners due to exabox.conf flag: "
                f"'{_SKIP_ATPSECLISTENER}'")
            return 2

        _dpairs = self.mReturnDom0DomUPair()

        # Set avoids duplicate elements
        _domU_ip_set = set()
        for _, _domU in _dpairs:

            _domU_mac = self.__machines.mGetMachineConfig(_domU)
            _domU_net_list = _domU_mac.mGetMacNetworks()

            _domU_ip_local = []
            for _net in _domU_net_list:
                _netcnf = self.__networks.mGetNetworkConfig(_net)
                if _netcnf.mGetNetType() == 'client':
                    _domU_ip_set.add(_netcnf.mGetNetIpAddr())
            #
            # Fetch Scan and VIP IPs
            #
            _domU_ip_set.update(self.mGetEbtablesScanVip(_domU))

        # Accept only traffic from Management (ECRA/OEDA) and other client IPs
        # for port 1521 and 2484(TCPS listener) until end of starterDB
        ebLogInfo('ATP: lock client listeners ports except for rack internal client/VIP ips: "{}"'.format(_domU_ip_set))

        def mGenerateCmds(aHost):

            _cmds = []
            _backend = ""

            if self.mIsHostOL8(aHost):

                _backend = "nftables"
                _cmds.append('add table ip filter')
                _cmds.append('add chain ip filter INPUT "{ type filter hook input priority filter; policy accept; }"')
                _cmds.append('add chain ip filter FORWARD "{ type filter hook input priority filter; policy accept; }"')
                _cmds.append('add chain ip filter OUTPUT "{ type filter hook input priority filter; policy accept; }"')
                _cmds.append('add rule ip filter INPUT iifname "lo" counter accept')
                _cmds.append('add rule ip filter INPUT iifname "eth0" tcp dport 1521 counter accept')
                _cmds.append('add rule ip filter INPUT iifname "eth0" tcp dport 2484 counter accept')
                _cmds += list(map('add rule ip filter INPUT iifname "bondeth0" ip saddr {} tcp dport 1521 counter accept'.format, _domU_ip_set))
                _cmds += list(map('add rule ip filter INPUT iifname "bondeth0" ip saddr {} tcp dport 2484 counter accept'.format, _domU_ip_set))
                _cmds.append("add rule ip filter INPUT tcp dport 1521 counter drop")
                _cmds.append("add rule ip filter INPUT tcp dport 2484 counter drop")

            else:

                _backend = "iptables"
                _cmds.append("-A INPUT -i lo -j ACCEPT")
                _cmds.append("-A INPUT -i eth0 -p tcp --dport 1521 -j ACCEPT")
                _cmds.append("-A INPUT -i eth0 -p tcp --dport 2484 -j ACCEPT")
                _cmds += list(map("-A INPUT -i bondeth0 -p tcp -s {} --dport 1521 -j ACCEPT".format, _domU_ip_set))
                _cmds += list(map("-A INPUT -i bondeth0 -p tcp -s {} --dport 2484 -j ACCEPT".format, _domU_ip_set))
                _cmds.append("-A INPUT -p tcp --dport 1521 -j DROP")
                _cmds.append("-A INPUT -p tcp --dport 2484 -j DROP")

            return _backend, _cmds

        def _lock_domU(aNode, aCmds, aBackend):

            if aBackend == "iptables":

                for _cmd in aCmds:
                    aNode.mExecuteCmd('/usr/sbin/iptables ' + _cmd)

                #
                # Commit/Save rules for iptables v4
                #
                ebLogInfo('*** Saving iptables rules...')
                aNode.mExecuteCmdLog("/sbin/iptables-save > /etc/sysconfig/iptables")
                aNode.mExecuteCmdLog("/usr/bin/systemctl start iptables")
                aNode.mExecuteCmdLog("/usr/bin/systemctl enable iptables")

            else: # nftables

                for _cmd in aCmds:
                    aNode.mExecuteCmd('/usr/sbin/nft ' + _cmd)

                #
                # Commit/Save rules for nftables v4
                #
                ebLogInfo('*** Saving nftables rules...')
                aNode.mExecuteCmdLog("/usr/sbin/nft list ruleset > /etc/sysconfig/nftables.conf")
                aNode.mExecuteCmdLog("/usr/bin/systemctl start nftables")
                aNode.mExecuteCmdLog("/usr/bin/systemctl enable nftables")

        for _,_domU in _dpairs:
            with connect_to_host(_domU, get_gcontext()) as _node:
                _backend, _cmds = mGenerateCmds(_domU)
                _lock_domU(_node, _cmds, _backend)

    def mReleaseRemoteLockUnowned(self, aUUID, aLockName='Default'):
        """
        Release unowned lock for a given uuid
        """

        self.remote_lock.release_unowned(aUUID, aLockName)

    def mHandlerOperationCleanup(self, aOptions=None):

        if not aOptions: 
            aOptions = self.mGetArgsOptions()

        _db = ebGetDefaultDB()

        # Update requests table.
        _jconf = aOptions.jsonconf
        if _jconf is None or 'uuid' not in _jconf.keys():
            #This function is unable to proceed other UUID since
            #it relies on XML being passed matching UUID to kill
            _err = 'uuid is required in JSON payload for op_cleanup endpoint'
            ebLogError(_err)
            raise KeyError(_err)

        _uuid = _jconf['uuid']

        # Update entry in request table corresponding to the operation (uuid)
        from exabox.agent.Agent import ebGetRequestObj
        _reqobj = ebGetRequestObj(_uuid)
        _reqobj.mSetStatus('Done')
        _reqobj.mSetError('709')
        _reqobj.mSetErrorStr('Error in Execution: [Operation Cleanup endpoint called explicitly]')
        _db = ebGetDefaultDB()
        _db.mUpdateRequest(_reqobj)

        # Delete lock acquired.
        self.mReleaseRemoteLockUnowned(aUUID=_uuid)

        # kill the worker process running the operation (_uuid)
        _rqlist = literal_eval(_db.mDumpWorkers(_uuid))
        for _rq in _rqlist:
            _pid = _rq[8]
            if _pid.isdigit():
                ebLogInfo("*** op_cleanup endpoint, killing Worker with PID: {} for UUID {}".format(_pid,_uuid))
                os.kill(int(_pid),signal.SIGKILL)

        # Free locks Under acquisition by past UUID
        _locks = _db.mGetLocksByUUIDAndType(_uuid, ebDBLockTypes.DOM0_LOCK_ACQUIRING)
        for _lock in _locks:
            _dom0 = _lock['lock_hostname']
            try:
                _db.mDeleteLock(_uuid, ebDBLockTypes.DOM0_LOCK_ACQUIRING, _dom0)
            except Exception as e:
                ebLogWarn('Unable to cleanup UUID {} partially acquired lock: {}'.format(_uuid, e))

        # Remove entry from DB, corresponding to the worker process
        _db.mClearWorkers(_uuid)

        # Delete entry from registry table
        _db.mDelRegByUUID(_uuid)

        ebLogInfo("Operation cleanup completed.")

    def mCompareExadataModel(self, aExaModelFirst, aExaModelSecond):
        """
        Method for compare the integer part of two Exadata model
        X8 -> 8 integer part, X10 -> 10 integer part
        aExaModelFirst : String with the exadata model.
        aExaModelSecond : String with the exadata model.
        return: result_compare : Int
        result_compare:
                0  if two params are  equal.
                1  if the first param is greather.
                -1 if the second param is greather
        Example (assuming valid entries for exadata model)
            assert mCompareExadataModel("X10","X7") == 1
            assert mCompareExadataModel("X7","X7") == 0
            assert mCompareExadataModel("X6","X7") == -1
        """
        _result = 0
        _exa_first_list = [ _char for _char in aExaModelFirst if _char.isdigit() is True]
        _model_first = "".join(_exa_first_list)
        _exa_second_list = [ _char for _char in aExaModelSecond if _char.isdigit() is True]
        _model_second = "".join(_exa_second_list)
        return mCompareModel(_model_first, _model_second)


    def mAddUserDomU(self, aUser, aUID, aSudoAccess=False, aPasswordLess=False, aGID=None):
        def _addUserPerDomU(aDomU, aUser, aUID, aSudoAccess=False, aPasswordLess=False, aGID=None):
            _domU = aDomU
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_domU)

            if aGID:
                _node.mExecuteCmd('/usr/sbin/groupadd -g {0} {1}'.format(aGID, aUser))
                _node.mExecuteCmd('/usr/sbin/useradd -u {0} -g {1} -d /home/{2} -s /bin/bash {2}'.format(aUID, aGID, aUser))
            else:
                _node.mExecuteCmd('/usr/sbin/useradd -u {0} -d /home/{1} -s /bin/bash {1}'.format(aUID, aUser))

            if aSudoAccess:
                _node.mExecuteCmd("echo '{0} ALL=(ALL) NOPASSWD: ALL' >> /etc/sudoers".format(aUser))

            _node.mExecuteCmd("mkdir -p /home/{0}/.ssh".format(aUser))
            _node.mExecuteCmd("chmod 700 /home/{0}/.ssh".format(aUser))
            _node.mExecuteCmd("chown {0}:{0} /home/{0}/.ssh".format(aUser))

            _node.mDisconnect()
        
        _plist = ProcessManager()

        for _, _domU in self.mReturnDom0DomUPair():
            _p = ProcessStructure(_addUserPerDomU, [_domU, aUser, aUID, aSudoAccess, aPasswordLess, aGID])
            _p.mSetMaxExecutionTime(30*60) #30 minutes timeout
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)

        _plist.mJoinProcess()

        self.mAddUserPubKey(aUser)

        if aPasswordLess:
            self.mConfigurePasswordLessDomU(aUser)


    # mStoreDomUInterconnectIps: Stores the interconnect IP's of
    # the VM's in cluster_interconnect.dat
    def mStoreDomUInterconnectIps(self):

        _node = exaBoxNode(get_gcontext())
        _dpairs = self.mGetOrigDom0sDomUs()
        _vm = _dpairs[0][1]

        _user = 'oracle'
        _node.mConnect(aHost=_vm)

        try:
            _ls_cmd = node_cmd_abs_path_check(_node, "ls")
            _cmd = f"{_ls_cmd} -la /var/opt/oracle/dbaas_acfs | head -2"
            _i, _o, _e = _node.mExecuteCmd(_cmd)                                                                                                                                                 
            _out = _o.readlines()
            ebLogInfo(f'*** dbaas_acfs directory permissions: {_out}')
        except Exception as ex:
            ebLogError(f"Could not obtain dbaas_acfs permissions. Error: {ex}.")

        _node.mExecuteCmdLog("/usr/bin/chmod u+xw,g=xr /var/opt/oracle/dbaas_acfs/ovm_template/cluster_interconnect.dat")

        _cmd = "mkdir -p /var/opt/oracle/dbaas_acfs/ovm_template/; " + \
        "rm -f /var/opt/oracle/dbaas_acfs/ovm_template/cluster_interconnect.dat; " + \
        "touch /var/opt/oracle/dbaas_acfs/ovm_template/cluster_interconnect.dat;"

        _node.mExecuteCmdLog(_cmd)
        #The ovm_template extracted by dbaas has all files created with a non-standard uid of 1429878. Lets change ownership of all those files to oracle:oinstall
        _node.mExecuteCmdLog(f"/bin/chown -R `id -u {_user}`:`id -g {_user}` /var/opt/oracle/dbaas_acfs/ovm_template")

        for _, _domU in _dpairs:

            _domU_mac = self.__machines.mGetMachineConfig(_domU)
            if _domU_mac is None:
                ebLogWarn('*** failed to retrieve machine config for domu: %s' %(_domU))
                continue
            _domU_net_list = _domU_mac.mGetMacNetworks()
            _arr = []
            for _net in _domU_net_list:
                _netcnf = self.__networks.mGetNetworkConfig(_net)
                if _netcnf.mGetNetType() == 'private':
                    _pkeyname = _netcnf.mGetPkeyName()
                    # TODO, an alternative  way to obtain cluster_interconnect inteface:
                    # /<grid_home>/bind/oifcfg getif since when QinQ solution comes
                    # pkeys won't exist anymore
                    if self.mIsKVM():
                        _iname = _netcnf.mGetInterfaceName()
                        if self.__debug:
                            ebLogInfo('*** interface name: %s' % (str(_iname)))
                        if _iname is not None and _iname[:2] == 'cl':
                            _arr.append(_netcnf.mGetNetIpAddr())
                        elif _iname is not None and _iname[:2] == 'st':
                             if self.__debug:
                                 ebLogInfo('*** skipping storage interface: %s' % (_iname))
                        elif _iname == 'UNDEFINED':
                             # MR: This is only needed for older ROCE/KVM XML schema (e.g. pre cl/streX)
                             _arr.append(_netcnf.mGetNetIpAddr())
                    else:
                        if _pkeyname[:2] == 'cl':
                            _arr.append(_netcnf.mGetNetIpAddr())

            ebLogInfo("interconnect ips: %s: %s" % (_arr[0], _arr[1]))

            _node.mExecuteCmdLog("/usr/bin/chmod u+xw,g=xr /var/opt/oracle/dbaas_acfs/ovm_template/cluster_interconnect.dat")
            _cmd = '/bin/echo "%s|%s:%s" >> /var/opt/oracle/dbaas_acfs/ovm_template/cluster_interconnect.dat' % (_domU.split('.')[0], _arr[0], _arr[1])
            _node.mExecuteCmdLog(_cmd)
            try:
                _cat_cmd = node_cmd_abs_path_check(_node, "cat")
                _cmd = f"{_cat_cmd} /var/opt/oracle/dbaas_acfs/ovm_template/cluster_interconnect.dat"
                _i, _o, _e = _node.mExecuteCmd(_cmd)                                                                                                                                                 
                _out = _o.readlines()
                ebLogTrace(f'*** cluster_interconnect.dat file content: {_out}')
            except Exception as ex:
                ebLogWarn(f"Could not obtain cluster_interconnect.dat file contents. Error: {ex}.")
            _node.mExecuteCmdLog("/usr/bin/chmod u-w /var/opt/oracle/dbaas_acfs/ovm_template/cluster_interconnect.dat")

        _node.mDisconnect()

    def mForceDeleteDomainUnnamed(self, aOptions, aDom0DomUPair=None):

        if aDom0DomUPair:
            _ddpair = aDom0DomUPair
        else:
            _ddpair = self.mReturnDom0DomUPair()

        if self.mCheckConfigOption("_force_delete_unnamed", "True"):

            ebLogInfo("Running Force Delete Domain-Unnamed")

            for _dom0, _domU in _ddpair:

                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_dom0)

                _vmhandle = self.__CompRegistry.mGetComponent("vm_operations")
                _vmhandle.mSetOVMCtrl(aCtx=get_gcontext(), aNode=_node)

                # Print running Domus that are not in the XML
                if not self.__shared_env:

                    for _domuRunning in _vmhandle.mGetVmCtrl().mGetDomUs():
                        found = False
                        for _, _domu in self.mReturnDom0DomUPair():
                            if _domuRunning == _domu:
                                found = True
                                break
                        if not found:
                            ebLogWarn("DomU Running not present in XML: {0}".format(_domuRunning))

                _rc = _vmhandle.mDispatchEvent("destroy", aOptions, aVMId="Domain-Unnamed", aCluCtrlObj=self)

                if _rc == 0:
                    ebLogInfo("Domain-Unnamed removed")

        else:
            ebLogInfo("Skip Force Delete Domain-Unnamed")

    def mValidateTmpKeyVm(self, aOptions, aJConf=None):

        # Validate parameters
        if aJConf:
            _jconf = aJConf
        else:
            _jconf = aOptions.jsonconf

        if "user" not in _jconf.keys():
            raise ExacloudRuntimeError(0x0119, 0xA, "Missing 'user' key in jsonconf")

        if "vmName" not in _jconf.keys():
            raise ExacloudRuntimeError(0x0119, 0xA, "Missing 'vmName' key in jsonconf")

        _domUs = list(map(lambda x: x[1], self.mReturnDom0DomUPair()))

        if _jconf["vmName"] != "_all_" and _jconf["vmName"] not in _domUs:
                raise ExacloudRuntimeError(0x0119, 0xA, "Provided host is not domU {0}".format(_jconf["vmName"]))

        if 'badNodes' in _jconf.keys() and _jconf['badNodes']:
            _badNodes = _jconf['badNodes']
            for _badNode in _badNodes:
                for _dom0, _domU in self.mReturnDom0DomUPair():
                    if _badNode == _dom0.split('.')[0] or _badNode == _dom0:
                        _domUs.remove(_domU)

        for _domU in _domUs:
            if  _jconf['vmName']== "_all_" or _domU == _jconf['vmName']:

                try:
                    ebLogInfo("Try connection with: {0}/{1}".format(_domU, _jconf['user']))
                    _node = exaBoxNode(get_gcontext())

                    if _jconf['user'] != "root":
                        _node.mSetUser(_jconf['user'])

                    _node.mConnect(aHost=_domU)
                    _node.mDisconnect()
                except Exception as e:
                    ebLogWarn(e)
                    raise

        ebLogInfo("All connections works")


    def mGenerateTmpKeyVm(self, aOptions, aJConf=None):

        # Validate parameters
        if aJConf:
            _jconf = aJConf
        else:
            _jconf = aOptions.jsonconf

        if "user" not in _jconf.keys():
            raise ExacloudRuntimeError(0x0119, 0xA, "Missing 'user' key in jsonconf")

        if "vmName" not in _jconf.keys():
            raise ExacloudRuntimeError(0x0119, 0xA, "Missing 'vmName' key in jsonconf")

        _domUs = list(map(lambda x: x[1], self.mReturnDom0DomUPair()))

        if _jconf["vmName"] != "_all_" and _jconf["vmName"] not in _domUs:
                raise ExacloudRuntimeError(0x0119, 0xA, "Provided host is not domU {0}".format(_jconf["vmName"]))

        # Create one temporal key
        _generatedKeys = {}
        _exakms = get_gcontext().mGetExaKms()

        if 'badNodes' in _jconf.keys() and _jconf['badNodes']:
            _badNodes = _jconf['badNodes']
            for _badNode in _badNodes:
                for _dom0, _domU in self.mReturnDom0DomUPair():
                    if _badNode == _dom0.split('.')[0] or _badNode == _dom0:
                        _domUs.remove(_domU)

        # Generate new structure of return
        _generatedKeys["ssh_keys"] = []
        _userKeys = {}

        # Register the temporal key
        for _domU in _domUs:
            if _jconf['vmName'] == "_all_" or _domU == _jconf['vmName']:

                _payloadNode = {}

                _payloadNode["vmName"] = _domU
                _payloadNode["keys"] = []
                _userList = None

                if isinstance(_jconf['user'], list):
                    _userList = _jconf['user']
                else:
                    _userList = [_jconf['user']]

                for _user in _userList:

                    if _user not in _userKeys:
                        _userKeys[_user] = _exakms.mGetEntryClass().mGeneratePrivateKey()

                    _newEntry = _exakms.mBuildExaKmsEntry(
                        _domU,
                        _user,
                        _userKeys[_user],
                        ExaKmsHostType.DOMU
                    )

                    _exakms.mInsertExaKmsEntry(_newEntry)

                    _payloadNode["keys"].append({
                        "user": _user,
                        "public_key": _newEntry.mGetPublicKey("TEMPORAL_KEY")
                    })

                _generatedKeys["ssh_keys"].append(_payloadNode)

        ebLogInfo("Generate new temporal key sucess")

        # return the keys
        if aOptions is not None:
            _reqobj = self.mGetRequestObj()
            if _reqobj is not None:
                _reqobj.mSetData(json.dumps(_generatedKeys, sort_keys=True))
                _db = ebGetDefaultDB()
                _db.mUpdateRequest(_reqobj)
            else:
                ebLogInfo(json.dumps(_generatedKeys, sort_keys=True))

        return _generatedKeys


    def mCleanUpTmpKeyVm(self, aOptions, aJConf=None, aLiveDomUNode=None):

        # Validate parameters
        if aJConf:
            _jconf = aJConf
        else:
            _jconf = aOptions.jsonconf

        if "user" not in _jconf.keys():
            raise ExacloudRuntimeError(0x0119, 0xA, "Missing 'user' key in jsonconf")

        if "vmName" not in _jconf.keys():
            raise ExacloudRuntimeError(0x0119, 0xA, "Missing 'vmName' key in jsonconf")

        _domUs = list(map(lambda x: x[1], self.mReturnDom0DomUPair()))

        if _jconf["vmName"] != "_all_" and _jconf["vmName"] not in _domUs:
                raise ExacloudRuntimeError(0x0119, 0xA, "Provided host is not domU {0}".format(_jconf["vmName"]))

        if 'badNodes' in _jconf.keys() and _jconf['badNodes']:
            _badNodes = _jconf['badNodes']
            for _badNode in _badNodes:
                for _dom0, _domU in self.mReturnDom0DomUPair():
                    if _badNode == _dom0.split('.')[0] or _badNode == _dom0:
                        _domUs.remove(_domU)

        # Register the temporal key
        for _domU in _domUs:
            if  _jconf['vmName']== "_all_" or _domU == _jconf['vmName']:

                _ctx = get_gcontext()
                if _ctx.mCheckRegEntry('_natHN_' + _domU):
                    _domU = _ctx.mGetRegEntry('_natHN_' + _domU)

                # Connect to the nodes marked in the payload
                _user = _jconf['user']
                _internalNode = aLiveDomUNode
                _nodeConnected = False

                if not _internalNode:
                    _internalNode = exaBoxNode(get_gcontext())
                    _internalNode.mSetUser(_user)
                    if _internalNode.mIsConnectable(aHost=_domU):
                        _internalNode.mConnect(aHost=_domU)
                        _nodeConnected = True

                else:
                    _nodeConnected = True

                # Delete temporal key and do lockdown if in payload
                if _nodeConnected:
                    if not self.mIsAdbs():
                        ebLogInfo("Deleting from the DomU: {0}/{1}".format(_domU, _user))
                        _internalNode.mExecuteCmd("sed -i '/TEMPORAL_KEY/d' .ssh/authorized_keys")

                    if "lockdown" in _jconf and _jconf['lockdown'] == "true" and _user == "root":
                        ebLogInfo("Lockdown the DomU: {0}/{1}".format(_domU, _user))
                        _internalNode.mExecuteCmd("/opt/oracle.cellos/host_access_control rootssh -l")

                if not aLiveDomUNode and _nodeConnected:
                    _internalNode.mDisconnect()

                # Clean the keys in FS

                _exakms = get_gcontext().mGetExaKms()
                _cparam = {"FQDN": _domU, "user": _user}
                _entry = _exakms.mGetExaKmsEntry(_cparam)

                if _entry:
                    ebLogInfo("Clean the keys in KMS")
                    _exakms.mDeleteExaKmsEntry(_entry)

        ebLogInfo("Cleanup complete")

    def mSetMultiRequestKv(self, aKey, aValue):

        from exabox.agent.Agent import ebAgentInfo

        _agentInfo = ebAgentInfo()
        _agentInfo.mLoadAgentFromDB(get_gcontext().mGetConfigOptions()['agent_id'])

        _kv = json.loads(_agentInfo.mGetMisc())
        _kv[aKey] = aValue
        _agentInfo.mSetMisc(json.dumps(_kv))

        _db = ebGetDefaultDB()
        _db.mUpdateAgent(_agentInfo)

    def mGetMultiRequestKv(self, aKey):

        from exabox.agent.Agent import ebAgentInfo

        _agentInfo = ebAgentInfo()
        _agentInfo.mLoadAgentFromDB(get_gcontext().mGetConfigOptions()['agent_id'])

        _kv = json.loads(_agentInfo.mGetMisc())
        if aKey in _kv.keys():
            return _kv[aKey]
        else:
            return None

    def mCreatePreVmKeysOedaWorkDir(self):

        _exakms = get_gcontext().mGetExaKms()

        # Generate DomU keys
        _user = "root"
        for _, _domU in self.mReturnDom0DomUPair():

            _cparam = {"FQDN": _domU, "user": _user}
            _entry = _exakms.mGetExaKmsEntry(_cparam)

            if _entry:
                ebLogInfo(f"Kms entry already exists: {_entry}")
                _exakms.mDeleteExaKmsEntry(_entry)

            ebLogInfo(f"Generating new domU key: {_user}@{_domU}")

            _entry = _exakms.mBuildExaKmsEntry(
                _domU,
                _user,
                _exakms.mGetEntryClass().mGeneratePrivateKey(),
                ExaKmsHostType.DOMU
            )
            _exakms.mInsertExaKmsEntry(_entry)

        self.mHandlerGenerateSwitchesKeys()

    def mHandlerGenerateSwitchesKeys(self):

        _exakms = get_gcontext().mGetExaKms()

        # Get the switches from the XML
        _, _, _, _sws = self.mReturnAllClusterHosts()

        # Add keys for KVM-ROCE Switches
        _roceSwitches = []
        if get_gcontext().mCheckRegEntry("ROCE_SWITCHES"):
            _roceSwitches = get_gcontext().mGetRegEntry("ROCE_SWITCHES")
            _sws += _roceSwitches

        if _sws:

            _switches = _sws

            # In case of ibswitches is necessary to fetch the other switches connected
            if not get_gcontext().mCheckRegEntry("ROCE_SWITCHES"):

                _node = exaBoxNode(get_gcontext())
                _node.mConnect(_sws[0])

                # 36283497: ibswitches output may contain warnings which should not be parsed
                # Warning can be as below:
                # ibwarn: [17529] _do_madrpc: recv failed: Connection timed out (Need to avoid parsing these)
                _cmd = """ibswitches | grep Switch | awk '{print "echo -n "$10".; hostname -d"}' | bash"""
                _, _o, _ = _node.mExecuteCmd(_cmd)
                _switches = list(map(lambda x: x.strip(), _o.readlines()))

                _node.mDisconnect()

            # Create the keys for the switches
            for _switch in _switches:

                # For roce switches will be on admin
                _user = "root"
                if _switch in _roceSwitches:
                    _user = "admin"

                # Pregenerate kms entry
                _cparam = {"FQDN": _switch, "user": _user}
                _entry = _exakms.mGetExaKmsEntry(_cparam)

                if not _entry:
                    ebLogInfo(f"Kms entry not exists for: {_user}@{_switch}, Generating")

                    _entry = _exakms.mBuildExaKmsEntry(
                            _switch,
                            _user,
                            _exakms.mGetEntryClass().mGeneratePrivateKey(),
                            ExaKmsHostType.SWITCH
                    )

                    _exakms.mInsertExaKmsEntry(_entry)
                else:
                    ebLogInfo(f"Kms entry already exists for: {_user}@{_switch}")

                _pubKeyContent = _entry.mGetPublicKey("EXACLOUD KEY")

                # Inject the key on switch
                if _switch in _roceSwitches:

                    # In the case of roce-switches will be injected using the admin console
                    _node = exaBoxNode(get_gcontext())
                    try:
                        _cmds = []
                        _cmds.append(['#', 'configure terminal'])
                        _cmds.append(['#', 'username admin sshkey {0}'.format(_pubKeyContent)])
                        _cmds.append(['#', 'exit']) # exit configure terminal
                        _cmds.append(['#', 'exit']) # exit ssh connection

                        _lastpwd = self.mGetIlomPass()

                        _node.mSetUser(_user)
                        _node.mSetPassword(_lastpwd)

                        ebLogInfo("mGenerateSwitchesKeys: Try authentication: {0}".format(_switch))
                        _node.mConnectAuthInteractive(aHost=_switch)

                        ebLogInfo("mGenerateSwitchesKeys: Execute command on switch: {0}".format(_switch))
                        _node.mExecuteCmdsAuthInteractive(_cmds)

                        if self.__debug:
                            ebLogInfo("mGenerateSwitchesKeys: Read from socket: [{0}]".format(_node.mGetConsoleRawOutput()))

                        ebLogInfo("mGenerateSwitchesKeys: Key injection complete: {0}".format(_switch))

                    except Exception as e:
                        ebLogError("mGenerateSwitchesKeys: Could not inject key in {0}: ".format(_switch))
                        ebLogError(e)
                    finally:
                        _node.mDisconnect()

                else:

                    _node = exaBoxNode(get_gcontext())
                    try:
                        # In case of ibswitches will be injected using root on the .ssh/authorized keys
                        _node.mConnect(_switch)
                        # The public key content is having a \n at the end which is messing up with grep of
                        # it in the existing authorized keys file and it is always returning that the key is
                        # present in the authorized key file even if it is not there.
                        _pubKeyContent = _pubKeyContent.strip()
                        _cmd = """! cat /root/.ssh/authorized_keys | grep "{0}" && """.format(_pubKeyContent)
                        _cmd += """echo "{0}" >> /root/.ssh/authorized_keys""".format(_pubKeyContent)
                        _node.mExecuteCmd(_cmd)

                        ebLogInfo("mGenerateSwitchesKeys: Key injection complete: {0}".format(_switch))

                    except Exception as e:
                        ebLogError("mGenerateSwitchesKeys: Could not inject key in {0}: ".format(_switch))
                        ebLogError(e)
                    finally:
                        _node.mDisconnect()


    def mRestoreRootAccess(self, aOptions):

        if not self.mCheckConfigOption('enable_restore_root', 'True'):
            ebLogInfo("mRestoreRootAccess skipped since enable_restore_root is not True")
            return

        if not self.mCheckConfigOption('remove_root_access', 'True'):
            ebLogInfo("mRestoreRootAccess skipped since remove_root_access is not True")
            return

        _jconf = aOptions.jsonconf
        if _jconf and 'TargetType' in _jconf:
            if "domu" not in _jconf['TargetType']:
                ebLogInfo("mRestoreRootAccess skipped since targed is: {0}".format(_jconf["TargetType"]))
                return

        ebLogInfo("Running mRestoreRootAccess")

        # Set opc_enabled only for this operation
        get_gcontext().mSetRegEntry('opc_enabled', 'True')

        # Increase the counter of concurrent operations
        _regEntry = "{0}_tmp_keys".format(self.__key)
        _concurrent = self.mGetMultiRequestKv(_regEntry)

        if _concurrent is None or _concurrent <= 0:
            self.mSetMultiRequestKv(_regEntry, 1)
            ebLogInfo("Set Counter of {0} to {1}".format(_regEntry, 1))
        else:
            self.mSetMultiRequestKv(_regEntry, _concurrent+1)
            ebLogInfo("Set Counter of {0} to {1}".format(_regEntry, _concurrent+1))

        # Create temporal key if not exists
        if self.mGetMultiRequestKv(_regEntry) == 1:

            # Generate temporal key
            _conf = {"vmName": "_all_", "user": "root"}
            _tmpKey = self.mGenerateTmpKeyVm(None, aJConf=_conf)["ssh_keys"][0]["keys"][0]

            for _, _domU in self.mReturnDom0DomUPair():

                _node = exaBoxNode(get_gcontext())
                if _node.mIsConnectable(aHost=_domU):
                    _node.mConnect(aHost=_domU)

                    # Provide root access
                    _cmd = "sh -c 'echo \"{0}\" >> /root/.ssh/authorized_keys'".format(_tmpKey["public_key"].strip())
                    _node.mExecuteCmd(_cmd)
                    _node.mExecuteCmd("sh -c '/opt/oracle.cellos/host_access_control rootssh -u'")

                    _node.mDisconnect()
                else:
                    ebLogInfo("VM {} is not reachable".format(_domU))

        # Restore permision of opc enabled
        get_gcontext().mSetRegEntry('opc_enabled', "False")

    def mUnRestoreRootAccess(self, aOptions):

        if not self.mCheckConfigOption('enable_restore_root', 'True'):
            ebLogInfo("mUnRestoreRootAccess skipped since enable_restore_root is not True")
            return

        if not self.mCheckConfigOption('remove_root_access', 'True'):
            ebLogInfo("mUnRestoreRootAccess skipped since remove_root_access is not True")
            return

        _jconf = aOptions.jsonconf
        if _jconf and 'TargetType' in _jconf:
            if "domu" not in _jconf['TargetType']:
                ebLogInfo("mUnRestoreRootAccess skipped since targed is: {0}".format(_jconf["TargetType"]))
                return

        ebLogInfo("Running mUnRestoreRootAccess")

        # Decrement the counter of concurrent operations
        _regEntry = "{0}_tmp_keys".format(self.__key)
        _concurrent = self.mGetMultiRequestKv(_regEntry)

        if _concurrent is None or _concurrent <= 0:
            self.mSetMultiRequestKv(_regEntry, 0)
            ebLogInfo("Set Counter of {0} to {1}".format(_regEntry, 0))
        else:
            self.mSetMultiRequestKv(_regEntry, _concurrent-1)
            ebLogInfo("Set Counter of {0} to {1}".format(_regEntry, _concurrent-1))

        # Create temporal key if not exists
        if self.mGetMultiRequestKv(_regEntry) == 0:

            for _, _domU in self.mReturnDom0DomUPair():

                _node = exaBoxNode(get_gcontext())
                if _node.mIsConnectable(aHost=_domU):
                    _node.mConnect(aHost=_domU)

                    _conf = {"vmName": _domU, "user": "root"}
                    self.mCleanUpTmpKeyVm(None, aJConf=_conf, aLiveDomUNode=_node)

                    _node.mExecuteCmd("/opt/oracle.cellos/host_access_control rootssh -l")
                    _node.mDisconnect()
                else:
                    ebLogInfo("VM {} is not reachable".format(_domU))

    def mDeletePKeyCell(self):

        # Single VM and Bare Metal env
        if self.mIsExabm() and not self.mGetSharedEnv():

            ebLogInfo("Running mDeletePKeyCell")

            # Apply the commands to delete the pkeys of cells
            for _cell in  self.mReturnCellNodes():

                _node = exaBoxNode(get_gcontext(), Cluctrl = self)
                _node.mConnect(aHost=_cell)

                _node.mExecuteCmdLog("cellcli -e alter cell shutdown services all")
                _node.mExecuteCmdLog("cellcli -e list cell detail")
                _node.mExecuteCmdLog("ipconf -pkey-getruntime")
                _node.mExecuteCmdLog("ipconf -pkey-delete /var/log/cellos/pkey_backup/pkey/generated/Pkey_runtime.conf -pkey-apply")
                _node.mExecuteCmdLog("ip a sync")

                _node.mDisconnect()

            # Then Reboot the modified cells in parallel as well
            _plist = ProcessManager()

            for _cellToReboot in  self.mReturnCellNodes():
                ebLogInfo("Rebooting cell %s." % _cellToReboot)
                _p = ProcessStructure(self.mRebootNode, (_cellToReboot,), _cellToReboot, -1, None)
                _p.mSetMaxExecutionTime(60*60)
                _p.mSetJoinTimeout(30)
                _p.mSetLogTimeoutFx(ebLogWarn)
                _plist.mStartAppend(_p)

            _plist.mJoinProcess()

    def mSshDiagnostic(self):

        ebLogTrace("Running ssh diagnostic")

        _host_list = self.mGetHostList()

        for _host in _host_list:
            _node = exaBoxNode(get_gcontext())
            _isConnetable = _node.mIsConnectable(aHost=_host)
            ebLogTrace("Diag {0} is Connectable? {1}".format(_host, _isConnetable))


    def mAddUseDnsFlag(self):
        """
        Add 'UseDNS no' to the sshd_config file from te DomUs without repetitions
        """

        ebLogInfo("Add UseDns=no flag from sshd_config file")

        #Restore UseDNS=no
        for _ , _domu in self.mReturnDom0DomUPair():

            _node = exaBoxNode(get_gcontext())
            try:
                _node.mConnect(aHost=_domu)

                _cmd = "/bin/sed --follow-symlinks -i '/^#UseDNS\ /d' /etc/ssh/sshd_config"
                _node.mExecuteCmd(_cmd)
                _cmd = "/bin/sed --follow-symlinks -i 's/^UseDNS\ /#UseDNS\ /g' /etc/ssh/sshd_config"
                _node.mExecuteCmdLog(_cmd)
                _cmd = "/bin/echo 'UseDNS no' >> /etc/ssh/sshd_config"
                _node.mExecuteCmdLog(_cmd)
                _cmd = "/sbin/service sshd restart"
                _node.mExecuteCmdLog(_cmd)

            finally:
                _node.mDisconnect()


    def mRemoveUseDnsFlag(self):
        """
        Remove 'UseDNS no' from the sshd_config file
        """

        ebLogInfo("Remove UseDns flag from sshd_config file")

        #Restore UseDNS=no
        for _ , _domu in self.mReturnDom0DomUPair():

            _node = exaBoxNode(get_gcontext())
            try:
                _node.mConnect(aHost=_domu)

                _cmd = "/bin/sed --follow-symlinks -i '/^UseDNS\ /d' /etc/ssh/sshd_config"
                _node.mExecuteCmd(_cmd)
                _cmd = "/bin/sed --follow-symlinks -i 's/#UseDNS\ /UseDNS\ /g' /etc/ssh/sshd_config"
                _node.mExecuteCmd(_cmd)
                _cmd = "/sbin/service sshd restart"
                _node.mExecuteCmdLog(_cmd)

            finally:
                _node.mDisconnect()

    def mHandlerUnLockCellUsers(self):
        return self.mLockCellUsers(False)

    def mHandlerLockCellUsers(self):
        return self.mLockCellUsers(True)

    def mLockCellUsers(self, aMode=True):
        """
        Lock/Unlock password of the defined linux users on the cells
        :param aMode: Lock the linux users password if True. Unlock otherwise
        """
        def _mUpdate_cell_user_password(aCell, aCellMode):
            """
            Internal function for parallel framework
            :param aCell: Cell host
            :param aCellMode: Lock the linux users password if True. Unlock otherwise
            """
            _cell_node = exaBoxNode(get_gcontext())
            _cell_node.mConnect(aHost=aCell)
            for _cell_user in ('celladmin', 'cellmonitor'):
                if aCellMode:  # Lock , Random password
                    _cell_node.mExecuteCmd('echo {0}:$(openssl rand -base64 32) | chpasswd'.format(_cell_user))
                    _cell_node.mExecuteCmdLog('passwd -l {0}'.format(_cell_user))  # Lock passwd
                    ebLogInfo('*** Locked {0} account on cell {1}'.format(_cell_user, aCell))
                else:  # Unlock
                    _default_pwd = self.mCheckConfigOption("default_pwd")
                    _cell_pwd = b64decode(_default_pwd).decode('utf8')
                    _cell_node.mExecuteCmdLog('passwd -u {0}'.format(_cell_user))  # Unlock passwd
                    _cell_node.mExecuteCmd('echo {0}:{1} | chpasswd'.format(_cell_user, _cell_pwd))
                    ebLogInfo('*** Unlocked {0} account on cell {1}'.format(_cell_user, aCell))
            _cell_node.mDisconnect()
            return
        # Function Main flow
        _timeout_execution = 10 * 60
        _timeout_join = 10
        _proc_manager = ProcessManager()
        for _cell in  self.mReturnCellNodes():
            if not self.mPingHost(_cell):
                ebLogWarn('*** Cell {0} is not reachable. Aborting mLockCellUsers on this cell.'.format(_cell))
                continue
            _proc_structure = ProcessStructure( _mUpdate_cell_user_password, [_cell, aMode],\
                                                'lock_user_cell_{0}'.format(_cell))
            _proc_structure.mSetMaxExecutionTime(_timeout_execution)
            _proc_structure.mSetJoinTimeout(_timeout_join)
            _proc_structure.mSetLogTimeoutFx(ebLogWarn)
            _proc_manager.mStartAppend(_proc_structure)
        _proc_manager.mJoinProcess()
        return

    def mKillOngoingStartDomains(self, aDom0DomUPair=None):
        if aDom0DomUPair:
            _ddpair = aDom0DomUPair
        else:
            _ddpair = self.mReturnDom0DomUPair()

        def _kill_on_single_dom0(aDom0, aDomU):
            ebLogInfo('*** Kill Ongoing start domains for domU:{} on dom0:{}'.format(aDomU,aDom0))
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=aDom0)
            # 31349800: Kill dom0 process containing both start-domain and the _domU name
            # To avoid race condition between a start and remove
            _node.mExecuteCmdLog("ps ax | grep start-domain | grep '{0}' | awk '{{print $1}}' | xargs kill -9".format(aDomU))
            _node.mDisconnect()


        _processes = ProcessManager()
        for _dom0 , _domU in _ddpair:

            _p = ProcessStructure(_kill_on_single_dom0, aArgs=[_dom0,_domU],
                                  aId=_dom0, aMaxExecutionTime=10*60)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _processes.mStartAppend(_p)

        _processes.mJoinProcess()


    def mGetGeneratedSELinuxPolicies(self, aOptions):
        if aOptions is not None and aOptions.jsonconf is not None and aOptions.jsonconf.get("se_linux", None) is not None:
            if type(self.__options.jsonconf["se_linux"]) is str:
                _error_str = "Incorrect SE Linux configuration in request payload."
                raise ExacloudRuntimeError(0x0121, 0xA, _error_str, aStackTrace=False)

            _db = ebGetDefaultDB()
            dictHostToGeneratedPolicies = dict()
            listOfInfrastructureComponents = aOptions.jsonconf.get("se_linux").get("infraComponent", [])
            sendAll = aOptions.jsonconf.get("sendall", False)
            if sendAll:
                ebLogInfo("Exacloud will send back all generated policies irrespective of previous sync operation.")
            for component in listOfInfrastructureComponents:
                listOfNodes = component.get("targetComponentName", [])
                ebLogInfo("List of hosts to sync generated policy files: {}.".format(listOfNodes))
                for thisNode in listOfNodes:
                    targetHost = str(thisNode)
                    listOfPolicies = list()
                    if sendAll:
                        listOfPolicies = _db.mGetAllSELinuxPolicy(targetHost)
                    else:
                        listOfPolicies = _db.mGetUnsyncedSELinuxPolicy(targetHost)
                    if listOfPolicies is None or len(listOfPolicies) == 0:
                        dictHostToGeneratedPolicies[targetHost] = []
                    else:
                        finalListOfPolicies = list()
                        for thisRow in listOfPolicies:
                            finalListOfPolicies.append(thisRow[0])
                        dictHostToGeneratedPolicies[targetHost] = finalListOfPolicies
            _reqobj = self.mGetRequestObj()
            _reqobj.mSetStatusInfo(json.dumps(dictHostToGeneratedPolicies))
            ebLogDebug("Complete statusinfo of operation: {}".format(json.dumps(dictHostToGeneratedPolicies, indent = 4)))
            _db.mUpdateRequest(_reqobj)

            #Marking policies of all hosts currently synced with ECRA
            for component in listOfInfrastructureComponents:
                listOfNodes = component.get("targetComponentName", [])
                for thisNode in listOfNodes:
                    targetHost = str(thisNode)
                    _db.mUpdateAllPoliciesOfHostAsSynced(targetHost)

            return SELINUX_UPDATE_SUCCESS
        else:
            _error_str = "Missing required information in exacloud payload."
            raise ExacloudRuntimeError(0x0121, 0xA, _error_str, aStackTrace=False)


    def mGenerateCustomPolicyFileForThisRequest(self):
        reqObj = self.mGetRequestObj()
        if reqObj is not None:
            startTime = reqObj.mGetTimeStampStart()
            ebLogTrace("Request start time was: {}".format(startTime))
            dom0s = [dom0 for dom0, _ in self.mReturnDom0DomUPair()]
            generatedPolicyFileMap = dict()
            for dom0 in dom0s:
                checkNode = exaBoxNode(get_gcontext())
                checkNode.mSetUser("root")
                # The mIsConnectable have no retry hence this check makse sense here.
                if checkNode.mIsConnectable(aHost=dom0):
                    with connect_to_host(dom0, get_gcontext()) as dom0Node:

                        ebLogTrace("Checking if python3 exists on {}".format(dom0))
                        python3Executable = None
                        if dom0Node.mFileExists("/usr/bin/python3"):
                            python3Executable = "/usr/bin/python3"
                        if python3Executable is None and dom0Node.mFileExists("/bin/python3"):
                            python3Executable = "/bin/python3"

                        if python3Executable is None:
                            ebLogWarn("Python3 does not exist on node {}. Skipping custom SE Linux policy generation".format(dom0))
                            continue
                        customPolicyScriptPath = os.path.join(self.mGetBasePath(),"scripts/selinux/createCustomAuditLog.py")
                        remoteScriptPath = os.path.join("/root", "createCustomAuditLog.py")
                        try:
                            dom0Node.mCopyFile(customPolicyScriptPath, remoteScriptPath)
                        except:
                            continue
                        cmdToExecute = "{0} {1} -st \"{2}\"".format(python3Executable, remoteScriptPath, startTime)
                        ebLogTrace("Executing command: {}".format(cmdToExecute))
                        fin, fout, ferr = dom0Node.mExecuteCmd(cmdToExecute)

                        returnCode = dom0Node.mGetCmdExitStatus()
                        if returnCode != 0:
                            ebLogWarn("Failed to generate custom policies on {}".format(dom0))
                            err = ferr.readlines()
                            if err:
                                for e in err:
                                    ebLogWarn(e[:-1].encode('utf-8'))
                        else:
                            ebLogTrace("Command execution successful.")
                            out = fout.readlines()
                            if out:
                                for e in out:
                                    outputLine = e[:-1]
                                    ebLogTrace(outputLine)
                                    remoteFileLocation = None
                                    if outputLine.startswith("Custom policy file created:"):
                                        remoteFileLocation = outputLine.split(":")[1]
                                        baseName = os.path.basename(remoteFileLocation)
                                        strFileContent = dom0Node.mReadFile(remoteFileLocation)
                                        enc64Data = b64encode(strFileContent)
                                        strEncodedData = enc64Data.decode('utf-8')
                                        generatedPolicyFileMap[str(dom0)] = strEncodedData
                                        cmdToExecute = "/bin/rm {0}".format(remoteFileLocation)
                                        dom0Node.mExecuteCmd(cmdToExecute)
                        
                        cmdToExecute = "/bin/rm {0}".format(remoteScriptPath)
                        dom0Node.mExecuteCmd(cmdToExecute)

            if len(generatedPolicyFileMap.keys()) > 0:
                _db = ebGetDefaultDB()
                hostList = list(generatedPolicyFileMap.keys())
                for thisHost in hostList:
                    _db.mInsertGeneratedSELinuxPolicy(reqObj.mGetUUID(), thisHost,generatedPolicyFileMap.get(thisHost))


    def mUpdateListWithDomainNameIfRequired(self, listOfNodes, listOfAllHosts):

        hostNameToFQDNHostName = defaultdict(lambda: None)
        for thisHost in listOfAllHosts:
            hostName = thisHost.split(".")[0]
            hostNameToFQDNHostName[hostName] = thisHost
        
        for thisIndex, thisNode in enumerate(listOfNodes):
            if thisNode.find(".") == -1:
                if hostNameToFQDNHostName[thisNode] != None:
                    listOfNodes[thisIndex] = hostNameToFQDNHostName[thisNode]
                else:
                    return False

        return True


    def mProcessSELinuxUpdate(self, aOptions, isElastic=False):

        """Return status code meaning:
           0 : Successful.
           1 : mode update failure only.
           2 : policy update failure only.
           3 : mode and policy update failure."""

        #TODO: reboot nodes.
        _hasModeUpdateFailed = False
        _hasPolicyUpdateFailed = False
        if aOptions is not None and aOptions.jsonconf is not None and aOptions.jsonconf.get("se_linux", None) is not None:
            if type(self.__options.jsonconf["se_linux"]) is str:
                _error_str = "Incorrect SE Linux configuration in request payload."
                raise ExacloudRuntimeError(0x0121, 0xA, _error_str, aStackTrace=False)
            listOfInfrastructureComponents = aOptions.jsonconf.get("se_linux").get("infraComponent", [])
            statusOfComponents = list()
            ebLogInfo("Shared environment: {0}".format(self.__shared_env))
            listOfAllHosts = self.mGetHostList()
            for component in listOfInfrastructureComponents:
                completeStatus = dict()

                newMode = component["mode"]
                componentType = component["component"]
                if componentType == "domu" and not isElastic:
                    ebLogWarn("SE Linux update operations for domUs are supported only during provisioning and elastic scale compute.")
                    continue

                listOfNodes = component["targetComponentName"]
                preComputedList = True
                if type(listOfNodes) is str and listOfNodes == "all":
                    preComputedList = False
                    listOfNodes = list()
                    if componentType == "dom0":
                        listOfNodes = [dom0 for dom0, _ in self.mReturnDom0DomUPair()]
                    elif componentType == "cell":
                        listOfNodes = [cell for cell in self.mReturnCellNodes()]
                    elif componentType == "domu":
                        listOfNodes = [domU for _, domU in self.mReturnDom0DomUPair()]

                if type(listOfNodes) is not list:
                    ebLogError("Invalid value for list of nodes. Value = {0}".format(listOfNodes))
                    _hasModeUpdateFailed = True
                    _hasPolicyUpdateFailed = True
                    continue

                if preComputedList:
                    _ret = self.mUpdateListWithDomainNameIfRequired(listOfNodes, listOfAllHosts)
                    if not _ret:
                        _error_str = "Failed to obtain domain name for list of Nodes."
                        raise ExacloudRuntimeError(0x0123, 0xA, _error_str, aStackTrace=False)

                ebLogInfo("Processing SE Linux updates for {0}s: {1}".format(componentType, listOfNodes))

                completeStatus["componentType"] = componentType
                nodeStatusList = list()
                _reboot_set = set()
                for thisNode in listOfNodes:
                    with connect_to_host(thisNode, get_gcontext()) as _node:
                        operationStatus = dict()
                        operationStatus["modeUpdate"] = "Success"
                        operationStatus["policyUpdate"] = "Success"
                        if self.mSetSeLinux(_node, newMode, componentType, operationStatus):
                            _reboot_set.add(thisNode)

                        if operationStatus["modeUpdate"] == "Failure":
                            _hasModeUpdateFailed = True
                        if operationStatus["policyUpdate"] == "Failure":
                            _hasPolicyUpdateFailed = True
                        thisNodeStatus = dict()
                        thisNodeStatus["hostname"] = str(thisNode)
                        thisNodeStatus["status"] = operationStatus
                        nodeStatusList.append(thisNodeStatus)
                
                if _reboot_set:
                    self.mRebootNodesIfNoVMExists(_reboot_set, componentType)

                completeStatus["nodeStatus"] = nodeStatusList
                policyFileKey = "{0}_policy".format(componentType)
                if policyFileKey in aOptions.jsonconf.get("se_linux").keys():
                    policyFileList = aOptions.jsonconf.get("se_linux").get(policyFileKey)
                    for policyFile in policyFileList:
                        if os.path.exists(policyFile):
                            os.remove(policyFile)

                statusOfComponents.append(completeStatus)

            _reqobj = self.mGetRequestObj()
            detailedStatusInfo = dict()
            detailedStatusInfo["components"] = statusOfComponents
            _reqobj.mSetStatusInfo(json.dumps(detailedStatusInfo, indent = 4))
            ebLogDebug("Complete status of operation: {0}".format(json.dumps(detailedStatusInfo, indent = 4)))
            _db = ebGetDefaultDB()
            _db.mUpdateRequest(_reqobj)

            if _hasModeUpdateFailed and _hasPolicyUpdateFailed:
                _error_str = "Failed to update SE Linux mode and policies."
                raise ExacloudRuntimeError(0x0123, 0xA, _error_str, aStackTrace=False)
            elif _hasModeUpdateFailed:
                _error_str = "Failed to update SE Linux mode."
                raise ExacloudRuntimeError(0x0124, 0xA, _error_str, aStackTrace=False)
            elif _hasPolicyUpdateFailed:
                _error_str = "Failed to update SE Linux policies."
                raise ExacloudRuntimeError(0x0125, 0xA, _error_str, aStackTrace=False)
            else:
                return SELINUX_UPDATE_SUCCESS
        else:
            _error_str = "Missing SE Linux configuration in request payload."
            raise ExacloudRuntimeError(0x0121, 0xA, _error_str, aStackTrace=False)


    def mRebootNodesIfNoVMExists(self, aRebootSet: set, aComponentType: str) -> None:

        _reboot_set = aRebootSet
        _component_type = aComponentType
        if _component_type == "domu":
            _plist = ProcessManager()
            for _node in _reboot_set:
                ebLogInfo("Rebooting {0} ({1}).".format(_component_type, _node))
                _p = ProcessStructure(self.mRebootNode, [_node], _node)
                _p.mSetMaxExecutionTime(30*60) # 30 minutes timeout
                _p.mSetJoinTimeout(10)
                _p.mSetLogTimeoutFx(ebLogWarn)
                _plist.mStartAppend(_p)
            _plist.mJoinProcess()
            ebLogInfo(f"Reboot of domUs: {_reboot_set} was successful.")
            return
        if _component_type == "dom0":
            _any_vm_running = False
            for _node in _reboot_set:
                _vm_instance = getHVInstance(_node)
                _running_domus = _vm_instance.mRefreshDomUs()
                if _running_domus is not None and len(_running_domus) > 0:
                    _any_vm_running = True
                    break
            if not self.__shared_env or _any_vm_running is False:
                self.mAcquireRemoteLock()
                ebLogInfo(f"No VMs are running or it is a SVM environment for the dom0s: {_reboot_set}, Rebooting all dom0s for selinux changes to take effect.")
                _plist = ProcessManager()
                for _node in _reboot_set:
                    ebLogInfo("Rebooting {0} ({1}).".format(_component_type, _node))
                    _p = ProcessStructure(self.mRebootNode, [_node], _node)
                    _p.mSetMaxExecutionTime(30*60) # 30 minutes timeout
                    _p.mSetJoinTimeout(10)
                    _p.mSetLogTimeoutFx(ebLogWarn)
                    _plist.mStartAppend(_p)
                _plist.mJoinProcess()
                self.mReleaseRemoteLock()
                ebLogInfo(f"Reboot of dom0s: {_reboot_set} was successful.")
            else:
                ebLogWarn("Dom0s {0} needs to be rebooted manually since there are running VMs.".format(_reboot_set))
            return
        if _component_type == "cell":
            _all_dom0s = [dom0 for dom0, _ in self.mReturnDom0DomUPair()]
            _any_vm_running = False
            for dom0 in _all_dom0s:
                _vm_instance = getHVInstance(dom0)
                _running_domus = _vm_instance.mRefreshDomUs()
                if _running_domus is not None and len(_running_domus) > 0:
                    _any_vm_running = True
                    break
            if not self.__shared_env or _any_vm_running is False:
                ebLogInfo(f"No VMs are running or it is a SVM environment for the dom0s: {_all_dom0s}, Rebooting cells for selinux changes to take effect.")
                _plist = ProcessManager()
                for _node in _reboot_set:
                    ebLogInfo("Rebooting {0} ({1}).".format(_component_type, _node))
                    _p = ProcessStructure(self.mRebootNode, [_node], _node)
                    _p.mSetMaxExecutionTime(30*60) # 30 minutes timeout
                    _p.mSetJoinTimeout(10)
                    _p.mSetLogTimeoutFx(ebLogWarn)
                    _plist.mStartAppend(_p)
                _plist.mJoinProcess()
                ebLogInfo(f"Reboot of cells: {_reboot_set} was successful.")
            else:
                ebLogWarn("Cells {0} needs to be rebooted manually since there are running VMs.".format(_reboot_set))


    def mGetEnableFipsPayload(self, aOptions=None):

        _options = aOptions
        if not _options:
            _options = self.__options

        _enablefips = False
        _payloadValue = ""

        if _options:
            _jconf = _options.jsonconf

            if _jconf:

                if "fips_compliance" in _jconf:
                    _payloadValue = _jconf["fips_compliance"]

                elif _jconf and "dbaas_api" in _jconf and "params" in _jconf['dbaas_api']:
                    if "common" in _jconf['dbaas_api']['params']:
                        if "fips_compliance" in _jconf['dbaas_api']['params']['common']:
                            _payloadValue = _jconf['dbaas_api']['params']['common']['fips_compliance']

        if _payloadValue.lower() == "enabled":
            _enablefips = True

        if not _enablefips:
            if  self.mCheckConfigOption('enforce_fips_compliance', 'True'):
                ebLogInfo('*** Enforcing fips compliance from exabox.conf!')
                _enablefips = True

        ebLogInfo(f"Enable FIPs: {_enablefips}")
        return _enablefips


    def mGetSELinuxMode(self, aNodeType):

        if self.__options is not None and self.__options.jsonconf is not None and self.__options.jsonconf.get("se_linux", None) is not None:
            if type(self.__options.jsonconf["se_linux"]) is str:
                return None
            aNodeType = aNodeType.lower()
            listOfInfraComponents = self.__options.jsonconf["se_linux"]["infraComponent"]
            for infraComponent in listOfInfraComponents:
                if infraComponent["component"] == aNodeType:
                    return infraComponent["mode"]

        return None

    def mSetSeLinux(self, aNode, aStatus, anodeType = None, operationStatusDict = None):
        """
          aNode: node connected ready to execute commands
          aStatus: desired status of SELinux to have -> Enabled or Disabled
          Return type: boolean
              True: Reboot needed to achieve desired SELinux status (i.e. sed performed)
              False: No reboot needed because desired SELinux status was already present
        """

        aStatus = str(aStatus).lower()
        _permissible_vals = ["enforcing", "permissive", "disabled"]
        if aStatus not in _permissible_vals:
            ebLogError("*** Invalid SE_LINUX mode. Value: {0}".format(aStatus))
            if operationStatusDict is None:
                operationStatusDict = dict()
            operationStatusDict["modeUpdate"] = "Failure"
            operationStatusDict["policyUpdate"] = "Success"
            return False

        _modeUpdateStatus = "Success"
        _policyUpdateStatus = "Success"
        _node = aNode
        _cmdstr = "/bin/grep -i \"^\\s*SELINUX\" /etc/selinux/config"
        _f, _o, _e = _node.mExecuteCmd(_cmdstr)
        _rc = _node.mGetCmdExitStatus()
        _needs_change = False
        _current_status = "disabled"
        _lines = _o.readlines()

        if _rc == 0:
            for _line in _lines:
                _line = _line.replace(" ", "") 
                _line = _line.replace(os.linesep, "") 
                _vals = _line.split("=") #SELINUX=disabled/permissive/enforcing

                if len(_vals) == 2 and _vals[0] == "SELINUX":
                    if _vals[1] in _permissible_vals and _vals[1] != aStatus:
                        _current_status = _vals[1]
                        _needs_change = True
                        break

        _rv = False

        if _needs_change:
            _updateSELinuxMode = True
            if aStatus == "enforcing":
                _node.mExecuteCmdLog("/bin/touch /.autorelabel")
                _thisReturnCode = _node.mGetCmdExitStatus()
                if _thisReturnCode != 0:
                    ebLogError("Failed to create autorelabel file on the host. Will not update SELinux mode. Exception policies will be loaded if present.")
                    _updateSELinuxMode = False
                    _modeUpdateStatus = "Failure"
                else:
                    ebLogInfo("Successfully created the autorelabel file.")

            if _updateSELinuxMode:
                _cmdstr = "/bin/sed -i --follow-symlinks s/SELINUX={0}/SELINUX={1}/ /etc/selinux/config".format(_current_status, aStatus)
                _node.mExecuteCmdLog(_cmdstr)
                _thisReturnCode = _node.mGetCmdExitStatus()
                if _thisReturnCode != 0:
                    ebLogError("Failed to update mode to {0}".format(aStatus))
                    _modeUpdateStatus = "Failure"
                else:
                    _rv = True
                    ebLogInfo("SELinux will be {0}".format(aStatus))
        else:
            ebLogWarn("*** SE Linux value already at: {0}".format(aStatus))

        #Update policies on the node in case se linux status is set to enforcing.
        if aStatus == "enforcing" or aStatus == "permissive":
            if anodeType is None:
                ebLogWarn("*** Unable to update security policies for node type None.")
                _policyUpdateStatus = "Failure"
            else:
                anodeType = anodeType.lower()
                _keyName = "{0}_policy".format(anodeType)
                policyFileList = list()
                if type(self.__options.jsonconf.get("se_linux")) is str:
                    policyFileList = None
                else:
                    policyFileList = self.__options.jsonconf.get("se_linux").get(_keyName, None)

                if policyFileList is None:
                    ebLogWarn("*** Policy file details absent in payload. ***")
                else:
                    for _policy_file in policyFileList:
                        if not os.path.exists(_policy_file):
                            ebLogWarn("*** Unable to locate policy file: {0}".format(_policy_file))
                            _policyUpdateStatus = "Failure"
                        else:
                            simpleFileName = _policy_file.split("/")[-1]
                            currentUUID = simpleFileName.split(".")[0]
                            _dst_file = "/tmp/{0}_policies_{1}.pp".format(anodeType, currentUUID)
                            _node.mCopyFile(_policy_file, _dst_file)
                            _cmdstr = "/usr/sbin/semodule -i {0}".format(_dst_file)
                            _f, _o, _e = _node.mExecuteCmd(_cmdstr)
                            _rc = _node.mGetCmdExitStatus()
                            if _rc == 0:
                                ebLogInfo("{0} Security policies successfully loaded.".format(anodeType))
                            else:
                                ebLogError("Failed to load security policies for {0}.".format(anodeType))
                                _policyUpdateStatus = "Failure"

        if operationStatusDict is None:
            operationStatusDict = dict()
        operationStatusDict["modeUpdate"] = _modeUpdateStatus
        operationStatusDict["policyUpdate"] = _policyUpdateStatus
        return _rv

    def mGetOEDAExtraArgs(self) -> str:
        _args = self.mCheckConfigOption('oeda_extra_args')

        if not _args: #If option not present
            return ''

        if get_gcontext().mGetExaKms().mGetDefaultKeyAlgorithm() == "RSA":
            _args = f"{_args} -usersa"

        if ebCluCmdCheckOptions(self.__cmd, ['no_oeda_sshkeys']):
            _stripped_args = []
            # Remove enablessh and usesu
            for _arg in _args.split(' '):
                # Skip empty strings (multiple space cases) and enablessh
                # to allow delete service to proceed without domU key
                if not _arg or _arg in ('-usesu','-sshkeys'):
                    continue
                else:
                    _stripped_args.append(_arg)
            _args = ' '.join(_stripped_args)
        # Default case
        return str(_args)

    def mApplyExtraSrvctlConfig(self):

        ebLogInfo("Executing mApplyExtraSrvctlConfig")

        def _mApplySrvctl(aDomu, aExecOnce):

            _ExecOnce = aExecOnce
            _domU = aDomu
            _node = exaBoxNode(get_gcontext())
            _cmd2 = None

            try:

                _path, _ = self.mGetGridHome(_domU)

                # Apply Extra properties
                _node = exaBoxNode(get_gcontext())
                _node.mConnect(aHost=_domU)

                _cmd = '{0}/bin/srvctl setenv nodeapps -envs ORA_NET_DEEP_CHECK=10 -viponly'.format(_path)
                _node.mExecuteCmdLog(_cmd)

                _ver = re.search("([0-9]{2}[.][0-9])", _path)
                if _ver is not None:

                    if float(_ver.group(1)) >= 18.0:
                        _cmd = '{0}/bin/srvctl setenv nodeapps -envs ORA_NET_PING_TIMEOUT=300 -viponly'.format(_path)
                    else:
                        _cmd = '{0}/bin/srvctl setenv nodeapps -envs ORA_NET_PING_TIMEOUT=100 -viponly'.format(_path)

                    if float(_ver.group(1)) >= 19.0:
                        _cmd = '{0}/bin/srvctl setenv nodeapps -envs "ORA_VIP_GARP_AFTER=30,ORA_VIP_GARP_RETRIES=3" -viponly'.format(_path)
                        _cmd2 = '{0}/bin/crsctl modify res ora.scan1.vip -attr "USR_ORA_ENV=ORA_VIP_GARP_AFTER=30 ORA_VIP_GARP_RETRIES=3" -unsupported'.format(_path)

                _node.mExecuteCmdLog(_cmd)
                # Execute scan resource modify commands once per cluster
                if _ExecOnce and _cmd2 is not None:
                    _node.mExecuteCmdLog(_cmd2)
                    _cmd2 = '{0}/bin/crsctl modify res ora.scan2.vip -attr "USR_ORA_ENV=ORA_VIP_GARP_AFTER=30 ORA_VIP_GARP_RETRIES=3" -unsupported'.format(_path)
                    _node.mExecuteCmdLog(_cmd2)
                    _cmd2 = '{0}/bin/crsctl modify res ora.scan3.vip -attr "USR_ORA_ENV=ORA_VIP_GARP_AFTER=30 ORA_VIP_GARP_RETRIES=3" -unsupported'.format(_path)
                    _node.mExecuteCmdLog(_cmd2)

            finally:
                _node.mDisconnect()
        _plist = ProcessManager()
        _idx = 0
        for _dom0, _domU in self.mReturnDom0DomUPair():
            if _idx == 0:
                _idx = _idx + 1
                _p = ProcessStructure(_mApplySrvctl, [_domU, True])
            else:
                _p = ProcessStructure(_mApplySrvctl, [_domU, False])
            _p.mSetMaxExecutionTime(60*60)
            _p.mSetJoinTimeout(5)
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)
        _plist.mJoinProcess()

    def mCleanUpReconfig(self):

        # Execute reconfig
        _reconfig = self.__factoryPreprovReconfig.mCreateReconfig()
        _reconfig.mExecuteReconfig("CleanUp")

        if self.__debug:
            ebLogInfo(json.dumps(_reconfig.mGetStepRecord(), indent=4))

    def mCleanUpOvsDom0(self):

        if self.__shared_env:
            return

        if self.mIsKVM():
            return

        for _dom0, _domu in self.mReturnDom0DomUPair():

            _node = exaBoxNode(get_gcontext())

            try:
                _node.mConnect(aHost=_dom0)

                _node.mExecuteCmdLog("/bin/rm -rf /OVS/Repositories/*")
                _node.mExecuteCmdLog("/bin/rm -rf /etc/xen/auto/*")

            finally:
                _node.mDisconnect()


    def mCleanUpBackupsQemu(self, aDom0DomUPair=None):
        if aDom0DomUPair:
            _ddpair = aDom0DomUPair
        else:
            _ddpair = self.mReturnDom0DomUPair()

        if not self.mIsKVM():
            return

        for _dom0, _domu in _ddpair:

            _node = exaBoxNode(get_gcontext())

            try:
                _node.mConnect(aHost=_dom0)
                _node.mExecuteCmdLog("/bin/rm /etc/libvirt/qemu/*.backup.xml")

                # BUG 32289319: Verify and remove remaining VMs on "shut off" state.
                _domu_list = []
                _check_shut_off_vms_cmd = "/usr/sbin/vm_maker --list-domains | /usr/bin/grep -i 'shut off' | /usr/bin/awk '{print $1}'"
                _i, _o, _e = _node.mExecuteCmd(_check_shut_off_vms_cmd)
                if _o:
                    _o = _o.readlines()
                    for _line in _o:
                        _entry = _line.strip().split('(')[0]
                        if _entry.strip() == _domu:
                            _domu_list.append(_entry)

                for _domu in _domu_list:
                    _domu = _domu.strip()
                    _node.mExecuteCmdLog(f"ls /EXAVMIMAGES/GuestImages/{_domu}/*")
                    if not _node.mGetCmdExitStatus():
                        ebLogInfo(f"*** {_domu} will remain there. VM files are still present. ***")
                        continue
                    _undefine_vm_cmd = f'/usr/bin/virsh undefine {_domu}'
                    _node.mExecuteCmdLog(_undefine_vm_cmd)
                    if not _node.mGetCmdExitStatus():
                        ebLogInfo(f"*** {_domu} was still there, but it was successfully undefined. ***")
                    else:
                        ebLogWarn(f"*** {_domu} is still there. It couldn't be undefined! ***")

            finally:
                _node.mDisconnect()


    def mCleanUpStaleVm(self, aOedaCleanupSucess, aDom0DomUPair=None, aNodeRecovery=False):
        _node_recovery = aNodeRecovery
        if aDom0DomUPair:
            _ddpair = aDom0DomUPair
        else:
            _ddpair = self.mReturnDom0DomUPair()

        self.mAcquireRemoteLock()

        for _dom0, _domu in _ddpair:
            if not self.mIsKVM():
                _cmd_str  = f'/opt/exadata_ovm/exadata.img.domu_maker remove-domain {_domu} -force'
                if self.__exabm:
                    _cmd_str += ' ; /opt/exadata_ovm/exadata.img.domu_maker remove-bridge-dom0 vmeth100 -force'
                _cmd_str2 = '/usr/sbin/xm destroy '+_domu
                _chkvm_cmd = '/usr/sbin/xm list | /bin/grep -w '+_domu
                _hypervisor_service = 'xend'
            else:
                _cmd_str = '/opt/exadata_ovm/vm_maker --remove-domain '+_domu+' --force'
                _cmd_str2 = '/usr/sbin/vm_maker --stop-domain '+_domu + ' --force'
                _cmd_str3 = '/usr/bin/virsh undefine '+_domu
                _chkvm_cmd = '/usr/sbin/vm_maker --list-domains | /bin/grep -w '+_domu
                _hypervisor_service = 'libvirtd'
                if self.__exabm:
                    _cmd_str3 += ' ; /opt/exadata_ovm/vm_maker --remove-bridge vmeth200'
            _cmd_del_vmbkup = 'source /opt/python-vmbackup/bin/set-vmbackup-env.sh && vmbackup cleanall --vm '+_domu
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=_dom0)
            _del_vmbkup = self.mCheckConfigOption('delete_vmbackup')
            if _del_vmbkup is not None and _del_vmbkup == 'True' and not self.mIsKVM() and not _node_recovery:
                ebLogInfo("Deleting vmbackup using: " + _cmd_del_vmbkup)
                _node.mExecuteCmdLog(_cmd_del_vmbkup)
            elif self.mIsKVM():
                ebLogError('*** vmbackup is not support on KVM - FIXME')
            # VGE : only execute xm destroy + remove domain if OEDA failed/not ran
            if not aOedaCleanupSucess:
                self.mStartDom0Service(_hypervisor_service, _node, _dom0)
                self.mEnableDom0Service(_hypervisor_service, _node, _dom0)
                _state = _node.mSingleLineOutput(f'systemctl is-active {_hypervisor_service}').strip()                
                if _state != "active":                    
                    _error_str = f"Hypervisor service {_hypervisor_service} is not running on {_dom0}"                    
                    raise ExacloudRuntimeError(0x0801, 0xA, _error_str)
                ebLogInfo("Running VM destroy using: " + _cmd_str2)
                _node.mExecuteCmdLog(_cmd_str2)
                _rc = _node.mGetCmdExitStatus()
                if self.__debug :
                    ebLogInfo("** [Debug] The command '{0}' returned with the code '{1}'".format(_cmd_str2 , str(_rc)) )

                ebLogInfo('*** Running vm delete using: '+_cmd_str)
                _node.mExecuteCmdLog(_cmd_str)
                _rc = _node.mGetCmdExitStatus()
                if self.__debug :
                    ebLogInfo("** [Debug] The command '{0}' returned with the code '{1}'".format(_cmd_str , str(_rc)) )

            if self.mIsKVM():
                ebLogInfo('*** Running virsh undefine using : '+_cmd_str3)
                _node.mExecuteCmdLog(_cmd_str3)
                _rc = _node.mGetCmdExitStatus()
                ebLogInfo("** virsh undefine returned with the code '{0}'".format(str(_rc)) )

            _node.mExecuteCmdLog(_chkvm_cmd)
            # If VM is present, retry cmd to remove domain
            if not _node.mGetCmdExitStatus():

                ebLogInfo('*** VMs still listed, rerunning vm delete using: '+_cmd_str)
                _node.mExecuteCmdLog(_cmd_str)
                _node.mExecuteCmdLog(_chkvm_cmd)
                if not _node.mGetCmdExitStatus():
                    _error_str = 'Failed to delete VM {}'.format(_domu)
                    raise ExacloudRuntimeError(0x0801, 0xA, _error_str)

            # Bug 25506875 - ExaCM 16.4.2.1 ExaCM Instance Deletion and Re-creation Failed
            if self.mCheckConfigOption ('force_delete_vm', 'True') :
                _cmd = "ls /EXAVMIMAGES/GuestImages/"+_domu
                _node.mExecuteCmdLog (_cmd)
                _rc = _node.mGetCmdExitStatus()

                if _rc == 0 :
                    ebLogInfo ("*** Command '{0}' return a non cero value, cheking if the DomU files still present in the Dom0.".format(_cmd_str))
                    _cmd = "losetup -a"
                    _, _o, _e = _node.mExecuteCmd (_cmd)
                    out = _o.readlines()
                    if out :
                        for o in out :
                            if _domu in o :
                                _cmd = "losetup -d " + o.split(":")[0]
                                ebLogInfo ("*** running: '{0}'".format(_cmd))
                                _node.mExecuteCmdLog (_cmd)
                                _rc = _node.mGetCmdExitStatus()
                                if _rc :
                                    ebLogError ("*** ERROR: Force removal of the vm disk image failed.")
                                if self.__debug :
                                    ebLogInfo("** [Debug] The command '{0}' returned with the code '{1}'".format(_cmd , str(_rc)) )

                    _cmd = "rm -rf /EXAVMIMAGES/GuestImages/"+_domu
                    _node.mExecuteCmdLog (_cmd)
                    _cmd = "ls /EXAVMIMAGES/GuestImages/"+_domu
                    _node.mExecuteCmdLog (_cmd)
                    _rc = _node.mGetCmdExitStatus()
                    if _rc == 0 :
                        ebLogError ("*** ERROR: can't delete VM files from directory /EXAVMIMAGES/GuestImages/{0}".format(_domu))
            else: # force_delete_vm is not set, still delete GuestImages dir if domu_maker failed (vm.cfg not present)
                ebLogInfo("Deleting potential leftover domU Images")
                _cmd = "rm -rf /EXAVMIMAGES/GuestImages/"+_domu
                _node.mExecuteCmd(_cmd)

            # BUG 32055654: Verify VM is effectively gone on KVM environments.
            if self.mIsKVM():
                _cmd_str3 = '/usr/bin/virsh undefine '+_domu
                for _ in range(3):
                    _node.mExecuteCmdLog(_chkvm_cmd)
                    if not _node.mGetCmdExitStatus():
                        ebLogWarn('*** VM appears to be still there. Retrying to unregister it. ***')
                        ebLogInfo('*** Running virsh undefine using : '+_cmd_str3)
                        _node.mExecuteCmdLog(_cmd_str3)
                        _rc = _node.mGetCmdExitStatus()
                        ebLogInfo(f'*** virsh undefine returned with the code {_rc} ***')
                        ebLogInfo('*** Waiting a bit to see if VM is now deleted... ***')
                        time.sleep(60)
                    else:
                        break
                else:
                    ebLogError('*** Could not unregister VM. ***')

            _node.mDisconnect()

        # Remove backups of files
        self.mCleanUpBackupsQemu(aDom0DomUPair=_ddpair)
        self.mReleaseRemoteLock()

    def mAddEcraNatOnDomU(self) -> None:
        """
        This method will attempt to add to /etc/hosts an entry with the nat
        ecra ip, with should contain two fields:
            ip-address -> eth0-ip - 1
            hostname -> "nat-mvm1-hostname"
        raises ExacloudRuntimeError if an error occurs while attempting
                to add to /etc/hosts the new entry
        returns None
        """

        _etc_hosts = "/etc/hosts"
        _ecra_nat_hostname = "nat-mvm1-hostname"

        for _, _domU in self.mReturnDom0DomUPair():

            with connect_to_host(_domU, get_gcontext()) as _node:

                # Fetch commands binaries
                _ip_bin = node_cmd_abs_path_check(node=_node, cmd="ip", sbin=True)
                _grep_bin = node_cmd_abs_path_check(node=_node, cmd="grep")

                # Fetch eth0 ip address and netmask
                _eth0_ipaddr = node_exec_cmd(
                    node = _node,
                    cmd = f"{_ip_bin} -f inet addr show eth0 ")

                # If eth0 doesnt exist, this is nop for Exacloud
                if _eth0_ipaddr.exit_code:
                    ebLogInfo("Interface eth0 seems unexistent, this is nop. When"
                            f" attempting to fetch, received: '{_eth0_ipaddr.stdout}'")
                    return

                # Attempt creation of ip_interface, assume 1st position is
                # ip address
                try:
                    _eth0_regex = re.search(r"inet.+",  _eth0_ipaddr.stdout)
                    _eth0_interface = ip_interface(_eth0_regex.group().split()[1])

                except Exception as exp:
                    _err_msg = (f"Unable to fetch ip address from: "
                                f"{_eth0_ipaddr.stdout} on eth0 "
                                f"on: {_domU}, stderr: {_eth0_ipaddr.stderr}")
                    ebLogError(_err_msg)
                    raise ExacloudRuntimeError(0x97, 0xA, _err_msg) from exp

                # Exacloud should not add the entry in /etc/hosts if we are in ATP
                # To verify we are not in ATP, we check that the ip address of eth0
                # is part of the link local block CIDR. If it's not part of it, then
                # this is nop. This check is made this way since on elastic add compute
                # there might be a case where the ATP flag is not provided
                if not _eth0_interface.ip.is_link_local:
                    ebLogInfo(f"Ip address of eth0: '{_eth0_interface.ip}' is not "
                               "part of link local block. This is nop")
                    return

                # Compute ecra nat ip, which should be the IP of eth0 - 1
                _ecra_nat_ip = IPv4Address(_eth0_interface.ip) - 1

                if _ecra_nat_ip not in _eth0_interface.network.hosts():
                    _err_msg = (f"Unable to calculate a valid ecra nat ip for: '{_domU}'. "
                               f"Ip address {_ecra_nat_ip} not part "
                               f"of network {_eth0_interface.network}. Ignoring this error")
                    ebLogError(_err_msg)

                # Check if /etc/hosts contains ecra nat ip already
                # If is not present, add it
                _ecra_in_etchosts = node_exec_cmd(
                    node = _node,
                    cmd = f"{_grep_bin} -q '{_ecra_nat_ip}' {_etc_hosts}")

                if _ecra_in_etchosts.exit_code == 0:
                    ebLogInfo(f"This is nop, as '{_ecra_nat_ip}'"
                                  f" is already present in: {_domU} on {_etc_hosts}")
                elif _ecra_in_etchosts.exit_code == 1:
                    ebLogInfo(f"Adding '{_ecra_nat_ip}' to '{_etc_hosts}' in {_domU}")
                    _data = f"{_ecra_nat_ip} {_ecra_nat_hostname}\n"
                    node_write_text_file(_node, _etc_hosts, _data, append=True)
                else:
                    ebLogWarn(f"Unable to fetch '{_etc_hosts}' state, in {_domU}. ")
                
                # check if /etc/hosts contain patchserver 
                if self.mIsOciEXACC() and self.mIsFedramp():
                    _ecra_nat_patchserver = "patchserver"
                    _ecra_patchserver_in_etchosts = node_exec_cmd(node = _node, cmd = f"{_grep_bin} -q '{_ecra_nat_patchserver}' {_etc_hosts}")
                    
                    if _ecra_patchserver_in_etchosts.exit_code == 0:
                        ebLogInfo(f"This is nop, as {_ecra_nat_patchserver} host is already present in: {_domU} on {_etc_hosts}")
                    elif _ecra_patchserver_in_etchosts.exit_code == 1:
                        ebLogInfo(f"Adding '{_ecra_nat_ip}' with host as {_ecra_nat_patchserver} to '{_etc_hosts}' in {_domU}")
                        _data = f"{_ecra_nat_ip} {_ecra_nat_patchserver}\n"
                        node_write_text_file(_node, _etc_hosts, _data, append=True)
                    else:
                        ebLogWarn(f"Unable to fetch '{_etc_hosts}' state, in {_domU}. ")
                    
                    
    def mCleanupSingleVMNatBridge(self) -> None:
        """
        This function will do nothing in multiVM, but will clean leftover NAT
        bridge in SingleVM.
        A proper implementation to cleanup leftover bridges in MultiVM
        is to be done in the future
        """
        if self.__shared_env:
            return

        def _singleDom0Cleanup(aDom0: str, aIsKVM: bool) -> None:
            if aIsKVM:
                _bridge = 'vmeth200'
            else:
                _bridge = 'vmeth100'
            _node = exaBoxNode(get_gcontext())
            _node.mConnect(aHost=aDom0)
            _node.mExecuteCmd(f'/sbin/ip addr show {_bridge}')
            _rc = _node.mGetCmdExitStatus()
            if _rc:
                ebLogInfo(f'*** Bridge {_bridge} is not present on singleVM dom0: {aDom0}, skip cleanup')
            else:
                ebLogInfo(f'*** Bridge {_bridge} is present on singleVM dom0: {aDom0}, doing cleanup')
                if aIsKVM:
                    _cmd = f'/opt/exadata_ovm/vm_maker --remove-bridge {_bridge}'
                else:
                    _cmd = f'/opt/exadata_ovm/exadata.img.domu_maker remove-bridge-dom0 {_bridge} -force'

                _node.mExecuteCmdLog(_cmd)

            _node.mDisconnect()

        # Cleanup Dom0s in parallel
        _isKVM = self.mIsKVM()

        self.mAcquireRemoteLock()

        _plist = ProcessManager()
        for _dom0 in map(operator.itemgetter(0), self.mReturnDom0DomUPair()):
            _p = ProcessStructure(_singleDom0Cleanup, [_dom0, _isKVM], _dom0)
            _p.mSetMaxExecutionTime(5*60) # 5 Min is enough,
            _p.mSetLogTimeoutFx(ebLogWarn)
            _plist.mStartAppend(_p)
        _plist.mJoinProcess()

        self.mReleaseRemoteLock()

    def mGetOedaProperty(self, aPropertyName):

        with open(self.mGetOedaPath() + "/properties/es.properties", "r") as _f:
            _lines = _f.readlines()

            _propertyValue = ""
            _parseProperty = False

            for _line in _lines:

                if _parseProperty or re.match("{0}=".format(aPropertyName), _line):

                    _propertyValue += _line.replace("\\", "").strip()

                    if _line.strip().endswith("\\"):
                        _parseProperty = True
                    else:
                        _parseProperty = False

            if _propertyValue:
                _propertyValue = _propertyValue.split("=")[1]

            return _propertyValue

    def mGetNodeModel(self, aHostName: str) -> Optional[str]:
        """
            Connects to aHostName and gets the Exadata model (X5/X6/X7/X8 etc)
            Different ways to get that information
                1. ipmitool sunoem cli "show /SYS" | grep product_name
                2. dmidecode | grep Exadata | tail -1
                3. /opt/oracle.cellos/exadata.img.hw --get model | tail -1
        :param aHostName: Node to retrieve Exadata model name from, it must be
                          either a Dom0 or a Cell.
        :returns: Exadata model string with form 'XN' if successful, None otherwise.
        """
        _model = None
        _node = exaBoxNode(get_gcontext())
        _in, _out, _err = None, None, None
        if _node.mIsConnectable(aHost=aHostName):
            try:
                _node.mConnect(aHost=aHostName)

                _in, _out, _err = _node.mExecuteCmd("/opt/oracle.cellos/exadata.img.hw --get model")
                _out_str = _out.readline()
                _err_str = _err.read()

                if 'E4-2c' in _out_str:
                    _model = 'X9'
                elif 'E5-2L' in _out_str:
                    _model = 'X10'
                elif 'E6-2L' in _out_str:
                    _model = 'X11'
                #check if any addition nneded for x10m
                else:
                    _match = re.search('(X[0-9]+)-[0-9]', _out_str)

                    if _node.mGetCmdExitStatus() or not _match:
                        ebLogCritical(aString=f'Failed to get a valid Exadata model from '\
                                            f'host: {aHostName} stdout: {_out_str} '\
                                            f'stderr: {_err_str} exit code: {_node.mGetCmdExitStatus()}',
                                    aAction='Check ILOM health and potentially power-'\
                                            'cycle the Exadata if unresponsive')
                    else:
                        _model = _match.group(1)
            finally:
                if _in:
                    _in.close()
                if _out:
                    _out.close()
                if _err:
                    _err.close()
                _node.mDisconnect()
        else:
            ebLogError(f"Node {aHostName} is not connectable, skipping model fetch operation.")

        return _model

class ebGenerateDBGIProperties(object):
    """
       This class includes methods to generate OEDA properties for non
       supported BP (like April 2021 images with January 2021 OEDA)
    """
    @staticmethod
    def GetGeneralProperty(aClubox, aType):
        _prop_path = aClubox.mGetOedaPath() + '/properties/s_LinuxXen.properties'
        if aClubox.mIsKVM():
            _prop_path = aClubox.mGetOedaPath() + '/properties/s_LinuxKvm.properties'
        _tag =None
        _base_tag = "<tag>_<type>"
        _base_str = "={0}-klone-Linux-x86-64-<tag>.zip".format(aType)
        _config = None
        _final_cmd = ""

        if aType == "grid":
            _base_tag = _base_tag.replace("<type>", "grid")
            _config = aClubox.mGetGridConfig()

        elif aType == "db":
            _base_tag = _base_tag.replace("<type>", "db")
            _config = aClubox.mGetDBConfig()

        # Replace command with version
        _st = os.stat(_prop_path)
        if not (_st.st_mode & stat.S_IWUSR):
            os.chmod(_prop_path, _st.st_mode | stat.S_IWUSR)
        tag_dict = {}
        for version in _config.keys():
            try:
                if aClubox.mGetGiMultiImageSupport():
                    _tag = (_config[version][2] + _config[version][3]).replace(".", "")
                else:
                    _tag = "{0}{1}".format(_config[version][0].replace(".", ""), _config[version][3])
                if _tag.startswith("181"):
                    _tag = _tag.replace("181", "180", 1)
                new_tag = _base_tag.replace("<tag>", _tag)
                new_str= _base_str.replace("<tag>", _tag)

                tag_dict[new_tag] = new_str
                ebLogTrace(f'Generated tags for OEDA: {tag_dict}')
            except IndexError as error:
                ebLogInfo(error)

        current_tags = set()
        existing_tags = set(tag_dict.keys())
        with open(_prop_path, 'r+') as file:
            for line in file:
                for tag in existing_tags:
                    if tag in line:
                        current_tags.add(tag)

        new_tags = existing_tags - current_tags
        ebLogTrace(f'Tags to update : {new_tags}')
        with open(_prop_path, "a") as propfile :
            for _tag in new_tags:
                _new_str = "{0}{1}\n".format(_tag, tag_dict[_tag])
                propfile.write(_new_str)

    @staticmethod
    def GetCmdAddGIToOEDA(aClubox):
        return ebGenerateDBGIProperties.GetGeneralProperty(aClubox, "grid")

    @staticmethod
    def GetCmdAddDBToOEDA(aClubox):
        return ebGenerateDBGIProperties.GetGeneralProperty(aClubox, "db")


# end of file
